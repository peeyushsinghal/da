{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJtZSQV17M2uVEBQ8eWYvX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peeyushsinghal/da/blob/main/mitigating_bias_sa_da_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mitigating bias in sentiment analysis using domain adaptation"
      ],
      "metadata": {
        "id": "NAewnDQwhFKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ekphrasis # library to pre process twitter data\n",
        "! pip install emoji --upgrade #library to deal with emoji data\n",
        "! pip install NRCLex\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jYp6x5D5AYi",
        "outputId": "521977ad-1175-40fb-d979-c81e8214da76"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (6.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.64.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (0.4.5)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (5.5.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (2.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.7)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting NRCLex\n",
            "  Downloading NRCLex-4.0-py3-none-any.whl (4.4 kB)\n",
            "  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (from NRCLex) (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob->NRCLex) (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob->NRCLex) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob->NRCLex) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob->NRCLex) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob->NRCLex) (2022.6.2)\n",
            "Building wheels for collected packages: NRCLex\n",
            "  Building wheel for NRCLex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NRCLex: filename=NRCLex-3.0.0-py3-none-any.whl size=43329 sha256=0cf6f6f4660ebc1514b002c681bccad5d734fe9ab5330de55e1f5da43afc59e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/2c/9c/dfa19d1b65326c520b32850a9311f6d4eda679ac04dba26081\n",
            "Successfully built NRCLex\n",
            "Installing collected packages: NRCLex\n",
            "Successfully installed NRCLex-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "UN22BqAu8mbl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "110ce197-bef8-4dfe-bb7e-9a22b189a79c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ],
      "source": [
        "## Import statements\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import GloVe\n",
        "import numpy as np\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "import emoji\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import json\n",
        "\n",
        "from nrclex import NRCLex\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on:{}\".format(DEVICE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCz_btt5Saig",
        "outputId": "9e7ffc73-3917-470b-9cca-1f2503da7fb2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on:cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Handling"
      ],
      "metadata": {
        "id": "xWShQEZF9MlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting google drive for data in there"
      ],
      "metadata": {
        "id": "LDNN2hc-9SL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPXnt-PrRvGm",
        "outputId": "7e384afe-e3a6-4ada-b7ed-31f66ad52b68"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data configuration"
      ],
      "metadata": {
        "id": "v1MWCkgXSSv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/semeval-2018'\n",
        "DATA_DIR = os.path.join(BASE_PATH,'datasets')\n",
        "MODEL_DIR = os.path.join(BASE_PATH,'models')\n",
        "REF_DIR = os.path.join(BASE_PATH,'reference')\n",
        "\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "  os.makedirs(MODEL_DIR)\n",
        "  print(\"The new directory is created!\")\n",
        "\n",
        "domain_source = 0.0\n",
        "domain_target = 1.0"
      ],
      "metadata": {
        "id": "JAVHMCaNSIlW"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TASK1(object):\n",
        "  \n",
        "    EI_reg = {\n",
        "        'anger': {\n",
        "            'train': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/training/EI-reg-En-anger-train.txt'),\n",
        "            'dev': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/development/2018-EI-reg-En-anger-dev.txt'),\n",
        "            'gold': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/test-gold/2018-EI-reg-En-anger-test-gold.txt')\n",
        "                }\n",
        "        }\n",
        "\n",
        "    V_reg = {\n",
        "        'train': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-train.txt'),\n",
        "        'dev': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-dev.txt'),\n",
        "        'gold': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-test-gold.txt')\n",
        "             }\n",
        "\n",
        "    EEC = {\n",
        "        'eec': os.path.join(\n",
        "            DATA_DIR, 'task1/Equity-Evaluation-Corpus/Equity-Evaluation-Corpus.csv')\n",
        "             }"
      ],
      "metadata": {
        "id": "_Z4pcQiGSyXX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataloading and Parsing"
      ],
      "metadata": {
        "id": "UfWsmb-8V9Er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Source Data"
      ],
      "metadata": {
        "id": "XwMIowbv_YIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing Emotion and Valence regression data : `format [ID\tTweet\tAffect Dimension\tIntensity Score]`"
      ],
      "metadata": {
        "id": "CtLKXfb-Xi79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_reg(data_file, label_format='tuple')-> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    This is for datasets for the EI-reg and V-reg English tasks \n",
        "    Returns:\n",
        "        df: dataframe with columns in the first row of file [ID-Tweet-Affect Dimension-Intensity Score]\n",
        "    \"\"\"\n",
        "    with open(data_file, 'r') as fd:\n",
        "      data = [l.strip().split('\\t') for l in fd.readlines()]\n",
        "    \n",
        "    df = pd.DataFrame (data[1:],columns=data[0])\n",
        "    df['domain'] = domain_source\n",
        "    return df"
      ],
      "metadata": {
        "id": "7-CzssqIr68Z"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target Data"
      ],
      "metadata": {
        "id": "I1Xdl7OR_ogd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "parsing EEC data : `format [ID\tSentence\tTemplate\tPerson\tGender\tRace Emotion\tEmotion word]`"
      ],
      "metadata": {
        "id": "FNEQISzDY6ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_eec()->pd.DataFrame:\n",
        "  \"\"\"\n",
        "  This is for EEC Dataset, it is a csv file\n",
        "  Returns:\n",
        "        df_eec: dataframe \n",
        "  \"\"\"\n",
        "  data_train = TASK1.EEC['eec']\n",
        "  df_eec = pd.read_csv(data_train)\n",
        "  df_eec['domain'] = domain_target\n",
        "  return df_eec\n"
      ],
      "metadata": {
        "id": "afnCKQmtYhW3"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse(task, dataset, emotion='anger') -> pd.DataFrame:\n",
        "    if task == 'EI-reg':\n",
        "        data_train = TASK1.EI_reg[emotion][dataset]\n",
        "        df = parse_reg(data_train)\n",
        "        df[df.columns[-1]] = df[df.columns[-1]].astype(float)\n",
        "        return df\n",
        "    elif task == 'V-reg':\n",
        "        data_train = TASK1.V_reg[dataset]\n",
        "        df = parse_reg(data_train)\n",
        "        df[df.columns[-1]] = df[df.columns[-1]].astype(float)\n",
        "        return df\n",
        "    else:\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "0J7zs1GzXh8F"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating Dataframes\n",
        "df_EI_reg_train = parse('EI-reg','train')\n",
        "df_EI_reg_val = parse('EI-reg','dev')\n",
        "df_EI_reg_test = parse('EI-reg','gold')\n",
        "df_V_reg_train = parse('V-reg','train')\n",
        "df_V_reg_val = parse('V-reg','dev')\n",
        "df_V_reg_test = parse('V-reg','gold')\n",
        "\n",
        "dict_df= {'df_EI_reg_train':df_EI_reg_train, \n",
        "          'df_EI_reg_val':df_EI_reg_val, \n",
        "          'df_EI_reg_test':df_EI_reg_test, \n",
        "          'df_V_reg_train': df_V_reg_train, \n",
        "          'df_V_reg_val':df_V_reg_val, \n",
        "          'df_V_reg_test': df_V_reg_test \n",
        "          }"
      ],
      "metadata": {
        "id": "9CgnnxH-aU6B"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_df['df_V_reg_test']\n",
        "count = 0\n",
        "for name, df in dict_df.items():\n",
        "  if \"train\" in name:\n",
        "    count = count+ len(df)\n",
        "    print(len(df))\n",
        "\n",
        "print(\"count\", count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_nzUv2qA3Kh",
        "outputId": "3d16bd01-1a35-48e5-ad69-19e3e72f337d"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1701\n",
            "1181\n",
            "count 2882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating EEC Dataframe\n",
        "df_EEC = parse_eec()\n",
        "df_EEC.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VBKz_gjQRs3r",
        "outputId": "94784cd4-66f0-4005-800c-3d89bcbe8a2f"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      ID                 Sentence  \\\n",
              "0  2018-En-mystery-05498      Alonzo feels angry.   \n",
              "1  2018-En-mystery-11722    Alonzo feels furious.   \n",
              "2  2018-En-mystery-11364  Alonzo feels irritated.   \n",
              "3  2018-En-mystery-14320    Alonzo feels enraged.   \n",
              "4  2018-En-mystery-14114    Alonzo feels annoyed.   \n",
              "\n",
              "                                 Template  Person Gender              Race  \\\n",
              "0  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
              "1  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
              "2  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
              "3  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
              "4  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
              "\n",
              "  Emotion Emotion word  domain  \n",
              "0   anger        angry     1.0  \n",
              "1   anger      furious     1.0  \n",
              "2   anger    irritated     1.0  \n",
              "3   anger      enraged     1.0  \n",
              "4   anger      annoyed     1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b33c991c-93e0-4893-ae54-762ae86481b5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Template</th>\n",
              "      <th>Person</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Race</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Emotion word</th>\n",
              "      <th>domain</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-En-mystery-05498</td>\n",
              "      <td>Alonzo feels angry.</td>\n",
              "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
              "      <td>Alonzo</td>\n",
              "      <td>male</td>\n",
              "      <td>African-American</td>\n",
              "      <td>anger</td>\n",
              "      <td>angry</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-En-mystery-11722</td>\n",
              "      <td>Alonzo feels furious.</td>\n",
              "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
              "      <td>Alonzo</td>\n",
              "      <td>male</td>\n",
              "      <td>African-American</td>\n",
              "      <td>anger</td>\n",
              "      <td>furious</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-En-mystery-11364</td>\n",
              "      <td>Alonzo feels irritated.</td>\n",
              "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
              "      <td>Alonzo</td>\n",
              "      <td>male</td>\n",
              "      <td>African-American</td>\n",
              "      <td>anger</td>\n",
              "      <td>irritated</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-En-mystery-14320</td>\n",
              "      <td>Alonzo feels enraged.</td>\n",
              "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
              "      <td>Alonzo</td>\n",
              "      <td>male</td>\n",
              "      <td>African-American</td>\n",
              "      <td>anger</td>\n",
              "      <td>enraged</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-En-mystery-14114</td>\n",
              "      <td>Alonzo feels annoyed.</td>\n",
              "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
              "      <td>Alonzo</td>\n",
              "      <td>male</td>\n",
              "      <td>African-American</td>\n",
              "      <td>anger</td>\n",
              "      <td>annoyed</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b33c991c-93e0-4893-ae54-762ae86481b5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b33c991c-93e0-4893-ae54-762ae86481b5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b33c991c-93e0-4893-ae54-762ae86481b5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unique_templates = df_EEC['Template'].unique()\n",
        "\n",
        "# dict_templates ={}\n",
        "# for index, template in enumerate(unique_templates):\n",
        "#   name = \"T_\"+str(index)\n",
        "#   dict_templates[template] = name"
      ],
      "metadata": {
        "id": "lT7hzmj2SRBX"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_EEC['Template_Number'] = df_EEC['Template']\n",
        "# df_EEC['Template_Number'] = df_EEC['Template_Number'].map(dict_templates)"
      ],
      "metadata": {
        "id": "Zh7ZO2MgT2Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_EEC_subset_anger = df_EEC[df_EEC['Emotion']=='anger']"
      ],
      "metadata": {
        "id": "PmXOykvSU_Ps"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(df_EEC_subset_anger)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-kpSbmWVBX6",
        "outputId": "72395045-593c-4e80-c14d-7d0652db31e0"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2100"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PreProcess Twitter Data"
      ],
      "metadata": {
        "id": "vgcR6rW84jVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reference : https://github.com/cbaziotis/ekphrasis\n",
        "\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1DwsLjWqvEx",
        "outputId": "37885d0f-d4a3-4ef4-bd86-2e14621ab045"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n",
            "Reading twitter - 1grams ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #### Example checks of pre-processing\n",
        "# sentences = [\n",
        "#     \"CANT WAIT for the new season of #TwinPeaks ï¼¼(^o^)ï¼!!! #davidlynch #tvseries :)))\",\n",
        "#     \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies :/\",\n",
        "#     \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! :-D http://sentimentsymposium.com/.\",\n",
        "#     \"@MGBarbieri @SpalkTalk a@b.com And just saw your LinkedIn comment after I sent this! Thanks for the message :) ðŸ˜€\",\n",
        "#     \"ðŸ’™ðŸ’›ðŸ† @GeorgeePitman Young Player of The Season ðŸ†ðŸ’›ðŸ’™ #irony #actuallyseventy\"\n",
        "# ]\n",
        "\n",
        "# for s in sentences:\n",
        "#     print(\" \".join(text_processor.pre_process_doc(s)))\n",
        "# # print ([text_processor.pre_process_doc(s) for s in sentences])"
      ],
      "metadata": {
        "id": "iK5Ifwyhq0Ex"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_tweets(df)-> pd.DataFrame:\n",
        "  tweets = df.Tweet.to_list()\n",
        "  # df['TweetTokens'] = [emoji.demojize(text_processor.pre_process_doc(tweet),language = 'en') for tweet in tweets] # Translates emoji in to word and preprocesss\n",
        "  # df['TweetTokens'] = [text_processor.pre_process_doc(tweet) for tweet in tweets] # preprocesss\n",
        "  # tweets_processed = [text_processor.pre_process_doc(tweet) for tweet in tweets] # preprocesss\n",
        "  # for tweet in tweets_processed:\n",
        "  #   for index, token in enumerate(tweet):\n",
        "  #     if emoji.is_emoji(token):\n",
        "  #       tweet[index] = emoji.demojize(token, language = 'en')\n",
        "\n",
        "  tweets_processed = [\" \".join(text_processor.pre_process_doc(tweet)) for tweet in tweets] # preprocesss\n",
        "  # print (tweets_processed)\n",
        "  for index, tweet in enumerate(tweets_processed):\n",
        "      tweets_processed[index] = emoji.demojize(tweet, language = 'en')\n",
        "  \n",
        "  df['TweetTokens'] = tweets_processed\n",
        "  # print(df)\n",
        "  return df"
      ],
      "metadata": {
        "id": "pfmJjAHrujuX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_EI_reg_train = preprocess_tweets(df_EI_reg_train)\n",
        "# df_V_reg_train = preprocess_tweets(df_V_reg_train)\n",
        "\n",
        "for name, df in dict_df.items():\n",
        "  df = preprocess_tweets(df)\n"
      ],
      "metadata": {
        "id": "hxt99UC2xI2x"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_V_reg_test"
      ],
      "metadata": {
        "id": "-sRSUuUl7BfC"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : \n",
        "* remove stop words\n",
        "* stem\n",
        "* lemmetize\n"
      ],
      "metadata": {
        "id": "3los9BZbk2m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_V_reg_train.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtRWK8D6mMzk",
        "outputId": "b83d223e-60b1-4e67-8aec-90bf2d10be91"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'Tweet', 'Affect Dimension', 'Intensity Score', 'domain',\n",
              "       'TweetTokens'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def subset_df(df):\n",
        "  return df[['TweetTokens','Intensity Score','domain']]"
      ],
      "metadata": {
        "id": "_zoGr_W3mYgK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_df_subset ={name+\"_subset\": subset_df(df) for name, df in dict_df.items() }"
      ],
      "metadata": {
        "id": "kDsCZb5ExgA9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (dict_df_subset)"
      ],
      "metadata": {
        "id": "hcw_mO9Y1bP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Pytorch Datasets"
      ],
      "metadata": {
        "id": "Wi-dG9KP14u8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Vocabulary\n",
        "Before we create the Dataset, we need to define a process to build our vocabulary. For this,\n",
        "Weâ€™ll create a â€œVocabularyâ€ class which will create the word-to-index and index-to-word mappings using only the train dataframe we created before\n",
        "Also, the â€œVocabularyâ€ class returns the numericalized version of each sentence in our dataframe. Eg: [â€˜iâ€™, â€˜loveâ€™, â€˜appleâ€™] -> [23, 54, 1220]. We need to convert the words to numbers as models expect each word in our vocabulary to be represented by a number"
      ],
      "metadata": {
        "id": "IUsh5RUf2BxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#               Define Vocabulary Class\n",
        "#######################################################\n",
        "\n",
        "class Vocabulary:\n",
        "  \n",
        "    '''\n",
        "    __init__ method is called by default as soon as an object of this class is initiated\n",
        "    we use this method to initiate our vocab dictionaries\n",
        "    '''\n",
        "    def __init__(self, freq_threshold = 1, max_size = 10000):\n",
        "        '''\n",
        "        freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
        "        max_size : max source vocab size. Eg. if set to 10,000, we pick the top 10,000 most frequent words and discard others\n",
        "        '''\n",
        "        #initiate the index to token dict\n",
        "        ## <PAD> -> padding, used for padding the shorter sentences in a batch to match the length of longest sentence in the batch\n",
        "        ## <UNK> -> words which are not found in the vocab are replace by this token\n",
        "        # self.itos = {0: '<PAD>', 1: '<UNK>', 2:'<NUMBER>', 3: '<CURRENCY>', 4: '<URL>'}\n",
        "        self.itos = {0: '<PAD>', 1: '<UNK>'}\n",
        "        \n",
        "        \n",
        "        #initiate the token to index dict\n",
        "        self.stoi = {k:j for j,k in self.itos.items()}\n",
        "        self.original_stoi = self.stoi.copy()\n",
        "#         print(self.stoi)\n",
        "        \n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.max_size = max_size\n",
        "    \n",
        "    '''\n",
        "    __len__ is used by dataloader later to create batches\n",
        "    '''\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "    \n",
        "    '''\n",
        "    a simple tokenizer to split on space and converts the sentence to list of words\n",
        "    '''\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "#         return [tok.strip() for tok in text.split(' ')]\n",
        "        return [tok.lower().strip() for tok in text.split(' ')] # this is commented out to avoid <NUMBER> ,<UNK> lowering\n",
        "#         return [tok.lower().strip() for tok in text.split(' ') if tok not in list(self.stoi.keys())] \n",
        "    \n",
        "    '''\n",
        "    build the vocab: create a dictionary mapping of index to string (itos) and string to index (stoi)\n",
        "    output ex. for stoi -> {'the':6, 'a':7, 'an':8}\n",
        "    '''\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        #calculate the frequencies of each word first to remove the words with freq < freq_threshold\n",
        "#         frequencies = {}  #init the freq dict\n",
        "        frequencies = {k:self.max_size+1 for _,k in self.itos.items()}  # updated so that intial ones are also part of this\n",
        "        \n",
        "        # idx = 5 #index from which we want our dict to start. We already used 4 indexes for pad, unk...\n",
        "        idx = len(self.original_stoi)\n",
        "        \n",
        "        #calculate freq of words\n",
        "        for sentence in sentence_list:\n",
        "            list_word = [tok.lower().strip() for tok in sentence.split(' ') if tok not in list(self.stoi.keys())] \n",
        "            for word in list_word:\n",
        "#             for word in self.tokenizer(sentence):\n",
        "                \n",
        "                if word not in frequencies.keys():\n",
        "                    frequencies[word]=1\n",
        "                else:\n",
        "                    \n",
        "                    frequencies[word]+=1\n",
        "                    \n",
        "#         print (\"----2-----\\n\",frequencies)\n",
        "        \n",
        "        #limit vocab by removing low freq words\n",
        "        frequencies = {k:v for k,v in frequencies.items() if v>self.freq_threshold} \n",
        "        \n",
        "#         print (\"----3-----\\n\",frequencies)\n",
        "        \n",
        "        #limit vocab to the max_size specified\n",
        "        frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx =5 for pad, unk...\n",
        "        \n",
        "#         print (\"----4-----\\n\",frequencies)\n",
        "            \n",
        "        #create vocab\n",
        "        for key in set(self.stoi.keys()):\n",
        "            frequencies.pop(key)\n",
        "        \n",
        "#         print (\"----5-----\\n\",frequencies)\n",
        "        \n",
        "        for word in frequencies.keys():\n",
        "            self.stoi[word] = idx\n",
        "            self.itos[idx] = word\n",
        "            idx+=1\n",
        "        \n",
        "#         print (\"----6-----\\n\",self.stoi)\n",
        "        \n",
        "    '''\n",
        "    convert the list of words to a list of corresponding indexes\n",
        "    '''    \n",
        "    def numericalize(self, text):\n",
        "        #tokenize text\n",
        "#         tokenized_text = self.tokenizer(text)\n",
        "#         print(\"---------\\n\",self.original_stoi.keys())\n",
        "        tokenized_text = []\n",
        "        for tok in text.split(' '):\n",
        "            if tok not in list(self.original_stoi.keys()):\n",
        "                tokenized_text.append(tok.lower().strip())\n",
        "            else:\n",
        "                tokenized_text.append(tok.strip())\n",
        "                \n",
        "#         tokenized_text = [tok.lower().strip() for tok in text.split(' ') if tok not in list(self.original_stoi.keys())]\n",
        "        numericalized_text = []\n",
        "        for token in tokenized_text:\n",
        "            if token in self.stoi.keys():\n",
        "                numericalized_text.append(self.stoi[token])\n",
        "            else: #out-of-vocab words are represented by UNK token index\n",
        "                numericalized_text.append(self.stoi['<UNK>'])\n",
        "                \n",
        "        return numericalized_text"
      ],
      "metadata": {
        "id": "F7xZg1XcwLtt"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # #create a vocab class with freq_threshold=0 and max_size=100\n",
        "# voc = Vocabulary(0, 100)\n",
        "# sentence_list = ['that is a cat CAT', 'that is not a dog']\n",
        "# #build vocab\n",
        "# voc.build_vocabulary(sentence_list)\n",
        "\n",
        "# print('index to string: ',voc.itos)\n",
        "# print('string to index:',voc.stoi)\n",
        "\n",
        "# print('numericalize -> cat and a dog <URL>: ', voc.numericalize('cat and a dog <NUMBER>'))"
      ],
      "metadata": {
        "id": "COQ10m7Pv-sR"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating sentence list of all the training dataframes to create vocabulary later, this would mean a more robust vocab\n",
        "sentence_list = []\n",
        "for name,df in dict_df_subset.items():\n",
        "  if \"train\" in name or 'val' in name :\n",
        "    sentence_list.extend(df.TweetTokens.to_list())\n",
        "print(len(sentence_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vuabth4RCSO",
        "outputId": "be4eec25-32cb-4c64-9b24-1a32fccc7bcb"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Adding more words - especially emotions and sentiments from NRCLex\n",
        "\n",
        "lexicon_file = os.path.join(REF_DIR,'nrc_en.json')\n",
        "print(os.path.isfile(lexicon_file) )\n",
        "print(lexicon_file)\n",
        "\n",
        "with open(lexicon_file, 'r') as json_file:\n",
        "  lexicon_json= json.loads(json_file.read())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZcWtNY5fdR6",
        "outputId": "998e5eb6-b11b-4aa3-d4ab-722e7d765e9a"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "/content/drive/MyDrive/semeval-2018/reference/nrc_en.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_lexicon_all = []\n",
        "for name, value in lexicon_json.items():\n",
        "  list_lexicon_all.append(name)\n",
        "\n",
        "list_lexicon_all = list(set(list_lexicon_all))\n",
        "print(len(list_lexicon_all),list_lexicon_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckFtBlghlnxK",
        "outputId": "82de6c20-5ea0-489e-adcc-1f3a898e14c0"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6468 ['study', 'whip', 'knowledge', 'kudos', 'buried', 'flu', 'lament', 'passionate', 'virus', 'irreducible', 'intended', 'radio', 'tyranny', 'pleasurable', 'advised', 'tanned', 'monsoon', 'father', 'manslaughter', 'blast', 'displace', 'blatant', 'inert', 'independence', 'disallowed', 'arraignment', 'efficacy', 'darling', 'obey', 'babysitter', 'mud', 'doomed', 'rage', 'absolute', 'aggravation', 'fatty', 'chant', 'cradle', 'deplore', 'depressive', 'unbreakable', 'misuse', 'outcast', 'polygamy', 'peace', 'peerless', 'unfortunate', 'black', 'delirium', 'drooping', 'watery', 'disobedient', 'patter', 'evolution', 'holy', 'irritation', 'grate', 'unimpressed', 'forming', 'mucus', 'astonishingly', 'fundamental', 'excitation', 'regress', 'prophecy', 'dolphin', 'liking', 'homicidal', 'nullify', 'commonplace', 'relevant', 'preeminent', 'rack', 'undesirable', 'clearness', 'injured', 'mail', 'defect', 'recklessness', 'copy', 'abrasion', 'prank', 'confession', 'aloha', 'donation', 'serene', 'intolerance', 'interesting', 'illicit', 'marketable', 'depreciation', 'erudite', 'bother', 'disobedience', 'untrustworthy', 'gypsy', 'controversy', 'action', 'doldrums', 'arouse', 'cursory', 'thrift', 'maxim', 'fort', 'remarkably', 'invite', 'magician', 'myopia', 'outcry', 'leaky', 'hearing', 'embezzlement', 'disengagement', 'broadside', 'cute', 'princess', 'stripe', 'relinquish', 'disintegration', 'volatility', 'reunion', 'errant', 'officer', 'screech', 'trade', 'uneasiness', 'degeneracy', 'tiling', 'moorings', 'sermon', 'alb', 'jam', 'transcendence', 'segregated', 'nether', 'tale', 'inquirer', 'banish', 'jurisprudence', 'acceptable', 'forego', 'hitherto', 'mutant', 'carnivorous', 'career', 'discipline', 'chastisement', 'optional', 'hypocrisy', 'regulate', 'deference', 'music', 'stable', 'breakup', 'bummer', 'creaking', 'predict', 'lenient', 'tumble', 'older', 'mortgage', 'alienated', 'moat', 'fret', 'dance', 'bearer', 'tributary', 'disapproved', 'procrastinate', 'uneasy', 'resisting', 'unbearable', 'slanderous', 'coronation', 'secondhand', 'smashed', 'blush', 'villainous', 'bugaboo', 'watch', 'desirous', 'oversight', 'laborious', 'ministry', 'horoscope', 'disabled', 'atheism', 'grist', 'shatter', 'barricade', 'obstetrician', 'plight', 'commemorative', 'perishable', 'chop', 'formless', 'constable', 'undertaker', 'vulture', 'blues', 'accountant', 'unconstrained', 'rehabilitate', 'striking', 'asymmetry', 'senseless', 'wreck', 'nobleman', 'susceptible', 'authenticate', 'ecstatic', 'shambles', 'asshole', 'buxom', 'evacuate', 'jab', 'ship', 'manhood', 'drainage', 'painfully', 'wonderfully', 'cab', 'pimp', 'priest', 'shrapnel', 'threatening', 'scanty', 'arbitration', 'rupture', 'suggest', 'disgraceful', 'allegation', 'breakfast', 'angry', 'misplace', 'eject', 'considerate', 'equilibrium', 'jolt', 'quicken', 'culmination', 'executor', 'sour', 'stall', 'aching', 'dishonor', 'excrement', 'cube', 'love', 'scientist', 'preservative', 'defeated', 'ace', 'ruth', 'rigor', 'pry', 'respectability', 'honor', 'recesses', 'harlot', 'hurry', 'consistency', 'cytomegalovirus', 'playground', 'fortress', 'enforce', 'bylaw', 'pet', 'illusion', 'unholy', 'tolerate', 'patronage', 'succeeding', 'charm', 'primacy', 'overestimated', 'almighty', 'loafer', 'homicide', 'aground', 'establish', 'include', 'evident', 'inadequate', 'ruinous', 'defending', 'polarity', 'luxury', 'felicity', 'recorder', 'repudiation', 'unjustified', 'farm', 'pity', 'legibility', 'ineffective', 'pique', 'convent', 'enjoying', 'blot', 'urgency', 'bloody', 'lockup', 'orderly', 'sea', 'smattering', 'abbot', 'gratitude', 'coercion', 'communicate', 'injury', 'stricken', 'optimism', 'inexplicable', 'hostilities', 'shell', 'differently', 'lewd', 'spa', 'expertise', 'sinning', 'nurse', 'jerk', 'scripture', 'abomination', 'encroachment', 'nicotine', 'oration', 'pomp', 'bloodshed', 'rascal', 'consciousness', 'amortization', 'radiant', 'romantic', 'sterling', 'react', 'cataract', 'forewarned', 'immaturity', 'substantiate', 'incrimination', 'patriarchal', 'unforeseen', 'smother', 'acrobat', 'ashes', 'offended', 'treachery', 'condolence', 'reprimand', 'horrid', 'lagging', 'shrink', 'inspire', 'nasty', 'necessity', 'coursing', 'endanger', 'interlocutory', 'inventor', 'boasting', 'store', 'suffering', 'derogatory', 'dislike', 'obese', 'unmanageable', 'misrepresentation', 'incompatible', 'revolt', 'sculpture', 'unaware', 'forecast', 'interminable', 'magnificent', 'splinter', 'sugar', 'unwarranted', 'befall', 'fellowship', 'midwifery', 'exhausted', 'fecal', 'conjure', 'hunch', 'indecision', 'rusty', 'hazardous', 'ferocious', 'musical', 'alleviation', 'suicide', 'sensibility', 'trusty', 'birch', 'infantile', 'committee', 'tandem', 'obliterated', 'credential', 'incredulous', 'retard', 'enthusiast', 'muscular', 'mishap', 'paragon', 'perchance', 'limp', 'salvation', 'tabulate', 'divan', 'tearful', 'scrumptious', 'depart', 'blackness', 'renewal', 'bless', 'whine', 'spouse', 'rags', 'implacable', 'painful', 'critique', 'disheartened', 'unfulfilled', 'casualty', 'seductive', 'conductivity', 'unintentionally', 'ignorant', 'subvert', 'offshoot', 'overt', 'disparity', 'mediocre', 'differential', 'uncaring', 'concordance', 'spit', 'elders', 'burlesque', 'perjury', 'confusion', 'spirit', 'childhood', 'merit', 'frenetic', 'beholden', 'disrespectful', 'frigid', 'reluctant', 'horrifying', 'brute', 'intrepid', 'plagiarism', 'follower', 'overflow', 'exposed', 'grudgingly', 'expedient', 'terribly', 'buzz', 'attainment', 'trainer', 'fluke', 'terrorism', 'chairwoman', 'sundial', 'carelessness', 'attachment', 'skeptical', 'fumble', 'leeches', 'formidable', 'gubernatorial', 'guile', 'endless', 'corrupt', 'weight', 'laser', 'epic', 'argumentation', 'scalpel', 'contrasted', 'greed', 'dilapidated', 'hope', 'await', 'bride', 'rapid', 'rest', 'demanding', 'wimp', 'gazette', 'casket', 'preserve', 'acid', 'prophylactic', 'scotch', 'snarl', 'glee', 'omit', 'parsimonious', 'vainly', 'desert', 'idiocy', 'gonorrhea', 'tribute', 'captivity', 'ferment', 'parrot', 'restful', 'cautious', 'jealous', 'war', 'supreme', 'offset', 'nefarious', 'embarrassment', 'included', 'penitentiary', 'sordid', 'rejection', 'devastate', 'cooperation', 'funk', 'toils', 'boldness', 'atone', 'confederate', 'betrayal', 'physique', 'scarcely', 'revocation', 'incarceration', 'confess', 'phonetic', 'pillow', 'symptom', 'wrath', 'reassure', 'fuming', 'cue', 'decry', 'incapacity', 'beam', 'famine', 'diarrhoea', 'trance', 'coma', 'gullible', 'fade', 'academy', 'insignificance', 'awkwardness', 'landslide', 'refutation', 'wis', 'savings', 'burglar', 'communicative', 'herpes', 'ferocity', 'unfavorable', 'amour', 'intense', 'script', 'blessing', 'operation', 'voluptuous', 'incestuous', 'depository', 'darken', 'arsenic', 'hidden', 'procedure', 'catastrophe', 'insulting', 'translation', 'weirdo', 'compliment', 'imprudent', 'psalm', 'draft', 'concealed', 'impotent', 'celestial', 'cruel', 'baseless', 'sickness', 'restriction', 'advocate', 'tort', 'presentable', 'orc', 'bark', 'inmate', 'emaciated', 'skirmish', 'presumptuous', 'pretty', 'strangle', 'label', 'sarcasm', 'deadly', 'sex', 'slender', 'chaos', 'coalesce', 'credited', 'thumping', 'illegality', 'subjected', 'whore', 'captivating', 'solidity', 'gregarious', 'bogus', 'impair', 'practice', 'dispose', 'spirits', 'coroner', 'illegitimate', 'guru', 'clap', 'decline', 'pickle', 'conflict', 'lack', 'influential', 'ceremony', 'prominently', 'relics', 'argue', 'grief', 'lyrical', 'diverse', 'hermit', 'exterminate', 'expulsion', 'bee', 'injection', 'strained', 'robber', 'carol', 'punt', 'canker', 'lead', 'irons', 'faulty', 'irritability', 'vixen', 'revoke', 'winnings', 'indict', 'reliability', 'upset', 'advocacy', 'beverage', 'mar', 'sorely', 'mouth', 'infirm', 'voyage', 'palliative', 'fright', 'collision', 'detain', 'possessed', 'oblige', 'downfall', 'objection', 'despotism', 'edition', 'scar', 'fool', 'filth', 'religion', 'profession', 'rash', 'cannon', 'exemption', 'signature', 'mania', 'unsympathetic', 'sib', 'thresh', 'zany', 'cold', 'blinded', 'killing', 'flirt', 'punish', 'affluent', 'reinforcements', 'skillful', 'forerunner', 'destroying', 'inflict', 'agreeable', 'detonate', 'throne', 'schism', 'shopping', 'obstruct', 'tramp', 'truss', 'obedience', 'clamor', 'lateness', 'deception', 'provoking', 'reflex', 'dependence', 'grin', 'respect', 'marvelous', 'convincing', 'rollicking', 'expenditure', 'lustrous', 'definitive', 'fireball', 'voodoo', 'soil', 'communion', 'ail', 'recast', 'captivate', 'inoperative', 'enslavement', 'uncomfortable', 'uninitiated', 'faeces', 'cautionary', 'contribute', 'clump', 'commentator', 'unprofitable', 'downright', 'charitable', 'petroleum', 'unemployed', 'lash', 'atrocity', 'compassionate', 'lighthouse', 'possess', 'veritable', 'savagery', 'maniac', 'pad', 'exhaustion', 'expect', 'inconsiderate', 'agonizing', 'overdue', 'bane', 'bran', 'sweetie', 'secretariat', 'mage', 'plea', 'exhaust', 'trustee', 'accusation', 'ass', 'forget', 'ordnance', 'anarchism', 'sneeze', 'sprite', 'mystery', 'cripple', 'judicial', 'enable', 'persistent', 'senate', 'avarice', 'prisoner', 'staunch', 'troll', 'barred', 'scorn', 'prejudice', 'lunge', 'defend', 'overpower', 'fraternal', 'vacuous', 'romanticism', 'abnormal', 'ache', 'cramp', 'usurped', 'customer', 'vanquish', 'uneducated', 'beastly', 'excruciating', 'touchy', 'durability', 'outdo', 'bottom', 'cross', 'condescension', 'expletive', 'outstanding', 'hopeful', 'teasing', 'certify', 'guise', 'diabolical', 'impossible', 'unjust', 'conviction', 'tax', 'gross', 'incite', 'conveyancing', 'annihilated', 'individuality', 'defunct', 'recreation', 'intact', 'blackmail', 'inheritance', 'expel', 'separatist', 'grammar', 'church', 'unique', 'summons', 'drool', 'nutritious', 'motion', 'derogation', 'septic', 'artistic', 'ban', 'faculty', 'volunteer', 'malignancy', 'simplify', 'save', 'uninfected', 'mobile', 'author', 'rainy', 'livid', 'marry', 'rapping', 'soreness', 'prevalent', 'handy', 'slash', 'lesser', 'temperate', 'glaring', 'recline', 'punishment', 'categorical', 'inhuman', 'disturbed', 'aberration', 'cognitive', 'fireman', 'moron', 'ballot', 'woeful', 'securities', 'blaze', 'disorder', 'tough', 'ally', 'esthetic', 'candidate', 'coax', 'confirmation', 'rabble', 'astray', 'lacking', 'accusing', 'actionable', 'tiff', 'manipulate', 'pivot', 'regatta', 'prefer', 'demoralized', 'miraculous', 'monstrosity', 'pirate', 'randomly', 'imminent', 'demolition', 'resentful', 'ambulance', 'warlock', 'doctrine', 'mayor', 'settlor', 'douche', 'marrow', 'genial', 'wretch', 'conjuring', 'indestructible', 'violation', 'trend', 'marriage', 'opiate', 'syncope', 'partner', 'piles', 'rail', 'uniformly', 'chloroform', 'puffy', 'inauguration', 'safeguard', 'bonanza', 'fury', 'artiste', 'tutelage', 'festive', 'antiquated', 'inexperienced', 'effort', 'absenteeism', 'bigot', 'combat', 'airport', 'sob', 'diversion', 'lush', 'radiation', 'homework', 'invalid', 'pillage', 'ire', 'termination', 'pistol', 'watchdog', 'auction', 'divination', 'frenzied', 'balk', 'congregation', 'overdose', 'shoulder', 'compassion', 'stallion', 'consult', 'atherosclerosis', 'barb', 'deluge', 'dungeon', 'abandoned', 'denunciation', 'cessation', 'subpoena', 'reliable', 'aristocracy', 'clan', 'frown', 'difficulty', 'heartache', 'ardent', 'obstinate', 'hysteria', 'perdition', 'recommend', 'verve', 'stingy', 'authorization', 'harshness', 'tangled', 'glad', 'commutation', 'bury', 'team', 'malevolent', 'cleaning', 'polish', 'grandeur', 'sufferer', 'regiment', 'provide', 'sunny', 'arrogant', 'nectar', 'sardonic', 'spelling', 'boring', 'diligence', 'sterility', 'intercede', 'inhumanity', 'lordship', 'buss', 'heavenly', 'knowing', 'bouquet', 'misunderstand', 'forlorn', 'homeless', 'meandering', 'illumination', 'affirmative', 'blowout', 'ecstasy', 'anarchist', 'imprisoned', 'mindfulness', 'plentiful', 'euthanasia', 'disregarded', 'prestige', 'strut', 'cabinet', 'gem', 'plummet', 'fault', 'gracious', 'silly', 'congestion', 'seals', 'law', 'disqualified', 'chosen', 'growth', 'incest', 'nervous', 'partake', 'prognostic', 'loom', 'counsellor', 'influenza', 'ornate', 'eviction', 'forward', 'infirmity', 'friction', 'trophy', 'goblin', 'jump', 'insipid', 'watchman', 'parole', 'compliance', 'abominable', 'freakish', 'outcome', 'kick', 'remiss', 'involution', 'gain', 'luxuriant', 'eavesdropping', 'university', 'fermentation', 'pith', 'youth', 'disappoint', 'articulation', 'distortion', 'oath', 'corroboration', 'shortly', 'hug', 'stab', 'compensate', 'tantamount', 'butler', 'sentry', 'impersonate', 'mucous', 'thug', 'popularity', 'unfinished', 'boxing', 'poll', 'policeman', 'organic', 'bickering', 'baptismal', 'booze', 'worm', 'swift', 'sanctify', 'acuity', 'choke', 'slime', 'substance', 'unrequited', 'adapt', 'ingenious', 'mocking', 'successful', 'dexterity', 'dubious', 'impropriety', 'messenger', 'heartless', 'rational', 'magnetite', 'familiarity', 'weary', 'seasoned', 'palsy', 'conscience', 'abject', 'commemorate', 'apprehension', 'shanghai', 'contravention', 'labored', 'decay', 'defenseless', 'temptation', 'lowering', 'sensuous', 'hopelessness', 'supported', 'conglomerate', 'pristine', 'confinement', 'eschew', 'enchanted', 'scapegoat', 'constant', 'cheer', 'verge', 'approve', 'providing', 'cigarette', 'suck', 'untoward', 'negligence', 'retraction', 'genius', 'sprain', 'escape', 'kite', 'articulate', 'relapse', 'theorem', 'pollution', 'deal', 'adrift', 'grasping', 'shack', 'impolite', 'surprise', 'snide', 'resistance', 'encumbrance', 'secretion', 'deport', 'diversified', 'loyalty', 'unconscious', 'immortality', 'advancement', 'graciously', 'glorification', 'hog', 'lamenting', 'edict', 'dissolution', 'scold', 'warped', 'neutral', 'litigious', 'brunt', 'cemetery', 'affirmation', 'terminal', 'watchful', 'budget', 'fearful', 'disability', 'jackpot', 'render', 'irregular', 'irritable', 'badly', 'punctual', 'overpowering', 'stalemate', 'penchant', 'friendly', 'journeyman', 'disruption', 'reciprocate', 'machine', 'popularized', 'preparedness', 'rend', 'tragedy', 'authentic', 'dictum', 'fasting', 'gloomy', 'bomber', 'prudent', 'mimicry', 'parliament', 'lightning', 'pointedly', 'coalition', 'notion', 'scrimmage', 'err', 'humbled', 'sensuality', 'existence', 'forgiven', 'vendetta', 'academic', 'relative', 'hale', 'scaffold', 'shortage', 'homesick', 'prudence', 'firstborn', 'phantom', 'splitting', 'assessor', 'habitat', 'towering', 'evade', 'qualifying', 'esteem', 'brag', 'merciless', 'reflux', 'inexcusable', 'sickly', 'emergency', 'pledge', 'mire', 'wasting', 'slayer', 'jaws', 'endowment', 'wishful', 'hide', 'eminence', 'extinct', 'advanced', 'keepsake', 'priestly', 'obvious', 'savage', 'affluence', 'stately', 'gaining', 'dispel', 'outpost', 'lessen', 'taught', 'duel', 'honorable', 'reproach', 'nuisance', 'relegation', 'excavation', 'foolishness', 'ineptitude', 'scorpion', 'taunt', 'inspired', 'credible', 'uplift', 'endocarditis', 'noted', 'robbery', 'assignee', 'vernal', 'received', 'outburst', 'rapture', 'soiled', 'peculiarities', 'conciliation', 'godless', 'witness', 'congratulatory', 'mischief', 'crouch', 'swear', 'reading', 'aggressive', 'crazy', 'fuss', 'willingly', 'smiling', 'glum', 'alarm', 'resent', 'dashing', 'martial', 'guilty', 'retaliatory', 'reticent', 'perk', 'question', 'wages', 'supplies', 'dispersion', 'lump', 'picket', 'denounce', 'sunshine', 'genuine', 'spiteful', 'vigorous', 'sewage', 'catch', 'leak', 'chronic', 'disintegrate', 'spotless', 'slam', 'garnet', 'united', 'fallacy', 'daft', 'elimination', 'intervention', 'belligerent', 'frigate', 'concluding', 'afford', 'complementary', 'exaltation', 'ominous', 'deceiving', 'falling', 'manifested', 'accounts', 'helmet', 'monetary', 'climax', 'accountability', 'authority', 'beauty', 'younger', 'trump', 'neurotic', 'benefactor', 'gasping', 'morbid', 'alerts', 'long', 'offhand', 'degradation', 'flake', 'occupation', 'parting', 'vacation', 'amphetamines', 'hyperbole', 'crouching', 'gallantry', 'disconnection', 'unbroken', 'thermometer', 'coast', 'charade', 'reparation', 'infringement', 'tawny', 'irresponsible', 'difficulties', 'engaged', 'complicate', 'deaf', 'embrace', 'detritus', 'probation', 'subito', 'disbelieve', 'result', 'stainless', 'option', 'whimsical', 'legitimacy', 'strengthening', 'patron', 'staggering', 'corrosive', 'makeshift', 'compact', 'aspiration', 'iron', 'illegible', 'center', 'impracticable', 'allegro', 'chagrin', 'cutting', 'ardor', 'thump', 'respectable', 'healthful', 'cathedral', 'resigned', 'projectiles', 'shot', 'standing', 'noncompliance', 'desist', 'overpriced', 'exquisite', 'affection', 'ken', 'snare', 'liberate', 'scandalous', 'aggression', 'oppression', 'restraint', 'morgue', 'memorable', 'leakage', 'metropolitan', 'sump', 'recession', 'pinnacle', 'modify', 'bonus', 'mutilation', 'physiology', 'guarded', 'imitation', 'exchange', 'neatly', 'smoothness', 'retaliation', 'prognosis', 'tree', 'persistence', 'defiant', 'clothe', 'bounty', 'linguist', 'juvenile', 'doodle', 'trepidation', 'standoff', 'injustice', 'detract', 'frugal', 'bliss', 'escaped', 'legal', 'bonne', 'bizarre', 'paddle', 'protecting', 'leukemia', 'useless', 'damage', 'concealment', 'infant', 'ridiculous', 'disaster', 'nourishment', 'jubilee', 'torn', 'confound', 'battery', 'monk', 'precision', 'corrosion', 'disheartening', 'agitated', 'lifeless', 'hardy', 'mismatch', 'debate', 'degree', 'preclude', 'veto', 'untenable', 'envious', 'rampage', 'bore', 'tribune', 'skilled', 'marshal', 'miracle', 'abortion', 'mosquito', 'indecent', 'forte', 'growling', 'mortification', 'confident', 'lemon', 'crunch', 'shapely', 'insurgent', 'procrastination', 'raving', 'smelling', 'conflicting', 'delusional', 'exalted', 'gull', 'subsist', 'cough', 'content', 'fraudulent', 'surrogate', 'vicar', 'contact', 'informer', 'deliberate', 'gruesome', 'adequacy', 'backward', 'intimately', 'merry', 'erratic', 'cannibal', 'difficult', 'bribery', 'lettered', 'superfluous', 'toxin', 'aspire', 'applicant', 'bargain', 'unbelievable', 'possession', 'inflation', 'quickness', 'revolutionary', 'writer', 'sue', 'brigade', 'elated', 'caution', 'hollow', 'inhibit', 'slimy', 'technology', 'greedy', 'madman', 'morrow', 'parish', 'prolific', 'squeamish', 'starvation', 'luck', 'supply', 'reason', 'building', 'mandamus', 'countryman', 'smuggler', 'distasteful', 'serpent', 'flagship', 'enema', 'shaking', 'breakneck', 'usefulness', 'prologue', 'intuition', 'loving', 'belittle', 'reinforcement', 'redemption', 'lucky', 'unwitting', 'rhythm', 'hemorrhage', 'guide', 'decency', 'stationary', 'pains', 'dun', 'deadlock', 'abacus', 'intimidate', 'corroborate', 'saber', 'twitch', 'spider', 'posse', 'busted', 'prodigal', 'arrogance', 'dictatorial', 'reverie', 'soup', 'thrash', 'untie', 'courage', 'indemnity', 'happen', 'impurity', 'foreclose', 'fainting', 'axiom', 'outlaw', 'litter', 'fickle', 'distillation', 'chivalry', 'symmetrical', 'rapt', 'lurk', 'potable', 'deflation', 'deserve', 'perpetuation', 'earl', 'commendable', 'impede', 'spree', 'catheter', 'rout', 'inefficiency', 'wildcat', 'secular', 'unification', 'meadow', 'regurgitation', 'vital', 'perturbation', 'discord', 'eagerness', 'colic', 'faultless', 'mortal', 'cream', 'fascinating', 'chairman', 'usher', 'graduation', 'glorify', 'punch', 'haunted', 'perfect', 'unaccountable', 'pretended', 'mourn', 'tumult', 'opposition', 'satisfied', 'seclusion', 'suppression', 'defendant', 'atonement', 'rheumatism', 'mange', 'sever', 'superhuman', 'wondrous', 'excel', 'withstand', 'commission', 'erroneous', 'rat', 'smell', 'notable', 'indistinct', 'hustler', 'salute', 'top', 'tranquil', 'oasis', 'pious', 'thoughtful', 'astronaut', 'horde', 'distaste', 'sadness', 'resignation', 'outlandish', 'prediction', 'deformed', 'excess', 'board', 'unreliable', 'snort', 'odor', 'newcomer', 'pare', 'subject', 'considerable', 'berserk', 'unpredictable', 'contradict', 'believed', 'commotion', 'allowable', 'nag', 'plush', 'king', 'enterprising', 'martyr', 'aroma', 'placard', 'subordinate', 'banger', 'wen', 'incoherent', 'leery', 'apology', 'credibility', 'warned', 'baptism', 'professional', 'doubting', 'unbeaten', 'proud', 'frisky', 'scavenger', 'diminish', 'assistance', 'renovation', 'john', 'hygienic', 'cane', 'uncertain', 'autocratic', 'allure', 'kitten', 'forsake', 'nadir', 'lower', 'asylum', 'imitated', 'provocation', 'stunted', 'prodigy', 'incompetent', 'lion', 'manners', 'representing', 'starved', 'nauseous', 'lazy', 'uncontrollable', 'involuntary', 'underwrite', 'offense', 'rook', 'unknown', 'scarecrow', 'surgery', 'vent', 'jarring', 'germination', 'misnomer', 'harbor', 'community', 'mutiny', 'pact', 'hero', 'panache', 'condone', 'indeterminate', 'bayonet', 'grope', 'secretive', 'overwhelm', 'smuggle', 'sponge', 'surprised', 'nameless', 'flaunt', 'engaging', 'rumor', 'canons', 'stalk', 'puppy', 'base', 'coexisting', 'reprint', 'stranger', 'deceit', 'visitor', 'martyrdom', 'innocent', 'armed', 'drown', 'incontinence', 'hermaphrodite', 'accursed', 'complication', 'footing', 'defense', 'stifled', 'intrigue', 'passivity', 'gallows', 'partnership', 'belt', 'detachment', 'reformation', 'holocaust', 'unfairness', 'earthquake', 'covenant', 'waver', 'movable', 'bilingual', 'efficiency', 'forbidding', 'helplessness', 'vision', 'infrequent', 'improving', 'thinker', 'heathen', 'worse', 'spaniel', 'regretting', 'shackle', 'interment', 'keystone', 'wise', 'lemma', 'prerequisite', 'happily', 'regent', 'cacophony', 'bereaved', 'impart', 'forgotten', 'leave', 'flood', 'appalling', 'sewerage', 'police', 'hail', 'profuse', 'bruise', 'electorate', 'deed', 'harass', 'depravity', 'accompaniment', 'levy', 'defiance', 'nul', 'torpedo', 'witty', 'structural', 'knack', 'inclement', 'reporter', 'phoenix', 'underline', 'offend', 'horrified', 'pretending', 'repay', 'fete', 'perpetrator', 'treadmill', 'associate', 'irrevocable', 'beautiful', 'crusty', 'gape', 'drone', 'exaggerate', 'putative', 'discontinuity', 'demented', 'tarry', 'professorship', 'daughter', 'campaigning', 'grenade', 'defamation', 'loaf', 'sucker', 'unitary', 'apathy', 'gold', 'smash', 'reconstruction', 'chocolate', 'ravine', 'precinct', 'muzzle', 'superman', 'unfair', 'surpassing', 'fondness', 'larger', 'fake', 'brawl', 'death', 'rubric', 'usurp', 'danger', 'enmity', 'tempered', 'aye', 'chorus', 'aiding', 'pope', 'absurd', 'resistive', 'battled', 'recombination', 'imposition', 'phony', 'despairing', 'digit', 'patrol', 'deputy', 'gentry', 'declination', 'smut', 'moot', 'apparition', 'chicken', 'retract', 'omniscient', 'unconscionable', 'foreman', 'blister', 'adjudicate', 'ripe', 'calculating', 'crack', 'demonic', 'agile', 'coldly', 'requiem', 'paraphrase', 'discriminate', 'duplicity', 'remove', 'arrest', 'fraud', 'experiment', 'pulpit', 'capture', 'sultan', 'debris', 'duress', 'tenacity', 'bewildered', 'relieving', 'dangerous', 'mangle', 'creep', 'oppress', 'policy', 'endure', 'masculine', 'accomplishment', 'acceptance', 'instructions', 'challenge', 'ungrateful', 'misrepresent', 'terrific', 'tension', 'fidelity', 'cupping', 'stolen', 'launch', 'polished', 'bastion', 'needful', 'endow', 'hit', 'sufficiency', 'safekeeping', 'wither', 'inefficient', 'pull', 'incomprehensible', 'rancid', 'shortcoming', 'admirable', 'forthcoming', 'humorist', 'stroke', 'pill', 'disfigured', 'obscene', 'scolding', 'adaptable', 'crumbling', 'insecure', 'leaning', 'voucher', 'courtesy', 'environ', 'monopolist', 'worry', 'didactic', 'importance', 'blessings', 'dignity', 'loo', 'tribunal', 'superior', 'deteriorated', 'scientific', 'vanish', 'swelling', 'synod', 'transaction', 'nation', 'rambling', 'jubilant', 'case', 'destination', 'locust', 'outsider', 'allied', 'chowder', 'fixed', 'sweetheart', 'salutary', 'broken', 'febrile', 'ethics', 'infamous', 'sepsis', 'expenses', 'bitterly', 'amnesty', 'maggot', 'politic', 'bleeding', 'refusal', 'insanity', 'drunken', 'moronic', 'discolored', 'wickedness', 'paralysis', 'enclave', 'judicious', 'hoax', 'routine', 'grant', 'household', 'obtuse', 'jabber', 'regulatory', 'install', 'jovial', 'prescient', 'cooperating', 'elder', 'seizure', 'languishing', 'fiend', 'finally', 'autopsy', 'averse', 'courtship', 'ruin', 'obtainable', 'stuffy', 'conserve', 'feat', 'shady', 'hippie', 'unbelief', 'clerical', 'incisive', 'cooperative', 'angelic', 'league', 'talent', 'strife', 'beg', 'coerce', 'bunker', 'curable', 'disappointed', 'unwell', 'detest', 'break', 'repress', 'weigh', 'shoddy', 'sticky', 'destined', 'mildew', 'task', 'humorous', 'enlightenment', 'fusion', 'exotic', 'disliked', 'discoloration', 'saloon', 'reactionary', 'thankful', 'civil', 'disregard', 'animosity', 'happy', 'garbage', 'fragrant', 'lethal', 'parentage', 'instinctive', 'egotistical', 'connective', 'fang', 'drowsiness', 'worrying', 'remorse', 'pose', 'trumpet', 'incursion', 'sham', 'slowness', 'embarrassing', 'dagger', 'inter', 'sorter', 'interrupt', 'words', 'poisoning', 'picketing', 'drunkenness', 'shrunk', 'conducive', 'untrained', 'deceived', 'impartial', 'collectively', 'infection', 'leader', 'blindly', 'perplexed', 'furious', 'idiot', 'smile', 'problem', 'splendor', 'regal', 'glide', 'deficiency', 'forgive', 'liberty', 'yelp', 'devious', 'failing', 'snarling', 'finery', 'sloth', 'tit', 'sudden', 'pungent', 'ruse', 'vindictive', 'bitterness', 'cumbersome', 'clean', 'babble', 'barter', 'depression', 'opponent', 'proviso', 'rue', 'elect', 'hilarious', 'triumph', 'wrestling', 'supporting', 'spike', 'wrongly', 'convince', 'treasurer', 'insolent', 'barbaric', 'feces', 'panic', 'humbly', 'mastery', 'wanting', 'fell', 'unresolved', 'famously', 'landed', 'depress', 'obstructive', 'perversion', 'meddle', 'strive', 'guarantee', 'sinner', 'unsuccessful', 'incalculable', 'polite', 'lobbyist', 'alcoholism', 'revise', 'falsify', 'answerable', 'counselor', 'unprecedented', 'violent', 'series', 'gospel', 'misrepresented', 'retrenchment', 'missing', 'geranium', 'amusing', 'cholera', 'radiate', 'ailing', 'engulf', 'sonorous', 'rejected', 'taboo', 'prodigious', 'tyrannical', 'interlude', 'enchant', 'entertain', 'ancestral', 'loudness', 'solidarity', 'antipathy', 'stereotype', 'whimper', 'flowery', 'barrow', 'specie', 'reliance', 'sentimental', 'annihilation', 'runaway', 'veneration', 'breakdown', 'lesson', 'desolation', 'remedy', 'feudal', 'honeymoon', 'classics', 'swastika', 'unofficial', 'fulfill', 'heal', 'malaise', 'fitting', 'infidelity', 'complexity', 'delegate', 'deteriorate', 'destructive', 'perceptible', 'possibility', 'dull', 'disinfection', 'angina', 'effective', 'entanglement', 'vigil', 'trickery', 'smith', 'crusade', 'winning', 'enslaved', 'fabricate', 'variable', 'badge', 'dragon', 'conservation', 'boisterous', 'employ', 'grating', 'departure', 'tender', 'premises', 'fain', 'supremely', 'vomiting', 'obliterate', 'cur', 'battlefield', 'fervor', 'exigent', 'antagonism', 'influence', 'corporeal', 'somatic', 'interrogation', 'cushion', 'maintenance', 'feudalism', 'apostle', 'standstill', 'thief', 'spasm', 'suffer', 'suspense', 'wail', 'assail', 'linger', 'prince', 'resist', 'debt', 'therapeutics', 'postponement', 'impersonation', 'ordinance', 'track', 'aplomb', 'twinkle', 'isolated', 'leisurely', 'ownership', 'mistress', 'staccato', 'thorn', 'growl', 'raucous', 'goo', 'dermatologist', 'crucifixion', 'rigidity', 'combative', 'cogent', 'fussy', 'prohibited', 'concord', 'escort', 'guts', 'accueil', 'cleanse', 'melodrama', 'scarcity', 'undisclosed', 'adder', 'sing', 'excretion', 'companion', 'quash', 'firmness', 'gut', 'spanking', 'wrongdoing', 'humane', 'torrid', 'infidel', 'persecution', 'exhortation', 'neutrality', 'segregate', 'ease', 'surety', 'subversive', 'tenant', 'intolerant', 'jealousy', 'stools', 'frayed', 'numbness', 'accredited', 'disciple', 'pus', 'showy', 'bereavement', 'freedom', 'harrowing', 'astringent', 'dire', 'injurious', 'generous', 'owing', 'paramount', 'toleration', 'unwashed', 'cowardly', 'assets', 'sly', 'guidance', 'biased', 'cloister', 'invalidate', 'amicable', 'obesity', 'trashy', 'banquet', 'willful', 'unforgiving', 'hardship', 'hostile', 'purely', 'pow', 'taint', 'recruits', 'vengeance', 'degrade', 'feature', 'chilly', 'powerless', 'pop', 'indolent', 'thirteenth', 'pioneer', 'exhilaration', 'overburden', 'regretted', 'flush', 'intermission', 'designation', 'treat', 'exclaim', 'fascination', 'player', 'unfaithful', 'plum', 'disagree', 'indelible', 'dandy', 'homology', 'camouflaged', 'ascent', 'termite', 'immigrant', 'wits', 'temperance', 'embolism', 'falsehood', 'infect', 'warning', 'injure', 'reaffirm', 'hilarity', 'servitude', 'furiously', 'epitome', 'birthday', 'deviation', 'traitor', 'poisoned', 'criminality', 'hot', 'innovation', 'warranty', 'unbiased', 'sunset', 'knell', 'tedium', 'batter', 'quack', 'alarming', 'schizophrenia', 'level', 'frankness', 'bleak', 'sweat', 'enablement', 'murky', 'unverified', 'cursing', 'jornada', 'bookshop', 'legendary', 'chaff', 'stark', 'intruder', 'reassurance', 'cliff', 'comptroller', 'virtuous', 'riotous', 'letter', 'lord', 'flee', 'intellectual', 'inaudible', 'appendicitis', 'furor', 'refractory', 'mystic', 'resident', 'pernicious', 'gratify', 'duke', 'coward', 'bravado', 'gunpowder', 'kind', 'harvest', 'aghast', 'humanitarian', 'befriend', 'arduous', 'dreadfully', 'dictatorship', 'dart', 'disagreeing', 'surround', 'confidence', 'meditate', 'assassin', 'cede', 'perceive', 'unsafe', 'pompous', 'delight', 'atrocious', 'succinct', 'granted', 'swig', 'spacious', 'rubble', 'actual', 'imprisonment', 'charity', 'intercession', 'nestle', 'questionable', 'anxiety', 'calls', 'eradication', 'hypocritical', 'bankrupt', 'believer', 'hanging', 'gaby', 'instability', 'mindful', 'legislator', 'margin', 'extensive', 'probability', 'dreadful', 'aesthetic', 'unfurnished', 'havoc', 'crushed', 'soak', 'uphill', 'hymn', 'auditor', 'tasty', 'tempest', 'incompetence', 'normality', 'premature', 'ancient', 'marked', 'intolerable', 'ineffable', 'verified', 'governor', 'cutthroat', 'sore', 'placid', 'commonwealth', 'occupant', 'loss', 'creature', 'bias', 'kennel', 'assessment', 'boomerang', 'contradictory', 'apologetic', 'kern', 'executioner', 'aid', 'haven', 'pontiff', 'stint', 'lint', 'magical', 'declaratory', 'dishonest', 'vouch', 'arrive', 'majority', 'journalism', 'evil', 'excite', 'scorching', 'caress', 'serve', 'uninspired', 'destitute', 'journey', 'yell', 'divested', 'unimportant', 'humiliation', 'loathing', 'main', 'convert', 'octopus', 'infrequently', 'pregnancy', 'rocket', 'aptitude', 'surrendering', 'harmful', 'constitute', 'waste', 'fat', 'ridicule', 'fee', 'discretion', 'flop', 'emptiness', 'struggle', 'communication', 'pensive', 'nun', 'delay', 'unhealthy', 'accuser', 'solid', 'frustrated', 'enhance', 'civilized', 'arbiter', 'blind', 'tactics', 'greasy', 'status', 'majesty', 'thrilling', 'imperfection', 'gray', 'epitaph', 'unyielding', 'turbulent', 'thriving', 'wimpy', 'patience', 'eruption', 'handbook', 'overwhelming', 'discharge', 'thermocouple', 'resection', 'opium', 'young', 'devastation', 'joy', 'aftermath', 'flea', 'irrationality', 'dolor', 'steal', 'heady', 'praiseworthy', 'trust', 'elegance', 'seek', 'denying', 'presumption', 'reconciliation', 'revolting', 'administrative', 'suspicion', 'stiff', 'blower', 'servant', 'impairment', 'alienate', 'colonel', 'impermeable', 'slip', 'ready', 'truthful', 'vitality', 'continue', 'amen', 'boycott', 'enlighten', 'talk', 'repellant', 'stupidity', 'agreeing', 'apologize', 'blasphemous', 'widespread', 'joke', 'troublesome', 'faith', 'advisable', 'gauging', 'avatar', 'sensual', 'orthodoxy', 'oaf', 'perverted', 'embroiled', 'severance', 'tipsy', 'sense', 'hood', 'padding', 'untidy', 'pretense', 'rob', 'righteous', 'exquisitely', 'checklist', 'achieve', 'classic', 'groundless', 'conflagration', 'revere', 'soulmate', 'confine', 'vigor', 'sorrowful', 'conceal', 'ethical', 'forgiving', 'attacking', 'superstition', 'dark', 'secrete', 'defender', 'misfortune', 'monogamy', 'cider', 'negligent', 'endemic', 'extremity', 'bonds', 'sap', 'reconsideration', 'gladiator', 'agreement', 'reorganize', 'sterile', 'plodding', 'green', 'empower', 'grizzly', 'pastor', 'listless', 'unspeakable', 'slut', 'blockade', 'fierce', 'balance', 'pine', 'amazingly', 'attendant', 'expire', 'staring', 'unguarded', 'cheerfulness', 'junk', 'indifference', 'gladness', 'fruitful', 'heroism', 'wit', 'excise', 'cohesive', 'wasted', 'thirsty', 'authorize', 'constrain', 'ruined', 'instigation', 'prowl', 'veer', 'precious', 'exacting', 'celebrated', 'forfeited', 'libel', 'muddle', 'sadly', 'uninformed', 'beautification', 'retribution', 'detention', 'incline', 'prospectively', 'scourge', 'defeat', 'stiffness', 'rabid', 'organize', 'pedestrian', 'tenable', 'debenture', 'advantage', 'perilous', 'kerosene', 'demolish', 'squall', 'unity', 'president', 'mafia', 'time', 'animate', 'uncover', 'ballet', 'bribe', 'knickers', 'truck', 'gamble', 'abandonment', 'regularity', 'critic', 'menial', 'sensational', 'solvency', 'usual', 'matrimony', 'deceased', 'resolutely', 'hashish', 'emir', 'derision', 'ignorance', 'shun', 'crippled', 'theoretical', 'weighty', 'lonesome', 'onerous', 'unexpectedly', 'fully', 'garnish', 'expected', 'revenge', 'vague', 'doit', 'undertaking', 'earnest', 'traps', 'disgust', 'din', 'starlight', 'rape', 'underestimate', 'fictitious', 'litigate', 'prostitute', 'respecting', 'cultivation', 'sympathetic', 'succumb', 'kill', 'cheerful', 'leisure', 'scribe', 'unconstitutional', 'defraud', 'bog', 'chafing', 'evict', 'astonishment', 'dishonesty', 'communist', 'needle', 'exorcism', 'cry', 'ordeal', 'powerfully', 'blight', 'protected', 'precarious', 'pilot', 'mortgagor', 'wrecked', 'pessimism', 'orphan', 'bestial', 'callous', 'deserved', 'hiding', 'heartworm', 'strike', 'undoubted', 'espionage', 'ribbon', 'ordained', 'grab', 'vindication', 'perplexity', 'lying', 'assent', 'audience', 'bewilderment', 'overture', 'pastry', 'incense', 'gap', 'indignation', 'diaper', 'complicated', 'radon', 'humble', 'blighted', 'invalidity', 'gloom', 'nurture', 'subsidy', 'eventuality', 'burdensome', 'clown', 'completely', 'rectify', 'porcupine', 'spoil', 'flinch', 'insignificant', 'circumvention', 'mercenary', 'title', 'luster', 'unstable', 'penalty', 'erosion', 'error', 'anomaly', 'throttle', 'reputable', 'feeling', 'toast', 'indignant', 'pornographic', 'memorials', 'bearish', 'numbers', 'theocratic', 'entertainment', 'chaotic', 'cardiomyopathy', 'operatic', 'burnt', 'hypocrite', 'woefully', 'success', 'banshee', 'unwise', 'bigoted', 'barbarian', 'profanity', 'aphid', 'acquire', 'credence', 'gory', 'mum', 'betterment', 'consul', 'cad', 'compatibility', 'vindicate', 'competency', 'dust', 'dumps', 'heighten', 'perverse', 'prosecute', 'brocade', 'conceited', 'encourage', 'award', 'regrettable', 'hoot', 'gorilla', 'fulfillment', 'shoplifting', 'avoiding', 'intractable', 'accommodation', 'inimitable', 'straits', 'tasteless', 'compulsion', 'amused', 'culpable', 'floral', 'secluded', 'cautiously', 'culinary', 'latent', 'philanthropy', 'tirade', 'blindfold', 'precede', 'dilute', 'decrement', 'isolation', 'surveying', 'strategist', 'wealth', 'freely', 'durable', 'contemplation', 'attraction', 'demon', 'expecting', 'mate', 'quandary', 'negative', 'haunt', 'propaganda', 'hospitality', 'filibuster', 'litigant', 'dismissal', 'urchin', 'slay', 'dissatisfaction', 'throb', 'crave', 'crabby', 'bastard', 'stunned', 'undo', 'malformation', 'blather', 'farcical', 'courier', 'bulldog', 'preliminary', 'snags', 'sisterhood', 'armaments', 'interior', 'wan', 'weakened', 'delivery', 'understanding', 'amenity', 'onset', 'marquis', 'ulcer', 'acumen', 'irregularity', 'malfeasance', 'magnetism', 'expired', 'attorney', 'money', 'parietal', 'council', 'unthinkable', 'transgression', 'visionary', 'dislocated', 'mirth', 'pigeon', 'director', 'threat', 'languish', 'bile', 'antisocial', 'abovementioned', 'split', 'larceny', 'louse', 'snob', 'school', 'late', 'vocabulary', 'cracking', 'rosy', 'elaboration', 'screwed', 'gash', 'scrambling', 'cruciate', 'surge', 'ungodly', 'creeping', 'slate', 'intuitive', 'advice', 'imputation', 'assassinate', 'affront', 'announcement', 'barf', 'gang', 'cash', 'avoid', 'infiltration', 'fenced', 'doer', 'gruff', 'inattention', 'classify', 'indemnify', 'obliging', 'resign', 'gratuitous', 'beneficial', 'erase', 'numb', 'unintentional', 'velvet', 'unchangeable', 'manipulation', 'hearse', 'white', 'facts', 'evacuation', 'mountain', 'cove', 'contributor', 'tragic', 'aga', 'frighten', 'thorny', 'disused', 'strengthen', 'spew', 'dogma', 'unquestionably', 'palatable', 'hurtful', 'drab', 'disinformation', 'breach', 'joined', 'unlicensed', 'excluding', 'morality', 'consecration', 'confiscate', 'heroic', 'solace', 'convenience', 'hydrocephalus', 'sovereign', 'spoke', 'ashamed', 'accident', 'tower', 'electric', 'resumption', 'felony', 'slag', 'warp', 'varicose', 'cleanliness', 'travail', 'immaculate', 'constrained', 'prosecution', 'spokesman', 'elbow', 'hooked', 'verdict', 'bureaucracy', 'oblivion', 'psychosis', 'toilet', 'whiteness', 'crushing', 'brandy', 'constraint', 'depend', 'tranquility', 'overpaid', 'absolution', 'cheap', 'loot', 'preponderance', 'represented', 'falsity', 'speck', 'deportation', 'increase', 'proceeding', 'dissection', 'gnome', 'nepotism', 'principal', 'tobacco', 'brutal', 'shabby', 'wrongful', 'expedition', 'raging', 'confessional', 'sinister', 'bridal', 'giggle', 'imperfectly', 'turmoil', 'chemist', 'wilderness', 'antidote', 'originality', 'affiliated', 'brutality', 'commend', 'dirty', 'fleet', 'luscious', 'anger', 'discourage', 'offer', 'insecurity', 'bombard', 'leading', 'yawning', 'falsification', 'alertness', 'evanescence', 'blessed', 'undesired', 'magnet', 'discredit', 'lust', 'laughing', 'invoke', 'ultimatum', 'forcibly', 'physician', 'needy', 'nobility', 'antifungal', 'model', 'obligor', 'cadaver', 'admiral', 'saturated', 'weeds', 'gigantic', 'forage', 'perfection', 'delinquent', 'fangs', 'zip', 'constitutional', 'scrutiny', 'quote', 'invader', 'morn', 'ethereal', 'redundant', 'humanity', 'progression', 'perished', 'disservice', 'vigilant', 'purgatory', 'malignant', 'validity', 'disappointment', 'laughable', 'educate', 'brimstone', 'caricature', 'stale', 'talons', 'pessimist', 'foe', 'unlimited', 'proverbial', 'warfare', 'lynch', 'projectile', 'anathema', 'pray', 'favorite', 'gob', 'crisp', 'displeasure', 'welcomed', 'trifle', 'visitation', 'intoxicated', 'abolition', 'mentor', 'departed', 'extricate', 'dear', 'complaint', 'carnage', 'abhor', 'pride', 'merge', 'emancipation', 'angling', 'nomination', 'cosmopolitan', 'servile', 'justifiable', 'payback', 'bloodless', 'corrective', 'alleviate', 'retention', 'bite', 'guidebook', 'shaky', 'afflict', 'naughty', 'involvement', 'loathe', 'enrich', 'moribund', 'colossal', 'fun', 'caretaker', 'grisly', 'bravery', 'neglected', 'stillborn', 'neighbor', 'objectionable', 'inclusion', 'despotic', 'disconnected', 'confuse', 'retirement', 'lusty', 'military', 'indigent', 'horrific', 'submit', 'premeditated', 'scaly', 'criminal', 'immerse', 'replete', 'distraught', 'custody', 'confounded', 'remake', 'show', 'ghastly', 'hellish', 'encyclopedia', 'stoppage', 'dole', 'devastating', 'incompleteness', 'integrity', 'clearance', 'accolade', 'chisel', 'performer', 'nervousness', 'brazen', 'unpublished', 'madden', 'surmise', 'loyal', 'diseased', 'riot', 'pasture', 'unruly', 'incendiary', 'burke', 'loon', 'tiresome', 'scoff', 'wait', 'food', 'avenger', 'noise', 'menace', 'apprentice', 'starry', 'revels', 'eminently', 'drinking', 'prevail', 'siren', 'innovate', 'uproar', 'bug', 'symphony', 'disqualify', 'sullen', 'malpractice', 'friendship', 'shame', 'underpaid', 'anchorage', 'lumpy', 'contracted', 'waterproof', 'baccalaureate', 'dementia', 'parasite', 'recalcitrant', 'priceless', 'tumour', 'poisonous', 'adversity', 'demise', 'productive', 'cobra', 'drudgery', 'amaze', 'refurbish', 'deferral', 'bacteria', 'orchestra', 'vengeful', 'roadster', 'toothache', 'warrior', 'heartfelt', 'asserting', 'prove', 'hospital', 'inactivity', 'contradiction', 'damned', 'antithesis', 'heft', 'torture', 'glowing', 'notoriety', 'corse', 'desecration', 'discovery', 'slur', 'mortality', 'couch', 'razor', 'shape', 'ascendancy', 'risky', 'allege', 'awful', 'unimaginable', 'lieutenant', 'hire', 'unattractive', 'cherry', 'special', 'satanic', 'opportunity', 'lamb', 'apprehensive', 'dogged', 'mite', 'fighting', 'advance', 'blob', 'tinsel', 'husbandry', 'neurosis', 'behemoth', 'clue', 'slim', 'anarchy', 'arrival', 'fatigued', 'automatic', 'denial', 'mosque', 'elucidate', 'foul', 'neglect', 'recurrent', 'overjoyed', 'idler', 'fleshy', 'stormy', 'gusset', 'renaissance', 'sanctification', 'sting', 'unsustainable', 'tornado', 'premier', 'teach', 'crowning', 'saint', 'notification', 'sarcoma', 'leeway', 'complacency', 'consternation', 'untimely', 'dominion', 'resources', 'continuation', 'strongly', 'undiscovered', 'meritorious', 'malice', 'stinking', 'marvelously', 'quiz', 'piety', 'gambling', 'exile', 'soothe', 'hell', 'amnesia', 'indefensible', 'stealth', 'furrow', 'harassing', 'strategic', 'fall', 'confined', 'convinced', 'corporal', 'hap', 'assembly', 'found', 'disparaging', 'presto', 'accelerate', 'determinate', 'mad', 'restoring', 'appeal', 'distract', 'aunt', 'wound', 'captor', 'fair', 'governess', 'sloppy', 'roulette', 'depressing', 'birth', 'critter', 'bier', 'furnace', 'admire', 'detect', 'abundance', 'coop', 'lowlands', 'withered', 'changeable', 'indenture', 'unscrupulous', 'convergence', 'enslave', 'sanguine', 'displeased', 'rider', 'chuckle', 'grim', 'boast', 'intercourse', 'conquest', 'fill', 'virulence', 'victory', 'countdown', 'endangered', 'tighten', 'sciatica', 'oracle', 'celebrating', 'coherent', 'rattlesnake', 'puke', 'hype', 'complement', 'certainty', 'compatible', 'kiss', 'lovemaking', 'lie', 'inequality', 'amusement', 'omen', 'cruelty', 'descent', 'measles', 'prime', 'culpability', 'obstacle', 'revival', 'shark', 'torrent', 'bequest', 'misbehavior', 'charmed', 'cremation', 'garish', 'hateful', 'devout', 'headache', 'surrender', 'failure', 'accomplished', 'reform', 'terrorize', 'exacerbate', 'bursary', 'overturn', 'monochrome', 'murder', 'prohibition', 'brilliant', 'flounder', 'practiced', 'alliance', 'athlete', 'declining', 'donkey', 'chieftain', 'ground', 'shooter', 'samurai', 'outhouse', 'antagonistic', 'designer', 'scrub', 'compelling', 'cuddle', 'insidious', 'inexperience', 'energetic', 'buttery', 'sewer', 'plunder', 'dummy', 'art', 'mannered', 'develop', 'inspector', 'flesh', 'prepare', 'disease', 'thirst', 'panacea', 'conversational', 'intestate', 'inquiry', 'endurance', 'dispassionate', 'turbulence', 'sweetness', 'stocks', 'correctness', 'quiver', 'menses', 'qualified', 'hive', 'admiration', 'distinction', 'brother', 'bothering', 'competition', 'corpse', 'shelved', 'amend', 'nest', 'devolution', 'hairy', 'fanfare', 'horizon', 'awry', 'falter', 'rencontre', 'stereotyped', 'afraid', 'hash', 'foreseen', 'mill', 'conjecture', 'earn', 'impending', 'fearing', 'effeminate', 'rehabilitation', 'improve', 'eloquent', 'willingness', 'sluggish', 'reprisal', 'succeed', 'hobo', 'arid', 'deformity', 'cosy', 'digress', 'brains', 'howl', 'composure', 'nappy', 'tackle', 'trash', 'wild', 'reward', 'oppressive', 'motherhood', 'achievement', 'interviewer', 'rigorous', 'clash', 'polio', 'salient', 'filthy', 'grace', 'tolerant', 'complicity', 'innocuous', 'manifestation', 'degenerate', 'delightful', 'mournful', 'ooze', 'mime', 'fortify', 'aberrant', 'cyanide', 'exalt', 'rift', 'unanimity', 'sincerity', 'interest', 'patronizing', 'professor', 'equality', 'unprepared', 'reader', 'sonar', 'assuredly', 'radioactive', 'confidentially', 'picnic', 'disillusionment', 'fear', 'dreary', 'public', 'bard', 'suspicious', 'hostility', 'bland', 'rescission', 'beast', 'ditty', 'architecture', 'bugle', 'chance', 'crap', 'maiden', 'foreign', 'ponderous', 'seriousness', 'miscarriage', 'captive', 'peri', 'adhering', 'burial', 'crew', 'chargeable', 'harry', 'bout', 'doom', 'lavish', 'contrary', 'phlegm', 'ability', 'court', 'diatribe', 'comprehensive', 'wretched', 'mathematical', 'recovery', 'friendliness', 'lustful', 'nausea', 'nerve', 'ambiguous', 'prying', 'mainstay', 'present', 'affliction', 'housekeeping', 'dizziness', 'gun', 'bale', 'mediocrity', 'tremor', 'athletic', 'entrust', 'chatty', 'authentication', 'raptors', 'lowest', 'wop', 'stink', 'camouflage', 'vesicular', 'observant', 'shockingly', 'hideous', 'hangman', 'ringer', 'conformance', 'shrill', 'related', 'glow', 'fruitless', 'incubus', 'humbug', 'refuse', 'politics', 'erratum', 'enigmatic', 'olfactory', 'smitten', 'fender', 'retrograde', 'pointless', 'purge', 'misunderstanding', 'sneer', 'prophet', 'purify', 'onus', 'accord', 'expectancy', 'vertigo', 'unfold', 'salary', 'sentinel', 'deplorable', 'cutter', 'heritage', 'respite', 'easygoing', 'evasion', 'fearless', 'facilitate', 'immovable', 'incineration', 'immature', 'suffocating', 'seal', 'sissy', 'trembling', 'screaming', 'soldier', 'jest', 'overthrow', 'diplomacy', 'lurid', 'utility', 'committal', 'enliven', 'confirmed', 'ensemble', 'stupor', 'moderator', 'sublimation', 'unkind', 'sorcery', 'payment', 'grieve', 'untamed', 'lapse', 'blissful', 'frailty', 'blossom', 'renowned', 'blitz', 'fallow', 'remarkable', 'renegade', 'gymnast', 'uninteresting', 'vanity', 'cracked', 'cranky', 'dentistry', 'instructor', 'tasteful', 'insufficiency', 'tumultuous', 'woe', 'prosper', 'wholesome', 'disorganized', 'smug', 'disable', 'gateway', 'retardation', 'competence', 'gouge', 'nigger', 'absence', 'rear', 'quarrel', 'brotherhood', 'losing', 'victor', 'alienation', 'responsive', 'shattered', 'silk', 'captain', 'halting', 'expatriate', 'laurel', 'framework', 'chore', 'felon', 'unison', 'commanding', 'progress', 'pleased', 'mug', 'mistaken', 'exemplary', 'lithe', 'secession', 'anaconda', 'careful', 'witch', 'overcast', 'laden', 'prison', 'gifted', 'creditable', 'exhaustive', 'acknowledgment', 'greater', 'lovable', 'renounce', 'marvel', 'abolish', 'homosexuality', 'pertussis', 'upheaval', 'weatherproof', 'transcendental', 'wane', 'speculative', 'betrothed', 'ash', 'shining', 'criticism', 'politeness', 'boo', 'referee', 'altercation', 'bloodthirsty', 'comfort', 'affirm', 'soot', 'gaunt', 'guardian', 'retort', 'unanimous', 'obscurity', 'wart', 'innocently', 'impeachment', 'stain', 'fairly', 'compulsory', 'proxy', 'protestant', 'wot', 'purity', 'dismemberment', 'gulp', 'childish', 'instigate', 'quinine', 'heinous', 'flabby', 'dispute', 'slap', 'promising', 'intelligence', 'cohesion', 'victimized', 'slump', 'bombed', 'courteous', 'patient', 'slouch', 'astrologer', 'attenuation', 'conversant', 'culprit', 'diplomatic', 'negotiator', 'forsaken', 'tariff', 'daring', 'obstruction', 'teeming', 'pathetic', 'antiseptic', 'impatience', 'admonition', 'habitual', 'inclusive', 'mighty', 'misconduct', 'fortune', 'exciting', 'cancer', 'murderous', 'cultivated', 'statement', 'coyote', 'reverend', 'clock', 'excellence', 'craftsman', 'artisan', 'vaccine', 'delighted', 'cowardice', 'depreciated', 'thundering', 'drugged', 'beggar', 'annul', 'economy', 'frolic', 'refine', 'sympathize', 'sir', 'inadmissible', 'blameless', 'agreed', 'paranoia', 'lovely', 'barren', 'playful', 'unhappiness', 'mutual', 'govern', 'chronicle', 'futile', 'immortal', 'inquisitive', 'dissident', 'fiasco', 'untitled', 'embarrass', 'seize', 'hindrance', 'coach', 'nothingness', 'bye', 'hamper', 'jurist', 'defended', 'profane', 'adventurous', 'unintelligible', 'lethargy', 'greeting', 'slave', 'liaison', 'scrutinize', 'grave', 'receiving', 'mob', 'hysterical', 'zest', 'carcass', 'debauchery', 'zealous', 'illegal', 'precipice', 'detainee', 'physics', 'salve', 'slough', 'sorrow', 'spectral', 'coolness', 'saliva', 'knotted', 'remedial', 'inaccessible', 'medal', 'banishment', 'liquor', 'spite', 'lackluster', 'boilerplate', 'rejects', 'accountable', 'butt', 'plunge', 'overload', 'avalanche', 'disseminate', 'wreak', 'royalty', 'backbone', 'proposition', 'centurion', 'exacerbation', 'unattainable', 'mediate', 'fawn', 'calculator', 'rule', 'animated', 'manage', 'don', 'destruction', 'ephemeris', 'disembodied', 'government', 'landmark', 'signify', 'familiar', 'worthless', 'weariness', 'communism', 'warn', 'contagion', 'misinterpretation', 'addresses', 'bankruptcy', 'impression', 'bellows', 'rod', 'peaceful', 'explore', 'armament', 'infectious', 'melancholic', 'quake', 'vomit', 'reconstruct', 'pandemic', 'ordination', 'gaol', 'interference', 'infliction', 'homage', 'aloof', 'obit', 'real', 'flying', 'delayed', 'stern', 'readily', 'balsam', 'maternal', 'freezing', 'prop', 'booby', 'nab', 'cover', 'agility', 'coup', 'guess', 'jail', 'unavoidable', 'galore', 'regressive', 'malicious', 'intonation', 'hazard', 'cartridge', 'spook', 'mockery', 'prosperous', 'choice', 'repose', 'indictment', 'cage', 'naive', 'elite', 'benign', 'coming', 'comatose', 'abrogate', 'speech', 'grinding', 'bowels', 'masterpiece', 'unpaid', 'quiet', 'adore', 'dinner', 'ignore', 'entangled', 'accidentally', 'congenial', 'deflate', 'antagonist', 'victorious', 'discards', 'attendance', 'dispossessed', 'obi', 'headway', 'planning', 'bound', 'edification', 'spurious', 'cancel', 'refused', 'force', 'solution', 'distorted', 'scarce', 'commerce', 'excluded', 'harmony', 'hurried', 'potency', 'slaughter', 'matchmaker', 'sturdy', 'abandon', 'essential', 'sectarian', 'philosopher', 'parade', 'manure', 'sneaking', 'quagmire', 'serial', 'umpire', 'prostitution', 'thanksgiving', 'accessible', 'moderate', 'artillery', 'lawyer', 'default', 'oxidation', 'brotherly', 'interrupted', 'fertile', 'hating', 'intrusion', 'mischievous', 'recipient', 'humiliating', 'longing', 'truth', 'oak', 'prejudiced', 'snag', 'unquestionable', 'cuckold', 'skid', 'pained', 'grit', 'inoculation', 'armor', 'trappings', 'attack', 'goodwill', 'brightness', 'traumatic', 'doll', 'procure', 'uninterested', 'piracy', 'riveting', 'passenger', 'merciful', 'gawk', 'seniority', 'invitation', 'mediation', 'hooded', 'glitter', 'smirk', 'bureaucrat', 'doctor', 'hypertrophy', 'repulsion', 'composer', 'unhappy', 'dumb', 'stellar', 'riddle', 'shriek', 'mutable', 'baby', 'soundness', 'leech', 'obnoxious', 'militia', 'inseparable', 'gradual', 'posthumous', 'austerity', 'reverence', 'languid', 'tussle', 'manly', 'neuralgia', 'aimless', 'partisan', 'nay', 'surly', 'adoration', 'lover', 'verbosity', 'buoy', 'occult', 'cooperate', 'dung', 'prevent', 'cyst', 'offing', 'afflicted', 'outward', 'committed', 'misguided', 'rescind', 'connoisseur', 'protect', 'cheesecake', 'invariably', 'pick', 'enroll', 'resplendent', 'sanctuary', 'martingale', 'carnal', 'rota', 'superficial', 'hawk', 'infallible', 'doubtless', 'argument', 'grandchildren', 'unequal', 'consort', 'production', 'synergistic', 'deletion', 'insurmountable', 'heartburn', 'restorative', 'visor', 'improvement', 'wallow', 'disgrace', 'ejection', 'entertained', 'foolish', 'jaundice', 'speedy', 'goodly', 'pinion', 'atrophy', 'protector', 'morbidity', 'powerful', 'decrepit', 'predominant', 'flattering', 'curse', 'manufacturer', 'lawful', 'womb', 'archaic', 'harbinger', 'dependent', 'epilepsy', 'slink', 'worship', 'fiesta', 'score', 'fraught', 'irrefutable', 'kicking', 'mortgagee', 'fib', 'wear', 'impound', 'borrower', 'clarify', 'restrict', 'cheat', 'muddy', 'hungry', 'lurch', 'attentive', 'hamstring', 'slaughterhouse', 'reserve', 'strikingly', 'darkened', 'efficient', 'depressed', 'sociable', 'subtract', 'unjustifiable', 'rickety', 'princely', 'lag', 'iris', 'affable', 'impervious', 'extend', 'pastime', 'personable', 'agriculture', 'beer', 'sincere', 'dawn', 'composed', 'massacre', 'conceit', 'delicious', 'terror', 'inconsequential', 'star', 'suppress', 'compress', 'vagueness', 'profusion', 'crude', 'unclean', 'rejoice', 'chattering', 'healing', 'constantly', 'canary', 'heroin', 'bang', 'speculation', 'horror', 'build', 'disgruntled', 'oligarchy', 'obscenity', 'derelict', 'peacock', 'shock', 'gorge', 'quit', 'irreconcilable', 'rejuvenated', 'warlike', 'income', 'statistical', 'grotesque', 'augment', 'authenticity', 'peculiarity', 'good', 'diamond', 'venerable', 'shiver', 'cultivate', 'procession', 'duet', 'exuberance', 'innocence', 'caste', 'overestimate', 'porn', 'surprisingly', 'transcript', 'valuable', 'beating', 'misconception', 'network', 'participation', 'attest', 'conscientious', 'spent', 'incase', 'godsend', 'impartiality', 'reinstate', 'cloudiness', 'negation', 'sequel', 'excellent', 'healthy', 'widower', 'justification', 'diagnosis', 'soothing', 'devour', 'plated', 'victim', 'lose', 'repression', 'ruthless', 'tiredness', 'improved', 'endeavor', 'mortar', 'pardon', 'lesbian', 'socialist', 'nap', 'animus', 'embargo', 'sky', 'indent', 'precedence', 'celebrity', 'completing', 'deform', 'ambassador', 'disclaim', 'truce', 'frantic', 'bereft', 'frightened', 'feigned', 'liar', 'splendid', 'scrapie', 'distracted', 'core', 'leprosy', 'eel', 'tumor', 'miserably', 'addiction', 'infinity', 'misery', 'mouthful', 'learning', 'august', 'fire', 'irrational', 'compost', 'quivering', 'cement', 'proficient', 'irreverent', 'basketball', 'adjunct', 'absorbed', 'priesthood', 'selfishness', 'infestation', 'civilization', 'lick', 'backwards', 'stone', 'distracting', 'ghost', 'sin', 'electricity', 'germ', 'censure', 'fanatic', 'disgraced', 'strip', 'ambush', 'peaceable', 'adversary', 'reclamation', 'assured', 'shameful', 'soulless', 'spoiler', 'scandal', 'straightforward', 'revolution', 'mother', 'occasional', 'sentence', 'perpetuate', 'emulate', 'destroyer', 'gossip', 'explosion', 'annoyance', 'begun', 'warden', 'revolver', 'debacle', 'feverish', 'fate', 'herpesvirus', 'illness', 'lava', 'gauche', 'supremacy', 'opposed', 'decoy', 'negligently', 'atom', 'thoroughbred', 'abysmal', 'hobby', 'masochism', 'tenderness', 'tripping', 'uneven', 'omnipotence', 'unsuspecting', 'tenement', 'fact', 'gag', 'loony', 'abuse', 'cater', 'adipose', 'decrease', 'foreboding', 'supplication', 'stud', 'immoral', 'worn', 'interrogate', 'malign', 'mandarin', 'encouragement', 'fragile', 'superstar', 'unanticipated', 'unsung', 'contemptible', 'flagging', 'typhoon', 'compliant', 'cystic', 'highest', 'precaution', 'slaughtering', 'bedrock', 'rejoicing', 'marine', 'adultery', 'tact', 'waffle', 'interested', 'readiness', 'antichrist', 'eagle', 'relief', 'demonstrative', 'fornication', 'attainable', 'purist', 'wicket', 'logical', 'predispose', 'bad', 'surveillance', 'tutor', 'liberation', 'lifeblood', 'unsupported', 'dutiful', 'glimmer', 'reversal', 'wench', 'judiciary', 'auspicious', 'disobey', 'helpful', 'superiority', 'dynamic', 'vanguard', 'holiness', 'selfish', 'regret', 'intend', 'subjugation', 'objective', 'medley', 'simmer', 'eligible', 'stumble', 'disapprove', 'delusion', 'wonderful', 'wrangling', 'teens', 'punitive', 'penance', 'reimburse', 'illustrate', 'episcopal', 'blurred', 'somber', 'blur', 'tomb', 'construct', 'forbearance', 'verily', 'subdue', 'cafe', 'materialism', 'monument', 'synchronize', 'fudge', 'hesitation', 'witchcraft', 'oppressor', 'polemic', 'curl', 'infallibility', 'outrageous', 'correspondence', 'plenary', 'extinguish', 'whim', 'convict', 'pay', 'rarity', 'groan', 'rife', 'county', 'wrong', 'deceive', 'preventive', 'brothel', 'replenish', 'detection', 'plump', 'ludicrous', 'unsophisticated', 'complain', 'choir', 'crime', 'resultant', 'gravitate', 'deacon', 'exhort', 'unprotected', 'frustrate', 'true', 'audacity', 'persecute', 'congruence', 'conspirator', 'credit', 'coldness', 'erotic', 'patent', 'candied', 'advent', 'noisy', 'talisman', 'optimist', 'grimy', 'diminished', 'equally', 'measure', 'leer', 'cruelly', 'fight', 'craft', 'treacherous', 'purr', 'consonant', 'sanitary', 'dare', 'moody', 'villain', 'microscope', 'prolong', 'equity', 'clumsy', 'anal', 'absurdity', 'damper', 'flange', 'luxurious', 'openness', 'suitable', 'chicane', 'reappear', 'hearsay', 'cyclone', 'destroyed', 'discreet', 'fever', 'dissension', 'hurrah', 'tetanus', 'dandruff', 'mortuary', 'distrust', 'timid', 'adulterated', 'stripped', 'ultimately', 'defy', 'storm', 'morsel', 'bomb', 'deterioration', 'swine', 'bartender', 'emeritus', 'football', 'restrain', 'jury', 'pound', 'serum', 'chirp', 'excuse', 'abscess', 'radar', 'mace', 'vivacious', 'bookworm', 'discouragement', 'hasty', 'sympathy', 'passe', 'folly', 'wrench', 'aura', 'bitch', 'allay', 'pain', 'akin', 'incur', 'deny', 'wince', 'coherence', 'defection', 'specter', 'unsteady', 'nose', 'sun', 'memento', 'fungus', 'virtue', 'dashed', 'agony', 'scholar', 'desirable', 'citizen', 'shank', 'collateral', 'terrorist', 'guilt', 'coffin', 'impenetrable', 'crucial', 'denied', 'bait', 'repellent', 'doubt', 'kindness', 'pacific', 'isolate', 'investigation', 'egregious', 'measured', 'general', 'negro', 'surprising', 'cull', 'horse', 'inane', 'guzzling', 'groundwork', 'gift', 'specialize', 'remission', 'passive', 'annoy', 'adventure', 'experienced', 'blackjack', 'journalist', 'proven', 'vegetative', 'incongruous', 'scoundrel', 'anticipation', 'buffet', 'poorly', 'respects', 'extraordinary', 'adept', 'slum', 'instruct', 'yawn', 'modest', 'cheering', 'lure', 'catechism', 'library', 'stamina', 'explosive', 'picturesque', 'inspiration', 'irritating', 'thoughtless', 'parachute', 'unequivocally', 'disuse', 'hypothesis', 'inferno', 'astronomer', 'punctuality', 'crying', 'goods', 'portable', 'coincidence', 'insane', 'restrictive', 'loneliness', 'fancy', 'eager', 'assassination', 'stigma', 'guard', 'encore', 'docked', 'ratify', 'rebellion', 'messy', 'nonsensical', 'aggravated', 'apprehend', 'shout', 'execution', 'rightly', 'affirmatively', 'methanol', 'spine', 'meaningless', 'monster', 'pressure', 'promotion', 'annoying', 'paucity', 'withdraw', 'boy', 'improvise', 'maximum', 'frowning', 'apostolic', 'misstatement', 'publicist', 'beautify', 'infamy', 'praise', 'stray', 'splash', 'savor', 'clashing', 'debtor', 'interdiction', 'divestment', 'smoker', 'agitation', 'squatter', 'battalion', 'rebel', 'inappropriate', 'contagious', 'porno', 'confidential', 'restitution', 'gorgeous', 'unbridled', 'latrines', 'harm', 'intestinal', 'fugitive', 'narcotic', 'unequivocal', 'unworthy', 'glut', 'rogue', 'babbling', 'sultry', 'shroud', 'labyrinth', 'absentee', 'console', 'menacing', 'sneak', 'birthplace', 'lax', 'poverty', 'stifle', 'shoot', 'traveling', 'beach', 'escalate', 'favoritism', 'insufficiently', 'jeopardize', 'suicidal', 'famous', 'dread', 'deleterious', 'physicist', 'march', 'boil', 'consequent', 'absent', 'effigy', 'mettle', 'demonstrable', 'approval', 'lumbering', 'copycat', 'poison', 'sharpen', 'apt', 'practise', 'invigorate', 'excited', 'educational', 'disinterested', 'bulbous', 'disreputable', 'responsible', 'pauper', 'forearm', 'mediator', 'grime', 'lonely', 'repelling', 'raffle', 'crook', 'shrewd', 'fixture', 'assault', 'discretionary', 'verification', 'remains', 'undivided', 'prepared', 'appropriation', 'fame', 'abba', 'devilish', 'biopsy', 'gibberish', 'smack', 'resistant', 'repent', 'deceptive', 'advantageous', 'trendy', 'violence', 'kindred', 'decompose', 'corporation', 'noxious', 'opportune', 'forefathers', 'treason', 'carefully', 'spruce', 'unacknowledged', 'dismay', 'lounge', 'bank', 'mistake', 'gallant', 'intimate', 'indoctrination', 'matron', 'heroics', 'ware', 'remodel', 'darkness', 'pimple', 'rabies', 'established', 'extrajudicial', 'touched', 'manna', 'spur', 'alimentation', 'dazed', 'female', 'verdant', 'promise', 'happiness', 'arbitrator', 'limited', 'starving', 'damn', 'flagrant', 'frenzy', 'romp', 'horribly', 'garrison', 'predatory', 'abrupt', 'bondage', 'discriminating', 'dedication', 'finesse', 'dizzy', 'heel', 'oppose', 'fleece', 'ahead', 'delectable', 'unquestioned', 'raid', 'insignia', 'crocodile', 'axiomatic', 'veal', 'perspiration', 'fellow', 'pornography', 'alive', 'immense', 'restrained', 'extra', 'candid', 'synonymous', 'vote', 'timidity', 'condemnation', 'gear', 'loath', 'abyss', 'registry', 'recognizable', 'inflammation', 'tired', 'countess', 'upright', 'organ', 'muss', 'insure', 'plan', 'cap', 'prey', 'tremendously', 'maroon', 'honey', 'invade', 'laxative', 'grumpy', 'bristle', 'sag', 'distress', 'skewed', 'apache', 'democracy', 'rebut', 'contravene', 'attestation', 'ballad', 'travesty', 'guillotine', 'infertility', 'charming', 'grudge', 'undermined', 'socialism', 'foiled', 'disapproving', 'steady', 'impure', 'belated', 'cherish', 'faithful', 'cabal', 'scare', 'petty', 'probity', 'discontent', 'accusative', 'overgrown', 'sally', 'spat', 'subjection', 'everlasting', 'accurate', 'dietary', 'bacterium', 'idol', 'knight', 'preparatory', 'balm', 'claw', 'amends', 'gentleman', 'muff', 'lawlessness', 'bath', 'distressed', 'fatal', 'periodicity', 'surcharge', 'complexed', 'unexpected', 'madness', 'fatigue', 'crash', 'smite', 'eradicate', 'gentleness', 'doubtful', 'determination', 'illiterate', 'slack', 'nursery', 'napkin', 'rock', 'rescue', 'unapproved', 'immunization', 'scholarship', 'torment', 'joker', 'illogical', 'malaria', 'venomous', 'proper', 'cascade', 'humility', 'discussion', 'urn', 'swim', 'analyst', 'capitalist', 'expose', 'merriment', 'unfriendly', 'valor', 'condescending', 'flexibility', 'crypt', 'damages', 'buck', 'bully', 'commodore', 'pestilence', 'mutter', 'deprivation', 'hindering', 'glib', 'prevention', 'republic', 'unsatisfied', 'domination', 'concerned', 'assuage', 'forced', 'promiscuous', 'stinging', 'disingenuous', 'benevolence', 'ogre', 'holiday', 'dove', 'notary', 'eyewitness', 'stealing', 'working', 'rightful', 'dastardly', 'decomposed', 'expert', 'rhythmical', 'incorrect', 'hardened', 'smudge', 'ovation', 'alimony', 'burglary', 'crawl', 'resentment', 'detrimental', 'dissenting', 'chase', 'acquiring', 'motive', 'corrupting', 'sunk', 'flatulence', 'statue', 'unlucky', 'farce', 'cramped', 'hush', 'horrors', 'grated', 'scum', 'offering', 'ineffectual', 'huff', 'keynote', 'permission', 'therapeutic', 'spinster', 'muddled', 'instrumental', 'formula', 'lunacy', 'producer', 'philanthropist', 'devil', 'mercy', 'impeccable', 'crazed', 'versus', 'plexus', 'romance', 'worthy', 'offender', 'trip', 'reimbursement', 'refreshing', 'collapse', 'despise', 'crescendo', 'bender', 'hurricane', 'adverse', 'criticize', 'magnificence', 'litigation', 'crystal', 'lyre', 'ransom', 'ripen', 'ghetto', 'despair', 'malady', 'exclusion', 'excitement', 'trick', 'poaching', 'drawback', 'approving', 'valiant', 'poke', 'fluctuation', 'quicksilver', 'corruption', 'disagreement', 'advise', 'worth', 'ravenous', 'glare', 'succulent', 'chancellor', 'anguish', 'melancholy', 'riddled', 'favorable', 'barbarism', 'amenable', 'intimidation', 'champion', 'bombardment', 'urgent', 'medical', 'meltdown', 'bluff', 'aftertaste', 'testament', 'steadfast', 'sheriff', 'mourning', 'sensibly', 'ultimate', 'pest', 'retain', 'whirlpool', 'singularly', 'displaced', 'unlawful', 'slop', 'violently', 'mule', 'sty', 'disapproval', 'vehement', 'abundant', 'injunction', 'tenacious', 'anxious', 'refugee', 'ram', 'prospect', 'ugly', 'detriment', 'judgment', 'proctor', 'precise', 'astute', 'secret', 'boon', 'laureate', 'weakness', 'irate', 'grievous', 'gore', 'noble', 'heyday', 'xenophobia', 'anticipatory', 'subsidence', 'enjoy', 'sludge', 'elevation', 'calamity', 'trig', 'singly', 'account', 'primer', 'word', 'heresy', 'occupy', 'deceitful', 'drought', 'wildfire', 'gall', 'transitional', 'swampy', 'organization', 'rugged', 'dank', 'grounded', 'accomplish', 'congress', 'antibiotics', 'predilection', 'attempt', 'majestic', 'microscopy', 'vanished', 'swab', 'clever', 'forge', 'cocaine', 'minimize', 'foundation', 'unseat', 'lender', 'scream', 'windfall', 'firearms', 'unwelcome', 'inapplicable', 'completeness', 'passion', 'thought', 'smuggling', 'ply', 'covet', 'pedigree', 'khan', 'cleave', 'indivisible', 'dictionary', 'overbearing', 'protective', 'irreparable', 'sage', 'lavatory', 'untold', 'uninvited', 'stalwart', 'lottery', 'treasure', 'bridesmaid', 'hedonism', 'anthrax', 'hut', 'whirlwind', 'desperate', 'fore', 'asp', 'ejaculation', 'serenity', 'sleet', 'primary', 'hunting', 'contaminate', 'estranged', 'eminent', 'precursor', 'molestation', 'cleansing', 'elf', 'lecturer', 'unselfish', 'syringe', 'hoary', 'hospice', 'cathartic', 'cerebral', 'erupt', 'laugh', 'bloated', 'bum', 'guerilla', 'assailant', 'discomfort', 'wicked', 'oust', 'disrespect', 'bolster', 'disappointing', 'betray', 'implicate', 'master', 'glittering', 'invocation', 'armored', 'compass', 'cess', 'joyful', 'unacceptable', 'playhouse', 'fatality', 'bear', 'counsel', 'deactivate', 'peril', 'heavily', 'annulment', 'unwavering', 'calf', 'bodyguard', 'shelter', 'bold', 'dike', 'congressman', 'whack', 'culture', 'wry', 'foreigner', 'learn', 'indecisive', 'mumble', 'expectation', 'ongoing', 'regression', 'concussion', 'prosperity', 'contempt', 'slush', 'cheery', 'revel', 'depreciate', 'conspiracy', 'vulgarity', 'empathy', 'join', 'inadequacy', 'applause', 'deserted', 'die', 'longevity', 'immediacy', 'gout', 'hatred', 'weeping', 'nihilism', 'insufficient', 'tribe', 'blue', 'chart', 'metastasis', 'coexist', 'virgin', 'confiscation', 'balanced', 'desiring', 'murderer', 'slug', 'arson', 'shudder', 'contentious', 'buzzed', 'intrusive', 'eloquence', 'falsely', 'noose', 'supporter', 'producing', 'organized', 'beaming', 'dabbling', 'swollen', 'gage', 'labor', 'chimera', 'stretcher', 'backwater', 'aesthetics', 'satin', 'vow', 'personal', 'lofty', 'deranged', 'uncanny', 'berth', 'anchor', 'shove', 'tickle', 'authorized', 'quarantine', 'alien', 'entrails', 'dysentery', 'symmetry', 'conspire', 'heroine', 'confide', 'privileged', 'incurable', 'lost', 'cursed', 'abort', 'tether', 'slavery', 'honesty', 'sketchy', 'easement', 'vindicated', 'lone', 'quest', 'villager', 'blindness', 'contamination', 'demand', 'abortive', 'closeness', 'bloom', 'miserable', 'cussed', 'lull', 'stillness', 'disgusting', 'pang', 'adviser', 'unintended', 'disorderly', 'ensign', 'banished', 'unrest', 'fabrication', 'decomposition', 'sappy', 'including', 'mamma', 'wasteful', 'revulsion', 'inferior', 'approaching', 'aggressor', 'frivolous', 'liberal', 'fortunate', 'collaborator', 'grandmother', 'reformer', 'wearily', 'lurking', 'hearth', 'presence', 'structure', 'sinful', 'halter', 'hydra', 'distressing', 'gent', 'storming', 'startling', 'usury', 'job', 'jungle', 'indomitable', 'naturalist', 'hardness', 'grateful', 'perish', 'kidnap', 'invasion', 'insolvency', 'combatant', 'topple', 'rust', 'haste', 'intellect', 'sleek', 'indecency', 'senile', 'punished', 'deduct', 'immediately', 'orgasm', 'weep', 'cool', 'yearning', 'opera', 'volcano', 'enforcement', 'triumphant', 'ornamented', 'fume', 'truthfulness', 'suffocation', 'theism', 'widow', 'plaintiff', 'survive', 'contour', 'fortitude', 'renovate', 'seduce', 'stagger', 'extermination', 'cop', 'baboon', 'biblical', 'competent', 'localize', 'rebate', 'contingent', 'chaplain', 'pneumonia', 'charger', 'earnestly', 'guardianship', 'idealism', 'friend', 'bandit', 'armory', 'arguments', 'impress', 'cleanly', 'feud', 'minority', 'renunciation', 'symbolic', 'rotting', 'ammonia', 'investigate', 'thwart', 'mumps', 'cartel', 'unsettled', 'irrelevant', 'unsurpassed', 'rifle', 'terrible', 'illuminate', 'odious', 'thoughtfulness', 'arrears', 'decayed', 'imprison', 'disposal', 'admirer', 'forfeiture', 'forfeit', 'angel', 'archaeology', 'stave', 'winner', 'reject', 'lace', 'infinite', 'ostensibly', 'daemon', 'mover', 'infanticide', 'chasm', 'desertion', 'correction', 'unsightly', 'broke', 'neighborhood', 'richness', 'presentment', 'geriatric', 'prick', 'sedition', 'compensatory', 'restlessness', 'rave', 'pitfall', 'clouded', 'unsavory', 'legalized', 'disposed', 'inviting', 'heartily', 'volunteers', 'untrue', 'buddy', 'opinionated', 'accused', 'begging', 'vacancy', 'unexplained', 'damnation', 'cuckoo', 'formative', 'retaliate', 'pharmaceutical', 'hissing', 'girder', 'plumb', 'uncontrolled', 'inconsistency', 'befitting', 'constipation', 'generosity', 'enchanting', 'threaten', 'incident', 'mistrust', 'contaminated', 'inventive', 'tyrant', 'navigable', 'battle', 'wont', 'elusive', 'testimony', 'stoned', 'utopian', 'robust', 'excitable', 'hurting', 'navigator', 'punishing', 'phalanx', 'penetration', 'vampire', 'pawn', 'aristocratic', 'zeal', 'setback', 'suddenly', 'abhorrent', 'diary', 'suspect', 'gelatin', 'rebuke', 'approbation', 'discontinue', 'unnatural', 'incessant', 'renown', 'timely', 'predicament', 'roughness', 'rubbish', 'inaction', 'enemy', 'peck', 'thrill', 'blasphemy', 'crank', 'gluttony', 'backer', 'apathetic', 'institute', 'blunder', 'racket', 'detonation', 'modesty', 'hallucination', 'graceful', 'unpretentious', 'depth', 'frank', 'ranger', 'shit', 'superb', 'insolvent', 'determined', 'comrade', 'paralyzed', 'squirm', 'helper', 'inability', 'substantive', 'commemoration', 'believing', 'honest', 'censor', 'lawsuit', 'vice', 'persuade', 'impassable', 'cunning', 'safe', 'fracture', 'grow', 'insulation', 'lunatic', 'nettle', 'antique', 'sabotage', 'unassuming', 'fireproof', 'palpable', 'pool', 'janitor', 'bookish', 'caries', 'prominence', 'gasp', 'forgery', 'undying', 'haze', 'rivalry', 'biennial', 'shepherd', 'entertaining', 'saintly', 'austere', 'resilient', 'handicap', 'musket', 'divinity', 'claimant', 'daily', 'immorality', 'pretensions', 'foresee', 'hankering', 'votive', 'vicious', 'headlight', 'impotence', 'jumble', 'authoritative', 'carcinoma', 'philanthropic', 'disdain', 'despicable', 'perishing', 'esprit', 'puma', 'iniquity', 'adorable', 'velvety', 'pervert', 'visit', 'flaw', 'secrecy', 'disclosed', 'mislead', 'neglecting', 'grandfather', 'idolatry', 'respectful', 'obituary', 'assure', 'spice', 'aspiring', 'boredom', 'interim', 'alluring', 'manual', 'fray', 'rejuvenate', 'unwillingness', 'productivity', 'gush', 'gate', 'formality', 'praised', 'inept', 'subversion', 'inform', 'pollute', 'startle', 'flog', 'tomorrow', 'vermin', 'ameliorate', 'hostage', 'dame', 'commandant', 'quaint', 'helpless', 'mess', 'funeral', 'panier', 'haughty', 'major', 'prophetic', 'dominate', 'relaxation', 'impatient', 'dying', 'rein', 'wisdom', 'toughness', 'dinosaur', 'god', 'badger', 'downy', 'patch', 'lodging', 'information', 'delirious', 'dealings', 'epidemic', 'misleading', 'official', 'remand', 'tarnish', 'genteel', 'impressionable', 'bountiful', 'elegant', 'proficiency', 'annihilate', 'bridegroom', 'fuse', 'virginity', 'creative', 'bottomless', 'fled', 'tardy', 'stress', 'important', 'leverage', 'vulnerability', 'glacial', 'frustration', 'flow', 'cleverness', 'examination', 'rowdy', 'collusion', 'futility', 'maniacal', 'hoarse', 'fits', 'worried', 'traditional', 'grumble', 'grievance', 'theology', 'mysterious', 'wary', 'laughter', 'stealthily', 'laudable', 'preparation', 'ugliness', 'simmering', 'appreciation', 'disconnect', 'exceed', 'brighten', 'ruffle', 'constancy', 'oddity', 'system', 'civility', 'unborn', 'sick', 'musty', 'mediterranean', 'alabaster', 'stealthy', 'fallible', 'savvy', 'small', 'pleasant', 'wrinkled', 'heavens', 'foray', 'craps', 'educated', 'hag', 'prejudicial', 'twin', 'ill', 'stagnant', 'cable', 'crisis', 'completion', 'spectacular', 'moan', 'agree', 'unscathed', 'shooting', 'wizard', 'blame', 'ghostly', 'disastrous', 'materialist', 'ay', 'sponsor', 'intelligent', 'removal', 'tantalizing', 'muck', 'contraband', 'sickening', 'concentric', 'exasperation', 'bulletproof', 'missionary', 'outrage', 'frostbite', 'terminate', 'lair', 'spank', 'toad', 'invalidation', 'scab', 'recherche', 'opaque', 'disqualification', 'undecided', 'nonsense', 'plaintive', 'quell', 'pertinent', 'celebration', 'management', 'wrought', 'reckless', 'drivel', 'meek', 'wring', 'tribulation', 'plague', 'feeble', 'recreational', 'revive', 'defective', 'morals', 'dignified', 'lowly', 'yellows', 'infarct', 'classical', 'privy', 'garden', 'controversial', 'contemptuous', 'rot', 'tenancy', 'midwife', 'inhospitable', 'stomach', 'calm', 'foregoing', 'viper', 'benefit', 'admissible', 'sizzle', 'hate', 'pig', 'godly', 'dependency', 'swamp', 'inalienable', 'errand', 'worsening', 'prickly', 'convenient', 'inexpensive', 'calculation', 'land', 'gambler', 'butcher', 'notables', 'shed', 'specialist', 'levee', 'trespass', 'junta', 'laurels', 'sobriety', 'melee', 'inaugural', 'sweet', 'defamatory', 'undersized', 'spectacle', 'legible', 'tease', 'clairvoyant', 'cashier', 'weakly', 'chastity', 'overdo', 'weird', 'risk', 'missile', 'change', 'improvisation', 'forbid', 'explain', 'progressive', 'dupe', 'theft', 'momentum', 'froth', 'sonnet', 'unattached', 'child', 'misplaced', 'tract', 'quiescent', 'expropriation', 'sear', 'reproductive', 'sacrifices', 'humiliate', 'avoidance', 'decent', 'hurt', 'veteran', 'perennial', 'delinquency', 'joking', 'courageous', 'massage', 'beware', 'fearfully', 'squelch', 'swarm', 'emphasize', 'faithless', 'accidental', 'infraction', 'dirt', 'dominant', 'sceptical', 'strength', 'sortie', 'spear', 'unsatisfactory', 'legislature', 'unauthorized', 'justice', 'aggravating', 'assist', 'inaccurate', 'discrimination', 'greatness', 'haggard', 'condemn', 'pavement', 'inequitable', 'earnestness', 'attractiveness', 'festival', 'consummate', 'jargon', 'mismanagement', 'rooted', 'loathsome', 'inferiority', 'stupid', 'lines', 'allegiance', 'evergreen', 'unimproved', 'share', 'ambition', 'eat', 'rekindle', 'snake', 'deficit', 'mausoleum', 'recoup', 'merchant', 'bailiff', 'commute', 'count', 'herbal', 'choral', 'amiable', 'jeopardy', 'steward', 'rating', 'harmoniously', 'airs', 'population', 'satisfactorily', 'gloss', 'flap', 'tortious', 'savory', 'unsuitable', 'mending', 'bawdy', 'association', 'ulterior', 'disappear', 'inflated', 'intuitively', 'attention', 'dismal', 'varicella', 'battered', 'moral', 'perpetuity', 'horrible', 'hiss', 'deliverance', 'badness', 'wireless', 'disturbance', 'expectant', 'vulgar', 'inimical', 'flaccid', 'cringe', 'fashionable', 'seduction', 'focus', 'obliteration', 'giving', 'sentimentality', 'shipwreck', 'vivid', 'swerve', 'attenuated', 'distraction', 'closure', 'whisky', 'coy', 'teacher', 'array', 'nascent', 'tedious', 'glory', 'minimum', 'devotional', 'inconvenient', 'confront', 'veracity', 'exaggerated', 'broil', 'shameless', 'explode', 'abduction', 'convention', 'superstitious', 'dwarfed', 'barrier', 'endowed', 'insurrection', 'amuse', 'hopeless', 'theological', 'skip', 'heretic', 'unproductive', 'craving', 'blanket', 'unable', 'overwhelmed', 'descriptive', 'purification', 'sonata', 'scintilla', 'suspension', 'recidivism', 'argumentative', 'dictator', 'disaffected', 'rising', 'row', 'penal', 'blemish', 'slander', 'giant', 'custodian', 'snicker', 'tardiness', 'scheme', 'sequestration', 'illustrious', 'sweets', 'aversion', 'impeach', 'conformity', 'cannibalism', 'cult', 'halfway', 'subscribe', 'comprehend', 'dour', 'hulk', 'disparage', 'frightful', 'vigilance', 'assurance', 'offensive', 'rigid', 'uprising', 'nightmare', 'redress', 'dissonance', 'divorce', 'unpleasant', 'cloudy', 'hemorrhoids', 'create', 'goodness', 'joyous', 'bovine', 'stranded', 'enthusiasm', 'paprika', 'proof', 'pretend', 'anonymous', 'toxic', 'eventual', 'curiosity', 'foresight', 'insult', 'depraved', 'sparkle', 'venom', 'circumcision', 'divergent', 'idiotic', 'theory', 'fallacious', 'degrading', 'hunter', 'full', 'banker', 'instruction', 'gutter', 'quail', 'start', 'radiance', 'shopkeeper', 'imaginative', 'mutilated', 'negotiate', 'doomsday', 'refinement', 'shine', 'cutters', 'fanaticism', 'unpopular']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sentence_list))\n",
        "sentence_list.extend(list_lexicon_all)\n",
        "print(len(sentence_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR3WCatnaQkl",
        "outputId": "b179d9d0-dfd3-4a37-f8f9-9cf9bf99f031"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3739\n",
            "10207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_list[3729:]"
      ],
      "metadata": {
        "id": "KmRh-CTfpjfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_threshold = 1\n",
        "vocab_max_size = 50000\n",
        "\n",
        "vocab = Vocabulary(freq_threshold, vocab_max_size)\n",
        "vocab.build_vocabulary(sentence_list)"
      ],
      "metadata": {
        "id": "WW44PGJKRbpS"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab.stoi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XpPIx3pdwbC",
        "outputId": "30a6cd7e-8d2c-46f4-8b50-7bbb4ebe9f52"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5068"
            ]
          },
          "metadata": {},
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Source Train Dataset\n",
        "We first inherit PyTorch's Dataset class.\n",
        "Then, we initialize and build the vocabs for subject in our train data frame.\n",
        "Then, we use the getitem() method to numericalize the subject 1 example at a time for the data loader (a function to load data in batches)."
      ],
      "metadata": {
        "id": "uwvxG4Op8YUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#               Define Train_Dataset class\n",
        "#######################################################\n",
        "\n",
        "class Train_Dataset(Dataset):\n",
        "    '''\n",
        "    Initiating Variables\n",
        "    df: the training dataframe\n",
        "    subject : the name of target text column in the dataframe\n",
        "    transform : If we want to add any augmentation\n",
        "    freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
        "    vocab_max_size : max  vocab size\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, df, subject, label_col, vocab , domain = domain_source, max_sentence_length = 150, transform=None, freq_threshold = 5,\n",
        "                vocab_max_size = 50000):\n",
        "    \n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        \n",
        "        #get body and label\n",
        "        self.subject_texts = self.df[subject]\n",
        "        self.labels = self.df[label_col].astype(float)\n",
        "        self.domain = domain\n",
        "        self.vocab = vocab\n",
        "        \n",
        "        # ##VOCAB class has been created above\n",
        "        # #Initialize vocab object and build vocabulary\n",
        "        # self.vocab = Vocabulary(freq_threshold, vocab_max_size)\n",
        "        # self.vocab.build_vocabulary(self.subject_texts.tolist())\n",
        "        self.max_sentence_length = max_sentence_length\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    '''\n",
        "    __getitem__ runs on 1 example at a time. Here, we get an example at index and return its numericalize source and\n",
        "    target values using the vocabulary objects we created in __init__\n",
        "    '''\n",
        "    def __getitem__(self, index):\n",
        "        subject_text = self.subject_texts[index]\n",
        "        label = self.labels[index]\n",
        "        domain_value = self.domain\n",
        "#         print(subject_text)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            subject_text = self.transform(subject_text)\n",
        "            \n",
        "        #numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
        "        numerialized_subject =[]\n",
        "        numerialized_subject += self.vocab.numericalize(subject_text)\n",
        "        \n",
        "        while len(numerialized_subject) < self.max_sentence_length:\n",
        "            numerialized_subject.append(0)\n",
        "        \n",
        "        #convert the list to tensor and return\n",
        "        return torch.tensor(numerialized_subject[:self.max_sentence_length]), torch.tensor(label), torch.tensor(domain_value)\n",
        "#         return torch.tensor(numerialized_subject[:self.train_dataset.max_sentence_length]),label"
      ],
      "metadata": {
        "id": "Ue3fipRlERZI"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_df_subset.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts9ZW7fxiDH0",
        "outputId": "cc0c12a2-cb33-4a83-c276-53f74af39b76"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['df_EI_reg_train_subset', 'df_EI_reg_val_subset', 'df_EI_reg_test_subset', 'df_V_reg_train_subset', 'df_V_reg_val_subset', 'df_V_reg_test_subset'])"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = dict_df_subset['df_EI_reg_train_subset']\n",
        "df_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "-XKkT1VABUHX",
        "outputId": "16d37ec9-2154-4985-ab3d-960c47a6b3fd"
      },
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            TweetTokens Intensity Score  \\\n",
              "0     <user> <user> shut up hashtags are cool <hasht...           0.562   \n",
              "1     it makes me so fucking irate jesus . nobody is...           0.750   \n",
              "2     lol adam the bull with his fake outrage . <rep...           0.417   \n",
              "3     <user> passed away early this morning in a fas...           0.354   \n",
              "4     <user> lol wow i was gonna say really ? ! <rep...           0.438   \n",
              "...                                                 ...             ...   \n",
              "1696  got a <money> tip from a drunk uber passenger ...           0.708   \n",
              "1697  <user> <user> <user> <user> fucker blocked me ...           0.625   \n",
              "1698                                <user> i look rabid           0.472   \n",
              "1699  <user> i am not surprised , i would be fuming ...           0.479   \n",
              "1700  <user> the pout tips me over the edge . i am m...           0.490   \n",
              "\n",
              "      domain  \n",
              "0        0.0  \n",
              "1        0.0  \n",
              "2        0.0  \n",
              "3        0.0  \n",
              "4        0.0  \n",
              "...      ...  \n",
              "1696     0.0  \n",
              "1697     0.0  \n",
              "1698     0.0  \n",
              "1699     0.0  \n",
              "1700     0.0  \n",
              "\n",
              "[1701 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e8fd3adc-7dc0-4432-b179-c0b4797f2a3f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TweetTokens</th>\n",
              "      <th>Intensity Score</th>\n",
              "      <th>domain</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;user&gt; &lt;user&gt; shut up hashtags are cool &lt;hasht...</td>\n",
              "      <td>0.562</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it makes me so fucking irate jesus . nobody is...</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lol adam the bull with his fake outrage . &lt;rep...</td>\n",
              "      <td>0.417</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;user&gt; passed away early this morning in a fas...</td>\n",
              "      <td>0.354</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;user&gt; lol wow i was gonna say really ? ! &lt;rep...</td>\n",
              "      <td>0.438</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1696</th>\n",
              "      <td>got a &lt;money&gt; tip from a drunk uber passenger ...</td>\n",
              "      <td>0.708</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1697</th>\n",
              "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; &lt;user&gt; fucker blocked me ...</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1698</th>\n",
              "      <td>&lt;user&gt; i look rabid</td>\n",
              "      <td>0.472</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1699</th>\n",
              "      <td>&lt;user&gt; i am not surprised , i would be fuming ...</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1700</th>\n",
              "      <td>&lt;user&gt; the pout tips me over the edge . i am m...</td>\n",
              "      <td>0.490</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1701 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8fd3adc-7dc0-4432-b179-c0b4797f2a3f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e8fd3adc-7dc0-4432-b179-c0b4797f2a3f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e8fd3adc-7dc0-4432-b179-c0b4797f2a3f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_train_dataset ={}\n",
        "for name,df in dict_df_subset.items():\n",
        "  if \"train\" in name:\n",
        "    dataset_name = name+\"_dataset\"\n",
        "    # vars()[dataset_name] = Train_Dataset(df,'TweetTokens','Intensity Score', max_sentence_length =200) # dynamically assigning datasetname\n",
        "    dict_train_dataset[dataset_name] = Train_Dataset(df,'TweetTokens','Intensity Score', vocab, domain = domain_source, max_sentence_length =200) # dynamically assigning datasetname"
      ],
      "metadata": {
        "id": "MWMm8D1t9dcs"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict_train_dataset.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeQ0pW6shCpT",
        "outputId": "cf2699d7-c8bb-42e9-c005-e662718d2b19"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['df_EI_reg_train_subset_dataset', 'df_V_reg_train_subset_dataset'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = random.randint(0,len(dict_train_dataset['df_EI_reg_train_subset_dataset']))\n",
        "print(dict_df_subset['df_EI_reg_train_subset'].loc[i][['TweetTokens','Intensity Score','domain']])\n",
        "print((dict_train_dataset['df_EI_reg_train_subset_dataset'][i][0]))\n",
        "print((dict_train_dataset['df_EI_reg_train_subset_dataset'][i][1]))\n",
        "print((dict_train_dataset['df_EI_reg_train_subset_dataset'][i][2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHSbz2sgGrS8",
        "outputId": "9e24847e-3c83-4040-8343-b4c4bbd5a5a7"
      },
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TweetTokens        why is it always me picking up the pieces <ann...\n",
            "Intensity Score                                                0.812\n",
            "domain                                                           0.0\n",
            "Name: 800, dtype: object\n",
            "tensor([  83,   14,   17,  124,   24, 3587,   48,    7, 1955, 1034,    2,   86,\n",
            "           3,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0])\n",
            "tensor(0.8120, dtype=torch.float64)\n",
            "tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset_obj = Train_Dataset(df_train,'TweetTokens','Intensity Score', vocab,max_sentence_length =200)"
      ],
      "metadata": {
        "id": "NBq8Pz1JA7wO"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i = random.randint(0,len(train_dataset_obj))\n",
        "# print(df_train.loc[i][['TweetTokens','Intensity Score']])\n",
        "# print((train_dataset_obj[i][1]))\n",
        "# print(len(train_dataset_obj[i][0]))\n",
        "# print(train_dataset_obj[i][0])"
      ],
      "metadata": {
        "id": "cxIIq74eCBTI"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df_EI_reg_train_subset_dataset)"
      ],
      "metadata": {
        "id": "arW3Nf90AwCV"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i = random.randint(0,len(df_EI_reg_train_subset_dataset))\n",
        "# # print(train.loc[i][['body','label']])\n",
        "# print(type(df_EI_reg_train_subset_dataset[i][1]))\n",
        "# len(df_EI_reg_train_subset_dataset[i][0])"
      ],
      "metadata": {
        "id": "5csRi0f3AsAA"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kRfZcZJPQ0ej"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Source Validation and Test Dataset"
      ],
      "metadata": {
        "id": "t0ji62MwaU8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#               Define Validation / Test Dataset Class\n",
        "#######################################################\n",
        "\n",
        "class Validation_Dataset(Dataset):\n",
        "    def __init__(self, train_dataset, df, subject, label_col, domain = domain_source, transform = None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        \n",
        "        #train dataset will be used as lookup for vocab\n",
        "        self.train_dataset = train_dataset\n",
        "        \n",
        "        #get body and label\n",
        "        self.subject_texts = self.df[subject]\n",
        "        self.labels = self.df[label_col].astype(float)\n",
        "        self.domain = domain\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        \n",
        "        subject_text = self.subject_texts[index]\n",
        "        label = self.labels[index]\n",
        "        domain_value = self.domain\n",
        "#         print(subject_text)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            subject_text = self.transform(subject_text)\n",
        "            \n",
        "\n",
        "        #numericalize texts ['cat', 'in', 'a', 'bag'] -> [12,2,9,24]\n",
        "        numerialized_subject = []\n",
        "        numerialized_subject += self.train_dataset.vocab.numericalize(subject_text)\n",
        "#         print(\"max sentence length\", self.train_dataset.max_sentence_length)\n",
        "        while len(numerialized_subject) < self.train_dataset.max_sentence_length:\n",
        "            numerialized_subject.append(0)\n",
        "            \n",
        "        #convert the list to tensor and return\n",
        "#         return torch.tensor(numerialized_subject),label\n",
        "\n",
        "#         #convert the list to tensor and return\n",
        "        return torch.tensor(numerialized_subject[:self.train_dataset.max_sentence_length]),torch.tensor(label), torch.tensor(domain_value)\n"
      ],
      "metadata": {
        "id": "Bzk0WNQmaiO7"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_val_dataset = {}\n",
        "for name,df in dict_df_subset.items():\n",
        "  if \"train\" in name:\n",
        "    train_dataset_name = name+\"_dataset\"\n",
        "    val_df_name = name[:len(name)-13]+\"_val_subset\"\n",
        "    val_dataset_name = val_df_name + \"_dataset\"\n",
        "    dict_val_dataset[val_dataset_name] = Validation_Dataset(dict_train_dataset[train_dataset_name], dict_df_subset[val_df_name], 'TweetTokens','Intensity Score', transform = None)"
      ],
      "metadata": {
        "id": "OtFQ6zwgdWIw"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_test_dataset = {}\n",
        "for name,df in dict_df_subset.items():\n",
        "  if \"train\" in name:\n",
        "    train_dataset_name = name+\"_dataset\"\n",
        "    test_df_name = name[:len(name)-13]+\"_test_subset\"\n",
        "    test_dataset_name = test_df_name + \"_dataset\"\n",
        "    dict_test_dataset[test_dataset_name] = Validation_Dataset(dict_train_dataset[train_dataset_name], dict_df_subset[test_df_name], 'TweetTokens','Intensity Score', transform = None)"
      ],
      "metadata": {
        "id": "i1Eq5adiMhR6"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dict_val_dataset.keys()"
      ],
      "metadata": {
        "id": "pQR41i-MiPnq"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dict_test_dataset.keys()"
      ],
      "metadata": {
        "id": "doOOpHk1M_IA"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(dict_test_dataset['df_EI_reg_test_subset_dataset'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HNAAJe4NFvp",
        "outputId": "d05e2502-c59d-43d9-bdb3-bbdd166cca07"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Validation_Dataset at 0x7f4f06386f50>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = random.randint(0,len(dict_val_dataset['df_EI_reg_val_subset_dataset']))\n",
        "# i = 3774\n",
        "print(\"i=\",i)\n",
        "print(dict_df_subset['df_EI_reg_val_subset'].loc[i][['TweetTokens','Intensity Score','domain']])\n",
        "\n",
        "print((dict_val_dataset['df_EI_reg_val_subset_dataset'][i][0]))\n",
        "print((dict_val_dataset['df_EI_reg_val_subset_dataset'][i][1]))\n",
        "print((dict_val_dataset['df_EI_reg_val_subset_dataset'][i][2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spuu-fgFk-k8",
        "outputId": "67c0f89a-fa63-445d-d747-b32a091e27b7"
      },
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i= 107\n",
            "TweetTokens        tonight ' s run . <repeated> <hashtag> restles...\n",
            "Intensity Score                                                0.484\n",
            "domain                                                           0.0\n",
            "Name: 107, dtype: object\n",
            "tensor([ 282,   11,   22,  421,    4,   19,    2, 1075,    3,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0])\n",
            "tensor(0.4840, dtype=torch.float64)\n",
            "tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Target (DA) dataset\n",
        "EEC dataframe does not have label column,\n",
        "Therefore, we would write another dataset class"
      ],
      "metadata": {
        "id": "bzlEwu7BNf4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#               Define DA class\n",
        "#######################################################\n",
        "\n",
        "class DA_Dataset(Dataset):\n",
        "    '''\n",
        "    Initiating Variables\n",
        "    df: the training dataframe\n",
        "    subject : the name of target text column in the dataframe\n",
        "    transform : If we want to add any augmentation\n",
        "    freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
        "    vocab_max_size : max  vocab size\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, df, subject, vocab , domain = domain_target, max_sentence_length = 150, transform=None, freq_threshold = 5,\n",
        "                vocab_max_size = 50000):\n",
        "    \n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        \n",
        "        #get body and label\n",
        "        self.subject_texts = self.df[subject]\n",
        "        self.domain = domain\n",
        "        self.vocab = vocab\n",
        "        \n",
        "        self.max_sentence_length = max_sentence_length\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    '''\n",
        "    __getitem__ runs on 1 example at a time. Here, we get an example at index and return its numericalize source and\n",
        "    target values using the vocabulary objects we created in __init__\n",
        "    '''\n",
        "    def __getitem__(self, index):\n",
        "        subject_text = self.subject_texts[index]\n",
        "        domain_value = self.domain\n",
        "#         print(subject_text)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            subject_text = self.transform(subject_text)\n",
        "            \n",
        "        #numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
        "        numerialized_subject =[]\n",
        "        numerialized_subject += self.vocab.numericalize(subject_text)\n",
        "        \n",
        "        while len(numerialized_subject) < self.max_sentence_length:\n",
        "            numerialized_subject.append(0)\n",
        "        \n",
        "        #convert the list to tensor and return\n",
        "        return torch.tensor(numerialized_subject[:self.max_sentence_length]), torch.tensor(domain_value)"
      ],
      "metadata": {
        "id": "kEEOlcqQO8pc"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "da_dataset = DA_Dataset(df_EEC,'Sentence', vocab, domain = domain_target, max_sentence_length =200) "
      ],
      "metadata": {
        "id": "FdOb-tOGNuLu"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = random.randint(0,len(da_dataset))\n",
        "print(\"i=\",i)\n",
        "print(df_EEC.loc[i][['Sentence','domain']])\n",
        "print(da_dataset[i][0])\n",
        "print(da_dataset[i][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF4tH9rwP-1S",
        "outputId": "8d410367-861c-42b1-f67a-b20bc3b7e853"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i= 5945\n",
            "Sentence    Betsy found herself in a depressing situation.\n",
            "domain                                                 1.0\n",
            "Name: 5945, dtype: object\n",
            "tensor([   1,  362, 3101,   21,   10,  424,    1,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0])\n",
            "tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dataloader\n"
      ],
      "metadata": {
        "id": "YW_4wbJOm8Ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#######################################################\n",
        "#            Define Dataloader Functions\n",
        "#######################################################\n",
        "\n",
        "# If we run a next(iter(data_loader)) we get an output of batch_size * (num_workers+1)\n",
        "def get_loader(dataset, batch_size, num_workers=1, shuffle=True, pin_memory=True): #increase num_workers according to CPU\n",
        "    loader = DataLoader(dataset, batch_size = batch_size, num_workers = num_workers,\n",
        "                        shuffle=shuffle,\n",
        "                       pin_memory=pin_memory)\n",
        "    \n",
        "    return loader\n"
      ],
      "metadata": {
        "id": "aibKT1fey5Ij"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_train_loader = {}\n",
        "batch_size = 8\n",
        "for name, dataset in dict_train_dataset.items():\n",
        "  name_dataloader = name+\"_dataloader\"\n",
        "  dict_train_loader[name_dataloader] = get_loader(dataset, batch_size)\n",
        "  x = next(iter(dict_train_loader[name_dataloader]))\n",
        "  print(name_dataloader, x[0].shape, x[1].shape, type(x[0]), type (x[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZGamL_ovHwX",
        "outputId": "310d20c2-5a87-4e0d-a6c1-b0b993a0286d"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_EI_reg_train_subset_dataset_dataloader torch.Size([8, 200]) torch.Size([8]) <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "df_V_reg_train_subset_dataset_dataloader torch.Size([8, 200]) torch.Size([8]) <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_val_loader = {}\n",
        "# batch_size = 8 \n",
        "for name, dataset in dict_val_dataset.items():\n",
        "  name_dataloader = name+\"_dataloader\"\n",
        "  # dict_val_loader[name_dataloader] = get_loader(dataset,batch_size)\n",
        "  dict_val_loader[name_dataloader] = get_loader(dataset,len(dataset))\n",
        "  x = next(iter(dict_val_loader[name_dataloader]))\n",
        "  print(name_dataloader, x[0].shape, x[1].shape, type(x[0]), type (x[1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unm6tZwcwK9Q",
        "outputId": "ce8e8c81-83ef-42e7-de5c-6027ffa64fed"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_EI_reg_val_subset_dataset_dataloader torch.Size([388, 200]) torch.Size([388]) <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "df_V_reg_val_subset_dataset_dataloader torch.Size([449, 200]) torch.Size([449]) <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_test_loader = {}\n",
        "# batch_size = 8 \n",
        "for name, dataset in dict_test_dataset.items():\n",
        "  name_dataloader = name+\"_dataloader\"\n",
        "  dict_test_loader[name_dataloader] = get_loader(dataset,len(dataset))\n",
        "  x = next(iter(dict_test_loader[name_dataloader]))\n",
        "  print(name_dataloader, x[0].shape, x[1].shape, type(x[0]), type (x[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bnKsN85NPds",
        "outputId": "bf584e18-9c0e-4ec8-fab0-c1f7b970f08f"
      },
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_EI_reg_test_subset_dataset_dataloader torch.Size([1002, 200]) torch.Size([1002]) <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "df_V_reg_test_subset_dataset_dataloader torch.Size([937, 200]) torch.Size([937]) <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "da_dataset_loader = get_loader(da_dataset, len(da_dataset))"
      ],
      "metadata": {
        "id": "7Ot4CKbirVqf"
      },
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(dict_val_loader['df_EI_reg_val_subset_dataset_dataloader'] ))"
      ],
      "metadata": {
        "id": "-E1ZxVrHz9YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(da_dataset_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L4d6gd-rkC0",
        "outputId": "fa04c9fe-99c6-44d7-c60a-87cab5c35918"
      },
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[  23, 1230,  362,  ...,    0,    0,    0],\n",
              "         [   1,  217,   24,  ...,    0,    0,    0],\n",
              "         [  42,  339, 1012,  ...,    0,    0,    0],\n",
              "         ...,\n",
              "         [   1,  785,    1,  ...,    0,    0,    0],\n",
              "         [   7, 1037,   38,  ...,    0,    0,    0],\n",
              "         [   7,  947,  222,  ...,    0,    0,    0]]),\n",
              " tensor([1., 1., 1.,  ..., 1., 1., 1.])]"
            ]
          },
          "metadata": {},
          "execution_count": 295
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings "
      ],
      "metadata": {
        "id": "Ju9qo16gfYpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Creating sentence list of all the training dataframes to create vocabulary later, this would mean a more robust vocab\n",
        "# sentence_list = []\n",
        "# for name,df in dict_df_subset.items():\n",
        "#   if \"train\" in name:\n",
        "#     sentence_list.extend(df.TweetTokens.to_list())\n",
        "# print(len(sentence_list))"
      ],
      "metadata": {
        "id": "QE30YEqsh8jh"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freq_threshold = 3\n",
        "# vocab_max_size = 50000\n",
        "\n",
        "# vocab = Vocabulary(freq_threshold, vocab_max_size)\n",
        "# vocab.build_vocabulary(sentence_list)"
      ],
      "metadata": {
        "id": "dXKmUgn6i5O0"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_vectors = GloVe(name='840B', dim=300)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_cbk8uefeq_",
        "outputId": "17102470-cb0b-45e4-c66a-f51ada20947a"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.840B.300d.zip: 2.18GB [06:53, 5.26MB/s]                            \n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2196016/2196017 [04:50<00:00, 7568.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(global_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8oxfF8ycwR5",
        "outputId": "440dc113-5246-490b-8933-c126e3fa13be"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchtext.vocab.vectors.GloVe"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_pretrained_vectors(word2idx, embedding_name = 'glove', embedding_file = global_vectors):\n",
        "    \"\"\"Load pretrained vectors and create embedding layers.\n",
        "    \n",
        "    Args:\n",
        "        word2idx - vocab.stoi (Dict): Vocabulary built from the corpus\n",
        "        embedding_name (str): the type of embedding - glove for GloVe or word2vec for word2vec\n",
        "        embedding_file (object) :optional embedding file\n",
        "\n",
        "    Returns:\n",
        "        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n",
        "            the size of word2idx and d is embedding dimension\n",
        "    \"\"\"\n",
        "\n",
        "    if embedding_name == 'glove':\n",
        "      print(\"Loading pretrained vectors...\")\n",
        "      if embedding_file:\n",
        "        global_vectors = embedding_file\n",
        "      else:\n",
        "        global_vectors = GloVe(name='840B', dim=300)\n",
        "\n",
        "      print(\"Processing pretrained vectors...\")\n",
        "      d = 300\n",
        "      print(\"\\ndimension of pretained embedding: \", d)\n",
        "\n",
        "      # Initilize random embeddings\n",
        "      embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
        "      embeddings[word2idx['<PAD>']] = np.zeros((d,))\n",
        "      \n",
        "      # Load pretrained vectors\n",
        "      count = 0 \n",
        "      for word in global_vectors.stoi:\n",
        "        if word in word2idx:\n",
        "            count +=1\n",
        "            embeddings[word2idx[word]] = global_vectors[word]\n",
        "      print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n",
        "        \n",
        "      print(\"Process Completed...\")\n",
        "      return embeddings\n",
        "\n",
        "    else:\n",
        "      print(\" Embedding not implemented, returning zero embedding\")\n",
        "      return np.zeros(len(word2idx), 300)\n",
        "    \n",
        "    \n",
        "#     # downloaded word2vec from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "\n",
        "#     word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
        "    \n",
        "#     print(\"Processing pretrained vectors...\")\n",
        "#     d = word2vec.vector_size\n",
        "#     print(\"\\ndimension of pretained embedding: \", d)\n",
        "    \n",
        "#     # Initilize random embeddings\n",
        "#     embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
        "#     embeddings[word2idx['<PAD>']] = np.zeros((d,))\n",
        "\n",
        "#     # Load pretrained vectors\n",
        "#     count = 0 \n",
        "#     for word in word2vec.key_to_index:\n",
        "#         if word in word2idx:\n",
        "#             count +=1\n",
        "#             embeddings[word2idx[word]] = word2vec.get_vector(word)\n",
        "    \n",
        "    \n",
        "# #     count = 0\n",
        "# #     for line in tqdm_notebook(fin):\n",
        "# #         tokens = line.rstrip().split(' ')\n",
        "# #         word = tokens[0]\n",
        "# #         if word in word2idx:\n",
        "# #             count += 1\n",
        "# #             embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n",
        "\n",
        "#     print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n",
        "    \n",
        "#     print(\"Process Completed...\")\n",
        "#     return embeddings\n"
      ],
      "metadata": {
        "id": "PUeTKJw_gAGn"
      },
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_embeddings = load_pretrained_vectors(vocab.stoi, embedding_name = 'glove', embedding_file = global_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6wQMWKQz9cf",
        "outputId": "7ec134a9-4397-4e4f-e9d0-122d97999e4a"
      },
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained vectors...\n",
            "Processing pretrained vectors...\n",
            "\n",
            "dimension of pretained embedding:  300\n",
            "There are 4805 / 5068 pretrained vectors found.\n",
            "Process Completed...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pre_trained_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5yEsQYDUEA0",
        "outputId": "90657416-dc32-40da-8d03-8c1a2d09d8b3"
      },
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5068, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_tensor = torch.tensor(pre_trained_embeddings)\n",
        "embedding_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ISwnOQS0TWH",
        "outputId": "ab493a67-3a62-44af-f890-50025480c923"
      },
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5068, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation"
      ],
      "metadata": {
        "id": "Q1MBzRYR0--X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Reversal Layer Function"
      ],
      "metadata": {
        "id": "Ecg7Pzsm1CkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "class ReverseLayerF(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None"
      ],
      "metadata": {
        "id": "Bgxif9of08dQ"
      },
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN 1-D Model\n",
        "\n",
        "Reference: A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification, Ye Zhang,Â Byron Wallace 2015\n",
        "\n",
        "Difference: \n",
        "\n",
        "1.   use of embedding\n",
        "2.   use of sigmoid function, as we are having a regression model not a classififer as the main task\n",
        "\n"
      ],
      "metadata": {
        "id": "Hgp_oFzHTPP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_SA(nn.Module):\n",
        "    \"\"\"An 1D Convulational Neural Network for Sentiment Analysis.\"\"\"\n",
        "    def __init__(self,\n",
        "                 pretrained_embedding=None,\n",
        "                 freeze_embedding=False,\n",
        "                 vocab_size=None,\n",
        "                 embed_dim=300,\n",
        "                 filter_sizes=[1, 2, 3, 4, 5],\n",
        "                 num_filters=[ 100, 100, 100, 100, 100],\n",
        "                 num_classes=2,\n",
        "                 dropout=0.25):\n",
        "        \"\"\"\n",
        "        The constructor for CNN_SA class.\n",
        "\n",
        "        Args:\n",
        "            pretrained_embedding (torch.Tensor): Pretrained embeddings with\n",
        "                shape (vocab_size, embed_dim)\n",
        "            freeze_embedding (bool): Set to False to fine-tune pretraiend\n",
        "                vectors. Default: False\n",
        "            vocab_size (int): Need to be specified when not pretrained word\n",
        "                embeddings are not used.\n",
        "            embed_dim (int): Dimension of word vectors. Need to be specified\n",
        "                when pretrained word embeddings are not used. Default: 300\n",
        "            filter_sizes (List[int]): List of filter sizes. Default: [2, 3, 4, 5]\n",
        "            num_filters (List[int]): List of number of filters, has the same\n",
        "                length as `filter_sizes`. Default: [100, 100, 100, 100]\n",
        "            n_classes (int): Number of classes (domain classification usage). \n",
        "            Default: 2\n",
        "            dropout (float): Dropout rate. Default: 0.25\n",
        "        \"\"\"\n",
        "\n",
        "        super(CNN_SA, self).__init__()\n",
        "        \n",
        "         #---------------------Feature Extractor Network----------------------#\n",
        "        # Embedding layer\n",
        "        if pretrained_embedding is not None:\n",
        "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
        "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
        "                                                          freeze=freeze_embedding)\n",
        "        else:\n",
        "            self.embed_dim = embed_dim\n",
        "            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                          embedding_dim=self.embed_dim,\n",
        "                                          padding_idx=0,\n",
        "                                          max_norm=5.0)\n",
        "        # Conv Network\n",
        "        self.feature_extractor = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=self.embed_dim,\n",
        "                      out_channels=num_filters[i],\n",
        "                      kernel_size=filter_sizes[i])\n",
        "            for i in range(len(filter_sizes))\n",
        "        ])\n",
        "        \n",
        "        #---------------------Regression Network------------------------#\n",
        "        # Fully-connected layer and Dropout\n",
        "        self.regression = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(np.sum(num_filters), np.sum(num_filters)//2),\n",
        "            nn.ReLU(),\n",
        "#             nn.BatchNorm1d(np.sum(num_filters)//2),\n",
        "            # nn.Linear(np.sum(num_filters)//2, num_classes), # for classification\n",
        "            nn.Linear(np.sum(num_filters)//2, 1), # for regression\n",
        "            # nn.LogSoftmax(dim=1) # for classification\n",
        "            nn.Sigmoid() # for regession (values between 0 and 1)\n",
        "        )\n",
        "        \n",
        "        #---------------------Domain Classifier Network------------------------#\n",
        "        # Fully-connected layer and Dropout\n",
        "        self.domain_classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(np.sum(num_filters), np.sum(num_filters)//2),\n",
        "            nn.ReLU(),\n",
        "#             nn.BatchNorm1d(np.sum(num_filters)//2),\n",
        "            nn.Linear(np.sum(num_filters)//2, num_classes),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, input_ids,alpha=1):\n",
        "        \"\"\"Perform a forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): A tensor of token ids with shape\n",
        "                (batch_size, max_sent_length)\n",
        "\n",
        "        Returns:\n",
        "            sigmoid (torch.Tensor) : Output sigmoid \n",
        "            logits (torch.Tensor): Output logits with shape (batch_size,\n",
        "                n_classes)\n",
        "        \"\"\"\n",
        "\n",
        "        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n",
        "#         input_ids = torch.tensor(input_ids).to(torch.int64)\n",
        "        input_ids = input_ids.clone().detach().to(torch.int64)\n",
        "        # print(\"input_ids.shape\", input_ids.shape)\n",
        "        \n",
        "        x_embed = self.embedding((input_ids)).float()\n",
        "        # print(\"x_embed.shape\", x_embed.shape)\n",
        "\n",
        "\n",
        "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
        "        # Output shape: (b, embed_dim, max_len)\n",
        "        x_reshaped = x_embed.permute(0, 2, 1)\n",
        "        # print(\"x_reshaped.shape\", x_reshaped.shape)\n",
        "        \n",
        "\n",
        "#         # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
        "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.feature_extractor]\n",
        "        # print(\"x_conv_list[1].shape\", x_conv_list[1].shape)\n",
        "        # print(\"x_conv_list[2].shape\", x_conv_list[2].shape)\n",
        "\n",
        "#         # Max pooling. Output shape: (b, num_filters[i], 1)\n",
        "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
        "            for x_conv in x_conv_list]\n",
        "        # print( \"x_pool_list[3].shape\", x_pool_list[3].shape)\n",
        "        \n",
        "#         # Concatenate x_pool_list to feed the fully connected layer.\n",
        "#         # Output shape: (b, sum(num_filters))\n",
        "        x_feature = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
        "                         dim=1)\n",
        "        # print(\"x_feature.shape\", x_feature.shape)\n",
        "        \n",
        "# #         # Compute logits. Output shape: (b, n_classes)\n",
        "#         logits = self.fc(self.dropout(x_feature))\n",
        "#         print(logits)\n",
        "\n",
        "        reverse_feature = ReverseLayerF.apply(x_feature, alpha)\n",
        "        # print(\"reverse_feature\",reverse_feature)\n",
        "    \n",
        "        regression_output = self.regression(x_feature)\n",
        "    \n",
        "        domain_classifier_output = self.domain_classifier(reverse_feature)\n",
        "#         print(domain_classifier_logits.shape)\n",
        "\n",
        "#         return logits\n",
        "        return regression_output, domain_classifier_output\n"
      ],
      "metadata": {
        "id": "Hn38ChCwTT-W"
      },
      "execution_count": 349,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = CNN_SA(pretrained_embedding=embedding_tensor,freeze_embedding=True).to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eCFudXBYHjM",
        "outputId": "d87649d5-818c-43d1-f8b6-b779c74a8bdc"
      },
      "execution_count": 350,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "CNN_SA(\n",
            "  (embedding): Embedding(5068, 300)\n",
            "  (feature_extractor): ModuleList(\n",
            "    (0): Conv1d(300, 100, kernel_size=(1,), stride=(1,))\n",
            "    (1): Conv1d(300, 100, kernel_size=(2,), stride=(1,))\n",
            "    (2): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
            "    (3): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
            "    (4): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (regression): Sequential(\n",
            "    (0): Dropout(p=0.25, inplace=False)\n",
            "    (1): Linear(in_features=500, out_features=250, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=250, out_features=1, bias=True)\n",
            "    (4): Sigmoid()\n",
            "  )\n",
            "  (domain_classifier): Sequential(\n",
            "    (0): Dropout(p=0.25, inplace=False)\n",
            "    (1): Linear(in_features=500, out_features=250, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=250, out_features=2, bias=True)\n",
            "    (4): LogSoftmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for name, train_loader in dict_train_loader.items():\n",
        "  tweet, intensity, domain = next(iter(train_loader))\n",
        "  print (name, \"\\n\", tweet, intensity)\n",
        "  a, b = model(tweet)\n",
        "  print (\"a - sigmoid output\\n\", a)\n",
        "  print (\"b - softmax output\\n\", b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpCF2qvRYkK4",
        "outputId": "570b728d-13ce-4b3c-e8b6-23bc432a71fa"
      },
      "execution_count": 351,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_EI_reg_train_subset_dataset_dataloader \n",
            " tensor([[   5,    5,    5,  ...,    0,    0,    0],\n",
            "        [   5,  519,  115,  ...,    0,    0,    0],\n",
            "        [   5,    5,   52,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   5,  134,   58,  ...,    0,    0,    0],\n",
            "        [  13,   44, 1816,  ...,    0,    0,    0],\n",
            "        [  49,   15,  341,  ...,    0,    0,    0]]) tensor([0.3330, 0.3330, 0.6460, 0.9000, 0.5000, 0.5420, 0.7080, 0.6250],\n",
            "       dtype=torch.float64)\n",
            "a - sigmoid output\n",
            " tensor([[0.4808],\n",
            "        [0.4904],\n",
            "        [0.4559],\n",
            "        [0.4635],\n",
            "        [0.4850],\n",
            "        [0.4949],\n",
            "        [0.4556],\n",
            "        [0.4687]], grad_fn=<SigmoidBackward0>)\n",
            "b - softmax output\n",
            " tensor([[-0.7309, -0.6568],\n",
            "        [-0.6815, -0.7049],\n",
            "        [-0.6494, -0.7389],\n",
            "        [-0.6257, -0.7655],\n",
            "        [-0.6589, -0.7286],\n",
            "        [-0.6507, -0.7375],\n",
            "        [-0.6691, -0.7178],\n",
            "        [-0.6758, -0.7108]], grad_fn=<LogSoftmaxBackward0>)\n",
            "df_V_reg_train_subset_dataset_dataloader \n",
            " tensor([[519,   1,   9,  ...,   0,   0,   0],\n",
            "        [  5, 519,   1,  ...,   0,   0,   0],\n",
            "        [ 10, 524,   8,  ...,   0,   0,   0],\n",
            "        ...,\n",
            "        [  5,   5,  12,  ...,   0,   0,   0],\n",
            "        [  5,   6,  30,  ...,   0,   0,   0],\n",
            "        [ 41,  93,   6,  ...,   0,   0,   0]]) tensor([0.4020, 0.5170, 0.8710, 0.8280, 0.3040, 0.6090, 0.3670, 0.2420],\n",
            "       dtype=torch.float64)\n",
            "a - sigmoid output\n",
            " tensor([[0.4763],\n",
            "        [0.4849],\n",
            "        [0.4389],\n",
            "        [0.4727],\n",
            "        [0.4504],\n",
            "        [0.4542],\n",
            "        [0.4484],\n",
            "        [0.4612]], grad_fn=<SigmoidBackward0>)\n",
            "b - softmax output\n",
            " tensor([[-0.6906, -0.6957],\n",
            "        [-0.6545, -0.7334],\n",
            "        [-0.6451, -0.7436],\n",
            "        [-0.6732, -0.7135],\n",
            "        [-0.6110, -0.7826],\n",
            "        [-0.6927, -0.6936],\n",
            "        [-0.6295, -0.7611],\n",
            "        [-0.6578, -0.7298]], grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Test Function"
      ],
      "metadata": {
        "id": "VjBXTHz2cXcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Typical Training, Test Function (without Domain Adaptation)"
      ],
      "metadata": {
        "id": "8nPEeiSFcc-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Typical Training Function (without domain adapatation)"
      ],
      "metadata": {
        "id": "pRqVO5RUpFHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Function\n",
        "\n",
        "from tqdm import tqdm # for beautiful model training updates\n",
        "\n",
        "\n",
        "def train_model(model, device, train_loader, optimizer, epoch):\n",
        "    model.train() # setting the model in training mode\n",
        "    pbar = tqdm(train_loader) # putting the iterator in pbara\n",
        "    correct = 0 # for accuracy numerator\n",
        "    processed =0 # for accuracy denominator\n",
        "    epoch_loss = 0.0\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "\n",
        "        tweets, intensities = batch[0].to(device), batch[1].float().to(device)  # plural, we are not interested in domain\n",
        "        #sending data to CPU or GPU as per device\n",
        "\n",
        "        optimizer.zero_grad() # setting gradients to zero to avoid accumulation\n",
        "\n",
        "        y_preds, _ = model(tweets) # forward pass, result captured in y_preds (plural as there are many body in a batch)\n",
        "        # we are not interested in domain prediction\n",
        "        # the predictions are in one hot vector\n",
        "\n",
        "        loss = F.mse_loss(y_preds,intensities.unsqueeze(1)) # Computing loss\n",
        "\n",
        "        train_losses.append(loss) # to capture loss over many epochs\n",
        "\n",
        "        loss.backward() # backpropagation\n",
        "        optimizer.step() # updating the params\n",
        "\n",
        "        # preds = y_preds.argmax(dim=1, keepdim=True)  # get the index olf the max log-probability\n",
        "        # correct += preds.eq(labels.view_as(preds)).sum().item()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        processed += len(tweets)\n",
        "\n",
        "        pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Epoch Average loss={100*epoch_loss/processed:0.4f}')\n",
        "    train_accuracy.append(100*epoch_loss/len(train_loader))"
      ],
      "metadata": {
        "id": "bulUVa2VcrHm"
      },
      "execution_count": 352,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Typical Test Function "
      ],
      "metadata": {
        "id": "FZu9qYllpPDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model,device, test_loader, mode= 'test'):\n",
        "    model.eval() # setting the model in evaluation mode\n",
        "    loss = 0\n",
        "    correct = 0 # for accuracy numerator\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (tweets, intensities, domain) in test_loader:\n",
        "\n",
        "            tweets, intensities  = tweets.to(device),intensities.float().to(device), #sending data to CPU or GPU as per device\n",
        "            # we are not interested in domains\n",
        "            \n",
        "            outputs,_ = model(tweets) # forward pass, result captured in outputs (plural as there are many bodies in a batch)\n",
        "            # the outputs are in batch size x one hot vector \n",
        "            # not interested in domain output\n",
        "\n",
        "            loss = F.mse_loss(outputs,intensities.unsqueeze(1))\n",
        "\n",
        "\n",
        "        loss /= len(test_loader.dataset) # average test loss\n",
        "        if mode == 'test':\n",
        "          test_losses.append(loss) # to capture loss over many batches\n",
        "          print('...Average test loss: {:.8f}'.format(loss))\n",
        "        else:\n",
        "          val_losses.append(loss) # to capture loss over many batches\n",
        "          print('...Average val loss: {:.8f}'.format(loss))\n"
      ],
      "metadata": {
        "id": "-zcgoz0-qJia"
      },
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing loop"
      ],
      "metadata": {
        "id": "vKNwBgxhsfJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXECUTION\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "typical_model =  CNN_SA(pretrained_embedding=embedding_tensor,\n",
        "                  freeze_embedding=True,\n",
        "                 ).to(device)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(typical_model.parameters(), lr=0.001)\n",
        "\n",
        "# train_losses = [] # to capture train losses over training epochs\n",
        "# train_accuracy = [] # to capture train accuracy over training epochs\n",
        "# val_losses = [] # to capture validation loss\n",
        "# test_losses = [] # to capture test losses \n",
        "# test_accuracy = [] # to capture test accuracy \n",
        "\n",
        "EPOCHS = 2\n",
        "# EPOCHS = 5\n",
        "dict_val_loss = {}\n",
        "dict_test_loss = {}\n",
        "\n",
        "for train_name, train_loader in dict_train_loader.items():\n",
        "  name = \"_\".join(train_name.split(\"_\")[1:3])\n",
        "  train_losses = [] # to capture train losses over training epochs\n",
        "  train_accuracy = [] # to capture train accuracy over training epochs\n",
        "  val_losses = [] # to capture validation loss\n",
        "  test_losses = [] # to capture test losses \n",
        "  print(f'----------------------training started for {name}-----------------')\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch+1)\n",
        "    train_model(typical_model, device, train_loader, optimizer, epoch)\n",
        "\n",
        "    print(\"\\nfor validation.......\")\n",
        "    val_name = train_name.replace(\"train\", \"val\" )\n",
        "    test_model(typical_model, device, dict_val_loader[val_name], mode = 'val')\n",
        "    \n",
        "\n",
        "\n",
        "    print(\"for test  .......\")\n",
        "    test_name = train_name.replace(\"train\", \"test\" )\n",
        "    test_model(typical_model, device, dict_test_loader[test_name], mode = 'test')\n",
        "\n",
        "  dict_val_loss[name] = val_losses\n",
        "  dict_test_loss[name] = test_losses\n",
        "\n",
        "  model_name = name+\".pt\"\n",
        "  torch.save(typical_model.state_dict(), os.path.join(MODEL_DIR, model_name))\n",
        "  print(f'----------------------training complete for {name}-----------------')\n",
        "print(dict_val_loss.items())\n",
        "print(dict_test_loss.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FrceNxng81k",
        "outputId": "b69ffb5e-c009-4306-8904-e02c2234a366"
      },
      "execution_count": 354,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "----------------------training started for EI_reg-----------------\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.021468039602041245 Batch_id=212 Epoch Average loss=0.2867: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 213/213 [00:25<00:00,  8.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "for validation.......\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...Average val loss: 0.00007197\n",
            "for test  .......\n",
            "...Average test loss: 0.00002696\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.010277757421135902 Batch_id=212 Epoch Average loss=0.1514: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 213/213 [00:27<00:00,  7.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "for validation.......\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...Average val loss: 0.00006732\n",
            "for test  .......\n",
            "...Average test loss: 0.00002614\n",
            "----------------------training complete for EI_reg-----------------\n",
            "----------------------training started for V_reg-----------------\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08481224626302719 Batch_id=147 Epoch Average loss=0.4969: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:16<00:00,  8.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "for validation.......\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...Average val loss: 0.00004720\n",
            "for test  .......\n",
            "...Average test loss: 0.00002206\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.009592588059604168 Batch_id=147 Epoch Average loss=0.1911: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148/148 [00:17<00:00,  8.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "for validation.......\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...Average val loss: 0.00004452\n",
            "for test  .......\n",
            "...Average test loss: 0.00002076\n",
            "----------------------training complete for V_reg-----------------\n",
            "dict_items([('EI_reg', [tensor(7.1968e-05), tensor(6.7316e-05)]), ('V_reg', [tensor(4.7198e-05), tensor(4.4523e-05)])])\n",
            "dict_items([('EI_reg', [tensor(2.6958e-05), tensor(2.6142e-05)]), ('V_reg', [tensor(2.2058e-05), tensor(2.0761e-05)])])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict_val_loss.items())\n",
        "print(dict_test_loss.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wo9pgE9JqHy",
        "outputId": "5951d731-58c0-45ea-f500-ca568ded6a5e"
      },
      "execution_count": 356,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('EI_reg', [tensor(7.1968e-05), tensor(6.7316e-05)]), ('V_reg', [tensor(4.7198e-05), tensor(4.4523e-05)])])\n",
            "dict_items([('EI_reg', [tensor(2.6958e-05), tensor(2.6142e-05)]), ('V_reg', [tensor(2.2058e-05), tensor(2.0761e-05)])])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# next(iter(dict_train_loader['df_EI_reg_train_subset_dataset_dataloader']))"
      ],
      "metadata": {
        "id": "ly-mL3FUD5pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Domain Adaptation Training Function (using DANN)"
      ],
      "metadata": {
        "id": "zNuo16p-skka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DA Training Function (using DANN)"
      ],
      "metadata": {
        "id": "RbaUmBTX0Cva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DANN Training Function\n",
        "\n",
        "def dann_train_model(model, device, train_source_loader, train_target_loader,optimizer, epoch, num_epochs):\n",
        "    model.train() # setting the model in training mode\n",
        "    len_dataloader = min(len(train_source_loader), len(train_target_loader)) # training for minimum of two dataloaders\n",
        "    \n",
        "    i = 0 # as the training progresses the alpha changes\n",
        "    while i < len_dataloader -1:\n",
        "        \n",
        "        # implementation of alpha as per paper\n",
        "        p = float(i + epoch * len_dataloader) / (num_epochs * len_dataloader)\n",
        "        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "        alpha = torch.tensor(alpha)\n",
        "        \n",
        "        # training model using source data\n",
        "        source_batch = next(iter(train_source_loader))\n",
        "        source_bodies, source_labels, source_domains = source_batch[0].to(device),source_batch[1].float().to(device),source_batch[2].long().to(device) # plural\n",
        "        source_batch_size = len(source_labels)\n",
        "        \n",
        "        optimizer.zero_grad() # setting gradients to zero to avoid accumulation\n",
        "        \n",
        "        y_preds, source_domain_outputs  = model(source_bodies, alpha = alpha) # forward pass, plural results\n",
        "        \n",
        "        loss_source_label =  F.mse_loss(y_preds,source_labels.unsqueeze(1)) # Computing loss, regression loss\n",
        "        loss_source_domain = F.nll_loss(source_domain_outputs, source_domains) # classificaiton loss\n",
        "        \n",
        "\n",
        "    #     train_losses.append(loss) # to capture loss over many epochs\n",
        "\n",
        "    #     loss.backward() # backpropagation\n",
        "    #     optimizer.step() # updating the params\n",
        "\n",
        "    #     # preds = y_preds.argmax(dim=1, keepdim=True)  # get the index olf the max log-probability\n",
        "    #     # correct += preds.eq(labels.view_as(preds)).sum().item()\n",
        "    #     epoch_loss += loss.item()\n",
        "\n",
        "    #     processed += len(tweets)\n",
        "\n",
        "    #     pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Epoch Average loss={100*epoch_loss/processed:0.4f}')\n",
        "    # train_accuracy.append(100*epoch_loss/len(train_loader))\n",
        "\n",
        "\n",
        "\n",
        "        # training model using target data\n",
        "        target_batch = next(iter(train_target_loader))\n",
        "#         target_bodies, _ , target_domains = tuple(t.to(device) for t in target_batch) # plural, we are not interesed in  label\n",
        "        target_bodies, target_domains = target_batch[0].to(device),target_batch[2].long().to(device) # plural, we are not interesed in  label\n",
        "\n",
        "        target_batch_size = len(target_domains)\n",
        "        \n",
        "        _ , target_domain_outputs = model(target_bodies, alpha = alpha) # forward pass, plural results, we are not interested in label\n",
        "        \n",
        "        loss_target_domain = F.nll_loss(target_domain_outputs, target_domains)\n",
        "        \n",
        "        loss = loss_source_label + loss_source_domain + loss_target_domain\n",
        "        \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        loss.backward() # backpropagation\n",
        "        optimizer.step() # updating the params\n",
        "        \n",
        "        if ((i + 1) % 100 == 0):\n",
        "                print(\"Epoch [{}/{}] Step [{}/{}]: domain_loss_target={:.4f} / domain_loss_source={:.4f} / class_loss_source={:.4f}\"\n",
        "                      .format(epoch + 1,\n",
        "                              num_epochs,\n",
        "                              i + 1,\n",
        "                              len_dataloader,\n",
        "                              loss_target_domain.item()\n",
        "                              ,loss_source_domain.item()\n",
        "                              ,loss_source_label.item()))\n",
        "        \n",
        "        i = i+1\n",
        "        \n",
        "    # torch.save(model.state_dict(),\"DANN-{}.pt\".format(epoch + 1))\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "-CAN3m78smwU"
      },
      "execution_count": 355,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DA Test Function (using DANN)"
      ],
      "metadata": {
        "id": "Ve3HTV-Buwfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import logProcesses\n",
        "# DANN Test Function\n",
        "\n",
        "def dann_test_model(model, device, test_loader , domain = domain_source, mode = 'test'):\n",
        "    model.eval() # setting the model in evaluation mode\n",
        "    alpha = 0\n",
        "    loss = 0\n",
        "    correct = 0  # for accuracy numerator\n",
        "    if domain == domain_source:\n",
        "        domain_str = \"source\"\n",
        "    else:\n",
        "        domain_str = \"target\"\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for (bodies,labels,domains) in test_loader:\n",
        "            \n",
        "            bodies, labels, domains = bodies.to(device), labels.float().to(device), domains.long().to(device)#sending data to CPU or GPU as per device\n",
        "            outputs, _  = model(bodies, alpha = alpha) # forward pass, plural results\n",
        "            # the outputs are in batch size x one hot vector \n",
        "            # not interested in domain output\n",
        "\n",
        "            loss = F.mse_loss(outputs,labels.unsqueeze(1))\n",
        "\n",
        "\n",
        "            \n",
        "        loss /= len(test_loader.dataset) # average test loss\n",
        "            \n",
        "        if mode == 'test':\n",
        "          dann_test_losses.append(loss) # to capture loss over many batches\n",
        "          print('...Average test loss: {:.8f},  domain: {} \\n'.format(loss, domain_str))\n",
        "        else:\n",
        "          dann_val_losses.append(loss) # to capture loss over many batches\n",
        "          print('...Average val loss: {:.8f},  domain: {} \\n'.format(loss, domain_str))\n",
        "                        \n",
        "        "
      ],
      "metadata": {
        "id": "4ZD5YWwIuvBu"
      },
      "execution_count": 358,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DA (DANN) Train and Test Loop"
      ],
      "metadata": {
        "id": "OJucAFRoykyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "dann_model =  CNN_SA(pretrained_embedding=embedding_tensor,\n",
        "                  freeze_embedding=True).to(device)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(dann_model.parameters(), lr=0.001)\n",
        "\n",
        "# train_losses = [] # to capture train losses over training epochs\n",
        "# train_accuracy = [] # to capture train accuracy over training epochs\n",
        "\n",
        "# test_losses_source = [] # to capture test losses \n",
        "# test_losses_target = [] # to capture test losses\n",
        "# test_accuracy_source = [] # to capture test accuracy \n",
        "# test_accuracy_target = [] # to capture test accuracy\n",
        "\n",
        "# EPOCHS = 2\n",
        "EPOCHS = 10\n",
        "dict_dann_val_loss = {}\n",
        "dict_dann_test_loss = {}\n",
        "\n",
        "for train_name, train_loader in dict_train_loader.items():\n",
        "  name = \"_\".join(train_name.split(\"_\")[1:3])\n",
        "\n",
        "  dann_train_losses = [] # to capture train losses over training epochs\n",
        "  dann_train_accuracy = [] # to capture train accuracy over training epochs\n",
        "  dann_val_losses = [] # to capture validation loss\n",
        "  dann_test_losses = [] # to capture test losses \n",
        "\n",
        "  print(f'----------------------training started for {name}-----------------')\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch+1)\n",
        "    dann_train_model(dann_model, device, train_loader, da_dataset_loader, optimizer, epoch, num_epochs = EPOCHS)\n",
        "    print(\"for val.......\")\n",
        "    val_name = train_name.replace(\"train\", \"val\" )\n",
        "    dann_test_model(dann_model, device, dict_val_loader[val_name], domain = domain_source, mode = 'val')\n",
        "    print(\"for test......\")\n",
        "    test_name = train_name.replace(\"train\", \"test\" )\n",
        "    dann_test_model(dann_model, device, dict_test_loader[test_name], domain = domain_source, mode = 'test')\n",
        "  print(f'----------------------training complete for {name}-----------------')\n",
        "  \n",
        "  dict_dann_val_loss[name] = dann_val_losses\n",
        "  dict_dann_test_loss[name] = dann_test_losses\n",
        "\n",
        "  model_name = name+\"_dann.pt\"\n",
        "  torch.save(dann_model.state_dict(), os.path.join(MODEL_DIR, model_name))\n",
        "  print(f'----------------------Model: {model_name } saved complete for {name}-----------------')  \n",
        "print(\"---------------------DANN--training complete-----------------\")\n",
        "print(dict_dann_val_loss.items())\n",
        "print(dict_dann_test_loss.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtOU12iZxRM6",
        "outputId": "eb13c0c0-dab8-4ded-8be7-6ec30ac50fbd"
      },
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "----------------------training started for EI_reg-----------------\n",
            "EPOCH: 1\n",
            "for val.......\n",
            "...Average val loss: 0.00010251,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00003599,  domain: source \n",
            "\n",
            "EPOCH: 2\n",
            "for val.......\n",
            "...Average val loss: 0.00010251,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00003599,  domain: source \n",
            "\n",
            "EPOCH: 3\n",
            "for val.......\n",
            "...Average val loss: 0.00010251,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00003599,  domain: source \n",
            "\n",
            "EPOCH: 4\n",
            "for val.......\n",
            "...Average val loss: 0.00010251,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00003599,  domain: source \n",
            "\n",
            "EPOCH: 5\n",
            "for val.......\n",
            "...Average val loss: 0.00010251,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00003599,  domain: source \n",
            "\n",
            "EPOCH: 6\n",
            "for val.......\n",
            "...Average val loss: 0.00010251,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00003599,  domain: source \n",
            "\n",
            "EPOCH: 7\n",
            "for val.......\n",
            "...Average val loss: 0.00010251,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00003599,  domain: source \n",
            "\n",
            "EPOCH: 8\n",
            "for val.......\n",
            "...Average val loss: 0.00010251,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00003599,  domain: source \n",
            "\n",
            "EPOCH: 9\n",
            "for val.......\n",
            "...Average val loss: 0.00010251,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00003599,  domain: source \n",
            "\n",
            "EPOCH: 10\n",
            "for val.......\n",
            "...Average val loss: 0.00010251,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00003599,  domain: source \n",
            "\n",
            "----------------------training complete for EI_reg-----------------\n",
            "----------------------Model: EI_reg_dann.pt saved complete for EI_reg-----------------\n",
            "----------------------training started for V_reg-----------------\n",
            "EPOCH: 1\n",
            "for val.......\n",
            "...Average val loss: 0.00011898,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00005077,  domain: source \n",
            "\n",
            "EPOCH: 2\n",
            "for val.......\n",
            "...Average val loss: 0.00011898,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00005077,  domain: source \n",
            "\n",
            "EPOCH: 3\n",
            "for val.......\n",
            "...Average val loss: 0.00011898,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00005077,  domain: source \n",
            "\n",
            "EPOCH: 4\n",
            "for val.......\n",
            "...Average val loss: 0.00011898,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00005077,  domain: source \n",
            "\n",
            "EPOCH: 5\n",
            "for val.......\n",
            "...Average val loss: 0.00011898,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00005077,  domain: source \n",
            "\n",
            "EPOCH: 6\n",
            "for val.......\n",
            "...Average val loss: 0.00011898,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00005077,  domain: source \n",
            "\n",
            "EPOCH: 7\n",
            "for val.......\n",
            "...Average val loss: 0.00011898,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00005077,  domain: source \n",
            "\n",
            "EPOCH: 8\n",
            "for val.......\n",
            "...Average val loss: 0.00011898,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00005077,  domain: source \n",
            "\n",
            "EPOCH: 9\n",
            "for val.......\n",
            "...Average val loss: 0.00011898,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00005077,  domain: source \n",
            "\n",
            "EPOCH: 10\n",
            "for val.......\n",
            "...Average val loss: 0.00011898,  domain: source \n",
            "\n",
            "for test......\n",
            "...Average test loss: 0.00005077,  domain: source \n",
            "\n",
            "----------------------training complete for V_reg-----------------\n",
            "----------------------Model: V_reg_dann.pt saved complete for V_reg-----------------\n",
            "---------------------DANN--training complete-----------------\n",
            "dict_items([('EI_reg', [tensor(0.0001), tensor(0.0001), tensor(0.0001), tensor(0.0001), tensor(0.0001), tensor(0.0001), tensor(0.0001), tensor(0.0001), tensor(0.0001), tensor(0.0001)]), ('V_reg', [tensor(0.0001), tensor(0.0001), tensor(0.0001), tensor(0.0001), tensor(0.0001), tensor(0.0001), tensor(0.0001), tensor(0.0001), tensor(0.0001), tensor(0.0001)])])\n",
            "dict_items([('EI_reg', [tensor(3.5990e-05), tensor(3.5990e-05), tensor(3.5990e-05), tensor(3.5990e-05), tensor(3.5990e-05), tensor(3.5990e-05), tensor(3.5990e-05), tensor(3.5990e-05), tensor(3.5990e-05), tensor(3.5990e-05)]), ('V_reg', [tensor(5.0767e-05), tensor(5.0767e-05), tensor(5.0767e-05), tensor(5.0767e-05), tensor(5.0767e-05), tensor(5.0767e-05), tensor(5.0767e-05), tensor(5.0767e-05), tensor(5.0767e-05), tensor(5.0767e-05)])])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict_dann_val_loss.items())\n",
        "print(dict_dann_test_loss.items())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz5oN4e6H6oj",
        "outputId": "3d87dc21-b0e4-42d1-ea9f-2f0eb2dff00e"
      },
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('EI_reg', [tensor(0.0001), tensor(0.0001)]), ('V_reg', [tensor(0.0001), tensor(0.0001)])])\n",
            "dict_items([('EI_reg', [tensor(3.5896e-05), tensor(3.5896e-05)]), ('V_reg', [tensor(5.0222e-05), tensor(5.0222e-05)])])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'df_EI_reg_val_subset_dataset_dataloader'\n",
        "\"_\".join(name.split(\"_\")[1:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dbta-TYq67WL",
        "outputId": "6f419e46-72c6-4598-fef7-87ab5a41ce7a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'EI_reg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}