{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQpJfUEUfqcLKoJu02a8hh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peeyushsinghal/da/blob/main/mitigating_bias_sa_da.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ekphrasis # library to pre process twitter data\n",
        "! pip install emoji --upgrade #library to deal with emoji data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jYp6x5D5AYi",
        "outputId": "75808e32-ccb5-4870-850d-0a7d17c968ed"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.64.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.7)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (0.4.5)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (2.0.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (6.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.21.6)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (5.5.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "UN22BqAu8mbl"
      },
      "outputs": [],
      "source": [
        "## Import statements\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import GloVe\n",
        "import numpy as np\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "import emoji\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on:{}\".format(DEVICE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCz_btt5Saig",
        "outputId": "ddf58bf4-c058-4380-c004-e6d05f97de8b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on:cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Handling"
      ],
      "metadata": {
        "id": "xWShQEZF9MlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting google drive for data in there"
      ],
      "metadata": {
        "id": "LDNN2hc-9SL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPXnt-PrRvGm",
        "outputId": "7ef5e0c3-968c-4ed9-90a2-48487d47158e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data configuration"
      ],
      "metadata": {
        "id": "v1MWCkgXSSv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/semeval-2018'\n",
        "DATA_DIR = os.path.join(BASE_PATH,'datasets')"
      ],
      "metadata": {
        "id": "JAVHMCaNSIlW"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TASK1(object):\n",
        "  \n",
        "    EI_reg = {\n",
        "        'anger': {\n",
        "            'train': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/training/EI-reg-En-anger-train.txt'),\n",
        "            'dev': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/development/2018-EI-reg-En-anger-dev.txt'),\n",
        "            'gold': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/test-gold/2018-EI-reg-En-anger-test-gold.txt')\n",
        "                }\n",
        "        }\n",
        "\n",
        "    V_reg = {\n",
        "        'train': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-train.txt'),\n",
        "        'dev': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-dev.txt'),\n",
        "        'gold': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-test-gold.txt')\n",
        "             }\n",
        "\n",
        "    EEC = {\n",
        "        'eec': os.path.join(\n",
        "            DATA_DIR, 'task1/Equity-Evaluation-Corpus/Equity-Evaluation-Corpus.csv')\n",
        "             }"
      ],
      "metadata": {
        "id": "_Z4pcQiGSyXX"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataloading and Parsing"
      ],
      "metadata": {
        "id": "UfWsmb-8V9Er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing regression data : `format [ID\tTweet\tAffect Dimension\tIntensity Score]`"
      ],
      "metadata": {
        "id": "CtLKXfb-Xi79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_reg(data_file, label_format='tuple')-> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    This is for datasets for the EI-reg and V-reg English tasks \n",
        "    Returns:\n",
        "        df: dataframe with columns in the first row of file [ID-Tweet-Affect Dimension-Intensity Score]\n",
        "    \"\"\"\n",
        "    with open(data_file, 'r') as fd:\n",
        "      data = [l.strip().split('\\t') for l in fd.readlines()]\n",
        "    \n",
        "    df = pd.DataFrame (data[1:],columns=data[0])\n",
        "    return df"
      ],
      "metadata": {
        "id": "7-CzssqIr68Z"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def parse_reg(data_file, label_format='tuple')-> (list, list):\n",
        "#     \"\"\"\n",
        "#     This is for datasets for the EI-reg and V-reg English tasks \n",
        "#     Returns:\n",
        "#         X: a list of tweets\n",
        "#         y: a list of (affect dimension, v) tuples corresponding to\n",
        "#          the regression targets of the tweets\n",
        "#     \"\"\"\n",
        "#     with open(data_file, 'r') as fd:\n",
        "#         data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
        "#     X = [d[1] for d in data]\n",
        "#     y = [(d[2], float(d[3])) for d in data]\n",
        "#     if label_format == 'list':\n",
        "#         y = [l[1] for l in y]\n",
        "#     return X, y"
      ],
      "metadata": {
        "id": "F_A5KcxfV_4N"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "parsing EEC data : `format [ID\tSentence\tTemplate\tPerson\tGender\tRace Emotion\tEmotion word]`"
      ],
      "metadata": {
        "id": "FNEQISzDY6ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_eec()->pd.DataFrame:\n",
        "  \"\"\"\n",
        "  This is for EEC Dataset, it is a csv file\n",
        "  Returns:\n",
        "        df_eec: dataframe \n",
        "  \"\"\"\n",
        "  data_train = TASK1.EEC['eec']\n",
        "  df_eec = pd.read_csv(data_train)\n",
        "  return df_eec\n"
      ],
      "metadata": {
        "id": "afnCKQmtYhW3"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse(task, dataset, emotion='anger') -> pd.DataFrame:\n",
        "    if task == 'EI-reg':\n",
        "        data_train = TASK1.EI_reg[emotion][dataset]\n",
        "        df = parse_reg(data_train)\n",
        "        df[df.columns[-1]] = df[df.columns[-1]].astype(float)\n",
        "        return df\n",
        "    elif task == 'V-reg':\n",
        "        data_train = TASK1.V_reg[dataset]\n",
        "        df = parse_reg(data_train)\n",
        "        df[df.columns[-1]] = df[df.columns[-1]].astype(float)\n",
        "        return df\n",
        "    else:\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "0J7zs1GzXh8F"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating Dataframes\n",
        "df_EI_reg_train = parse('EI-reg','train')\n",
        "df_EI_reg_val = parse('EI-reg','dev')\n",
        "df_EI_reg_test = parse('EI-reg','gold')\n",
        "df_V_reg_train = parse('V-reg','train')\n",
        "df_V_reg_val = parse('V-reg','dev')\n",
        "df_V_reg_test = parse('V-reg','gold')\n",
        "\n",
        "dict_df= {'df_EI_reg_train':df_EI_reg_train, \n",
        "          'df_EI_reg_val':df_EI_reg_val, \n",
        "          'df_EI_reg_test':df_EI_reg_test, \n",
        "          'df_V_reg_train': df_V_reg_train, \n",
        "          'df_V_reg_val':df_V_reg_val, \n",
        "          'df_V_reg_test': df_V_reg_test \n",
        "          }"
      ],
      "metadata": {
        "id": "9CgnnxH-aU6B"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PreProcess Twitter Data"
      ],
      "metadata": {
        "id": "vgcR6rW84jVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reference : https://github.com/cbaziotis/ekphrasis\n",
        "\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1DwsLjWqvEx",
        "outputId": "f9ed8d3d-b7ed-494e-f5a6-c557de98804e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #### Example checks of pre-processing\n",
        "# sentences = [\n",
        "#     \"CANT WAIT for the new season of #TwinPeaks ＼(^o^)／!!! #davidlynch #tvseries :)))\",\n",
        "#     \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies :/\",\n",
        "#     \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! :-D http://sentimentsymposium.com/.\",\n",
        "#     \"@MGBarbieri @SpalkTalk a@b.com And just saw your LinkedIn comment after I sent this! Thanks for the message :) 😀\",\n",
        "#     \"💙💛🏆 @GeorgeePitman Young Player of The Season 🏆💛💙 #irony #actuallyseventy\"\n",
        "# ]\n",
        "\n",
        "# for s in sentences:\n",
        "#     print(\" \".join(text_processor.pre_process_doc(s)))\n",
        "# # print ([text_processor.pre_process_doc(s) for s in sentences])"
      ],
      "metadata": {
        "id": "iK5Ifwyhq0Ex"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_tweets(df)-> pd.DataFrame:\n",
        "  tweets = df.Tweet.to_list()\n",
        "  # df['TweetTokens'] = [emoji.demojize(text_processor.pre_process_doc(tweet),language = 'en') for tweet in tweets] # Translates emoji in to word and preprocesss\n",
        "  # df['TweetTokens'] = [text_processor.pre_process_doc(tweet) for tweet in tweets] # preprocesss\n",
        "  # tweets_processed = [text_processor.pre_process_doc(tweet) for tweet in tweets] # preprocesss\n",
        "  # for tweet in tweets_processed:\n",
        "  #   for index, token in enumerate(tweet):\n",
        "  #     if emoji.is_emoji(token):\n",
        "  #       tweet[index] = emoji.demojize(token, language = 'en')\n",
        "\n",
        "  tweets_processed = [\" \".join(text_processor.pre_process_doc(tweet)) for tweet in tweets] # preprocesss\n",
        "  # print (tweets_processed)\n",
        "  for index, tweet in enumerate(tweets_processed):\n",
        "      tweets_processed[index] = emoji.demojize(tweet, language = 'en')\n",
        "  \n",
        "  df['TweetTokens'] = tweets_processed\n",
        "  # print(df)\n",
        "  return df"
      ],
      "metadata": {
        "id": "pfmJjAHrujuX"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_EI_reg_train = preprocess_tweets(df_EI_reg_train)\n",
        "# df_V_reg_train = preprocess_tweets(df_V_reg_train)\n",
        "\n",
        "for name, df in dict_df.items():\n",
        "  df = preprocess_tweets(df)\n"
      ],
      "metadata": {
        "id": "hxt99UC2xI2x"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_V_reg_train"
      ],
      "metadata": {
        "id": "-sRSUuUl7BfC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : \n",
        "* remove stop words\n",
        "* stem\n",
        "* lemmetize\n"
      ],
      "metadata": {
        "id": "3los9BZbk2m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_V_reg_train.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtRWK8D6mMzk",
        "outputId": "d473ff86-210c-41ea-af3a-35c851a0b4ef"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'Tweet', 'Affect Dimension', 'Intensity Score', 'TweetTokens'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def subset_df(df):\n",
        "  return df[['TweetTokens','Intensity Score']]"
      ],
      "metadata": {
        "id": "_zoGr_W3mYgK"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_df_subset ={name+\"_subset\": subset_df(df) for name, df in dict_df.items() }"
      ],
      "metadata": {
        "id": "kDsCZb5ExgA9"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (dict_df_subset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcw_mO9Y1bP-",
        "outputId": "abb94638-d837-46c0-9cea-ad73d91d4b13"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'df_EI_reg_train_subset':                                             TweetTokens  Intensity Score\n",
            "0     <user> <user> shut up hashtags are cool <hasht...            0.562\n",
            "1     it makes me so fucking irate jesus . nobody is...            0.750\n",
            "2     lol adam the bull with his fake outrage . <rep...            0.417\n",
            "3     <user> passed away early this morning in a fas...            0.354\n",
            "4     <user> lol wow i was gonna say really ? ! <rep...            0.438\n",
            "...                                                 ...              ...\n",
            "1696  got a <money> tip from a drunk uber passenger ...            0.708\n",
            "1697  <user> <user> <user> <user> fucker blocked me ...            0.625\n",
            "1698                                <user> i look rabid            0.472\n",
            "1699  <user> i am not surprised , i would be fuming ...            0.479\n",
            "1700  <user> the pout tips me over the edge . i am m...            0.490\n",
            "\n",
            "[1701 rows x 2 columns], 'df_EI_reg_val_subset':                                            TweetTokens  Intensity Score\n",
            "0    ' we need to do something . something must be ...            0.517\n",
            "1    <user> <user> <user> <user> <user> we ' d be f...            0.726\n",
            "2    caleb had a nightmare about zombies . i had a ...            0.250\n",
            "3    <hashtag> cnn </hashtag> really needs to get o...            0.641\n",
            "4    <hashtag> dm me </hashtag> <hashtag> kik me </...            0.313\n",
            "..                                                 ...              ...\n",
            "383  time to <hashtag> impeach trump </hashtag> . h...            0.813\n",
            "384  had frustration dream that left me utterly f**...            0.844\n",
            "385  <user> i love you , but between online order d...            0.778\n",
            "386  quick note about insta stories how the f do yo...            0.500\n",
            "387  <hashtag> revenge </hashtag> or <hashtag> chan...            0.397\n",
            "\n",
            "[388 rows x 2 columns], 'df_EI_reg_test_subset':                                             TweetTokens  Intensity Score\n",
            "0     <user> i know you mean well but i am offended ...            0.734\n",
            "1     let go of resentment , it will hold you back ,...            0.422\n",
            "2     no , i am not ' depressed because of the weath...            0.663\n",
            "3     <hashtag> amarnath terror attack </hashtag> mu...            0.703\n",
            "4     prepare to suffer the sting of ghost rider ' s...            0.719\n",
            "...                                                 ...              ...\n",
            "997   that morning when you get half - way to work a...            0.629\n",
            "998   <user> <user> <user> i bet he needed to take a...            0.550\n",
            "999   <user> <user> <user> <user> ring a ring of ros...            0.424\n",
            "1000  have to go to a occupational services place fo...            0.597\n",
            "1001       of course molina <hashtag> bitter </hashtag>            0.547\n",
            "\n",
            "[1002 rows x 2 columns], 'df_V_reg_train_subset':                                             TweetTokens  Intensity Score\n",
            "0                    <user> yeah ! <happy> playing well            0.600\n",
            "1     at least i do not have a guy trying to discour...            0.484\n",
            "2     <allcaps> uplift </allcaps> : if you are still...            0.563\n",
            "3     . <repeated> at your age , the heyday in the b...            0.450\n",
            "4     i was so embarrassed when she saw us i was lik...            0.233\n",
            "...                                                 ...              ...\n",
            "1176  <user> do not think ian knew of pavel . he kne...            0.613\n",
            "1177  i lost my wallet lol . <repeated> again . <rep...            0.234\n",
            "1178  repentance , and trusting in christ . it is lo...            0.700\n",
            "1179  <user> :flushed_face: chewing what ? <hashtag>...            0.565\n",
            "1180  i am so anxious all the time leaving the house...            0.278\n",
            "\n",
            "[1181 rows x 2 columns], 'df_V_reg_val_subset':                                            TweetTokens  Intensity Score\n",
            "0    so <user> site crashes everytime i try to book...            0.141\n",
            "1    theme of week : ask the lord for strength & pe...            0.317\n",
            "2    <user> why announcing so late , it will be har...            0.145\n",
            "3    the greatest happiness is seeing someone you l...            0.813\n",
            "4    omg so grateful to have an education but ive b...            0.672\n",
            "..                                                 ...              ...\n",
            "444  idk why but when i help someone , a smile come...            0.903\n",
            "445  i think ' sleep ' is my favorite from how did ...            0.328\n",
            "446  what does amelia want ? ! <repeated> sarah was...            0.690\n",
            "447  it ’ s lack of <hashtag> faith </hashtag> that...            0.500\n",
            "448  james clapper ' scary and disturbing ' . <hash...            0.296\n",
            "\n",
            "[449 rows x 2 columns], 'df_V_reg_test_subset':                                            TweetTokens  Intensity Score\n",
            "0         gm and have a <hashtag> tuesday </hashtag> !            0.589\n",
            "1    <user> but you have a lot of time for tweeting...            0.500\n",
            "2    i graduated yesterday and already had <number>...            0.550\n",
            "3    <user> seriously . <repeated> i have been sitt...            0.633\n",
            "4    whether my glass is half empty or its half ful...            0.750\n",
            "..                                                 ...              ...\n",
            "932  premier league teams should fear next seasons ...            0.507\n",
            "933  how are you my love ? <user> love youu ! <repe...            0.867\n",
            "934  ' she is the clothed with strength and dignity...            0.516\n",
            "935  my dads big day is only less than <number> wee...            0.823\n",
            "936  and let the depression take the stage once mor...            0.141\n",
            "\n",
            "[937 rows x 2 columns]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZT_7DbHv8W44"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Pytorch Datasets"
      ],
      "metadata": {
        "id": "Wi-dG9KP14u8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Vocabulary\n",
        "Before we create the Dataset, we need to define a process to build our vocabulary. For this,\n",
        "We’ll create a “Vocabulary” class which will create the word-to-index and index-to-word mappings using only the train dataframe we created before\n",
        "Also, the “Vocabulary” class returns the numericalized version of each sentence in our dataframe. Eg: [‘i’, ‘love’, ‘apple’] -> [23, 54, 1220]. We need to convert the words to numbers as models expect each word in our vocabulary to be represented by a number"
      ],
      "metadata": {
        "id": "IUsh5RUf2BxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#               Define Vocabulary Class\n",
        "#######################################################\n",
        "\n",
        "class Vocabulary:\n",
        "  \n",
        "    '''\n",
        "    __init__ method is called by default as soon as an object of this class is initiated\n",
        "    we use this method to initiate our vocab dictionaries\n",
        "    '''\n",
        "    def __init__(self, freq_threshold = 3, max_size = 10000):\n",
        "        '''\n",
        "        freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
        "        max_size : max source vocab size. Eg. if set to 10,000, we pick the top 10,000 most frequent words and discard others\n",
        "        '''\n",
        "        #initiate the index to token dict\n",
        "        ## <PAD> -> padding, used for padding the shorter sentences in a batch to match the length of longest sentence in the batch\n",
        "        ## <UNK> -> words which are not found in the vocab are replace by this token\n",
        "        # self.itos = {0: '<PAD>', 1: '<UNK>', 2:'<NUMBER>', 3: '<CURRENCY>', 4: '<URL>'}\n",
        "        self.itos = {0: '<PAD>', 1: '<UNK>'}\n",
        "        \n",
        "        \n",
        "        #initiate the token to index dict\n",
        "        self.stoi = {k:j for j,k in self.itos.items()}\n",
        "        self.original_stoi = self.stoi.copy()\n",
        "#         print(self.stoi)\n",
        "        \n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.max_size = max_size\n",
        "    \n",
        "    '''\n",
        "    __len__ is used by dataloader later to create batches\n",
        "    '''\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "    \n",
        "    '''\n",
        "    a simple tokenizer to split on space and converts the sentence to list of words\n",
        "    '''\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "#         return [tok.strip() for tok in text.split(' ')]\n",
        "        return [tok.lower().strip() for tok in text.split(' ')] # this is commented out to avoid <NUMBER> ,<UNK> lowering\n",
        "#         return [tok.lower().strip() for tok in text.split(' ') if tok not in list(self.stoi.keys())] \n",
        "    \n",
        "    '''\n",
        "    build the vocab: create a dictionary mapping of index to string (itos) and string to index (stoi)\n",
        "    output ex. for stoi -> {'the':6, 'a':7, 'an':8}\n",
        "    '''\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        #calculate the frequencies of each word first to remove the words with freq < freq_threshold\n",
        "#         frequencies = {}  #init the freq dict\n",
        "        frequencies = {k:self.max_size+1 for _,k in self.itos.items()}  # updated so that intial ones are also part of this\n",
        "        \n",
        "        # idx = 5 #index from which we want our dict to start. We already used 4 indexes for pad, unk...\n",
        "        idx = len(self.original_stoi)\n",
        "        \n",
        "        #calculate freq of words\n",
        "        for sentence in sentence_list:\n",
        "            list_word = [tok.lower().strip() for tok in sentence.split(' ') if tok not in list(self.stoi.keys())] \n",
        "            for word in list_word:\n",
        "#             for word in self.tokenizer(sentence):\n",
        "                \n",
        "                if word not in frequencies.keys():\n",
        "                    frequencies[word]=1\n",
        "                else:\n",
        "                    \n",
        "                    frequencies[word]+=1\n",
        "                    \n",
        "#         print (\"----2-----\\n\",frequencies)\n",
        "        \n",
        "        #limit vocab by removing low freq words\n",
        "        frequencies = {k:v for k,v in frequencies.items() if v>self.freq_threshold} \n",
        "        \n",
        "#         print (\"----3-----\\n\",frequencies)\n",
        "        \n",
        "        #limit vocab to the max_size specified\n",
        "        frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx =5 for pad, unk...\n",
        "        \n",
        "#         print (\"----4-----\\n\",frequencies)\n",
        "            \n",
        "        #create vocab\n",
        "        for key in set(self.stoi.keys()):\n",
        "            frequencies.pop(key)\n",
        "        \n",
        "#         print (\"----5-----\\n\",frequencies)\n",
        "        \n",
        "        for word in frequencies.keys():\n",
        "            self.stoi[word] = idx\n",
        "            self.itos[idx] = word\n",
        "            idx+=1\n",
        "        \n",
        "#         print (\"----6-----\\n\",self.stoi)\n",
        "        \n",
        "    '''\n",
        "    convert the list of words to a list of corresponding indexes\n",
        "    '''    \n",
        "    def numericalize(self, text):\n",
        "        #tokenize text\n",
        "#         tokenized_text = self.tokenizer(text)\n",
        "#         print(\"---------\\n\",self.original_stoi.keys())\n",
        "        tokenized_text = []\n",
        "        for tok in text.split(' '):\n",
        "            if tok not in list(self.original_stoi.keys()):\n",
        "                tokenized_text.append(tok.lower().strip())\n",
        "            else:\n",
        "                tokenized_text.append(tok.strip())\n",
        "                \n",
        "#         tokenized_text = [tok.lower().strip() for tok in text.split(' ') if tok not in list(self.original_stoi.keys())]\n",
        "        numericalized_text = []\n",
        "        for token in tokenized_text:\n",
        "            if token in self.stoi.keys():\n",
        "                numericalized_text.append(self.stoi[token])\n",
        "            else: #out-of-vocab words are represented by UNK token index\n",
        "                numericalized_text.append(self.stoi['<UNK>'])\n",
        "                \n",
        "        return numericalized_text"
      ],
      "metadata": {
        "id": "F7xZg1XcwLtt"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # #create a vocab class with freq_threshold=0 and max_size=100\n",
        "# voc = Vocabulary(0, 100)\n",
        "# sentence_list = ['that is a cat CAT', 'that is not a dog']\n",
        "# #build vocab\n",
        "# voc.build_vocabulary(sentence_list)\n",
        "\n",
        "# print('index to string: ',voc.itos)\n",
        "# print('string to index:',voc.stoi)\n",
        "\n",
        "# print('numericalize -> cat and a dog <URL>: ', voc.numericalize('cat and a dog <NUMBER>'))"
      ],
      "metadata": {
        "id": "COQ10m7Pv-sR"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating sentence list of all the training dataframes to create vocabulary later, this would mean a more robust vocab\n",
        "sentence_list = []\n",
        "for name,df in dict_df_subset.items():\n",
        "  if \"train\" in name:\n",
        "    sentence_list.extend(df.TweetTokens.to_list())\n",
        "print(len(sentence_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vuabth4RCSO",
        "outputId": "0370f471-2dcc-44bb-cd9b-379bc1054c82"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_threshold = 3\n",
        "vocab_max_size = 50000\n",
        "\n",
        "vocab = Vocabulary(freq_threshold, vocab_max_size)\n",
        "vocab.build_vocabulary(sentence_list)"
      ],
      "metadata": {
        "id": "WW44PGJKRbpS"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xa1pUuSH8Uoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Train_Dataset\n",
        "We first inherit PyTorch's Dataset class.\n",
        "Then, we initialize and build the vocabs for subject in our train data frame.\n",
        "Then, we use the getitem() method to numericalize the subject 1 example at a time for the data loader (a function to load data in batches)."
      ],
      "metadata": {
        "id": "uwvxG4Op8YUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#               Define Train_Dataset class\n",
        "#######################################################\n",
        "\n",
        "class Train_Dataset(Dataset):\n",
        "    '''\n",
        "    Initiating Variables\n",
        "    df: the training dataframe\n",
        "    subject : the name of target text column in the dataframe\n",
        "    transform : If we want to add any augmentation\n",
        "    freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
        "    vocab_max_size : max  vocab size\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, df, subject, label_col, vocab , max_sentence_length = 150, transform=None, freq_threshold = 5,\n",
        "                vocab_max_size = 50000):\n",
        "    \n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        \n",
        "        #get body and label\n",
        "        self.subject_texts = self.df[subject]\n",
        "        self.labels = self.df[label_col]\n",
        "        self.vocab = vocab\n",
        "        \n",
        "        # ##VOCAB class has been created above\n",
        "        # #Initialize vocab object and build vocabulary\n",
        "        # self.vocab = Vocabulary(freq_threshold, vocab_max_size)\n",
        "        # self.vocab.build_vocabulary(self.subject_texts.tolist())\n",
        "        self.max_sentence_length = max_sentence_length\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    '''\n",
        "    __getitem__ runs on 1 example at a time. Here, we get an example at index and return its numericalize source and\n",
        "    target values using the vocabulary objects we created in __init__\n",
        "    '''\n",
        "    def __getitem__(self, index):\n",
        "        subject_text = self.subject_texts[index]\n",
        "        label = self.labels[index]\n",
        "#         print(subject_text)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            subject_text = self.transform(subject_text)\n",
        "            \n",
        "        #numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
        "        numerialized_subject =[]\n",
        "        numerialized_subject += self.vocab.numericalize(subject_text)\n",
        "        \n",
        "        while len(numerialized_subject) < self.max_sentence_length:\n",
        "            numerialized_subject.append(0)\n",
        "        \n",
        "        #convert the list to tensor and return\n",
        "        return torch.tensor(numerialized_subject[:self.max_sentence_length]),torch.tensor(label)\n",
        "#         return torch.tensor(numerialized_subject[:self.train_dataset.max_sentence_length]),label"
      ],
      "metadata": {
        "id": "LRsK6LkK8iij"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_df_subset.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts9ZW7fxiDH0",
        "outputId": "9198029e-91b1-4197-ca23-e101424d0cb4"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['df_EI_reg_train_subset', 'df_EI_reg_val_subset', 'df_EI_reg_test_subset', 'df_V_reg_train_subset', 'df_V_reg_val_subset', 'df_V_reg_test_subset'])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = dict_df_subset['df_EI_reg_train_subset']\n",
        "df_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "-XKkT1VABUHX",
        "outputId": "acdfb176-9025-446c-f2d8-032821862dde"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            TweetTokens  Intensity Score\n",
              "0     <user> <user> shut up hashtags are cool <hasht...            0.562\n",
              "1     it makes me so fucking irate jesus . nobody is...            0.750\n",
              "2     lol adam the bull with his fake outrage . <rep...            0.417\n",
              "3     <user> passed away early this morning in a fas...            0.354\n",
              "4     <user> lol wow i was gonna say really ? ! <rep...            0.438\n",
              "...                                                 ...              ...\n",
              "1696  got a <money> tip from a drunk uber passenger ...            0.708\n",
              "1697  <user> <user> <user> <user> fucker blocked me ...            0.625\n",
              "1698                                <user> i look rabid            0.472\n",
              "1699  <user> i am not surprised , i would be fuming ...            0.479\n",
              "1700  <user> the pout tips me over the edge . i am m...            0.490\n",
              "\n",
              "[1701 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e390272-f59e-4210-9f04-0c704ee0364c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TweetTokens</th>\n",
              "      <th>Intensity Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;user&gt; &lt;user&gt; shut up hashtags are cool &lt;hasht...</td>\n",
              "      <td>0.562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it makes me so fucking irate jesus . nobody is...</td>\n",
              "      <td>0.750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lol adam the bull with his fake outrage . &lt;rep...</td>\n",
              "      <td>0.417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;user&gt; passed away early this morning in a fas...</td>\n",
              "      <td>0.354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;user&gt; lol wow i was gonna say really ? ! &lt;rep...</td>\n",
              "      <td>0.438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1696</th>\n",
              "      <td>got a &lt;money&gt; tip from a drunk uber passenger ...</td>\n",
              "      <td>0.708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1697</th>\n",
              "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; &lt;user&gt; fucker blocked me ...</td>\n",
              "      <td>0.625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1698</th>\n",
              "      <td>&lt;user&gt; i look rabid</td>\n",
              "      <td>0.472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1699</th>\n",
              "      <td>&lt;user&gt; i am not surprised , i would be fuming ...</td>\n",
              "      <td>0.479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1700</th>\n",
              "      <td>&lt;user&gt; the pout tips me over the edge . i am m...</td>\n",
              "      <td>0.490</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1701 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e390272-f59e-4210-9f04-0c704ee0364c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8e390272-f59e-4210-9f04-0c704ee0364c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8e390272-f59e-4210-9f04-0c704ee0364c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_train_dataset ={}\n",
        "for name,df in dict_df_subset.items():\n",
        "  if \"train\" in name:\n",
        "    dataset_name = name+\"_dataset\"\n",
        "    # vars()[dataset_name] = Train_Dataset(df,'TweetTokens','Intensity Score', max_sentence_length =200) # dynamically assigning datasetname\n",
        "    dict_train_dataset[dataset_name] = Train_Dataset(df,'TweetTokens','Intensity Score', vocab, max_sentence_length =200) # dynamically assigning datasetname"
      ],
      "metadata": {
        "id": "MWMm8D1t9dcs"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict_train_dataset.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeQ0pW6shCpT",
        "outputId": "f2d40304-3ec7-4a77-d29e-971799210d5f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['df_EI_reg_train_subset_dataset', 'df_V_reg_train_subset_dataset'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i = random.randint(0,len(dict_train_dataset['df_EI_reg_train_subset_dataset']))\n",
        "# print(dict_df_subset['df_EI_reg_train_subset'].loc[i][['TweetTokens','Intensity Score']])\n",
        "# print((dict_train_dataset['df_EI_reg_train_subset_dataset'][i][0]))\n",
        "# print((dict_train_dataset['df_EI_reg_train_subset_dataset'][i][1]))"
      ],
      "metadata": {
        "id": "CHSbz2sgGrS8"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_obj = Train_Dataset(df_train,'TweetTokens','Intensity Score', vocab,max_sentence_length =200)"
      ],
      "metadata": {
        "id": "NBq8Pz1JA7wO"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i = random.randint(0,len(train_dataset_obj))\n",
        "# print(df_train.loc[i][['TweetTokens','Intensity Score']])\n",
        "# print((train_dataset_obj[i][1]))\n",
        "# print(len(train_dataset_obj[i][0]))\n",
        "# print(train_dataset_obj[i][0])"
      ],
      "metadata": {
        "id": "cxIIq74eCBTI"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df_EI_reg_train_subset_dataset)"
      ],
      "metadata": {
        "id": "arW3Nf90AwCV"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i = random.randint(0,len(df_EI_reg_train_subset_dataset))\n",
        "# # print(train.loc[i][['body','label']])\n",
        "# print(type(df_EI_reg_train_subset_dataset[i][1]))\n",
        "# len(df_EI_reg_train_subset_dataset[i][0])"
      ],
      "metadata": {
        "id": "5csRi0f3AsAA"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kRfZcZJPQ0ej"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Validation Dataset"
      ],
      "metadata": {
        "id": "t0ji62MwaU8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#               Define Dataset Class\n",
        "#######################################################\n",
        "\n",
        "class Validation_Dataset(Dataset):\n",
        "    def __init__(self, train_dataset, df, subject, label_col, transform = None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        \n",
        "        #train dataset will be used as lookup for vocab\n",
        "        self.train_dataset = train_dataset\n",
        "        \n",
        "        #get body and label\n",
        "        self.subject_texts = self.df[subject]\n",
        "        self.labels = self.df[label_col]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        \n",
        "        subject_text = self.subject_texts[index]\n",
        "        label = self.labels[index]\n",
        "#         print(subject_text)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            subject_text = self.transform(subject_text)\n",
        "            \n",
        "            \n",
        "\n",
        "        #numericalize texts ['cat', 'in', 'a', 'bag'] -> [12,2,9,24]\n",
        "        numerialized_subject = []\n",
        "        numerialized_subject += self.train_dataset.vocab.numericalize(subject_text)\n",
        "#         print(\"max sentence length\", self.train_dataset.max_sentence_length)\n",
        "        while len(numerialized_subject) < self.train_dataset.max_sentence_length:\n",
        "            numerialized_subject.append(0)\n",
        "            \n",
        "        #convert the list to tensor and return\n",
        "#         return torch.tensor(numerialized_subject),label\n",
        "\n",
        "#         #convert the list to tensor and return\n",
        "        return torch.tensor(numerialized_subject[:self.train_dataset.max_sentence_length]),torch.tensor(label)\n"
      ],
      "metadata": {
        "id": "Bzk0WNQmaiO7"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_val_dataset = {}\n",
        "for name,df in dict_df_subset.items():\n",
        "  if \"train\" in name:\n",
        "    train_dataset_name = name+\"_dataset\"\n",
        "    val_df_name = name[:len(name)-13]+\"_val_subset\"\n",
        "    val_dataset_name = val_df_name + \"_dataset\"\n",
        "    dict_val_dataset[val_dataset_name] = Validation_Dataset(dict_train_dataset[train_dataset_name], dict_df_subset[val_df_name], 'TweetTokens','Intensity Score', transform = None)"
      ],
      "metadata": {
        "id": "OtFQ6zwgdWIw"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_val_dataset.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQR41i-MiPnq",
        "outputId": "1c8cf963-96bd-405b-90f2-0723ce66ea41"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['df_EI_reg_val_subset_dataset', 'df_V_reg_val_subset_dataset'])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i = random.randint(0,len(dict_val_dataset['df_EI_reg_val_subset_dataset']))\n",
        "# # i = 3774\n",
        "# print(\"i=\",i)\n",
        "# print(dict_df_subset['df_EI_reg_val_subset'].loc[i][['TweetTokens','Intensity Score']])\n",
        "\n",
        "# print((dict_val_dataset['df_EI_reg_val_subset_dataset'][i][0]))\n",
        "# print((dict_val_dataset['df_EI_reg_val_subset_dataset'][i][1]))"
      ],
      "metadata": {
        "id": "spuu-fgFk-k8"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dataloader\n"
      ],
      "metadata": {
        "id": "YW_4wbJOm8Ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#######################################################\n",
        "#            Define Dataloader Functions\n",
        "#######################################################\n",
        "\n",
        "# If we run a next(iter(data_loader)) we get an output of batch_size * (num_workers+1)\n",
        "def get_loader(dataset, batch_size, num_workers=1, shuffle=True, pin_memory=True): #increase num_workers according to CPU\n",
        "    loader = DataLoader(dataset, batch_size = batch_size, num_workers = num_workers,\n",
        "                        shuffle=shuffle,\n",
        "                       pin_memory=pin_memory)\n",
        "    \n",
        "    return loader\n"
      ],
      "metadata": {
        "id": "aibKT1fey5Ij"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_train_loader = {}\n",
        "batch_size = 8\n",
        "for name, dataset in dict_train_dataset.items():\n",
        "  name_dataloader = name+\"_dataloader\"\n",
        "  dict_train_loader[name_dataloader] = get_loader(dataset, batch_size)\n",
        "  x = next(iter(dict_train_loader[name_dataloader]))\n",
        "  print(name_dataloader, x[0].shape, x[1].shape, type(x[0]), type (x[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZGamL_ovHwX",
        "outputId": "ffb5f822-1b8b-43bd-b600-03dff6fe71d3"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_EI_reg_train_subset_dataset_dataloader torch.Size([8, 200]) torch.Size([8]) <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "df_V_reg_train_subset_dataset_dataloader torch.Size([8, 200]) torch.Size([8]) <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_val_loader = {}\n",
        "batch_size = 8 \n",
        "for name, dataset in dict_val_dataset.items():\n",
        "  name_dataloader = name+\"_dataloader\"\n",
        "  dict_val_loader[name_dataloader] = get_loader(dataset,batch_size)\n",
        "  x = next(iter(dict_val_loader[name_dataloader]))\n",
        "  print(name_dataloader, x[0].shape, x[1].shape, type(x[0]), type (x[1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unm6tZwcwK9Q",
        "outputId": "243d7a5a-2827-43e3-8582-61680060d655"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_EI_reg_val_subset_dataset_dataloader torch.Size([8, 200]) torch.Size([8]) <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "df_V_reg_val_subset_dataset_dataloader torch.Size([8, 200]) torch.Size([8]) <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(dict_val_loader['df_EI_reg_val_subset_dataset_dataloader'] ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E1ZxVrHz9YR",
        "outputId": "696c4520-1435-4f4b-cad7-91ec6fd14fec"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[598,  43,   1,  ...,   0,   0,   0],\n",
              "         [  1,   1,  87,  ...,   0,   0,   0],\n",
              "         [  5,  33, 185,  ...,   0,   0,   0],\n",
              "         ...,\n",
              "         [  5,   1,  78,  ...,   0,   0,   0],\n",
              "         [ 25, 655,   4,  ...,   0,   0,   0],\n",
              "         [  5,  78, 337,  ...,   0,   0,   0]]),\n",
              " tensor([0.2810, 0.5160, 0.6450, 0.3770, 0.6560, 0.3330, 0.4060, 0.4700],\n",
              "        dtype=torch.float64)]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings "
      ],
      "metadata": {
        "id": "Ju9qo16gfYpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Creating sentence list of all the training dataframes to create vocabulary later, this would mean a more robust vocab\n",
        "# sentence_list = []\n",
        "# for name,df in dict_df_subset.items():\n",
        "#   if \"train\" in name:\n",
        "#     sentence_list.extend(df.TweetTokens.to_list())\n",
        "# print(len(sentence_list))"
      ],
      "metadata": {
        "id": "QE30YEqsh8jh"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freq_threshold = 3\n",
        "# vocab_max_size = 50000\n",
        "\n",
        "# vocab = Vocabulary(freq_threshold, vocab_max_size)\n",
        "# vocab.build_vocabulary(sentence_list)"
      ],
      "metadata": {
        "id": "dXKmUgn6i5O0"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab.stoi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf74AQ4glAkg",
        "outputId": "b2256fb0-2bb6-4051-e3cf-35bbfa2f255f"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1619"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "global_vectors = GloVe(name='840B', dim=300)\n"
      ],
      "metadata": {
        "id": "G_cbk8uefeq_"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_pretrained_vectors(word2idx, embedding_name = 'glove', embedding_file = global_vectors):\n",
        "    \"\"\"Load pretrained vectors and create embedding layers.\n",
        "    \n",
        "    Args:\n",
        "        word2idx - vocab.stoi (Dict): Vocabulary built from the corpus\n",
        "        embedding_name (str): the type of embedding - glove for GloVe or word2vec for word2vec\n",
        "        embedding_file (object) :optional embedding file\n",
        "\n",
        "    Returns:\n",
        "        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n",
        "            the size of word2idx and d is embedding dimension\n",
        "    \"\"\"\n",
        "\n",
        "    if embedding_name == 'glove':\n",
        "      print(\"Loading pretrained vectors...\")\n",
        "      if embedding_file:\n",
        "        global_vectors = embedding_file\n",
        "      else:\n",
        "        global_vectors = GloVe(name='840B', dim=300)\n",
        "\n",
        "      print(\"Processing pretrained vectors...\")\n",
        "      d = 300\n",
        "      print(\"\\ndimension of pretained embedding: \", d)\n",
        "\n",
        "      # Initilize random embeddings\n",
        "      embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
        "      embeddings[word2idx['<PAD>']] = np.zeros((d,))\n",
        "      \n",
        "      # Load pretrained vectors\n",
        "      count = 0 \n",
        "      for word in global_vectors.stoi:\n",
        "        if word in word2idx:\n",
        "            count +=1\n",
        "            embeddings[word2idx[word]] = global_vectors[word]\n",
        "      print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n",
        "        \n",
        "      print(\"Process Completed...\")\n",
        "      return embeddings\n",
        "\n",
        "    else:\n",
        "      print(\" Embedding not implemented, returning zero embedding\")\n",
        "      return np.zeros(len(word2idx), 300)\n",
        "    \n",
        "    \n",
        "#     # downloaded word2vec from https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
        "\n",
        "#     word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
        "    \n",
        "#     print(\"Processing pretrained vectors...\")\n",
        "#     d = word2vec.vector_size\n",
        "#     print(\"\\ndimension of pretained embedding: \", d)\n",
        "    \n",
        "#     # Initilize random embeddings\n",
        "#     embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
        "#     embeddings[word2idx['<PAD>']] = np.zeros((d,))\n",
        "\n",
        "#     # Load pretrained vectors\n",
        "#     count = 0 \n",
        "#     for word in word2vec.key_to_index:\n",
        "#         if word in word2idx:\n",
        "#             count +=1\n",
        "#             embeddings[word2idx[word]] = word2vec.get_vector(word)\n",
        "    \n",
        "    \n",
        "# #     count = 0\n",
        "# #     for line in tqdm_notebook(fin):\n",
        "# #         tokens = line.rstrip().split(' ')\n",
        "# #         word = tokens[0]\n",
        "# #         if word in word2idx:\n",
        "# #             count += 1\n",
        "# #             embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n",
        "\n",
        "#     print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n",
        "    \n",
        "#     print(\"Process Completed...\")\n",
        "#     return embeddings\n"
      ],
      "metadata": {
        "id": "PUeTKJw_gAGn"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_embeddings = load_pretrained_vectors(vocab.stoi, embedding_name = 'glove', embedding_file = global_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6wQMWKQz9cf",
        "outputId": "6a109eba-2659-41c5-95bd-992944643666"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained vectors...\n",
            "Processing pretrained vectors...\n",
            "\n",
            "dimension of pretained embedding:  300\n",
            "There are 1548 / 1619 pretrained vectors found.\n",
            "Process Completed...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pre_trained_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5yEsQYDUEA0",
        "outputId": "704930ed-fb70-4a52-d7ac-4096c000a1f4"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1619, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_tensor = torch.tensor(pre_trained_embeddings)\n",
        "embedding_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ISwnOQS0TWH",
        "outputId": "daf08535-cf6d-437f-9909-7d333c1326a4"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1619, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation"
      ],
      "metadata": {
        "id": "Q1MBzRYR0--X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Reversal Layer Function"
      ],
      "metadata": {
        "id": "Ecg7Pzsm1CkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "class ReverseLayerF(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None"
      ],
      "metadata": {
        "id": "Bgxif9of08dQ"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN 1-D Model\n",
        "\n",
        "Reference: A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification, Ye Zhang, Byron Wallace 2015\n",
        "\n",
        "Difference: \n",
        "\n",
        "1.   use of embedding\n",
        "2.   use of sigmoid function, as we are having a regression model not a classififer as the main task\n",
        "\n"
      ],
      "metadata": {
        "id": "Hgp_oFzHTPP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_SA(nn.Module):\n",
        "    \"\"\"An 1D Convulational Neural Network for Sentiment Analysis.\"\"\"\n",
        "    def __init__(self,\n",
        "                 pretrained_embedding=None,\n",
        "                 freeze_embedding=False,\n",
        "                 vocab_size=None,\n",
        "                 embed_dim=300,\n",
        "                 filter_sizes=[2, 3, 4, 5],\n",
        "                 num_filters=[ 100, 100, 100, 100],\n",
        "                 num_classes=2,\n",
        "                 dropout=0.25):\n",
        "        \"\"\"\n",
        "        The constructor for CNN_SA class.\n",
        "\n",
        "        Args:\n",
        "            pretrained_embedding (torch.Tensor): Pretrained embeddings with\n",
        "                shape (vocab_size, embed_dim)\n",
        "            freeze_embedding (bool): Set to False to fine-tune pretraiend\n",
        "                vectors. Default: False\n",
        "            vocab_size (int): Need to be specified when not pretrained word\n",
        "                embeddings are not used.\n",
        "            embed_dim (int): Dimension of word vectors. Need to be specified\n",
        "                when pretrained word embeddings are not used. Default: 300\n",
        "            filter_sizes (List[int]): List of filter sizes. Default: [2, 3, 4, 5]\n",
        "            num_filters (List[int]): List of number of filters, has the same\n",
        "                length as `filter_sizes`. Default: [100, 100, 100, 100]\n",
        "            n_classes (int): Number of classes (domain classification usage). \n",
        "            Default: 2\n",
        "            dropout (float): Dropout rate. Default: 0.25\n",
        "        \"\"\"\n",
        "\n",
        "        super(CNN_SA, self).__init__()\n",
        "        \n",
        "         #---------------------Feature Extractor Network----------------------#\n",
        "        # Embedding layer\n",
        "        if pretrained_embedding is not None:\n",
        "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
        "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
        "                                                          freeze=freeze_embedding)\n",
        "        else:\n",
        "            self.embed_dim = embed_dim\n",
        "            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                          embedding_dim=self.embed_dim,\n",
        "                                          padding_idx=0,\n",
        "                                          max_norm=5.0)\n",
        "        # Conv Network\n",
        "        self.feature_extractor = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=self.embed_dim,\n",
        "                      out_channels=num_filters[i],\n",
        "                      kernel_size=filter_sizes[i])\n",
        "            for i in range(len(filter_sizes))\n",
        "        ])\n",
        "        \n",
        "        #---------------------Regression Network------------------------#\n",
        "        # Fully-connected layer and Dropout\n",
        "        self.regression = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(np.sum(num_filters), np.sum(num_filters)//2),\n",
        "            nn.ReLU(),\n",
        "#             nn.BatchNorm1d(np.sum(num_filters)//2),\n",
        "            # nn.Linear(np.sum(num_filters)//2, num_classes), # for classification\n",
        "            nn.Linear(np.sum(num_filters)//2, 1), # for regression\n",
        "            # nn.LogSoftmax(dim=1) # for classification\n",
        "            nn.Sigmoid() # for regession (values between 0 and 1)\n",
        "        )\n",
        "        \n",
        "        #---------------------Domain Classifier Network------------------------#\n",
        "        # Fully-connected layer and Dropout\n",
        "        self.domain_classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(np.sum(num_filters), np.sum(num_filters)//2),\n",
        "            nn.ReLU(),\n",
        "#             nn.BatchNorm1d(np.sum(num_filters)//2),\n",
        "            nn.Linear(np.sum(num_filters)//2, num_classes),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, input_ids,alpha=1):\n",
        "        \"\"\"Perform a forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): A tensor of token ids with shape\n",
        "                (batch_size, max_sent_length)\n",
        "\n",
        "        Returns:\n",
        "            sigmoid (torch.Tensor) : Output sigmoid \n",
        "            logits (torch.Tensor): Output logits with shape (batch_size,\n",
        "                n_classes)\n",
        "        \"\"\"\n",
        "\n",
        "        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n",
        "#         input_ids = torch.tensor(input_ids).to(torch.int64)\n",
        "        input_ids = input_ids.clone().detach().to(torch.int64)\n",
        "#         print(input_ids.shape)\n",
        "        \n",
        "        x_embed = self.embedding((input_ids)).float()\n",
        "#         print(x_embed.shape)\n",
        "\n",
        "\n",
        "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
        "        # Output shape: (b, embed_dim, max_len)\n",
        "        x_reshaped = x_embed.permute(0, 2, 1)\n",
        "#         print(x_reshaped.shape)\n",
        "        \n",
        "\n",
        "#         # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
        "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.feature_extractor]\n",
        "#         print(x_conv_list[1].shape)\n",
        "\n",
        "#         # Max pooling. Output shape: (b, num_filters[i], 1)\n",
        "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
        "            for x_conv in x_conv_list]\n",
        "#         print( x_pool_list[3].shape)\n",
        "        \n",
        "#         # Concatenate x_pool_list to feed the fully connected layer.\n",
        "#         # Output shape: (b, sum(num_filters))\n",
        "        x_feature = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
        "                         dim=1)\n",
        "#         print(x_feature.shape)\n",
        "        \n",
        "# #         # Compute logits. Output shape: (b, n_classes)\n",
        "#         logits = self.fc(self.dropout(x_feature))\n",
        "#         print(logits)\n",
        "\n",
        "        reverse_feature = ReverseLayerF.apply(x_feature, alpha)\n",
        "    \n",
        "        regression_output = self.regression(x_feature)\n",
        "    \n",
        "        domain_classifier_output = self.domain_classifier(reverse_feature)\n",
        "#         print(domain_classifier_logits.shape)\n",
        "\n",
        "#         return logits\n",
        "        return regression_output, domain_classifier_output\n"
      ],
      "metadata": {
        "id": "Hn38ChCwTT-W"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = CNN_SA(pretrained_embedding=embedding_tensor,freeze_embedding=True).to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eCFudXBYHjM",
        "outputId": "2b2cf64d-918c-4544-ef1c-14b8fdec6e7b"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "CNN_SA(\n",
            "  (embedding): Embedding(1619, 300)\n",
            "  (feature_extractor): ModuleList(\n",
            "    (0): Conv1d(300, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
            "    (3): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (regression): Sequential(\n",
            "    (0): Dropout(p=0.25, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=1, bias=True)\n",
            "    (4): Sigmoid()\n",
            "  )\n",
            "  (domain_classifier): Sequential(\n",
            "    (0): Dropout(p=0.25, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=2, bias=True)\n",
            "    (4): LogSoftmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for name, train_loader in dict_train_loader.items():\n",
        "  tweet, intensity = next(iter(train_loader))\n",
        "  print (name, \"\\n\", tweet, intensity)\n",
        "  a, b = model(tweet)\n",
        "  print (\"a - sigmoid output\\n\", a)\n",
        "  print (\"b - softmax output\\n\", b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpCF2qvRYkK4",
        "outputId": "38451080-1c50-4a79-f3e6-396080ed1b9a"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_EI_reg_train_subset_dataset_dataloader \n",
            " tensor([[ 53,  13,  40,  ...,   0,   0,   0],\n",
            "        [  5,  17,  11,  ...,   0,   0,   0],\n",
            "        [  5,   5,   1,  ...,   0,   0,   0],\n",
            "        ...,\n",
            "        [  5, 195,  48,  ...,   0,   0,   0],\n",
            "        [  1, 429,  31,  ...,   0,   0,   0],\n",
            "        [  5, 185,   9,  ...,   0,   0,   0]]) tensor([0.5830, 0.6460, 0.6670, 0.5210, 0.5830, 0.4580, 0.3540, 0.3960],\n",
            "       dtype=torch.float64)\n",
            "a - sigmoid output\n",
            " tensor([[0.5078],\n",
            "        [0.5082],\n",
            "        [0.5083],\n",
            "        [0.5106],\n",
            "        [0.4997],\n",
            "        [0.5144],\n",
            "        [0.5029],\n",
            "        [0.5007]], grad_fn=<SigmoidBackward0>)\n",
            "b - softmax output\n",
            " tensor([[-0.7902, -0.6047],\n",
            "        [-0.7487, -0.6405],\n",
            "        [-0.7778, -0.6151],\n",
            "        [-0.7245, -0.6628],\n",
            "        [-0.7425, -0.6461],\n",
            "        [-0.7311, -0.6566],\n",
            "        [-0.8051, -0.5924],\n",
            "        [-0.8086, -0.5897]], grad_fn=<LogSoftmaxBackward0>)\n",
            "df_V_reg_train_subset_dataset_dataloader \n",
            " tensor([[   5,    5,    5,  ...,    0,    0,    0],\n",
            "        [   7,   52,   38,  ...,    0,    0,    0],\n",
            "        [   7,   82,   19,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [1075,    7,   32,  ...,    0,    0,    0],\n",
            "        [   5,   33,  105,  ...,    0,    0,    0],\n",
            "        [   5,  800,   18,  ...,    0,    0,    0]]) tensor([0.5670, 0.4630, 0.8130, 0.6550, 0.6170, 0.5830, 0.6610, 0.2500],\n",
            "       dtype=torch.float64)\n",
            "a - sigmoid output\n",
            " tensor([[0.4940],\n",
            "        [0.4851],\n",
            "        [0.5037],\n",
            "        [0.5189],\n",
            "        [0.5103],\n",
            "        [0.5080],\n",
            "        [0.5059],\n",
            "        [0.5090]], grad_fn=<SigmoidBackward0>)\n",
            "b - softmax output\n",
            " tensor([[-0.7444, -0.6444],\n",
            "        [-0.7584, -0.6319],\n",
            "        [-0.8206, -0.5801],\n",
            "        [-0.7649, -0.6262],\n",
            "        [-0.8268, -0.5752],\n",
            "        [-0.7753, -0.6172],\n",
            "        [-0.7822, -0.6114],\n",
            "        [-0.7579, -0.6323]], grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Test Function"
      ],
      "metadata": {
        "id": "VjBXTHz2cXcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Typical Training, Test Function (without Domain Adaptation)"
      ],
      "metadata": {
        "id": "8nPEeiSFcc-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Function\n",
        "\n",
        "from tqdm import tqdm # for beautiful model training updates\n",
        "\n",
        "\n",
        "def train_model(model, device, train_loader, optimizer, epoch):\n",
        "    model.train() # setting the model in training mode\n",
        "    pbar = tqdm(train_loader) # putting the iterator in pbara\n",
        "    correct = 0 # for accuracy numerator\n",
        "    processed =0 # for accuracy denominator\n",
        "    current_loss = 0.0\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "\n",
        "        tweets, intensities = batch[0].to(device), batch[1].float().to(device)  # plural, we are not interested in domain\n",
        "        #sending data to CPU or GPU as per device\n",
        "\n",
        "        optimizer.zero_grad() # setting gradients to zero to avoid accumulation\n",
        "\n",
        "        y_preds, _ = model(tweets) # forward pass, result captured in y_preds (plural as there are many body in a batch)\n",
        "        # we are not interested in domain prediction\n",
        "        # the predictions are in one hot vector\n",
        "\n",
        "        loss = F.mse_loss(y_preds,intensities) # Computing loss        \n",
        "\n",
        "        train_losses.append(loss) # to capture loss over many epochs\n",
        "\n",
        "        loss.backward() # backpropagation\n",
        "        optimizer.step() # updating the params\n",
        "\n",
        "        # preds = y_preds.argmax(dim=1, keepdim=True)  # get the index olf the max log-probability\n",
        "        # correct += preds.eq(labels.view_as(preds)).sum().item()\n",
        "        current_loss += loss.item()\n",
        "\n",
        "        processed += len(tweets)\n",
        "\n",
        "        pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Average loss={100*current_loss/processed:0.2f}')\n",
        "        current_loss = 0.0\n"
      ],
      "metadata": {
        "id": "bulUVa2VcrHm"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "typical_model =  CNN_SA(pretrained_embedding=embedding_tensor,\n",
        "                  freeze_embedding=True,\n",
        "                 ).to(device)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(typical_model.parameters(), lr=0.001)\n",
        "\n",
        "train_losses = [] # to capture train losses over training epochs\n",
        "train_accuracy = [] # to capture train accuracy over training epochs\n",
        "test_losses = [] # to capture test losses \n",
        "test_accuracy = [] # to capture test accuracy \n",
        "\n",
        "EPOCHS = 2\n",
        "# EPOCHS = 10\n",
        "for train_name, train_loader in dict_train_loader.items():\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch+1)\n",
        "    train_model(typical_model, device, train_loader, optimizer, epoch)\n",
        "    print(\"for source validation.......\")\n",
        "    # val_name = train_name.replace(\"train\", \"val\" )\n",
        "    # test_model(typical_model, device, dict_val_loader[val_name])\n",
        "    # print(\"for target validation.......\")\n",
        "    # test_model(typical_model, device, val_target_loader)\n",
        "  break\n",
        "\n",
        "print(\"----------------------training complete-----------------\")\n",
        "# torch.save(typical_model.state_dict(),\"sentiment_analysis_typical.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FrceNxng81k",
        "outputId": "f8118e93-0c56-4d23-8d22-e3d5d18cba65"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/213 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "Loss=0.021369850262999535 Batch_id=211 Average loss=0.00: 100%|█████████▉| 212/213 [00:19<00:00, 11.22it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "Loss=0.03859928995370865 Batch_id=212 Average loss=0.00: 100%|██████████| 213/213 [00:19<00:00, 11.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for source validation.......\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.02852105163037777 Batch_id=212 Average loss=0.00: 100%|██████████| 213/213 [00:19<00:00, 11.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for source validation.......\n",
            "----------------------training complete-----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}