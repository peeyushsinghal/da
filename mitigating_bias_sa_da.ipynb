{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN0PhbbzOpth8yOoBZ5+Jd8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peeyushsinghal/da/blob/main/mitigating_bias_sa_da.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ekphrasis # library to pre process twitter data\n",
        "! pip install emoji --upgrade #library to deal with emoji data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jYp6x5D5AYi",
        "outputId": "1626054d-546f-403c-826d-2cf63cb1bc65"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.21.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (2.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.64.1)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (5.5.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (6.1.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (0.4.5)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.1.0.tar.gz (216 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 216 kB 5.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.1.0-py3-none-any.whl size=212392 sha256=19788328f3ea4287b1a563d0870d2a74935d6224058bf92ea6e1e97cefcc57c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/75/99/51c2a119f4cfd3af7b49cc57e4f737bed7e40b348a85d82804\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UN22BqAu8mbl"
      },
      "outputs": [],
      "source": [
        "## Import statements\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "import emoji\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on:{}\".format(DEVICE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCz_btt5Saig",
        "outputId": "a2c58122-78f5-4bcd-95f3-99308e2ab2d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on:cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Handling"
      ],
      "metadata": {
        "id": "xWShQEZF9MlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting google drive for data in there"
      ],
      "metadata": {
        "id": "LDNN2hc-9SL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPXnt-PrRvGm",
        "outputId": "ae8c50b0-fd8b-4a3b-c3f2-2ad596263bce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data configuration"
      ],
      "metadata": {
        "id": "v1MWCkgXSSv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/semeval-2018'\n",
        "DATA_DIR = os.path.join(BASE_PATH,'datasets')"
      ],
      "metadata": {
        "id": "JAVHMCaNSIlW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TASK1(object):\n",
        "  \n",
        "    EI_reg = {\n",
        "        'anger': {\n",
        "            'train': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/training/EI-reg-En-anger-train.txt'),\n",
        "            'dev': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/development/2018-EI-reg-En-anger-dev.txt'),\n",
        "            'gold': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/test-gold/2018-EI-reg-En-anger-test-gold.txt')\n",
        "                }\n",
        "        }\n",
        "\n",
        "    V_reg = {\n",
        "        'train': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-train.txt'),\n",
        "        'dev': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-dev.txt'),\n",
        "        'gold': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-test-gold.txt')\n",
        "             }\n",
        "\n",
        "    EEC = {\n",
        "        'eec': os.path.join(\n",
        "            DATA_DIR, 'task1/Equity-Evaluation-Corpus/Equity-Evaluation-Corpus.csv')\n",
        "             }"
      ],
      "metadata": {
        "id": "_Z4pcQiGSyXX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataloaders"
      ],
      "metadata": {
        "id": "UfWsmb-8V9Er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing regression data : `format [ID\tTweet\tAffect Dimension\tIntensity Score]`"
      ],
      "metadata": {
        "id": "CtLKXfb-Xi79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_reg(data_file, label_format='tuple')-> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    This is for datasets for the EI-reg and V-reg English tasks \n",
        "    Returns:\n",
        "        df: dataframe with columns in the first row of file [ID-Tweet-Affect Dimension-Intensity Score]\n",
        "    \"\"\"\n",
        "    with open(data_file, 'r') as fd:\n",
        "      data = [l.strip().split('\\t') for l in fd.readlines()]\n",
        "    \n",
        "    df = pd.DataFrame (data[1:],columns=data[0])\n",
        "    return df"
      ],
      "metadata": {
        "id": "7-CzssqIr68Z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def parse_reg(data_file, label_format='tuple')-> (list, list):\n",
        "#     \"\"\"\n",
        "#     This is for datasets for the EI-reg and V-reg English tasks \n",
        "#     Returns:\n",
        "#         X: a list of tweets\n",
        "#         y: a list of (affect dimension, v) tuples corresponding to\n",
        "#          the regression targets of the tweets\n",
        "#     \"\"\"\n",
        "#     with open(data_file, 'r') as fd:\n",
        "#         data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
        "#     X = [d[1] for d in data]\n",
        "#     y = [(d[2], float(d[3])) for d in data]\n",
        "#     if label_format == 'list':\n",
        "#         y = [l[1] for l in y]\n",
        "#     return X, y"
      ],
      "metadata": {
        "id": "F_A5KcxfV_4N"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "parsing EEC data : `format [ID\tSentence\tTemplate\tPerson\tGender\tRace Emotion\tEmotion word]`"
      ],
      "metadata": {
        "id": "FNEQISzDY6ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_eec()->pd.DataFrame:\n",
        "  \"\"\"\n",
        "  This is for EEC Dataset, it is a csv file\n",
        "  Returns:\n",
        "        df_eec: dataframe \n",
        "  \"\"\"\n",
        "  data_train = TASK1.EEC['eec']\n",
        "  df_eec = pd.read_csv(data_train)\n",
        "  return df_eec\n"
      ],
      "metadata": {
        "id": "afnCKQmtYhW3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse(task, dataset, emotion='anger') -> pd.DataFrame:\n",
        "    if task == 'EI-reg':\n",
        "        data_train = TASK1.EI_reg[emotion][dataset]\n",
        "        df = parse_reg(data_train)\n",
        "        return df\n",
        "    elif task == 'V-reg':\n",
        "        data_train = TASK1.V_reg[dataset]\n",
        "        df = parse_reg(data_train)\n",
        "        return df\n",
        "    else:\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "0J7zs1GzXh8F"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating Dataframes\n",
        "df_EI_reg_train = parse('EI-reg','train')\n",
        "df_EI_reg_val = parse('EI-reg','dev')\n",
        "df_EI_reg_test = parse('EI-reg','gold')\n",
        "df_V_reg_train = parse('V-reg','train')\n",
        "df_V_reg_val = parse('V-reg','dev')\n",
        "df_V_reg_test = parse('V-reg','gold')\n",
        "\n",
        "dict_df= {'df_EI_reg_train':df_EI_reg_train, \n",
        "          'df_EI_reg_val':df_EI_reg_val, \n",
        "          'df_EI_reg_test':df_EI_reg_test, \n",
        "          'df_V_reg_train': df_V_reg_train, \n",
        "          'df_V_reg_val':df_V_reg_val, \n",
        "          'df_V_reg_test': df_V_reg_test \n",
        "          }"
      ],
      "metadata": {
        "id": "9CgnnxH-aU6B"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PreProcess Twitter Data"
      ],
      "metadata": {
        "id": "vgcR6rW84jVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reference : https://github.com/cbaziotis/ekphrasis\n",
        "\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1DwsLjWqvEx",
        "outputId": "3d26a191-e982-4900-90b0-d23f1ee20eca"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #### Example checks of pre-processing\n",
        "sentences = [\n",
        "    \"CANT WAIT for the new season of #TwinPeaks Ôºº(^o^)Ôºè!!! #davidlynch #tvseries :)))\",\n",
        "    \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies :/\",\n",
        "    \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! :-D http://sentimentsymposium.com/.\",\n",
        "    \"@MGBarbieri @SpalkTalk a@b.com And just saw your LinkedIn comment after I sent this! Thanks for the message :) üòÄ\",\n",
        "    \"üíôüíõüèÜ @GeorgeePitman Young Player of The Season üèÜüíõüíô #irony #actuallyseventy\"\n",
        "]\n",
        "\n",
        "for s in sentences:\n",
        "    print(text_processor.pre_process_doc(s))\n",
        "# print ([text_processor.pre_process_doc(s) for s in sentences])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK5Ifwyhq0Ex",
        "outputId": "f662ee38-06e6-4f50-8681-ea12304568d5"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<allcaps>', 'cant', 'wait', '</allcaps>', 'for', 'the', 'new', 'season', 'of', '<hashtag>', 'twin', 'peaks', '</hashtag>', 'Ôºº(^o^)Ôºè', '!', '<repeated>', '<hashtag>', 'david', 'lynch', '</hashtag>', '<hashtag>', 'tv', 'series', '</hashtag>', '<happy>']\n",
            "['i', 'saw', 'the', 'new', '<hashtag>', 'john', 'doe', '</hashtag>', 'movie', 'and', 'it', 'sucks', '<elongated>', '!', '<repeated>', '<allcaps>', 'waisted', '</allcaps>', '<money>', '.', '<repeated>', '<hashtag>', 'bad', 'movies', '</hashtag>', '<annoyed>']\n",
            "['<user>', ':', 'can', 'not', 'wait', 'for', 'the', '<date>', '<hashtag>', 'sentiment', '</hashtag>', 'talks', '!', '<allcaps>', 'yay', '<elongated>', '</allcaps>', '!', '<repeated>', '<laugh>', '<url>']\n",
            "['<user>', '<user>', '<email>', 'and', 'just', 'saw', 'your', 'linkedin', 'comment', 'after', 'i', 'sent', 'this', '!', 'thanks', 'for', 'the', 'message', '<happy>', 'üòÄ']\n",
            "['üíô', 'üíõ', 'üèÜ', '<user>', 'young', 'player', 'of', 'the', 'season', 'üèÜ', 'üíõ', 'üíô', '<hashtag>', 'irony', '</hashtag>', '<hashtag>', 'actually', 'seventy', '</hashtag>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_tweets(df)-> pd.DataFrame:\n",
        "  tweets = df.Tweet.to_list()\n",
        "  # df['TweetTokens'] = [emoji.demojize(text_processor.pre_process_doc(tweet),language = 'en') for tweet in tweets] # Translates emoji in to word and preprocesss\n",
        "  # df['TweetTokens'] = [text_processor.pre_process_doc(tweet) for tweet in tweets] # preprocesss\n",
        "  tweets_processed = [text_processor.pre_process_doc(tweet) for tweet in tweets] # preprocesss\n",
        "  for tweet in tweets_processed:\n",
        "    for index, token in enumerate(tweet):\n",
        "      if emoji.is_emoji(token):\n",
        "        tweet[index] = emoji.demojize(token, language = 'en')\n",
        "  \n",
        "  df['TweetTokens'] = tweets_processed\n",
        "  # print(df)\n",
        "  return df"
      ],
      "metadata": {
        "id": "pfmJjAHrujuX"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_EI_reg_train = preprocess_tweets(df_EI_reg_train)\n",
        "# df_V_reg_train = preprocess_tweets(df_V_reg_train)\n",
        "\n",
        "for name, df in dict_df.items():\n",
        "  df = preprocess_tweets(df)\n"
      ],
      "metadata": {
        "id": "hxt99UC2xI2x"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : \n",
        "* remove stop words\n",
        "* stem\n",
        "* lemmetize\n"
      ],
      "metadata": {
        "id": "3los9BZbk2m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_V_reg_train.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtRWK8D6mMzk",
        "outputId": "46cfa401-a97b-4211-ad3a-7157aae9e11f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'Tweet', 'Affect Dimension', 'Intensity Score', 'TweetTokens'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def subset_df(df):\n",
        "  # df_reduced = df[['TweetTokens','Intensity Score']]\n",
        "  # print (df_reduced)\n",
        "  return df[['TweetTokens','Intensity Score']]"
      ],
      "metadata": {
        "id": "_zoGr_W3mYgK"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# subset_df_list = [subset_df(df) for df in df_list]"
      ],
      "metadata": {
        "id": "SZhg1IzgtzT6"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_df_subset ={name+\"_subset\": subset_df(df) for name, df in dict_df.items() }"
      ],
      "metadata": {
        "id": "kDsCZb5ExgA9"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print (dict_df_subset)"
      ],
      "metadata": {
        "id": "hcw_mO9Y1bP-"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Pytorch Datasets"
      ],
      "metadata": {
        "id": "Wi-dG9KP14u8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Vocabulary\n",
        "Before we create the Dataset, we need to define a process to build our vocabulary. For this,\n",
        "We‚Äôll create a ‚ÄúVocabulary‚Äù class which will create the word-to-index and index-to-word mappings using only the train dataframe we created before\n",
        "Also, the ‚ÄúVocabulary‚Äù class returns the numericalized version of each sentence in our dataframe. Eg: [‚Äòi‚Äô, ‚Äòlove‚Äô, ‚Äòapple‚Äô] -> [23, 54, 1220]. We need to convert the words to numbers as models expect each word in our vocabulary to be represented by a number"
      ],
      "metadata": {
        "id": "IUsh5RUf2BxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#               Define Vocabulary Class\n",
        "#######################################################\n",
        "\n",
        "class Vocabulary:\n",
        "  \n",
        "    '''\n",
        "    __init__ method is called by default as soon as an object of this class is initiated\n",
        "    we use this method to initiate our vocab dictionaries\n",
        "    '''\n",
        "    def __init__(self, freq_threshold = 3, max_size = 10000):\n",
        "        '''\n",
        "        freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
        "        max_size : max source vocab size. Eg. if set to 10,000, we pick the top 10,000 most frequent words and discard others\n",
        "        '''\n",
        "        #initiate the index to token dict\n",
        "        ## <PAD> -> padding, used for padding the shorter sentences in a batch to match the length of longest sentence in the batch\n",
        "        ## <UNK> -> words which are not found in the vocab are replace by this token\n",
        "        # self.itos = {0: '<PAD>', 1: '<UNK>', 2:'<NUMBER>', 3: '<CURRENCY>', 4: '<URL>'}\n",
        "        self.itos = {0: '<PAD>', 1: '<UNK>'}\n",
        "        \n",
        "        \n",
        "        #initiate the token to index dict\n",
        "        self.stoi = {k:j for j,k in self.itos.items()}\n",
        "        self.original_stoi = self.stoi.copy()\n",
        "#         print(self.stoi)\n",
        "        \n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.max_size = max_size\n",
        "    \n",
        "    '''\n",
        "    __len__ is used by dataloader later to create batches\n",
        "    '''\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "    \n",
        "    '''\n",
        "    a simple tokenizer to split on space and converts the sentence to list of words\n",
        "    '''\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "#         return [tok.strip() for tok in text.split(' ')]\n",
        "        return [tok.lower().strip() for tok in text.split(' ')] # this is commented out to avoid <NUMBER> ,<UNK> lowering\n",
        "#         return [tok.lower().strip() for tok in text.split(' ') if tok not in list(self.stoi.keys())] \n",
        "    \n",
        "    '''\n",
        "    build the vocab: create a dictionary mapping of index to string (itos) and string to index (stoi)\n",
        "    output ex. for stoi -> {'the':6, 'a':7, 'an':8}\n",
        "    '''\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        #calculate the frequencies of each word first to remove the words with freq < freq_threshold\n",
        "#         frequencies = {}  #init the freq dict\n",
        "        frequencies = {k:self.max_size+1 for _,k in self.itos.items()}  # updated so that intial ones are also part of this\n",
        "        \n",
        "        # idx = 5 #index from which we want our dict to start. We already used 4 indexes for pad, unk...\n",
        "        idx = len(self.original_stoi)\n",
        "        \n",
        "        #calculate freq of words\n",
        "        for sentence in sentence_list:\n",
        "            list_word = [tok.lower().strip() for tok in sentence.split(' ') if tok not in list(self.stoi.keys())] \n",
        "            for word in list_word:\n",
        "#             for word in self.tokenizer(sentence):\n",
        "                \n",
        "                if word not in frequencies.keys():\n",
        "                    frequencies[word]=1\n",
        "                else:\n",
        "                    \n",
        "                    frequencies[word]+=1\n",
        "                    \n",
        "#         print (\"----2-----\\n\",frequencies)\n",
        "        \n",
        "        #limit vocab by removing low freq words\n",
        "        frequencies = {k:v for k,v in frequencies.items() if v>self.freq_threshold} \n",
        "        \n",
        "#         print (\"----3-----\\n\",frequencies)\n",
        "        \n",
        "        #limit vocab to the max_size specified\n",
        "        frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx =5 for pad, unk...\n",
        "        \n",
        "#         print (\"----4-----\\n\",frequencies)\n",
        "            \n",
        "        #create vocab\n",
        "        for key in set(self.stoi.keys()):\n",
        "            frequencies.pop(key)\n",
        "        \n",
        "#         print (\"----5-----\\n\",frequencies)\n",
        "        \n",
        "        for word in frequencies.keys():\n",
        "            self.stoi[word] = idx\n",
        "            self.itos[idx] = word\n",
        "            idx+=1\n",
        "        \n",
        "#         print (\"----6-----\\n\",self.stoi)\n",
        "        \n",
        "    '''\n",
        "    convert the list of words to a list of corresponding indexes\n",
        "    '''    \n",
        "    def numericalize(self, text):\n",
        "        #tokenize text\n",
        "#         tokenized_text = self.tokenizer(text)\n",
        "#         print(\"---------\\n\",self.original_stoi.keys())\n",
        "        tokenized_text = []\n",
        "        for tok in text.split(' '):\n",
        "            if tok not in list(self.original_stoi.keys()):\n",
        "                tokenized_text.append(tok.lower().strip())\n",
        "            else:\n",
        "                tokenized_text.append(tok.strip())\n",
        "                \n",
        "#         tokenized_text = [tok.lower().strip() for tok in text.split(' ') if tok not in list(self.original_stoi.keys())]\n",
        "        numericalized_text = []\n",
        "        for token in tokenized_text:\n",
        "            if token in self.stoi.keys():\n",
        "                numericalized_text.append(self.stoi[token])\n",
        "            else: #out-of-vocab words are represented by UNK token index\n",
        "                numericalized_text.append(self.stoi['<UNK>'])\n",
        "                \n",
        "        return numericalized_text"
      ],
      "metadata": {
        "id": "F7xZg1XcwLtt"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #create a vocab class with freq_threshold=0 and max_size=100\n",
        "voc = Vocabulary(0, 100)\n",
        "sentence_list = ['that is a cat CAT', 'that is not a dog']\n",
        "#build vocab\n",
        "voc.build_vocabulary(sentence_list)\n",
        "\n",
        "print('index to string: ',voc.itos)\n",
        "print('string to index:',voc.stoi)\n",
        "\n",
        "print('numericalize -> cat and a dog <URL>: ', voc.numericalize('cat and a dog <NUMBER>'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COQ10m7Pv-sR",
        "outputId": "77874a69-9d50-47db-a576-6ca90559b08a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index to string:  {0: '<PAD>', 1: '<UNK>', 2: 'that', 3: 'is', 4: 'a', 5: 'cat', 6: 'not', 7: 'dog'}\n",
            "string to index: {'<PAD>': 0, '<UNK>': 1, 'that': 2, 'is': 3, 'a': 4, 'cat': 5, 'not': 6, 'dog': 7}\n",
            "numericalize -> cat and a dog <URL>:  [5, 1, 4, 7, 1]\n"
          ]
        }
      ]
    }
  ]
}