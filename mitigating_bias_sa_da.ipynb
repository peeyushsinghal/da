{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORHyzV86f9hg3o4eLSaVaf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peeyushsinghal/da/blob/main/mitigating_bias_sa_da.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ekphrasis # library to pre process twitter data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jYp6x5D5AYi",
        "outputId": "da8b03f2-c0d5-4a03-fbef-eee0d7b8f208"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ekphrasis\n",
            "  Downloading ekphrasis-0.5.4-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 884 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.21.6)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (5.5.0)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (2.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.7)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (2022.6.2)\n",
            "Installing collected packages: ftfy, colorama, ekphrasis\n",
            "Successfully installed colorama-0.4.5 ekphrasis-0.5.4 ftfy-6.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UN22BqAu8mbl"
      },
      "outputs": [],
      "source": [
        "## Import statements\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on:{}\".format(DEVICE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCz_btt5Saig",
        "outputId": "7f5419b4-a1db-4643-b578-2180819bd36b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on:cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Handling"
      ],
      "metadata": {
        "id": "xWShQEZF9MlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting google drive for data in there"
      ],
      "metadata": {
        "id": "LDNN2hc-9SL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPXnt-PrRvGm",
        "outputId": "2408b816-ee17-4a0b-fa34-2789647eda08"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data configuration"
      ],
      "metadata": {
        "id": "v1MWCkgXSSv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/semeval-2018'\n",
        "DATA_DIR = os.path.join(BASE_PATH,'datasets')"
      ],
      "metadata": {
        "id": "JAVHMCaNSIlW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TASK1(object):\n",
        "  \n",
        "    EI_reg = {\n",
        "        'anger': {\n",
        "            'train': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/training/EI-reg-En-anger-train.txt'),\n",
        "            'dev': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/development/2018-EI-reg-En-anger-dev.txt'),\n",
        "            'gold': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/test-gold/2018-EI-reg-En-anger-test-gold.txt')\n",
        "                }\n",
        "        }\n",
        "\n",
        "    V_reg = {\n",
        "        'train': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-train.txt'),\n",
        "        'dev': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-dev.txt'),\n",
        "        'gold': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-test-gold.txt')\n",
        "             }\n",
        "\n",
        "    EEC = {\n",
        "        'eec': os.path.join(\n",
        "            DATA_DIR, 'task1/Equity-Evaluation-Corpus/Equity-Evaluation-Corpus.csv')\n",
        "             }"
      ],
      "metadata": {
        "id": "_Z4pcQiGSyXX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataloaders"
      ],
      "metadata": {
        "id": "UfWsmb-8V9Er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing regression data : `format [ID\tTweet\tAffect Dimension\tIntensity Score]`"
      ],
      "metadata": {
        "id": "CtLKXfb-Xi79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_reg(data_file, label_format='tuple')-> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    This is for datasets for the EI-reg and V-reg English tasks \n",
        "    Returns:\n",
        "        df: dataframe with columns in the first row of file [ID-Tweet-Affect Dimension-Intensity Score]\n",
        "    \"\"\"\n",
        "    with open(data_file, 'r') as fd:\n",
        "      data = [l.strip().split('\\t') for l in fd.readlines()]\n",
        "    \n",
        "    df = pd.DataFrame (data[1:],columns=data[0])\n",
        "    return df"
      ],
      "metadata": {
        "id": "7-CzssqIr68Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def parse_reg(data_file, label_format='tuple')-> (list, list):\n",
        "#     \"\"\"\n",
        "#     This is for datasets for the EI-reg and V-reg English tasks \n",
        "#     Returns:\n",
        "#         X: a list of tweets\n",
        "#         y: a list of (affect dimension, v) tuples corresponding to\n",
        "#          the regression targets of the tweets\n",
        "#     \"\"\"\n",
        "#     with open(data_file, 'r') as fd:\n",
        "#         data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
        "#     X = [d[1] for d in data]\n",
        "#     y = [(d[2], float(d[3])) for d in data]\n",
        "#     if label_format == 'list':\n",
        "#         y = [l[1] for l in y]\n",
        "#     return X, y"
      ],
      "metadata": {
        "id": "F_A5KcxfV_4N"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "parsing EEC data : `format [ID\tSentence\tTemplate\tPerson\tGender\tRace Emotion\tEmotion word]`"
      ],
      "metadata": {
        "id": "FNEQISzDY6ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_eec()->pd.DataFrame:\n",
        "  \"\"\"\n",
        "  This is for EEC Dataset, it is a csv file\n",
        "  Returns:\n",
        "        df_eec: dataframe \n",
        "  \"\"\"\n",
        "  data_train = TASK1.EEC['eec']\n",
        "  df_eec = pd.read_csv(data_train)\n",
        "  return df_eec\n"
      ],
      "metadata": {
        "id": "afnCKQmtYhW3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse(task, dataset, emotion='anger') -> (list, list):\n",
        "    if task == 'EI-reg':\n",
        "        data_train = TASK1.EI_reg[emotion][dataset]\n",
        "        df = parse_reg(data_train)\n",
        "        return df\n",
        "    elif task == 'V-reg':\n",
        "        data_train = TASK1.V_reg[dataset]\n",
        "        df = parse_reg(data_train)\n",
        "        return df\n",
        "    else:\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "0J7zs1GzXh8F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Checking if the parsing is working \n",
        "# parse('EI-reg','train').head()\n",
        "# parse('V-reg','gold').head()\n",
        "# parse_eec().head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "9CgnnxH-aU6B",
        "outputId": "ca315310-db8b-4146-fb41-1a6f5d7c646d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             ﻿ID                                              Tweet  \\\n",
              "0  2018-En-01964                           Gm and have a  #Tuesday!   \n",
              "1  2018-En-01539  @realDonaldTrump But you have a lot of time fo...   \n",
              "2  2018-En-04235  I graduated yesterday and already had 8 family...   \n",
              "3  2018-En-03711  @jaimitoelcrack7 Seriously...I've been sitting...   \n",
              "4  2018-En-01177  Whether my glass is half empty or its half ful...   \n",
              "\n",
              "  Affect Dimension Intensity Score  \n",
              "0          valence           0.589  \n",
              "1          valence           0.500  \n",
              "2          valence           0.550  \n",
              "3          valence           0.633  \n",
              "4          valence           0.750  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-21a4915b-4021-4760-a181-58bb3820a3de\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>﻿ID</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Affect Dimension</th>\n",
              "      <th>Intensity Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-En-01964</td>\n",
              "      <td>Gm and have a  #Tuesday!</td>\n",
              "      <td>valence</td>\n",
              "      <td>0.589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-En-01539</td>\n",
              "      <td>@realDonaldTrump But you have a lot of time fo...</td>\n",
              "      <td>valence</td>\n",
              "      <td>0.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-En-04235</td>\n",
              "      <td>I graduated yesterday and already had 8 family...</td>\n",
              "      <td>valence</td>\n",
              "      <td>0.550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-En-03711</td>\n",
              "      <td>@jaimitoelcrack7 Seriously...I've been sitting...</td>\n",
              "      <td>valence</td>\n",
              "      <td>0.633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-En-01177</td>\n",
              "      <td>Whether my glass is half empty or its half ful...</td>\n",
              "      <td>valence</td>\n",
              "      <td>0.750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21a4915b-4021-4760-a181-58bb3820a3de')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-21a4915b-4021-4760-a181-58bb3820a3de button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-21a4915b-4021-4760-a181-58bb3820a3de');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PreProcess Twitter Data"
      ],
      "metadata": {
        "id": "vgcR6rW84jVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text, lowercase=True):\n",
        "  \"\"\"\n",
        "  Return:\n",
        "    text: tokenized text (split) and optionally lower case\n",
        "  \n",
        "  \"\"\"\n",
        "  if lowercase:\n",
        "    text = text.lower()\n",
        "  return text.split()"
      ],
      "metadata": {
        "id": "rZlHqOev40Xr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def twitter_preprocess():\n",
        "    preprocessor = TextPreProcessor(\n",
        "        normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "                   'time',\n",
        "                   'date', 'number'],\n",
        "        annotate={\"hashtag\", \"elongated\", \"allcaps\", \"repeated\", 'emphasis',\n",
        "                  'censored'},\n",
        "        all_caps_tag=\"wrap\",\n",
        "        fix_text=True,\n",
        "        segmenter=\"twitter_2018\",\n",
        "        corrector=\"twitter_2018\",\n",
        "        unpack_hashtags=True,\n",
        "        unpack_contractions=True,\n",
        "        spell_correct_elong=False,\n",
        "        tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "        dicts=[emoticons]\n",
        "    ).pre_process_doc\n",
        "\n",
        "    def preprocess(name, dataset):\n",
        "        desc = \"PreProcessing dataset {}...\".format(name)\n",
        "\n",
        "        data = [preprocessor(x)\n",
        "                for x in tqdm(dataset, desc=desc)]\n",
        "        return data\n",
        "\n",
        "    return preprocess"
      ],
      "metadata": {
        "id": "fqBBMxC9nRqm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def twitter_preprocess(name, dataset):\n",
        "#   \"\"\"\n",
        "#   reference : https://github.com/FengJiaChunFromSYSU/ntua-slp-semeval2018\n",
        "#   https://github.com/cbaziotis/ekphrasis\n",
        "#   \"\"\"\n",
        "#   preprocessor = TextPreProcessor(\n",
        "#     normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "#                 'time',\n",
        "#                 'date', 'number'],\n",
        "#     annotate={\"hashtag\", \"elongated\", \"allcaps\", \"repeated\", 'emphasis',\n",
        "#                 'censored'},\n",
        "#     all_caps_tag=\"wrap\",\n",
        "#     fix_text=True,\n",
        "#     segmenter=\"twitter_2018\",\n",
        "#     corrector=\"twitter_2018\",\n",
        "#     unpack_hashtags=True,\n",
        "#     unpack_contractions=True,\n",
        "#     spell_correct_elong=False,\n",
        "#     tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "#     dicts=[emoticons]\n",
        "#     ).pre_process_doc\n",
        "    \n",
        "#   desc = \"PreProcessing dataset {}...\".format(name)\n",
        "\n",
        "#   data = [preprocessor(x) for x in tqdm(dataset, desc=desc)]\n",
        "\n",
        "#   return data"
      ],
      "metadata": {
        "id": "Gv8y8cOz5-0O"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# reference : https://github.com/cbaziotis/ekphrasis\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1DwsLjWqvEx",
        "outputId": "fc31c8fb-352e-4684-e2fb-604afbe92f88"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n",
            "Reading twitter - 1grams ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"CANT WAIT for the new season of #TwinPeaks ＼(^o^)／!!! #davidlynch #tvseries :)))\",\n",
        "    \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies :/\",\n",
        "    \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! :-D http://sentimentsymposium.com/.\",\n",
        "    \"@MGBarbieri @SpalkTalk a@b.com And just saw your LinkedIn comment after I sent this! Thanks for the message :) 😀\",\n",
        "    \"💙💛🏆 @GeorgeePitman Young Player of The Season 🏆💛💙 #irony #actuallyseventy\"\n",
        "]\n",
        "\n",
        "for s in sentences:\n",
        "    print(text_processor.pre_process_doc(s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK5Ifwyhq0Ex",
        "outputId": "8a64fd56-f0f8-4f19-d7ab-afb90f242b67"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<allcaps>', 'cant', 'wait', '</allcaps>', 'for', 'the', 'new', 'season', 'of', '<hashtag>', 'twin', 'peaks', '</hashtag>', '＼(^o^)／', '!', '<repeated>', '<hashtag>', 'david', 'lynch', '</hashtag>', '<hashtag>', 'tv', 'series', '</hashtag>', '<happy>']\n",
            "['i', 'saw', 'the', 'new', '<hashtag>', 'john', 'doe', '</hashtag>', 'movie', 'and', 'it', 'sucks', '<elongated>', '!', '<repeated>', '<allcaps>', 'waisted', '</allcaps>', '<money>', '.', '<repeated>', '<hashtag>', 'bad', 'movies', '</hashtag>', '<annoyed>']\n",
            "['<user>', ':', 'can', 'not', 'wait', 'for', 'the', '<date>', '<hashtag>', 'sentiment', '</hashtag>', 'talks', '!', '<allcaps>', 'yay', '<elongated>', '</allcaps>', '!', '<repeated>', '<laugh>', '<url>']\n",
            "['<user>', '<user>', '<email>', 'and', 'just', 'saw', 'your', 'linkedin', 'comment', 'after', 'i', 'sent', 'this', '!', 'thanks', 'for', 'the', 'message', '<happy>', '😀']\n",
            "['💙', '💛', '🏆', '<user>', 'young', 'player', 'of', 'the', 'season', '🏆', '💛', '💙', '<hashtag>', 'irony', '</hashtag>', '<hashtag>', 'actually', 'seventy', '</hashtag>']\n"
          ]
        }
      ]
    }
  ]
}