{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbS9NxlNA2ZiSTHjh7jwGV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peeyushsinghal/da/blob/main/mitigating_bias_sa_da.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ekphrasis # library to pre process twitter data\n",
        "! pip install emoji --upgrade #library to deal with emoji data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jYp6x5D5AYi",
        "outputId": "1626054d-546f-403c-826d-2cf63cb1bc65"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.21.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (2.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.64.1)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (5.5.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (6.1.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (0.4.5)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.1.0.tar.gz (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 5.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.1.0-py3-none-any.whl size=212392 sha256=19788328f3ea4287b1a563d0870d2a74935d6224058bf92ea6e1e97cefcc57c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/75/99/51c2a119f4cfd3af7b49cc57e4f737bed7e40b348a85d82804\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UN22BqAu8mbl"
      },
      "outputs": [],
      "source": [
        "## Import statements\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "import emoji\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on:{}\".format(DEVICE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCz_btt5Saig",
        "outputId": "a2c58122-78f5-4bcd-95f3-99308e2ab2d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on:cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Handling"
      ],
      "metadata": {
        "id": "xWShQEZF9MlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting google drive for data in there"
      ],
      "metadata": {
        "id": "LDNN2hc-9SL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPXnt-PrRvGm",
        "outputId": "ae8c50b0-fd8b-4a3b-c3f2-2ad596263bce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data configuration"
      ],
      "metadata": {
        "id": "v1MWCkgXSSv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/semeval-2018'\n",
        "DATA_DIR = os.path.join(BASE_PATH,'datasets')"
      ],
      "metadata": {
        "id": "JAVHMCaNSIlW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TASK1(object):\n",
        "  \n",
        "    EI_reg = {\n",
        "        'anger': {\n",
        "            'train': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/training/EI-reg-En-anger-train.txt'),\n",
        "            'dev': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/development/2018-EI-reg-En-anger-dev.txt'),\n",
        "            'gold': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/test-gold/2018-EI-reg-En-anger-test-gold.txt')\n",
        "                }\n",
        "        }\n",
        "\n",
        "    V_reg = {\n",
        "        'train': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-train.txt'),\n",
        "        'dev': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-dev.txt'),\n",
        "        'gold': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-test-gold.txt')\n",
        "             }\n",
        "\n",
        "    EEC = {\n",
        "        'eec': os.path.join(\n",
        "            DATA_DIR, 'task1/Equity-Evaluation-Corpus/Equity-Evaluation-Corpus.csv')\n",
        "             }"
      ],
      "metadata": {
        "id": "_Z4pcQiGSyXX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataloaders"
      ],
      "metadata": {
        "id": "UfWsmb-8V9Er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing regression data : `format [ID\tTweet\tAffect Dimension\tIntensity Score]`"
      ],
      "metadata": {
        "id": "CtLKXfb-Xi79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_reg(data_file, label_format='tuple')-> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    This is for datasets for the EI-reg and V-reg English tasks \n",
        "    Returns:\n",
        "        df: dataframe with columns in the first row of file [ID-Tweet-Affect Dimension-Intensity Score]\n",
        "    \"\"\"\n",
        "    with open(data_file, 'r') as fd:\n",
        "      data = [l.strip().split('\\t') for l in fd.readlines()]\n",
        "    \n",
        "    df = pd.DataFrame (data[1:],columns=data[0])\n",
        "    return df"
      ],
      "metadata": {
        "id": "7-CzssqIr68Z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def parse_reg(data_file, label_format='tuple')-> (list, list):\n",
        "#     \"\"\"\n",
        "#     This is for datasets for the EI-reg and V-reg English tasks \n",
        "#     Returns:\n",
        "#         X: a list of tweets\n",
        "#         y: a list of (affect dimension, v) tuples corresponding to\n",
        "#          the regression targets of the tweets\n",
        "#     \"\"\"\n",
        "#     with open(data_file, 'r') as fd:\n",
        "#         data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
        "#     X = [d[1] for d in data]\n",
        "#     y = [(d[2], float(d[3])) for d in data]\n",
        "#     if label_format == 'list':\n",
        "#         y = [l[1] for l in y]\n",
        "#     return X, y"
      ],
      "metadata": {
        "id": "F_A5KcxfV_4N"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "parsing EEC data : `format [ID\tSentence\tTemplate\tPerson\tGender\tRace Emotion\tEmotion word]`"
      ],
      "metadata": {
        "id": "FNEQISzDY6ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_eec()->pd.DataFrame:\n",
        "  \"\"\"\n",
        "  This is for EEC Dataset, it is a csv file\n",
        "  Returns:\n",
        "        df_eec: dataframe \n",
        "  \"\"\"\n",
        "  data_train = TASK1.EEC['eec']\n",
        "  df_eec = pd.read_csv(data_train)\n",
        "  return df_eec\n"
      ],
      "metadata": {
        "id": "afnCKQmtYhW3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse(task, dataset, emotion='anger') -> pd.DataFrame:\n",
        "    if task == 'EI-reg':\n",
        "        data_train = TASK1.EI_reg[emotion][dataset]\n",
        "        df = parse_reg(data_train)\n",
        "        return df\n",
        "    elif task == 'V-reg':\n",
        "        data_train = TASK1.V_reg[dataset]\n",
        "        df = parse_reg(data_train)\n",
        "        return df\n",
        "    else:\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "0J7zs1GzXh8F"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating Dataframes\n",
        "df_EI_reg_train = parse('EI-reg','train')\n",
        "df_V_reg_train = parse('V-reg','train')"
      ],
      "metadata": {
        "id": "9CgnnxH-aU6B"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PreProcess Twitter Data"
      ],
      "metadata": {
        "id": "vgcR6rW84jVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reference : https://github.com/cbaziotis/ekphrasis\n",
        "\n",
        "# text_processor = TextPreProcessor(\n",
        "#     # terms that will be normalized\n",
        "#     normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "#         'time', 'url', 'date', 'number'],\n",
        "#     # terms that will be annotated\n",
        "#     annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "#         'emphasis', 'censored'},\n",
        "#     fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "#     # corpus from which the word statistics are going to be used \n",
        "#     # for word segmentation \n",
        "#     segmenter=\"twitter\", \n",
        "    \n",
        "#     # corpus from which the word statistics are going to be used \n",
        "#     # for spell correction\n",
        "#     corrector=\"twitter\", \n",
        "    \n",
        "#     unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "#     unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "#     spell_correct_elong=False,  # spell correction for elongated words\n",
        "    \n",
        "#     # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "#     # the tokenizer, should take as input a string and return a list of tokens\n",
        "#     tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    \n",
        "#     # list of dictionaries, for replacing tokens extracted from the text,\n",
        "#     # with other expressions. You can pass more than one dictionaries.\n",
        "#     dicts=[emoticons]\n",
        "# )\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1DwsLjWqvEx",
        "outputId": "167925e7-e220-40f9-bd02-478ab901df31"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #### Example checks of pre-processing\n",
        "# sentences = [\n",
        "#     \"CANT WAIT for the new season of #TwinPeaks ＼(^o^)／!!! #davidlynch #tvseries :)))\",\n",
        "#     \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies :/\",\n",
        "#     \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! :-D http://sentimentsymposium.com/.\",\n",
        "#     \"@MGBarbieri @SpalkTalk a@b.com And just saw your LinkedIn comment after I sent this! Thanks for the message :) 😀\",\n",
        "#     \"💙💛🏆 @GeorgeePitman Young Player of The Season 🏆💛💙 #irony #actuallyseventy\"\n",
        "# ]\n",
        "\n",
        "# for s in sentences:\n",
        "#     print(text_processor.pre_process_doc(s))\n",
        "# # print ([text_processor.pre_process_doc(s) for s in sentences])"
      ],
      "metadata": {
        "id": "iK5Ifwyhq0Ex"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_tweets(df)-> pd.DataFrame:\n",
        "  tweets = df.Tweet.to_list()\n",
        "  # df['TweetTokens'] = [emoji.demojize(text_processor.pre_process_doc(tweet),language = 'en') for tweet in tweets] # Translates emoji in to word and preprocesss\n",
        "  # df['TweetTokens'] = [text_processor.pre_process_doc(tweet) for tweet in tweets] # preprocesss\n",
        "  tweets_processed = [text_processor.pre_process_doc(tweet) for tweet in tweets] # preprocesss\n",
        "  for tweet in tweets_processed:\n",
        "    for index, token in enumerate(tweet):\n",
        "      if emoji.is_emoji(token):\n",
        "        tweet[index] = emoji.demojize(token, language = 'en')\n",
        "  \n",
        "  df['TweetTokens'] = tweets_processed\n",
        "  # print(df)\n",
        "  return df"
      ],
      "metadata": {
        "id": "pfmJjAHrujuX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_EI_reg_train = preprocess_tweets(df_EI_reg_train)\n",
        "df_V_reg_train = preprocess_tweets(df_V_reg_train)"
      ],
      "metadata": {
        "id": "hxt99UC2xI2x"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}