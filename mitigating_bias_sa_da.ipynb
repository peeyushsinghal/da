{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOa65t2NdAU2EXNQtzEG/2v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peeyushsinghal/da/blob/main/mitigating_bias_sa_da.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ekphrasis # library to pre process twitter data\n",
        "! pip install emoji --upgrade #library to deal with emoji data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jYp6x5D5AYi",
        "outputId": "7c7fae65-8143-4037-82ff-1607c1900b02"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (0.4.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.21.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.7)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (5.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.64.1)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (6.1.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (2.0.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "UN22BqAu8mbl"
      },
      "outputs": [],
      "source": [
        "## Import statements\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "import emoji\n",
        "from tqdm import tqdm\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on:{}\".format(DEVICE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCz_btt5Saig",
        "outputId": "d4af6d7a-d5bf-41a3-cea2-f39f0477c170"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on:cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Handling"
      ],
      "metadata": {
        "id": "xWShQEZF9MlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting google drive for data in there"
      ],
      "metadata": {
        "id": "LDNN2hc-9SL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPXnt-PrRvGm",
        "outputId": "956c6024-6f22-419c-9ae5-17e8771992b5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data configuration"
      ],
      "metadata": {
        "id": "v1MWCkgXSSv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/semeval-2018'\n",
        "DATA_DIR = os.path.join(BASE_PATH,'datasets')"
      ],
      "metadata": {
        "id": "JAVHMCaNSIlW"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TASK1(object):\n",
        "  \n",
        "    EI_reg = {\n",
        "        'anger': {\n",
        "            'train': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/training/EI-reg-En-anger-train.txt'),\n",
        "            'dev': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/development/2018-EI-reg-En-anger-dev.txt'),\n",
        "            'gold': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/test-gold/2018-EI-reg-En-anger-test-gold.txt')\n",
        "                }\n",
        "        }\n",
        "\n",
        "    V_reg = {\n",
        "        'train': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-train.txt'),\n",
        "        'dev': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-dev.txt'),\n",
        "        'gold': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-test-gold.txt')\n",
        "             }\n",
        "\n",
        "    EEC = {\n",
        "        'eec': os.path.join(\n",
        "            DATA_DIR, 'task1/Equity-Evaluation-Corpus/Equity-Evaluation-Corpus.csv')\n",
        "             }"
      ],
      "metadata": {
        "id": "_Z4pcQiGSyXX"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataloaders"
      ],
      "metadata": {
        "id": "UfWsmb-8V9Er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing regression data : `format [ID\tTweet\tAffect Dimension\tIntensity Score]`"
      ],
      "metadata": {
        "id": "CtLKXfb-Xi79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_reg(data_file, label_format='tuple')-> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    This is for datasets for the EI-reg and V-reg English tasks \n",
        "    Returns:\n",
        "        df: dataframe with columns in the first row of file [ID-Tweet-Affect Dimension-Intensity Score]\n",
        "    \"\"\"\n",
        "    with open(data_file, 'r') as fd:\n",
        "      data = [l.strip().split('\\t') for l in fd.readlines()]\n",
        "    \n",
        "    df = pd.DataFrame (data[1:],columns=data[0])\n",
        "    return df"
      ],
      "metadata": {
        "id": "7-CzssqIr68Z"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def parse_reg(data_file, label_format='tuple')-> (list, list):\n",
        "#     \"\"\"\n",
        "#     This is for datasets for the EI-reg and V-reg English tasks \n",
        "#     Returns:\n",
        "#         X: a list of tweets\n",
        "#         y: a list of (affect dimension, v) tuples corresponding to\n",
        "#          the regression targets of the tweets\n",
        "#     \"\"\"\n",
        "#     with open(data_file, 'r') as fd:\n",
        "#         data = [l.strip().split('\\t') for l in fd.readlines()][1:]\n",
        "#     X = [d[1] for d in data]\n",
        "#     y = [(d[2], float(d[3])) for d in data]\n",
        "#     if label_format == 'list':\n",
        "#         y = [l[1] for l in y]\n",
        "#     return X, y"
      ],
      "metadata": {
        "id": "F_A5KcxfV_4N"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "parsing EEC data : `format [ID\tSentence\tTemplate\tPerson\tGender\tRace Emotion\tEmotion word]`"
      ],
      "metadata": {
        "id": "FNEQISzDY6ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_eec()->pd.DataFrame:\n",
        "  \"\"\"\n",
        "  This is for EEC Dataset, it is a csv file\n",
        "  Returns:\n",
        "        df_eec: dataframe \n",
        "  \"\"\"\n",
        "  data_train = TASK1.EEC['eec']\n",
        "  df_eec = pd.read_csv(data_train)\n",
        "  return df_eec\n"
      ],
      "metadata": {
        "id": "afnCKQmtYhW3"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse(task, dataset, emotion='anger') -> pd.DataFrame:\n",
        "    if task == 'EI-reg':\n",
        "        data_train = TASK1.EI_reg[emotion][dataset]\n",
        "        df = parse_reg(data_train)\n",
        "        df[df.columns[-1]] = df[df.columns[-1]].astype(float)\n",
        "        return df\n",
        "    elif task == 'V-reg':\n",
        "        data_train = TASK1.V_reg[dataset]\n",
        "        df = parse_reg(data_train)\n",
        "        df[df.columns[-1]] = df[df.columns[-1]].astype(float)\n",
        "        return df\n",
        "    else:\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "0J7zs1GzXh8F"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating Dataframes\n",
        "df_EI_reg_train = parse('EI-reg','train')\n",
        "df_EI_reg_val = parse('EI-reg','dev')\n",
        "df_EI_reg_test = parse('EI-reg','gold')\n",
        "df_V_reg_train = parse('V-reg','train')\n",
        "df_V_reg_val = parse('V-reg','dev')\n",
        "df_V_reg_test = parse('V-reg','gold')\n",
        "\n",
        "dict_df= {'df_EI_reg_train':df_EI_reg_train, \n",
        "          'df_EI_reg_val':df_EI_reg_val, \n",
        "          'df_EI_reg_test':df_EI_reg_test, \n",
        "          'df_V_reg_train': df_V_reg_train, \n",
        "          'df_V_reg_val':df_V_reg_val, \n",
        "          'df_V_reg_test': df_V_reg_test \n",
        "          }"
      ],
      "metadata": {
        "id": "9CgnnxH-aU6B"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PreProcess Twitter Data"
      ],
      "metadata": {
        "id": "vgcR6rW84jVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reference : https://github.com/cbaziotis/ekphrasis\n",
        "\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1DwsLjWqvEx",
        "outputId": "081f1990-dc88-4085-9f99-0dbaf36b3bb6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #### Example checks of pre-processing\n",
        "# sentences = [\n",
        "#     \"CANT WAIT for the new season of #TwinPeaks ï¼¼(^o^)ï¼!!! #davidlynch #tvseries :)))\",\n",
        "#     \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies :/\",\n",
        "#     \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! :-D http://sentimentsymposium.com/.\",\n",
        "#     \"@MGBarbieri @SpalkTalk a@b.com And just saw your LinkedIn comment after I sent this! Thanks for the message :) ðŸ˜€\",\n",
        "#     \"ðŸ’™ðŸ’›ðŸ† @GeorgeePitman Young Player of The Season ðŸ†ðŸ’›ðŸ’™ #irony #actuallyseventy\"\n",
        "# ]\n",
        "\n",
        "# for s in sentences:\n",
        "#     print(\" \".join(text_processor.pre_process_doc(s)))\n",
        "# # print ([text_processor.pre_process_doc(s) for s in sentences])"
      ],
      "metadata": {
        "id": "iK5Ifwyhq0Ex"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_tweets(df)-> pd.DataFrame:\n",
        "  tweets = df.Tweet.to_list()\n",
        "  # df['TweetTokens'] = [emoji.demojize(text_processor.pre_process_doc(tweet),language = 'en') for tweet in tweets] # Translates emoji in to word and preprocesss\n",
        "  # df['TweetTokens'] = [text_processor.pre_process_doc(tweet) for tweet in tweets] # preprocesss\n",
        "  # tweets_processed = [text_processor.pre_process_doc(tweet) for tweet in tweets] # preprocesss\n",
        "  # for tweet in tweets_processed:\n",
        "  #   for index, token in enumerate(tweet):\n",
        "  #     if emoji.is_emoji(token):\n",
        "  #       tweet[index] = emoji.demojize(token, language = 'en')\n",
        "\n",
        "  tweets_processed = [\" \".join(text_processor.pre_process_doc(tweet)) for tweet in tweets] # preprocesss\n",
        "  # print (tweets_processed)\n",
        "  for index, tweet in enumerate(tweets_processed):\n",
        "      tweets_processed[index] = emoji.demojize(tweet, language = 'en')\n",
        "  \n",
        "  df['TweetTokens'] = tweets_processed\n",
        "  # print(df)\n",
        "  return df"
      ],
      "metadata": {
        "id": "pfmJjAHrujuX"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_EI_reg_train = preprocess_tweets(df_EI_reg_train)\n",
        "# df_V_reg_train = preprocess_tweets(df_V_reg_train)\n",
        "\n",
        "for name, df in dict_df.items():\n",
        "  df = preprocess_tweets(df)\n"
      ],
      "metadata": {
        "id": "hxt99UC2xI2x"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_V_reg_train"
      ],
      "metadata": {
        "id": "-sRSUuUl7BfC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO : \n",
        "* remove stop words\n",
        "* stem\n",
        "* lemmetize\n"
      ],
      "metadata": {
        "id": "3los9BZbk2m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_V_reg_train.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtRWK8D6mMzk",
        "outputId": "630ca44d-fe9f-4c81-9b8d-ab96d7de36e9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'Tweet', 'Affect Dimension', 'Intensity Score', 'TweetTokens'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def subset_df(df):\n",
        "  return df[['TweetTokens','Intensity Score']]"
      ],
      "metadata": {
        "id": "_zoGr_W3mYgK"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_df_subset ={name+\"_subset\": subset_df(df) for name, df in dict_df.items() }"
      ],
      "metadata": {
        "id": "kDsCZb5ExgA9"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (dict_df_subset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcw_mO9Y1bP-",
        "outputId": "8d4db71b-a5b1-4d58-b8c7-118b24559dba"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'df_EI_reg_train_subset':                                             TweetTokens  Intensity Score\n",
            "0     <user> <user> shut up hashtags are cool <hasht...            0.562\n",
            "1     it makes me so fucking irate jesus . nobody is...            0.750\n",
            "2     lol adam the bull with his fake outrage . <rep...            0.417\n",
            "3     <user> passed away early this morning in a fas...            0.354\n",
            "4     <user> lol wow i was gonna say really ? ! <rep...            0.438\n",
            "...                                                 ...              ...\n",
            "1696  got a <money> tip from a drunk uber passenger ...            0.708\n",
            "1697  <user> <user> <user> <user> fucker blocked me ...            0.625\n",
            "1698                                <user> i look rabid            0.472\n",
            "1699  <user> i am not surprised , i would be fuming ...            0.479\n",
            "1700  <user> the pout tips me over the edge . i am m...            0.490\n",
            "\n",
            "[1701 rows x 2 columns], 'df_EI_reg_val_subset':                                            TweetTokens  Intensity Score\n",
            "0    ' we need to do something . something must be ...            0.517\n",
            "1    <user> <user> <user> <user> <user> we ' d be f...            0.726\n",
            "2    caleb had a nightmare about zombies . i had a ...            0.250\n",
            "3    <hashtag> cnn </hashtag> really needs to get o...            0.641\n",
            "4    <hashtag> dm me </hashtag> <hashtag> kik me </...            0.313\n",
            "..                                                 ...              ...\n",
            "383  time to <hashtag> impeach trump </hashtag> . h...            0.813\n",
            "384  had frustration dream that left me utterly f**...            0.844\n",
            "385  <user> i love you , but between online order d...            0.778\n",
            "386  quick note about insta stories how the f do yo...            0.500\n",
            "387  <hashtag> revenge </hashtag> or <hashtag> chan...            0.397\n",
            "\n",
            "[388 rows x 2 columns], 'df_EI_reg_test_subset':                                             TweetTokens  Intensity Score\n",
            "0     <user> i know you mean well but i am offended ...            0.734\n",
            "1     let go of resentment , it will hold you back ,...            0.422\n",
            "2     no , i am not ' depressed because of the weath...            0.663\n",
            "3     <hashtag> amarnath terror attack </hashtag> mu...            0.703\n",
            "4     prepare to suffer the sting of ghost rider ' s...            0.719\n",
            "...                                                 ...              ...\n",
            "997   that morning when you get half - way to work a...            0.629\n",
            "998   <user> <user> <user> i bet he needed to take a...            0.550\n",
            "999   <user> <user> <user> <user> ring a ring of ros...            0.424\n",
            "1000  have to go to a occupational services place fo...            0.597\n",
            "1001       of course molina <hashtag> bitter </hashtag>            0.547\n",
            "\n",
            "[1002 rows x 2 columns], 'df_V_reg_train_subset':                                             TweetTokens  Intensity Score\n",
            "0                    <user> yeah ! <happy> playing well            0.600\n",
            "1     at least i do not have a guy trying to discour...            0.484\n",
            "2     <allcaps> uplift </allcaps> : if you are still...            0.563\n",
            "3     . <repeated> at your age , the heyday in the b...            0.450\n",
            "4     i was so embarrassed when she saw us i was lik...            0.233\n",
            "...                                                 ...              ...\n",
            "1176  <user> do not think ian knew of pavel . he kne...            0.613\n",
            "1177  i lost my wallet lol . <repeated> again . <rep...            0.234\n",
            "1178  repentance , and trusting in christ . it is lo...            0.700\n",
            "1179  <user> :flushed_face: chewing what ? <hashtag>...            0.565\n",
            "1180  i am so anxious all the time leaving the house...            0.278\n",
            "\n",
            "[1181 rows x 2 columns], 'df_V_reg_val_subset':                                            TweetTokens  Intensity Score\n",
            "0    so <user> site crashes everytime i try to book...            0.141\n",
            "1    theme of week : ask the lord for strength & pe...            0.317\n",
            "2    <user> why announcing so late , it will be har...            0.145\n",
            "3    the greatest happiness is seeing someone you l...            0.813\n",
            "4    omg so grateful to have an education but ive b...            0.672\n",
            "..                                                 ...              ...\n",
            "444  idk why but when i help someone , a smile come...            0.903\n",
            "445  i think ' sleep ' is my favorite from how did ...            0.328\n",
            "446  what does amelia want ? ! <repeated> sarah was...            0.690\n",
            "447  it â€™ s lack of <hashtag> faith </hashtag> that...            0.500\n",
            "448  james clapper ' scary and disturbing ' . <hash...            0.296\n",
            "\n",
            "[449 rows x 2 columns], 'df_V_reg_test_subset':                                            TweetTokens  Intensity Score\n",
            "0         gm and have a <hashtag> tuesday </hashtag> !            0.589\n",
            "1    <user> but you have a lot of time for tweeting...            0.500\n",
            "2    i graduated yesterday and already had <number>...            0.550\n",
            "3    <user> seriously . <repeated> i have been sitt...            0.633\n",
            "4    whether my glass is half empty or its half ful...            0.750\n",
            "..                                                 ...              ...\n",
            "932  premier league teams should fear next seasons ...            0.507\n",
            "933  how are you my love ? <user> love youu ! <repe...            0.867\n",
            "934  ' she is the clothed with strength and dignity...            0.516\n",
            "935  my dads big day is only less than <number> wee...            0.823\n",
            "936  and let the depression take the stage once mor...            0.141\n",
            "\n",
            "[937 rows x 2 columns]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZT_7DbHv8W44"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Pytorch Datasets"
      ],
      "metadata": {
        "id": "Wi-dG9KP14u8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Vocabulary\n",
        "Before we create the [link text](https://)Dataset, we need to define a process to build our vocabulary. For this,\n",
        "Weâ€™ll create a â€œVocabularyâ€ class which will create the word-to-index and index-to-word mappings using only the train dataframe we created before\n",
        "Also, the â€œVocabularyâ€ class returns the numericalized version of each sentence in our dataframe. Eg: [â€˜iâ€™, â€˜loveâ€™, â€˜appleâ€™] -> [23, 54, 1220]. We need to convert the words to numbers as models expect each word in our vocabulary to be represented by a number"
      ],
      "metadata": {
        "id": "IUsh5RUf2BxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#               Define Vocabulary Class\n",
        "#######################################################\n",
        "\n",
        "class Vocabulary:\n",
        "  \n",
        "    '''\n",
        "    __init__ method is called by default as soon as an object of this class is initiated\n",
        "    we use this method to initiate our vocab dictionaries\n",
        "    '''\n",
        "    def __init__(self, freq_threshold = 3, max_size = 10000):\n",
        "        '''\n",
        "        freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
        "        max_size : max source vocab size. Eg. if set to 10,000, we pick the top 10,000 most frequent words and discard others\n",
        "        '''\n",
        "        #initiate the index to token dict\n",
        "        ## <PAD> -> padding, used for padding the shorter sentences in a batch to match the length of longest sentence in the batch\n",
        "        ## <UNK> -> words which are not found in the vocab are replace by this token\n",
        "        # self.itos = {0: '<PAD>', 1: '<UNK>', 2:'<NUMBER>', 3: '<CURRENCY>', 4: '<URL>'}\n",
        "        self.itos = {0: '<PAD>', 1: '<UNK>'}\n",
        "        \n",
        "        \n",
        "        #initiate the token to index dict\n",
        "        self.stoi = {k:j for j,k in self.itos.items()}\n",
        "        self.original_stoi = self.stoi.copy()\n",
        "#         print(self.stoi)\n",
        "        \n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.max_size = max_size\n",
        "    \n",
        "    '''\n",
        "    __len__ is used by dataloader later to create batches\n",
        "    '''\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "    \n",
        "    '''\n",
        "    a simple tokenizer to split on space and converts the sentence to list of words\n",
        "    '''\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "#         return [tok.strip() for tok in text.split(' ')]\n",
        "        return [tok.lower().strip() for tok in text.split(' ')] # this is commented out to avoid <NUMBER> ,<UNK> lowering\n",
        "#         return [tok.lower().strip() for tok in text.split(' ') if tok not in list(self.stoi.keys())] \n",
        "    \n",
        "    '''\n",
        "    build the vocab: create a dictionary mapping of index to string (itos) and string to index (stoi)\n",
        "    output ex. for stoi -> {'the':6, 'a':7, 'an':8}\n",
        "    '''\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        #calculate the frequencies of each word first to remove the words with freq < freq_threshold\n",
        "#         frequencies = {}  #init the freq dict\n",
        "        frequencies = {k:self.max_size+1 for _,k in self.itos.items()}  # updated so that intial ones are also part of this\n",
        "        \n",
        "        # idx = 5 #index from which we want our dict to start. We already used 4 indexes for pad, unk...\n",
        "        idx = len(self.original_stoi)\n",
        "        \n",
        "        #calculate freq of words\n",
        "        for sentence in sentence_list:\n",
        "            list_word = [tok.lower().strip() for tok in sentence.split(' ') if tok not in list(self.stoi.keys())] \n",
        "            for word in list_word:\n",
        "#             for word in self.tokenizer(sentence):\n",
        "                \n",
        "                if word not in frequencies.keys():\n",
        "                    frequencies[word]=1\n",
        "                else:\n",
        "                    \n",
        "                    frequencies[word]+=1\n",
        "                    \n",
        "#         print (\"----2-----\\n\",frequencies)\n",
        "        \n",
        "        #limit vocab by removing low freq words\n",
        "        frequencies = {k:v for k,v in frequencies.items() if v>self.freq_threshold} \n",
        "        \n",
        "#         print (\"----3-----\\n\",frequencies)\n",
        "        \n",
        "        #limit vocab to the max_size specified\n",
        "        frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx =5 for pad, unk...\n",
        "        \n",
        "#         print (\"----4-----\\n\",frequencies)\n",
        "            \n",
        "        #create vocab\n",
        "        for key in set(self.stoi.keys()):\n",
        "            frequencies.pop(key)\n",
        "        \n",
        "#         print (\"----5-----\\n\",frequencies)\n",
        "        \n",
        "        for word in frequencies.keys():\n",
        "            self.stoi[word] = idx\n",
        "            self.itos[idx] = word\n",
        "            idx+=1\n",
        "        \n",
        "#         print (\"----6-----\\n\",self.stoi)\n",
        "        \n",
        "    '''\n",
        "    convert the list of words to a list of corresponding indexes\n",
        "    '''    \n",
        "    def numericalize(self, text):\n",
        "        #tokenize text\n",
        "#         tokenized_text = self.tokenizer(text)\n",
        "#         print(\"---------\\n\",self.original_stoi.keys())\n",
        "        tokenized_text = []\n",
        "        for tok in text.split(' '):\n",
        "            if tok not in list(self.original_stoi.keys()):\n",
        "                tokenized_text.append(tok.lower().strip())\n",
        "            else:\n",
        "                tokenized_text.append(tok.strip())\n",
        "                \n",
        "#         tokenized_text = [tok.lower().strip() for tok in text.split(' ') if tok not in list(self.original_stoi.keys())]\n",
        "        numericalized_text = []\n",
        "        for token in tokenized_text:\n",
        "            if token in self.stoi.keys():\n",
        "                numericalized_text.append(self.stoi[token])\n",
        "            else: #out-of-vocab words are represented by UNK token index\n",
        "                numericalized_text.append(self.stoi['<UNK>'])\n",
        "                \n",
        "        return numericalized_text"
      ],
      "metadata": {
        "id": "F7xZg1XcwLtt"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # #create a vocab class with freq_threshold=0 and max_size=100\n",
        "# voc = Vocabulary(0, 100)\n",
        "# sentence_list = ['that is a cat CAT', 'that is not a dog']\n",
        "# #build vocab\n",
        "# voc.build_vocabulary(sentence_list)\n",
        "\n",
        "# print('index to string: ',voc.itos)\n",
        "# print('string to index:',voc.stoi)\n",
        "\n",
        "# print('numericalize -> cat and a dog <URL>: ', voc.numericalize('cat and a dog <NUMBER>'))"
      ],
      "metadata": {
        "id": "COQ10m7Pv-sR"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xa1pUuSH8Uoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Train_Dataset\n",
        "We first inherit PyTorch's Dataset class.\n",
        "Then, we initialize and build the vocabs for subject in our train data frame.\n",
        "Then, we use the getitem() method to numericalize the subject 1 example at a time for the data loader (a function to load data in batches)."
      ],
      "metadata": {
        "id": "uwvxG4Op8YUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#               Define Train_Dataset class\n",
        "#######################################################\n",
        "\n",
        "class Train_Dataset(Dataset):\n",
        "    '''\n",
        "    Initiating Variables\n",
        "    df: the training dataframe\n",
        "    subject : the name of target text column in the dataframe\n",
        "    transform : If we want to add any augmentation\n",
        "    freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
        "    vocab_max_size : max  vocab size\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, df, subject, label_col, max_sentence_length = 150, transform=None, freq_threshold = 5,\n",
        "                vocab_max_size = 50000):\n",
        "    \n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        \n",
        "        #get body and label\n",
        "        self.subject_texts = self.df[subject]\n",
        "        self.labels = self.df[label_col]\n",
        "        \n",
        "        \n",
        "        ##VOCAB class has been created above\n",
        "        #Initialize vocab object and build vocabulary\n",
        "        self.vocab = Vocabulary(freq_threshold, vocab_max_size)\n",
        "        self.vocab.build_vocabulary(self.subject_texts.tolist())\n",
        "        self.max_sentence_length = max_sentence_length\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    '''\n",
        "    __getitem__ runs on 1 example at a time. Here, we get an example at index and return its numericalize source and\n",
        "    target values using the vocabulary objects we created in __init__\n",
        "    '''\n",
        "    def __getitem__(self, index):\n",
        "        subject_text = self.subject_texts[index]\n",
        "        label = self.labels[index]\n",
        "#         print(subject_text)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            subject_text = self.transform(subject_text)\n",
        "            \n",
        "        #numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
        "        numerialized_subject =[]\n",
        "        numerialized_subject += self.vocab.numericalize(subject_text)\n",
        "        \n",
        "        while len(numerialized_subject) < self.max_sentence_length:\n",
        "            numerialized_subject.append(0)\n",
        "        \n",
        "        #convert the list to tensor and return\n",
        "        return torch.tensor(numerialized_subject[:self.max_sentence_length]),torch.tensor(label)\n",
        "#         return torch.tensor(numerialized_subject[:self.train_dataset.max_sentence_length]),label"
      ],
      "metadata": {
        "id": "LRsK6LkK8iij"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = dict_df_subset['df_EI_reg_train_subset']\n",
        "df_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "-XKkT1VABUHX",
        "outputId": "1fcfb12e-e515-4db4-d073-8d517edbeb7e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            TweetTokens  Intensity Score\n",
              "0     <user> <user> shut up hashtags are cool <hasht...            0.562\n",
              "1     it makes me so fucking irate jesus . nobody is...            0.750\n",
              "2     lol adam the bull with his fake outrage . <rep...            0.417\n",
              "3     <user> passed away early this morning in a fas...            0.354\n",
              "4     <user> lol wow i was gonna say really ? ! <rep...            0.438\n",
              "...                                                 ...              ...\n",
              "1696  got a <money> tip from a drunk uber passenger ...            0.708\n",
              "1697  <user> <user> <user> <user> fucker blocked me ...            0.625\n",
              "1698                                <user> i look rabid            0.472\n",
              "1699  <user> i am not surprised , i would be fuming ...            0.479\n",
              "1700  <user> the pout tips me over the edge . i am m...            0.490\n",
              "\n",
              "[1701 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8fddddc-faba-41ca-89e5-72053febbb2b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TweetTokens</th>\n",
              "      <th>Intensity Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;user&gt; &lt;user&gt; shut up hashtags are cool &lt;hasht...</td>\n",
              "      <td>0.562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it makes me so fucking irate jesus . nobody is...</td>\n",
              "      <td>0.750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lol adam the bull with his fake outrage . &lt;rep...</td>\n",
              "      <td>0.417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;user&gt; passed away early this morning in a fas...</td>\n",
              "      <td>0.354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;user&gt; lol wow i was gonna say really ? ! &lt;rep...</td>\n",
              "      <td>0.438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1696</th>\n",
              "      <td>got a &lt;money&gt; tip from a drunk uber passenger ...</td>\n",
              "      <td>0.708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1697</th>\n",
              "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; &lt;user&gt; fucker blocked me ...</td>\n",
              "      <td>0.625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1698</th>\n",
              "      <td>&lt;user&gt; i look rabid</td>\n",
              "      <td>0.472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1699</th>\n",
              "      <td>&lt;user&gt; i am not surprised , i would be fuming ...</td>\n",
              "      <td>0.479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1700</th>\n",
              "      <td>&lt;user&gt; the pout tips me over the edge . i am m...</td>\n",
              "      <td>0.490</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1701 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8fddddc-faba-41ca-89e5-72053febbb2b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d8fddddc-faba-41ca-89e5-72053febbb2b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d8fddddc-faba-41ca-89e5-72053febbb2b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name,df in dict_df_subset.items():\n",
        "  if \"train\" in name:\n",
        "    dataset_name = name+\"_dataset\"\n",
        "    vars()[dataset_name] = Train_Dataset(df,'TweetTokens','Intensity Score', max_sentence_length =200) # dynamically assigning datasetname"
      ],
      "metadata": {
        "id": "MWMm8D1t9dcs"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = random.randint(0,len(df_EI_reg_train_subset_dataset))\n",
        "print(dict_df_subset['df_EI_reg_train_subset'].loc[i][['TweetTokens','Intensity Score']])\n",
        "print((df_EI_reg_train_subset_dataset[i][0]))\n",
        "print((df_EI_reg_train_subset_dataset[i][1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHSbz2sgGrS8",
        "outputId": "f642c06d-a34e-4ead-d1ad-6acad0a9db34"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TweetTokens        <user> * she could sense the anger stirring wi...\n",
            "Intensity Score                                                0.375\n",
            "Name: 1185, dtype: object\n",
            "tensor([  5, 267,  80, 253, 659,   7,  60,   1, 465, 130,  11, 444,  74,   1,\n",
            "         21,   1,   4, 267,  62, 325,  62,  65, 514,   1,  10,  65,  50, 337,\n",
            "         39,   1,   4,  34,  34,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0])\n",
            "tensor(0.3750, dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_obj = Train_Dataset(df_train,'TweetTokens','Intensity Score', max_sentence_length =200)"
      ],
      "metadata": {
        "id": "NBq8Pz1JA7wO"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = random.randint(0,len(train_dataset_obj))\n",
        "print(df_train.loc[i][['TweetTokens','Intensity Score']])\n",
        "print((train_dataset_obj[i][1]))\n",
        "print(len(train_dataset_obj[i][0]))\n",
        "print(train_dataset_obj[i][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxIIq74eCBTI",
        "outputId": "d8101b80-6c18-465c-cecc-b0ee00e8e709"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TweetTokens        <user> <user> <user> my snap is andriaprebles ...\n",
            "Intensity Score                                                0.271\n",
            "Name: 300, dtype: object\n",
            "tensor(0.2710, dtype=torch.float64)\n",
            "200\n",
            "tensor([ 5,  5,  5, 20, 82, 15,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df_EI_reg_train_subset_dataset)"
      ],
      "metadata": {
        "id": "arW3Nf90AwCV"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i = random.randint(0,len(df_EI_reg_train_subset_dataset))\n",
        "# # print(train.loc[i][['body','label']])\n",
        "# print(type(df_EI_reg_train_subset_dataset[i][1]))\n",
        "# len(df_EI_reg_train_subset_dataset[i][0])"
      ],
      "metadata": {
        "id": "5csRi0f3AsAA"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Validation Dataset"
      ],
      "metadata": {
        "id": "t0ji62MwaU8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#               Define Dataset Class\n",
        "#######################################################\n",
        "\n",
        "class Validation_Dataset(Dataset):\n",
        "    def __init__(self, train_dataset, df, subject, label_col, transform = None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        \n",
        "        #train dataset will be used as lookup for vocab\n",
        "        self.train_dataset = train_dataset\n",
        "        \n",
        "        #get body and label\n",
        "        self.subject_texts = self.df[subject]\n",
        "        self.labels = self.df[label_col]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        \n",
        "        subject_text = self.subject_texts[index]\n",
        "        label = self.labels[index]\n",
        "#         print(subject_text)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            subject_text = self.transform(subject_text)\n",
        "            \n",
        "            \n",
        "\n",
        "        #numericalize texts ['cat', 'in', 'a', 'bag'] -> [12,2,9,24]\n",
        "        numerialized_subject = []\n",
        "        numerialized_subject += self.train_dataset.vocab.numericalize(subject_text)\n",
        "#         print(\"max sentence length\", self.train_dataset.max_sentence_length)\n",
        "        while len(numerialized_subject) < self.train_dataset.max_sentence_length:\n",
        "            numerialized_subject.append(0)\n",
        "            \n",
        "\n",
        "        \n",
        "        #convert the list to tensor and return\n",
        "#         return torch.tensor(numerialized_subject),label\n",
        "\n",
        "#         #convert the list to tensor and return\n",
        "        return torch.tensor(numerialized_subject[:self.train_dataset.max_sentence_length]),torch.tensor(label)\n",
        "\n"
      ],
      "metadata": {
        "id": "Bzk0WNQmaiO7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}