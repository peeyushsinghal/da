{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK4BC+NUg3lADdLnFkmCPK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peeyushsinghal/da/blob/main/mitigating_bias_sa_da_v16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mitigating bias in sentiment analysis using domain adaptation"
      ],
      "metadata": {
        "id": "smhY2FyQoMJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchtext==0.10.0 --quiet # DOWNGRADE YOUR TORCHTEXT\n",
        "! pip install ekphrasis --quiet # library to pre process twitter data\n",
        "! pip install emoji --upgrade --quiet #library to deal with emoji data"
      ],
      "metadata": {
        "id": "RS_tIhlS2jzV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import statements\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.legacy.data import Dataset, Field, TabularDataset, BucketIterator\n",
        "from torchtext.vocab import GloVe\n",
        "import numpy as np\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "import emoji\n",
        "from torchtext.legacy.vocab import Vectors\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch.optim as optim\n",
        "import scipy.stats as stats\n",
        "from statistics import mean"
      ],
      "metadata": {
        "id": "ObCLSMyRohfP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on:{}\".format(DEVICE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzQzb_SFoqdX",
        "outputId": "b7596af9-181e-483c-c91a-1bd1feeb57d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on:cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading"
      ],
      "metadata": {
        "id": "xjIR3OrQoSEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rI_zEuloY4U",
        "outputId": "270911f9-acae-4628-c9c0-34189f5cbdb1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Configuration"
      ],
      "metadata": {
        "id": "o_kVI7furv8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/semeval-2018'\n",
        "\n",
        "DATA_DIR = os.path.join(BASE_PATH,'datasets')\n",
        "TARGET_DIR = os.path.join(BASE_PATH,'targetdataset')\n",
        "\n",
        "MODEL_DIR = os.path.join(BASE_PATH,'models')\n",
        "REF_DIR = os.path.join(BASE_PATH,'reference')\n",
        "\n",
        "MAX_SIZE = 50\n",
        "MAX_VOCAB_SIZE = 10000\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "TARGET_BATCH_SIZE = 8\n",
        "\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "  os.makedirs(MODEL_DIR)\n",
        "  print(\"The new directory is created!\")\n"
      ],
      "metadata": {
        "id": "052h66kcr6KV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data configuration\n",
        "\n",
        "class TASK1(object):\n",
        "  \n",
        "    EI_reg = {\n",
        "        'anger': {\n",
        "            'train': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/training/EI-reg-En-anger-train.txt'),\n",
        "            'dev': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/development/2018-EI-reg-En-anger-dev.txt'),\n",
        "            'gold': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/test-gold/2018-EI-reg-En-anger-test-gold.txt')\n",
        "                },\n",
        "        'joy': {\n",
        "                'train': os.path.join(\n",
        "                    DATA_DIR, 'task1/EI-reg/training/EI-reg-En-joy-train.txt'),\n",
        "                'dev': os.path.join(\n",
        "                    DATA_DIR, 'task1/EI-reg/development/2018-EI-reg-En-joy-dev.txt'),\n",
        "                'gold': os.path.join(\n",
        "                    DATA_DIR, 'task1/EI-reg/test-gold/2018-EI-reg-En-joy-test-gold.txt')\n",
        "                },\n",
        "        'fear': {\n",
        "            'train': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/training/EI-reg-En-fear-train.txt'),\n",
        "            'dev': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/development/2018-EI-reg-En-fear-dev.txt'),\n",
        "            'gold': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/test-gold/2018-EI-reg-En-fear-test-gold.txt')\n",
        "                },\n",
        "        'sadness': {\n",
        "            'train': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/training/EI-reg-En-sadness-train.txt'),\n",
        "            'dev': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/development/2018-EI-reg-En-sadness-dev.txt'),\n",
        "            'gold': os.path.join(\n",
        "                DATA_DIR, 'task1/EI-reg/test-gold/2018-EI-reg-En-sadness-test-gold.txt')\n",
        "                }                     \n",
        "        }\n",
        "\n",
        "    V_reg = {\n",
        "        'train': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-train.txt'),\n",
        "        'dev': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-dev.txt'),\n",
        "        'gold': os.path.join(\n",
        "            DATA_DIR, 'task1/V-reg/2018-Valence-reg-En-test-gold.txt')\n",
        "             }\n",
        "\n",
        "    EEC = {\n",
        "        'eec': os.path.join(\n",
        "            DATA_DIR, 'task1/Equity-Evaluation-Corpus/Equity-Evaluation-Corpus.csv')\n",
        "             }"
      ],
      "metadata": {
        "id": "qn0ixH3-sC1y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Source Data\n",
        "Parsing Emotion and Valence regression data : `format [ID\tTweet\tAffect Dimension\tIntensity Score]`"
      ],
      "metadata": {
        "id": "Gvy12RtIsqyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_reg(data_file, label_format='tuple'):\n",
        "    \"\"\"\n",
        "    This is for datasets for the EI-reg and V-reg English tasks \n",
        "    Returns:\n",
        "        df: dataframe with columns in the first row of file [ID-Tweet-Affect Dimension-Intensity Score]\n",
        "    \"\"\"\n",
        "    with open(data_file, 'r') as fd:\n",
        "      data = [l.strip().split('\\t') for l in fd.readlines()]\n",
        "    # print(data)\n",
        "    df = pd.DataFrame (data[1:],columns=data[0])\n",
        "    csv_file_name = (data_file.split(\"/\")[-1]).split('.')[0]+\".csv\"\n",
        "    csv_file = df.to_csv(str(csv_file_name))\n",
        "    return csv_file_name\n"
      ],
      "metadata": {
        "id": "Df43LU3ztDAk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generic Source Data Parser"
      ],
      "metadata": {
        "id": "VTmAiHnBZBKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_csv(task, dataset, emotion='anger'):\n",
        "    if task == 'EI-reg':\n",
        "        data_train = TASK1.EI_reg[emotion][dataset]\n",
        "        csv_file_name = parse_reg(data_train)\n",
        "        return csv_file_name\n",
        "\n",
        "    elif task == 'V-reg':\n",
        "        data_train = TASK1.V_reg[dataset]\n",
        "\n",
        "        csv_file_name = parse_reg(data_train)\n",
        "        return csv_file_name\n",
        "\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "e8eUaJgB0UhP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions = ['anger','joy','fear','sadness']\n",
        "dict_data ={'train':'train','dev':'val','gold':'test'}\n",
        "dict_file_name ={}\n",
        "for emotion in emotions:\n",
        "  for data_info, data_usage in dict_data.items():\n",
        "    file_name = str('file_EI_'+ emotion + \"_\" + data_usage)\n",
        "    dict_file_name[file_name] = parse_csv('EI-reg', data_info, emotion)\n",
        "\n",
        "    file_name2 = str('file_V_'+ data_usage)\n",
        "    dict_file_name[file_name2] = parse_csv('V-reg', data_info)\n",
        "\n",
        "(dict_file_name)"
      ],
      "metadata": {
        "id": "ilCPodjN0n1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a655c55-cf43-4072-eb31-e9e34a5dc4aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'file_EI_anger_train': 'EI-reg-En-anger-train.csv',\n",
              " 'file_V_train': '2018-Valence-reg-En-train.csv',\n",
              " 'file_EI_anger_val': '2018-EI-reg-En-anger-dev.csv',\n",
              " 'file_V_val': '2018-Valence-reg-En-dev.csv',\n",
              " 'file_EI_anger_test': '2018-EI-reg-En-anger-test-gold.csv',\n",
              " 'file_V_test': '2018-Valence-reg-En-test-gold.csv',\n",
              " 'file_EI_joy_train': 'EI-reg-En-joy-train.csv',\n",
              " 'file_EI_joy_val': '2018-EI-reg-En-joy-dev.csv',\n",
              " 'file_EI_joy_test': '2018-EI-reg-En-joy-test-gold.csv',\n",
              " 'file_EI_fear_train': 'EI-reg-En-fear-train.csv',\n",
              " 'file_EI_fear_val': '2018-EI-reg-En-fear-dev.csv',\n",
              " 'file_EI_fear_test': '2018-EI-reg-En-fear-test-gold.csv',\n",
              " 'file_EI_sadness_train': 'EI-reg-En-sadness-train.csv',\n",
              " 'file_EI_sadness_val': '2018-EI-reg-En-sadness-dev.csv',\n",
              " 'file_EI_sadness_test': '2018-EI-reg-En-sadness-test-gold.csv'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess tweets"
      ],
      "metadata": {
        "id": "9IO21vV_2wQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reference : https://github.com/cbaziotis/ekphrasis\n",
        "\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI6dZ6hY009m",
        "outputId": "18701c38-f422-47d5-82f1-2d9730456b05"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #### Example checks of text_processor\n",
        "# sentences = [\n",
        "#     \"CANT WAIT for the new season of #TwinPeaks ＼(^o^)／!!! #davidlynch #tvseries :)))\",\n",
        "#     \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies :/\",\n",
        "#     \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! :-D http://sentimentsymposium.com/.\",\n",
        "#     \"@MGBarbieri @SpalkTalk a@b.com And just saw your LinkedIn comment after I sent this! Thanks for the message :) 😀\",\n",
        "#     \"💙💛🏆 @GeorgeePitman Young Player of The Season 🏆💛💙 #irony #actuallyseventy\"\n",
        "# ]\n",
        "\n",
        "# for s in sentences:\n",
        "#     print(\" \".join(text_processor.pre_process_doc(s)))\n",
        "# # print ([text_processor.pre_process_doc(s) for s in sentences])"
      ],
      "metadata": {
        "id": "i8ixRElK29DM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_tweet(tweet): \n",
        "  tweet_processed = text_processor.pre_process_doc(tweet)\n",
        "  # print (tweet_processed)\n",
        "  final_list =[]\n",
        "  for index, tweet in enumerate(tweet_processed):\n",
        "      final_list.append(emoji.demojize(tweet, language = 'en'))\n",
        "  \n",
        "  # print(df)\n",
        "  return final_list"
      ],
      "metadata": {
        "id": "0CNdYx083SRU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #### Example checks of pre-processing\n",
        "# sentences = [\n",
        "#     \"CANT WAIT for the new season of #TwinPeaks ＼(^o^)／!!! #davidlynch #tvseries :)))\",\n",
        "#     \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies :/\",\n",
        "#     \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! :-D http://sentimentsymposium.com/.\",\n",
        "#     \"@MGBarbieri @SpalkTalk a@b.com And just saw your LinkedIn comment after I sent this! Thanks for the message :) 😀\",\n",
        "#     \"💙💛🏆 @GeorgeePitman Young Player of The Season 🏆💛💙 #irony #actuallyseventy\"\n",
        "# ]\n",
        "\n",
        "# for s in sentences:\n",
        "#   print(preprocess_tweet(s))\n",
        "#   # print(\" \".join(preprocess_tweet(s)))"
      ],
      "metadata": {
        "id": "WU-Ap0fS4Xu8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TorchText Treatment"
      ],
      "metadata": {
        "id": "x4MCSent0MSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_file_name.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_B3zfJbKcqya",
        "outputId": "583ca2e2-1577-480a-dd51-0ded6307cc1e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['file_EI_anger_train', 'file_V_train', 'file_EI_anger_val', 'file_V_val', 'file_EI_anger_test', 'file_V_test', 'file_EI_joy_train', 'file_EI_joy_val', 'file_EI_joy_test', 'file_EI_fear_train', 'file_EI_fear_val', 'file_EI_fear_test', 'file_EI_sadness_train', 'file_EI_sadness_val', 'file_EI_sadness_test'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_fields ={}\n",
        "list_name = list(set([\"_\".join(key.split(\"_\")[1:-1]) for key in list(dict_file_name.keys())]))\n",
        "\n",
        "for name in list_name:\n",
        "  field_tweet = Field(sequential=True, \n",
        "                      use_vocab = True, \n",
        "                      tokenize = preprocess_tweet, \n",
        "                      fix_length = MAX_SIZE, \n",
        "                      batch_first = True)\n",
        "  field_intensity = Field(sequential= False, \n",
        "                        dtype = torch.float,\n",
        "                        use_vocab = False)\n",
        "  fields = {\n",
        "    'Tweet':('tweet', field_tweet ), #\n",
        "    'Intensity Score': ('intensity',field_intensity) # Intensity Score is name of the dataset column, field_intensity is how we have defined the field, intensity is the name of the variable going fwd\n",
        "    }\n",
        "  \n",
        "  dict_fields[name] = fields\n",
        "\n",
        "  # dict_fields[name]= { 'field_tweet': Field(sequential=True,\n",
        "  #                                        use_vocab = True,\n",
        "  #                                        tokenize = preprocess_tweet,\n",
        "  #                                        fix_length = MAX_SIZE,\n",
        "  #                                        batch_first = True ), \n",
        "  #                           'field_intensity': Field(sequential= False,\n",
        "  #                                              dtype = torch.float,\n",
        "  #                                              use_vocab = False )}\n",
        "\n",
        "dict_fields"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Sdvf0mvi1UL",
        "outputId": "c818b0d8-81f4-4cfd-fbee-6904dfc2ba8f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'EI_sadness': {'Tweet': ('tweet',\n",
              "   <torchtext.legacy.data.field.Field at 0x7f0848e3a950>),\n",
              "  'Intensity Score': ('intensity',\n",
              "   <torchtext.legacy.data.field.Field at 0x7f0848e3a990>)},\n",
              " 'EI_anger': {'Tweet': ('tweet',\n",
              "   <torchtext.legacy.data.field.Field at 0x7f0848e3a9d0>),\n",
              "  'Intensity Score': ('intensity',\n",
              "   <torchtext.legacy.data.field.Field at 0x7f0848e3aa50>)},\n",
              " 'EI_fear': {'Tweet': ('tweet',\n",
              "   <torchtext.legacy.data.field.Field at 0x7f0848e3aa90>),\n",
              "  'Intensity Score': ('intensity',\n",
              "   <torchtext.legacy.data.field.Field at 0x7f0848e3a6d0>)},\n",
              " 'V': {'Tweet': ('tweet',\n",
              "   <torchtext.legacy.data.field.Field at 0x7f0848e3aad0>),\n",
              "  'Intensity Score': ('intensity',\n",
              "   <torchtext.legacy.data.field.Field at 0x7f0848e3ab10>)},\n",
              " 'EI_joy': {'Tweet': ('tweet',\n",
              "   <torchtext.legacy.data.field.Field at 0x7f0848e3ab50>),\n",
              "  'Intensity Score': ('intensity',\n",
              "   <torchtext.legacy.data.field.Field at 0x7f0848e3ab90>)}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_fields['EI_sadness']['Tweet'][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoPrAWhjjtx9",
        "outputId": "3641c426-e8ed-4c0a-c9a6-36b90c528367"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.legacy.data.field.Field at 0x7f0848e3a950>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WORKING CODE \n",
        "#dict_fields ={}\n",
        "# list_name = list(set([\"_\".join(key.split(\"_\")[1:-1]) for key in list(dict_file_name.keys())]))\n",
        "\n",
        "# for name in list_name:\n",
        "#   dict_fields[name]= { 'field_tweet': Field(sequential=True,\n",
        "#                                          use_vocab = True,\n",
        "#                                          tokenize = preprocess_tweet,\n",
        "#                                          fix_length = MAX_SIZE,\n",
        "#                                          batch_first = True ), \n",
        "#                             'field_intensity': Field(sequential= False,\n",
        "#                                                dtype = torch.float,\n",
        "#                                                use_vocab = False )}\n",
        "\n",
        "# dict_fields"
      ],
      "metadata": {
        "id": "QutIngnxc2Qt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dict_field_values = {}\n",
        "# for name in list_name:\n",
        "#   dict_field_values[name] = {\n",
        "      \n",
        "#   }\n",
        "\n",
        "\n",
        "  # fields = {\n",
        "#     'Tweet':('tweet', field_tweet ), #\n",
        "#     'Intensity Score': ('intensity',field_intensity) # Intensity Score is name of the dataset column, field_intensity is how we have defined the field, intensity is the name of the variable going fwd\n",
        "#     }"
      ],
      "metadata": {
        "id": "ATwAAtEcfI2k"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dict_fields ={}\n",
        "# for name in list(dict_dataset.keys()):\n",
        "#   print(name,dict_dataset[name][\"train_dataset\"])\n",
        "#   dict_fields[name]= { 'field_tweet': Field(sequential=True,\n",
        "#                                          use_vocab = True,\n",
        "#                                          tokenize = preprocess_tweet,\n",
        "#                                          fix_length = MAX_SIZE,\n",
        "#                                          batch_first = True ), \n",
        "#                             'field_intensity': Field(sequential= False,\n",
        "#                                                dtype = torch.float,\n",
        "#                                                use_vocab = False )}"
      ],
      "metadata": {
        "id": "IzYfk96YchuU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_dataset ={}\n",
        "for file_key, file_name in dict_file_name.items():\n",
        "  # print(file_key,file_name)\n",
        "  if \"train\" in (file_key.split(\"_\")[-1]):\n",
        "    head_name = \"_\".join(file_key.split(\"_\")[0:-1])\n",
        "    base_name = \"_\".join(file_key.split(\"_\")[1:-1])\n",
        "    # print(base_name)\n",
        "    train_file = dict_file_name[head_name+\"_train\"]\n",
        "    val_file = dict_file_name[head_name+\"_val\"]\n",
        "    test_file =  dict_file_name[head_name+\"_test\"]\n",
        "\n",
        "    train, val, test =TabularDataset.splits( path = './', \n",
        "                                            train = train_file, \n",
        "                                            validation = val_file, \n",
        "                                            test = test_file,\n",
        "                                            format = 'csv', \n",
        "                                            fields = dict_fields[base_name])\n",
        "    \n",
        "    # print(train_file,val_file,test_file)\n",
        "    \n",
        "    # dict_dataset[base_name+\"_train\"], dict_dataset[base_name+\"_val\"],dict_dataset[base_name+\"_test\"]=TabularDataset.splits( path = './',\n",
        "    #                                                                                                                        train = train_file,\n",
        "    #                                                                                                                        validation = val_file,\n",
        "    #                                                                                                                        test = test_file,\n",
        "    #                                                                                                                        format = 'csv',\n",
        "    #                                                                                                                        fields = fields)\n",
        "    dict_dataset[base_name] = {\"train_dataset\": train, \"val_dataset\":val,\"test_dataset\":test}"
      ],
      "metadata": {
        "id": "aSX_6QOxd9Pj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## BEFORE USING DICT_FIELDS, USING GENERIC FIELDS HERE\n",
        "\n",
        "# dict_dataset ={}\n",
        "# for file_key, file_name in dict_file_name.items():\n",
        "#   # print(file_key,file_name)\n",
        "#   if \"train\" in (file_key.split(\"_\")[-1]):\n",
        "#     head_name = \"_\".join(file_key.split(\"_\")[0:-1])\n",
        "#     base_name = \"_\".join(file_key.split(\"_\")[1:-1])\n",
        "#     # print(base_name)\n",
        "#     train_file = dict_file_name[head_name+\"_train\"]\n",
        "#     val_file = dict_file_name[head_name+\"_val\"]\n",
        "#     test_file =  dict_file_name[head_name+\"_test\"]\n",
        "\n",
        "#     train, val, test =TabularDataset.splits( path = './', \n",
        "#                                             train = train_file, \n",
        "#                                             validation = val_file, \n",
        "#                                             test = test_file,\n",
        "#                                             format = 'csv', \n",
        "#                                             fields = fields)\n",
        "    \n",
        "#     # print(train_file,val_file,test_file)\n",
        "    \n",
        "#     # dict_dataset[base_name+\"_train\"], dict_dataset[base_name+\"_val\"],dict_dataset[base_name+\"_test\"]=TabularDataset.splits( path = './',\n",
        "#     #                                                                                                                        train = train_file,\n",
        "#     #                                                                                                                        validation = val_file,\n",
        "#     #                                                                                                                        test = test_file,\n",
        "#     #                                                                                                                        format = 'csv',\n",
        "#     #                                                                                                                        fields = fields)\n",
        "#     dict_dataset[base_name] = {\"train_dataset\": train, \"val_dataset\":val,\"test_dataset\":test}"
      ],
      "metadata": {
        "id": "xbFJnISsl9Pm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVJn373elFpq",
        "outputId": "ed63f5b3-19b8-438f-da01-2fc4e46ee30f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'EI_anger': {'train_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f0872307cd0>,\n",
              "  'val_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f087ab4e1d0>,\n",
              "  'test_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f0848043910>},\n",
              " 'V': {'train_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f0848041a10>,\n",
              "  'val_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f084734c590>,\n",
              "  'test_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f0846a97290>},\n",
              " 'EI_joy': {'train_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f0846852dd0>,\n",
              "  'val_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f0847344350>,\n",
              "  'test_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f08489e0190>},\n",
              " 'EI_fear': {'train_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f0847c3bfd0>,\n",
              "  'val_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f08483f9f50>,\n",
              "  'test_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f0847f15e90>},\n",
              " 'EI_sadness': {'train_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f08489cb790>,\n",
              "  'val_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f08455fea50>,\n",
              "  'test_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f08452bc590>}}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdLacmxAtsmD",
        "outputId": "e3bc09fd-7fbd-4208-f398-ce2c9bafe311"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'EI_anger': {'train_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f0872307cd0>,\n",
              "  'val_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f087ab4e1d0>,\n",
              "  'test_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f0848043910>},\n",
              " 'V': {'train_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f0848041a10>,\n",
              "  'val_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f084734c590>,\n",
              "  'test_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f0846a97290>},\n",
              " 'EI_joy': {'train_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f0846852dd0>,\n",
              "  'val_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f0847344350>,\n",
              "  'test_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f08489e0190>},\n",
              " 'EI_fear': {'train_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f0847c3bfd0>,\n",
              "  'val_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f08483f9f50>,\n",
              "  'test_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f0847f15e90>},\n",
              " 'EI_sadness': {'train_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f08489cb790>,\n",
              "  'val_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f08455fea50>,\n",
              "  'test_dataset': <torchtext.legacy.data.dataset.TabularDataset at 0x7f08452bc590>}}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data, valid_data, test_data = TabularDataset.splits( path = './', \n",
        "#                                               train = file_EI_reg_train,\n",
        "#                                               validation = file_EI_reg_val, \n",
        "#                                               test = file_EI_reg_test,\n",
        "#                                               format = 'csv',\n",
        "#                                               fields = fields\n",
        "#                                           )"
      ],
      "metadata": {
        "id": "w8XrMJMu7jUa"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(train_data[0].__dict__.keys(),\"\\n\",valid_data[0].__dict__.values())"
      ],
      "metadata": {
        "id": "1rn6sv6QAuf1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in dict_dataset.items():\n",
        "  # count = 0\n",
        "  for name, dataset in value.items():\n",
        "    for example in dataset.examples:\n",
        "      print(key, name, example.tweet, example.intensity)\n",
        "      break\n",
        "\n",
        "\n",
        "# for example in test_data.examples:\n",
        "#   print(example.tweet, example.intensity)\n",
        "#   count += 1\n",
        "#   if count > 2:\n",
        "#     break"
      ],
      "metadata": {
        "id": "Sx-R5H6ALfwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b23ee35-6edf-4f29-d0a9-f722a844ce85"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EI_anger train_dataset ['<user>', '<user>', 'shut', 'up', 'hashtags', 'are', 'cool', '<hashtag>', 'offended', '</hashtag>'] 0.562\n",
            "EI_anger val_dataset [\"'\", 'we', 'need', 'to', 'do', 'something', '.', 'something', 'must', 'be', 'done', '!', '<repeated>', \"'\", '\\\\', 'n', '\\\\', 'nyour', 'anxiety', 'is', 'amusing', '.', 'nothing', 'will', 'be', 'done', '.', 'despair', '.'] 0.517\n",
            "EI_anger test_dataset ['<user>', 'i', 'know', 'you', 'mean', 'well', 'but', 'i', 'am', 'offended', '.', 'prick', '.'] 0.734\n",
            "V train_dataset ['<user>', 'yeah', '!', '<happy>', 'playing', 'well'] 0.600\n",
            "V val_dataset ['so', '<user>', 'site', 'crashes', 'everytime', 'i', 'try', 'to', 'book', '-', 'how', 'do', 'they', 'help', '?', 'tell', 'me', 'there', \"'\", 's', 'nothing', 'wrong', '&', 'hang', 'up', '<hashtag>', 'furious', '</hashtag>', '<hashtag>', 'helpless', '</hashtag>', '<user>'] 0.141\n",
            "V test_dataset ['gm', 'and', 'have', 'a', '<hashtag>', 'tuesday', '</hashtag>', '!'] 0.589\n",
            "EI_joy train_dataset ['<user>', 'quite', 'saddened', '.', '<repeated>', 'no', 'us', 'dates', ',', 'no', 'joyous', 'anticipation', 'of', 'attending', 'a', 'dg', 'concert', '(', 'since', '<number>', ')', '.', 'happy', 'you', 'are', 'keeping', 'busy', '.'] 0.140\n",
            "EI_joy val_dataset ['<user>', 'oh', '!', 'that', 'actually', 'was', 'my', 'first', 'guess', 'but', '.', '<repeated>', 'i', 'thought', 'he', 'was', 'too', 'dark', 'for', 'an', 'irish', 'man', 'from', '<allcaps>', 'bce', '</allcaps>', '.', 'thanks', 'for', 'clearing', 'it', 'up', '<laugh>'] 0.470\n",
            "EI_joy test_dataset ['people', 'are', 'truly', '<hashtag>', 'amazing', '</hashtag>', '.', '<hashtag>', 'inspiring', '</hashtag>', 'day'] 0.712\n",
            "EI_fear train_dataset ['<user>', '<user>', '<hashtag>', 'revolting', '</hashtag>', 'cocks', 'if', 'you', 'think', 'i', 'am', 'sexy', '!'] 0.292\n",
            "EI_fear val_dataset ['good', 'morning', 'and', 'happy', 'tuesday', '!', 'i', 'hope', 'you', 'have', 'a', 'terrific', 'day', '!', 'enjoy', 'it', 'tons', ':grinning_face_with_big_eyes:'] 0.100\n",
            "EI_fear test_dataset ['<user>', 'that', \"'\", 's', 'the', 'horrific', 'irony', ';', 'while', 'we', \"'\", 'cleanse', \"'\", 'society', 'of', 'moral', 'value', ',', 'a', 'new', 'religion', 'will', 'rise', 'up', '&', 'fill', 'the', 'gap', '-', 'unhindered'] 0.596\n",
            "EI_sadness train_dataset ['this', 'the', 'most', 'depressing', 'shit', 'ever'] 0.861\n",
            "EI_sadness val_dataset ['we', 'have', 'the', 'opportunity', 'to', 'move', 'away', 'from', 'the', 'gloomy', 'nihilism', 'that', 'characterizes', 'the', 'reign', 'of', 'our', 'deeply', 'patriarchal', ',', 'dominator', 'culture', '.'] 0.446\n",
            "EI_sadness test_dataset ['<user>', 'is', 'a', 'bully', '.', 'plain', 'and', 'simple', '.'] 0.550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building iterator and Vocabulary"
      ],
      "metadata": {
        "id": "V3CoFQZDLtVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #WORKING\n",
        "# dict_fields ={}\n",
        "# for name in list(dict_dataset.keys()):\n",
        "#   print(name,dict_dataset[name][\"train_dataset\"])\n",
        "#   dict_fields[name]= { 'field_tweet': Field(sequential=True,\n",
        "#                                          use_vocab = True,\n",
        "#                                          tokenize = preprocess_tweet,\n",
        "#                                          fix_length = MAX_SIZE,\n",
        "#                                          batch_first = True ), \n",
        "#                             'field_intensity': Field(sequential= False,\n",
        "#                                                dtype = torch.float,\n",
        "#                                                use_vocab = False )}\n",
        "#   # dict_fields[name]['field_intensity'] = Field(sequential= False,\n",
        "#   #                                              dtype = torch.float,\n",
        "#   #                                              use_vocab = False )\n",
        "\n",
        "# # field_tweet = Field(sequential=True,\n",
        "# #                     use_vocab = True,\n",
        "# #                     tokenize = preprocess_tweet, \n",
        "# #                     fix_length = MAX_SIZE, \n",
        "# #                     batch_first = True\n",
        "# #                     )\n",
        "\n",
        "# # field_intensity = Field(sequential= False, \n",
        "# #                         dtype = torch.float,\n",
        "# #                         use_vocab = False \n",
        "# #                         )"
      ],
      "metadata": {
        "id": "2I7Wbn5g1hK3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, value in dict_fields.items():\n",
        "  print(name, value['Tweet'][1])\n",
        "  value['Tweet'][1].build_vocab(dict_dataset[name]['train_dataset'],\n",
        "                                   max_size = MAX_VOCAB_SIZE,\n",
        "                                   min_freq = 1,\n",
        "                                   vectors = \"glove.6B.100d\",\n",
        "                                   unk_init=torch.Tensor.normal_)\n",
        "  value['Intensity Score'][1].build_vocab(dict_dataset[name]['train_dataset'])\n",
        "  # value['field_tweet'].build_vocab(dict_dataset[name]['train_dataset'],\n",
        "  #                                  max_size = MAX_VOCAB_SIZE,\n",
        "  #                                  min_freq = 1,\n",
        "  #                                  vectors = \"glove.6B.100d\",\n",
        "  #                                  unk_init=torch.Tensor.normal_)\n",
        "  # value['field_intensity'].build_vocab(dict_dataset[name]['train_dataset'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zfJr8Ctlfq8",
        "outputId": "1d35b3c6-ace6-4d5e-b5fe-239b1487dd3a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EI_sadness <torchtext.legacy.data.field.Field object at 0x7f0848e3a950>\n",
            "EI_anger <torchtext.legacy.data.field.Field object at 0x7f0848e3a9d0>\n",
            "EI_fear <torchtext.legacy.data.field.Field object at 0x7f0848e3aa90>\n",
            "V <torchtext.legacy.data.field.Field object at 0x7f0848e3aad0>\n",
            "EI_joy <torchtext.legacy.data.field.Field object at 0x7f0848e3ab50>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #WORKING\n",
        "# for name, value in dict_fields.items():\n",
        "#   value['field_tweet'].build_vocab(dict_dataset[name]['train_dataset'],\n",
        "#                                    max_size = MAX_VOCAB_SIZE,\n",
        "#                                    min_freq = 1,\n",
        "#                                    vectors = \"glove.6B.100d\",\n",
        "#                                    unk_init=torch.Tensor.normal_)\n",
        "#   value['field_intensity'].build_vocab(dict_dataset[name]['train_dataset'])"
      ],
      "metadata": {
        "id": "k4Khldql3Qxt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ORIGINAL\n",
        "# field_tweet.build_vocab(train_data, \n",
        "#                   max_size = MAX_VOCAB_SIZE,\n",
        "#                   min_freq = 1,\n",
        "#                   vectors = \"glove.6B.100d\",\n",
        "#                   unk_init=torch.Tensor.normal_)\n",
        "\n",
        "# vocab = field_tweet.build_vocab(train_data, \n",
        "#                   max_size = MAX_VOCAB_SIZE,\n",
        "#                   min_freq = 1,\n",
        "#                   vectors = \"glove.6B.100d\",\n",
        "#                   unk_init=torch.Tensor.normal_)\n",
        "# field_intensity.build_vocab(train_data)"
      ],
      "metadata": {
        "id": "IeSJaNSYT_D7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_iterator ={}\n",
        "for name, value in dict_dataset.items():\n",
        "  VALID_TEST_BATCH_SIZE= min(len(value['val_dataset']),len(value['test_dataset']) )\n",
        "  print(name, VALID_TEST_BATCH_SIZE)\n",
        "  train_iterator, val_iterator, test_iterator= BucketIterator.splits(\n",
        "      (value['train_dataset'], value['val_dataset'],value['test_dataset']),\n",
        "      batch_sizes= (BATCH_SIZE,VALID_TEST_BATCH_SIZE,VALID_TEST_BATCH_SIZE),\n",
        "      sort_key = lambda x: len(x.tweet),\n",
        "      sort_within_batch=True,\n",
        "      device = DEVICE,\n",
        "      shuffle= True)\n",
        "  \n",
        "  dict_iterator[name] = {\"train_iterator\": train_iterator, \"val_iterator\":val_iterator,\"test_iterator\":test_iterator}\n",
        "\n",
        "  # dict_iterator[name]['train_iterator'], dict_iterator[name]['val_iterator'], dict_iterator[name]['test_iterator'] = BucketIterator.splits((dict_dataset[name]['train_dataset'], dict_dataset[name]['val_dataset'],dict_dataset[name]['test_dataset']), \n",
        "  #                                                     batch_sizes= (BATCH_SIZE,VALID_TEST_BATCH_SIZE,VALID_TEST_BATCH_SIZE),\n",
        "  #                                                     sort_key = lambda x: len(x.tweet),\n",
        "  #                                                     sort_within_batch=True,\n",
        "  #                                                     device = DEVICE,\n",
        "  #                                                     shuffle= True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXfBt_u4oKJ5",
        "outputId": "54908aef-bdd9-4785-e551-dd5006dde835"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EI_anger 388\n",
            "V 449\n",
            "EI_joy 290\n",
            "EI_fear 389\n",
            "EI_sadness 397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_iterator.items()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0X9RtjD4ngO",
        "outputId": "506d0b6a-8c6f-43c5-b273-d37fa1dee5f6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('EI_anger', {'train_iterator': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f0848e35210>, 'val_iterator': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f0848e35050>, 'test_iterator': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f0848e2f210>}), ('V', {'train_iterator': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f0848e2f3d0>, 'val_iterator': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f0848e2f050>, 'test_iterator': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f0848e2f0d0>}), ('EI_joy', {'train_iterator': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f087ab4ee50>, 'val_iterator': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f087ab4e750>, 'test_iterator': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f0843347750>}), ('EI_fear', {'train_iterator': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f08433476d0>, 'val_iterator': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f0843347a50>, 'test_iterator': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f0843347950>}), ('EI_sadness', {'train_iterator': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f0843347d50>, 'val_iterator': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f0843347d90>, 'test_iterator': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f0843347cd0>})])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VALID_TEST_BATCH_SIZE = min (len(valid_data),len(test_data))\n",
        "# # print(VALID_TEST_BATCH_SIZE)\n",
        "\n",
        "# train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n",
        "#                                                       batch_sizes= (BATCH_SIZE,VALID_TEST_BATCH_SIZE,VALID_TEST_BATCH_SIZE),\n",
        "#                                                       sort_key = lambda x: len(x.tweet),\n",
        "#                                                       sort_within_batch=True,\n",
        "#                                                       device = DEVICE,\n",
        "#                                                       shuffle= True)"
      ],
      "metadata": {
        "id": "PYRrHy8HTcZM"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in dict_iterator.items():\n",
        "  for name, iterator in value.items():\n",
        "    for batch in iterator:\n",
        "      print(key, name, batch.tweet)\n",
        "      print(batch.intensity)\n",
        "      break\n",
        "    break\n",
        "  break\n",
        "    \n",
        "# count = 0a\n",
        "# for batch in train_iterator:\n",
        "#   print (batch.tweet)\n",
        "#   print (batch.intensity)\n",
        "#   count += 1\n",
        "#   if count > 2:\n",
        "#     break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si5ajBMXTb_m",
        "outputId": "a294932f-0413-4913-9799-ed620ab473ed"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EI_anger train_iterator tensor([[ 316,   21,   66,  802,  197,   19,  324,   95,  134,    4,    3,   66,\n",
            "            2,    3,  915,    2,    3,  767,    2,    3, 2142,    2,    3,  324,\n",
            "            2,    3,  802,    2,    3, 2006,    2,    3, 1496,    2,    3,  892,\n",
            "            2,    3,  240,    2,    3,  134,    2,    3,  290,    2,    1,    1,\n",
            "            1,    1],\n",
            "        [   3,    7,  325,   16,  234,    2,    3, 2339,    2,   10,    3, 2153,\n",
            "            2,    3, 2972,    2,   50,    3, 2918,    7, 2566,    2,   73,    3,\n",
            "         2565,    7, 2919,    2,  252,   17,   50,    3,  129,    2,   73,    3,\n",
            "          235,    7,  683,    2,    3,   62, 2259,    2,   18,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [ 136,    8,  680,   10,   33,  620,   81,    9,  603,  121,  136,   92,\n",
            "         2467,  126,    3,  136,    2,    3,  136,   23,    2,    3,  764,   23,\n",
            "            2,    3, 1037,   23,    2,    3,  500,    2,    3,  680,    2,    3,\n",
            "          680,  500,    2,    3,  357,    2,    3,  393,    2,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [ 175,   25,   53,   13,  125,    8,  133,   20,  533,   82,  104,  136,\n",
            "           23,   42, 1872, 1067,    3,  136,    2,    3,  136,   23,    2,    3,\n",
            "          187,    2,    3,   82,    2,    3,   82,   23,    2,    3, 1880,    2,\n",
            "            3, 2141,    2,    3, 2058,    2,    3,  832,    2,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [  41, 1727, 1400, 1582, 2447, 1303,    7,  287, 1079,   21, 1677,  254,\n",
            "            6,   57,  801, 1498,    3,  654,    2,    3,   60,    2,    3,  816,\n",
            "            2,    3,  379,    2,    3,  342,    2,    3,  379,    2,    3,  625,\n",
            "            2,  229,  229,  229,  229,  229,  229,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [  48,   65,   56,    9,  345,   16, 1977,  185,   42,   25,  104,   25,\n",
            "          101,   28,  295,  456,   17,  102,   39,  109,  374,  116,   74,  624,\n",
            "            3,   75,    2,    3, 2156,   49,   22,  264,    2,    3,  659,   87,\n",
            "          264,    2,    3,  850,  599, 1359,    2,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   3,  772,    2,   15,   69, 1499,   16,    3,  442,    2,  726,  252,\n",
            "            7,  726,   16,    3, 1645,    2,    4,    3, 1508,    2,    3,  904,\n",
            "            2,    3, 1486,    2,    3, 2240,    2,    3,  377,    2,    3,  132,\n",
            "            2,    3,   59,    2,    3,  499,    2,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   3,  361,    2,   15,  416,    9, 1874,  504, 1288,   16,  571,  539,\n",
            "            4, 1611,   25,   50,  592,    8, 2334,   17,   51,    4, 2007,   10,\n",
            "         2322,   11, 2016,  275,    4,   17,   12,   22,   90,    4,    6,   35,\n",
            "           52,    4, 1434,    3,   75,    2,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1]], device='cuda:0')\n",
            "tensor([0.6460, 0.4380, 0.2410, 0.3750, 0.8960, 0.6040, 0.5620, 0.6250],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name in list_name:\n",
        "  print(dict_fields[name]['Tweet'][1].vocab.stoi.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT4RpCQ3tsqC",
        "outputId": "123c74e2-2330-41fe-92c7-fce3f96f5780"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('<unk>', 0), ('<pad>', 1), ('</hashtag>', 2), ('<hashtag>', 3), ('.', 4), ('<user>', 5), ('i', 6), ('the', 7), (',', 8), ('to', 9), ('a', 10), (\"'\", 11), ('and', 12), ('not', 13), ('it', 14), ('of', 15), ('is', 16), ('you', 17), ('in', 18), ('<repeated>', 19), ('my', 20), ('s', 21), ('!', 22), ('for', 23), ('that', 24), ('\\\\', 25), ('-', 26), ('on', 27), ('do', 28), ('<number>', 29), ('have', 30), ('am', 31), ('are', 32), ('me', 33), ('?', 34), ('this', 35), ('so', 36), ('be', 37), ('but', 38), ('with', 39), ('</allcaps>', 40), ('<allcaps>', 41), ('can', 42), ('just', 43), ('n', 44), ('will', 45), ('all', 46), ('at', 47), ('sad', 48), ('when', 49), ('get', 50), ('your', 51), ('no', 52), ('&', 53), ('we', 54), ('lost', 55), ('like', 56), ('they', 57), ('depression', 58), ('was', 59), ('blues', 60), ('if', 61), ('he', 62), ('sadness', 63), (':sparkles:', 64), ('or', 65), ('up', 66), ('how', 67), ('what', 68), ('out', 69), ('sober', 70), ('about', 71), ('as', 72), ('serious', 73), ('one', 74), ('know', 75), ('day', 76), ('time', 77), ('too', 78), ('now', 79), ('dark', 80), ('from', 81), ('her', 82), ('there', 83), (':', 84), ('by', 85), ('unhappy', 86), ('been', 87), ('depressing', 88), ('his', 89), ('did', 90), ('life', 91), ('more', 92), ('good', 93), ('has', 94), ('people', 95), ('feel', 96), ('she', 97), ('love', 98), ('see', 99), ('our', 100), ('some', 101), ('u', 102), ('go', 103), ('only', 104), ('over', 105), ('who', 106), ('would', 107), (':face_with_tears_of_joy:', 108), ('grim', 109), ('think', 110), ('really', 111), ('does', 112), ('got', 113), ('had', 114), ('because', 115), ('despair', 116), ('still', 117), ('want', 118), ('an', 119), ('let', 120), ('their', 121), ('need', 122), ('off', 123), ('should', 124), ('someone', 125), ('today', 126), ('always', 127), ('dull', 128), ('even', 129), ('going', 130), ('never', 131), ('new', 132), ('sink', 133), ('them', 134), ('why', 135), ('/', 136), ('dreadful', 137), ('gloomy', 138), ('pout', 139), ('sadly', 140), ('then', 141), ('were', 142), ('could', 143), ('stayed', 144), ('back', 145), ('music', 146), ('pine', 147), ('than', 148), ('being', 149), ('discouraged', 150), ('him', 151), (')', 152), ('im', 153), ('mourn', 154), ('way', 155), ('weary', 156), ('again', 157), ('look', 158), ('man', 159), ('away', 160), ('d', 161), ('make', 162), ('most', 163), ('much', 164), ('play', 165), ('us', 166), ('well', 167), ('work', 168), ('frown', 169), ('sunk', 170), ('things', 171), ('very', 172), ('while', 173), ('<time>', 174), ('anxiety', 175), ('discourage', 176), ('find', 177), ('where', 178), ('hard', 179), ('night', 180), ('right', 181), ('(', 182), (':loudly_crying_face:', 183), ('<elongated>', 184), ('before', 185), ('god', 186), ('happy', 187), ('home', 188), ('into', 189), ('lol', 190), ('after', 191), ('bad', 192), ('down', 193), ('hate', 194), ('heart', 195), ('here', 196), ('week', 197), ('feeling', 198), ('help', 199), ('little', 200), ('t', 201), ('tell', 202), ('world', 203), ('worst', 204), ('better', 205), ('last', 206), ('oh', 207), ('wanna', 208), ('fucking', 209), ('getting', 210), ('having', 211), ('pretty', 212), ('take', 213), ('cause', 214), ('days', 215), ('end', 216), ('ever', 217), ('friends', 218), ('gloom', 219), ('hope', 220), ('looking', 221), ('many', 222), ('say', 223), ('twitter', 224), ('two', 225), ('yet', 226), ('any', 227), ('around', 228), ('depress', 229), ('first', 230), ('fret', 231), ('give', 232), ('gone', 233), ('long', 234), ('made', 235), ('moment', 236), ('other', 237), ('person', 238), ('rock', 239), ('shit', 240), ('bit', 241), ('dismal', 242), ('great', 243), ('pessimist', 244), ('real', 245), ('said', 246), ('same', 247), ('seen', 248), ('something', 249), ('sometimes', 250), ('sulk', 251), ('thing', 252), ('those', 253), ('tomorrow', 254), ('tonight', 255), ('yeah', 256), (';', 257), ('another', 258), ('doing', 259), ('either', 260), ('family', 261), ('high', 262), ('keep', 263), ('left', 264), ('old', 265), ('these', 266), ('thought', 267), ('trump', 268), ('wait', 269), ('watch', 270), ('wrong', 271), ('year', 272), ('—', 273), (':weary_face:', 274), ('alone', 275), ('anyone', 276), ('best', 277), ('leave', 278), ('melancholy', 279), ('myself', 280), ('nothing', 281), ('since', 282), ('start', 283), ('stop', 284), ('win', 285), ('yourself', 286), ('*', 287), ('2', 288), ('also', 289), ('bed', 290), ('come', 291), ('every', 292), ('fall', 293), ('full', 294), ('funny', 295), ('gbbo', 296), ('grieve', 297), ('head', 298), ('its', 299), ('live', 300), ('maybe', 301), ('next', 302), ('own', 303), ('team', 304), ('until', 305), ('years', 306), (':face_with_rolling_eyes:', 307), ('<happy>', 308), ('anything', 309), ('beautiful', 310), ('black', 311), ('both', 312), ('change', 313), ('everything', 314), ('face', 315), ('free', 316), ('frowning', 317), ('future', 318), ('honestly', 319), ('house', 320), ('makes', 321), ('season', 322), ('thank', 323), ('times', 324), ('turn', 325), ('watching', 326), ('weeks', 327), ('whole', 328), ('wow', 329), ('1', 330), ('affliction', 331), ('b', 332), ('between', 333), ('class', 334), ('die', 335), ('else', 336), ('guy', 337), ('guys', 338), ('health', 339), ('job', 340), ('knows', 341), ('late', 342), ('ll', 343), ('m', 344), ('may', 345), ('might', 346), ('money', 347), ('phone', 348), ('place', 349), ('r', 350), ('recovery', 351), ('show', 352), ('stay', 353), ('such', 354), ('thanks', 355), ('tired', 356), ('try', 357), ('trying', 358), ('’', 359), ('+', 360), ('0', 361), ('already', 362), ('broken', 363), ('call', 364), ('candice', 365), ('care', 366), ('close', 367), ('done', 368), ('game', 369), ('gonna', 370), ('laugh', 371), ('literally', 372), ('looks', 373), ('lord', 374), ('mean', 375), ('mh', 376), ('news', 377), ('ok', 378), ('once', 379), ('part', 380), ('playing', 381), ('point', 382), ('rather', 383), ('re', 384), ('rest', 385), ('room', 386), ('service', 387), ('sorry', 388), ('sure', 389), ('true', 390), ('w', 391), ('wish', 392), ('<sad>', 393), ('absolutely', 394), ('ask', 395), ('asked', 396), ('believe', 397), ('car', 398), ('chat', 399), ('comes', 400), ('confused', 401), ('don', 402), ('dont', 403), ('felt', 404), ('friend', 405), ('fun', 406), ('games', 407), ('girl', 408), ('hear', 409), ('hurt', 410), ('jazz', 411), ('least', 412), ('must', 413), ('needs', 414), ('ni', 415), ('police', 416), ('problem', 417), ('short', 418), ('situation', 419), ('soul', 420), ('super', 421), ('talking', 422), ('tears', 423), ('though', 424), ('through', 425), ('told', 426), ('top', 427), ('tv', 428), ('understand', 429), ('use', 430), ('without', 431), ('working', 432), ('yes', 433), ('“', 434), ('', 435), ('<annoyed>', 436), ('@', 437), ('almost', 438), ('become', 439), ('big', 440), ('boy', 441), ('customer', 442), ('drunk', 443), ('eyes', 444), ('fear', 445), ('follow', 446), ('fuck', 447), ('grow', 448), ('half', 449), ('happiness', 450), ('listening', 451), ('making', 452), ('mind', 453), ('mufc', 454), ('name', 455), ('nights', 456), ('pain', 457), ('please', 458), ('put', 459), ('safe', 460), ('saw', 461), ('saying', 462), ('seeing', 463), ('side', 464), ('smile', 465), ('song', 466), ('stars', 467), ('talk', 468), ('tweet', 469), ('ur', 470), ('used', 471), ('”', 472), ('<percent>', 473), ('>', 474), (']', 475), ('actually', 476), ('afraid', 477), ('ass', 478), ('awareness', 479), ('book', 480), ('business', 481), ('child', 482), ('clouded', 483), ('cold', 484), ('crying', 485), ('depressed', 486), ('disheartened', 487), ('doom', 488), ('dreary', 489), ('each', 490), ('eat', 491), ('eating', 492), ('everyone', 493), ('expect', 494), ('facts', 495), ('fail', 496), ('fan', 497), ('fans', 498), ('far', 499), ('football', 500), ('found', 501), ('goes', 502), ('haha', 503), ('history', 504), ('hours', 505), ('issues', 506), ('kill', 507), ('kind', 508), ('kinda', 509), ('lose', 510), ('loss', 511), ('mood', 512), ('morning', 513), ('murky', 514), ('optimist', 515), ('past', 516), ('peace', 517), ('pessimism', 518), ('photo', 519), ('poor', 520), ('power', 521), ('quite', 522), ('sat', 523), ('save', 524), ('school', 525), ('seriously', 526), ('shall', 527), ('sick', 528), ('single', 529), ('son', 530), ('stand', 531), ('starting', 532), ('story', 533), ('strong', 534), ('stuff', 535), ('support', 536), ('takes', 537), ('taking', 538), ('truth', 539), ('wants', 540), ('war', 541), ('weather', 542), ('#', 543), ('3', 544), ('5', 545), (':broken_heart:', 546), (':crying_face:', 547), ('<date>', 548), ('age', 549), ('anymore', 550), ('apple', 551), ('behind', 552), ('body', 553), ('bring', 554), ('btw', 555), ('cant', 556), ('cgi', 557), ('contact', 558), ('cover', 559), ('cup', 560), ('damper', 561), ('dashed', 562), ('dejected', 563), ('dog', 564), ('drop', 565), ('enough', 566), ('etc', 567), ('fact', 568), ('feels', 569), ('few', 570), ('final', 571), ('food', 572), ('forward', 573), ('friday', 574), ('front', 575), ('guess', 576), ('ha', 577), ('hand', 578), ('holiday', 579), ('hour', 580), ('inconsolable', 581), ('inside', 582), ('joy', 583), ('kitchen', 584), ('learning', 585), ('leaves', 586), ('less', 587), ('listen', 588), ('low', 589), ('major', 590), ('men', 591), ('mess', 592), ('milk', 593), ('mine', 594), ('miss', 595), ('mixed', 596), ('mope', 597), ('move', 598), ('nap', 599), ('near', 600), ('nice', 601), ('often', 602), ('opportunity', 603), ('penny', 604), ('pic', 605), ('post', 606), ('quote', 607), ('road', 608), ('saturday', 609), ('self', 610), ('sky', 611), ('sleep', 612), ('sobriety', 613), ('somber', 614), ('soon', 615), ('sort', 616), ('state', 617), ('step', 618), ('stupid', 619), ('tells', 620), ('waiting', 621), ('wasn', 622), ('wearing', 623), ('which', 624), ('word', 625), ('writing', 626), ('yesterday', 627), ('―', 628), ('2016', 629), ('<money>', 630), ('=', 631), ('[', 632), ('afternoon', 633), ('against', 634), ('american', 635), ('art', 636), ('band', 637), ('bc', 638), ('blame', 639), ('break', 640), ('cage', 641), ('cam', 642), ('came', 643), ('college', 644), ('cops', 645), ('course', 646), ('cry', 647), ('dad', 648), ('damn', 649), ('desolate', 650), ('difference', 651), ('dream', 652), ('dreams', 653), ('dropped', 654), ('ect', 655), ('evening', 656), ('evil', 657), ('fighting', 658), ('forest', 659), ('forever', 660), ('form', 661), ('grateful', 662), ('green', 663), ('guitar', 664), ('h', 665), ('hell', 666), ('hi', 667), ('hillary', 668), ('hit', 669), ('hot', 670), ('human', 671), ('ill', 672), ('imagine', 673), ('journey', 674), ('king', 675), ('lately', 676), ('level', 677), ('lives', 678), ('lmao', 679), ('longer', 680), ('lots', 681), ('lovely', 682), ('main', 683), ('manchester', 684), ('matter', 685), ('means', 686), ('members', 687), ('memories', 688), ('mental', 689), ('missed', 690), ('mom', 691), ('month', 692), ('moping', 693), ('nno', 694), ('obviously', 695), ('online', 696), ('order', 697), ('paint', 698), ('parents', 699), ('paul', 700), ('pay', 701), ('perfect', 702), ('perfection', 703), ('pitch', 704), ('pizza', 705), ('played', 706), ('players', 707), ('pour', 708), ('ppl', 709), ('probably', 710), ('protest', 711), ('questions', 712), ('race', 713), ('radio', 714), ('ran', 715), ('read', 716), ('regret', 717), ('remember', 718), ('rojo', 719), ('rooney', 720), ('run', 721), ('second', 722), ('sees', 723), ('series', 724), ('services', 725), ('shocking', 726), ('smh', 727), ('sound', 728), ('stopped', 729), ('strength', 730), ('sucks', 731), ('summer', 732), ('sunday', 733), ('ticket', 734), ('tms', 735), ('together', 736), ('tony', 737), ('tried', 738), ('trust', 739), ('ugly', 740), ('understanding', 741), ('vs', 742), ('walk', 743), ('watched', 744), ('weekend', 745), ('whatever', 746), ('women', 747), ('wonder', 748), ('words', 749), ('x', 750), ('young', 751), (':expressionless_face:', 752), (':nerd_face:', 753), (':see-no-evil_monkey:', 754), (':slightly_smiling_face:', 755), (':smiling_face_with_smiling_eyes:', 756), (':two_hearts:', 757), (':unamused_face:', 758), (':upside-down_face:', 759), ('ability', 760), ('able', 761), ('addiction', 762), ('advice', 763), ('af', 764), ('ago', 765), ('agree', 766), ('album', 767), ('anger', 768), ('argument', 769), ('arms', 770), ('asleep', 771), ('autumn', 772), ('baby', 773), ('bar', 774), ('beat', 775), ('beyond', 776), ('blue', 777), ('boring', 778), ('born', 779), ('bottom', 780), ('boys', 781), ('bright', 782), ('brown', 783), ('buy', 784), ('called', 785), ('campaign', 786), ('card', 787), ('city', 788), ('clean', 789), ('coffee', 790), ('coming', 791), ('completely', 792), ('cool', 793), ('country', 794), ('crutcher', 795), ('cure', 796), ('cut', 797), ('cute', 798), ('dead', 799), ('dear', 800), ('death', 801), ('deliver', 802), ('despondent', 803), ('different', 804), ('difficulty', 805), ('dishes', 806), ('dm', 807), ('doctor', 808), ('droop', 809), ('dude', 810), ('due', 811), ('e', 812), ('earlier', 813), ('early', 814), ('easy', 815), ('excited', 816), ('experience', 817), ('extreme', 818), ('facebook', 819), ('failure', 820), ('features', 821), ('film', 822), ('fml', 823), ('fortunate', 824), ('gets', 825), ('glad', 826), ('grand', 827), ('group', 828), ('hair', 829), ('happen', 830), ('healing', 831), ('heard', 832), ('heaviness', 833), ('held', 834), ('helping', 835), ('humor', 836), ('id', 837), ('idea', 838), ('india', 839), ('info', 840), ('issue', 841), ('justice', 842), ('k', 843), ('knowing', 844), ('lackadaisical', 845), ('lay', 846), ('line', 847), ('lips', 848), ('looked', 849), ('losing', 850), ('lot', 851), ('loved', 852), ('marcos', 853), ('market', 854), ('meeting', 855), ('met', 856), ('minutes', 857), ('momma', 858), ('months', 859), ('moving', 860), ('na', 861), ('noise', 862), ('nor', 863), ('november', 864), ('nthe', 865), ('nyou', 866), ('okay', 867), ('others', 868), ('outside', 869), ('park', 870), ('party', 871), ('pass', 872), ('pics', 873), ('positive', 874), ('possible', 875), ('praying', 876), ('punk', 877), ('question', 878), ('quit', 879), ('quotes', 880), ('racism', 881), ('random', 882), ('rd', 883), ('realize', 884), ('red', 885), ('relationships', 886), ('replaced', 887), ('report', 888), ('rn', 889), ('roblox', 890), ('running', 891), ('sadden', 892), ('sedate', 893), ('september', 894), ('shine', 895), ('sigh', 896), ('sleeping', 897), ('social', 898), ('solemn', 899), ('somewhere', 900), ('songs', 901), ('sounds', 902), ('st', 903), ('started', 904), ('starts', 905), ('stuck', 906), ('successes', 907), ('surprised', 908), ('sweet', 909), ('sydney', 910), ('szn', 911), ('taken', 912), ('thats', 913), ('tho', 914), ('took', 915), ('uk', 916), ('under', 917), ('upset', 918), ('using', 919), ('video', 920), ('videos', 921), ('vote', 922), ('wake', 923), ('wanted', 924), ('whenever', 925), ('white', 926), ('wild', 927), ('wind', 928), ('woke', 929), ('woman', 930), ('worse', 931), ('ya', 932), ('~', 933), ('…', 934), ('9', 935), (':OK_hand:', 936), (':disappointed_face:', 937), (':face_blowing_a_kiss:', 938), (':face_savoring_food:', 939), (':face_with_medical_mask:', 940), (':fallen_leaf:', 941), (':grimacing_face:', 942), (':hundred_points:', 943), (':light_skin_tone:', 944), (':maple_leaf:', 945), (':medium-dark_skin_tone:', 946), (':musical_notes:', 947), (':sleepy_face:', 948), (':smiling_face_with_heart-eyes:', 949), (':soccer_ball:', 950), (':thumbs_down:', 951), ('<wink>', 952), ('across', 953), ('added', 954), ('adult', 955), ('afl', 956), ('ah', 957), ('ahead', 958), ('air', 959), ('alcohol', 960), ('although', 961), ('annoying', 962), ('anybody', 963), ('anywhere', 964), ('arc', 965), ('attacks', 966), ('average', 967), ('awake', 968), ('awe', 969), ('awful', 970), ('badly', 971), ('bank', 972), ('bear', 973), ('beats', 974), ('birthday', 975), ('blessed', 976), ('blind', 977), ('block', 978), ('blog', 979), ('blow', 980), ('bob', 981), ('bored', 982), ('brad', 983), ('brain', 984), ('brexit', 985), ('bro', 986), ('broke', 987), ('brothers', 988), ('c', 989), ('cannot', 990), ('carry', 991), ('cat', 992), ('causing', 993), ('centre', 994), ('chair', 995), ('changed', 996), ('changes', 997), ('channel', 998), ('charlotte', 999), ('checked', 1000), ('cheer', 1001), ('childhood', 1002), ('chris', 1003), ('chronic', 1004), ('clear', 1005), ('clinton', 1006), ('clue', 1007), ('color', 1008), ('complains', 1009), ('constant', 1010), ('convinced', 1011), ('corrupt', 1012), ('count', 1013), ('coverage', 1014), ('covered', 1015), ('create', 1016), ('crime', 1017), ('cross', 1018), ('current', 1019), ('customers', 1020), ('dangerous', 1021), ('date', 1022), ('david', 1023), ('deep', 1024), ('definitely', 1025), ('delusional', 1026), ('depths', 1027), ('died', 1028), ('disappointed', 1029), ('disturbed', 1030), ('drive', 1031), ('dry', 1032), ('ears', 1033), ('economic', 1034), ('eliminate', 1035), ('emotions', 1036), ('empty', 1037), ('enemy', 1038), ('engine', 1039), ('episode', 1040), ('episodes', 1041), ('equinox', 1042), ('exhausted', 1043), ('experiences', 1044), ('expression', 1045), ('extra', 1046), ('fam', 1047), ('fast', 1048), ('ffs', 1049), ('figured', 1050), ('filled', 1051), ('fine', 1052), ('finished', 1053), ('flight', 1054), ('floor', 1055), ('folk', 1056), ('followers', 1057), ('forget', 1058), ('forlorn', 1059), ('funeral', 1060), ('gad', 1061), ('gaming', 1062), ('girls', 1063), ('goal', 1064), ('goodbye', 1065), ('growing', 1066), ('guests', 1067), ('hands', 1068), ('hanging', 1069), ('happened', 1070), ('harder', 1071), ('headed', 1072), ('hearing', 1073), ('heavy', 1074), ('hey', 1075), ('higher', 1076), ('hold', 1077), ('hopelessness', 1078), ('horror', 1079), ('hunger', 1080), ('hurting', 1081), ('ice', 1082), ('idk', 1083), ('ignorant', 1084), ('incredibly', 1085), ('insomnia', 1086), ('instantly', 1087), ('interesting', 1088), ('internet', 1089), ('ive', 1090), ('j', 1091), ('jack', 1092), ('joke', 1093), ('joyless', 1094), ('keeps', 1095), ('knew', 1096), ('known', 1097), ('l', 1098), ('lack', 1099), ('laptop', 1100), ('later', 1101), ('laughing', 1102), ('learn', 1103), ('lie', 1104), ('light', 1105), ('lights', 1106), ('likes', 1107), ('lip', 1108), ('lit', 1109), ('load', 1110), ('lonely', 1111), ('losers', 1112), ('lover', 1113), ('luckily', 1114), ('ma', 1115), ('mad', 1116), ('madness', 1117), ('mark', 1118), ('married', 1119), ('match', 1120), ('mate', 1121), ('meaning', 1122), ('meant', 1123), ('media', 1124), ('meds', 1125), ('melancholic', 1126), ('member', 1127), ('missing', 1128), ('mistake', 1129), ('mother', 1130), ('movies', 1131), ('nah', 1132), ('nasty', 1133), ('natural', 1134), ('nd', 1135), ('netflix', 1136), ('network', 1137), ('nigga', 1138), ('nl', 1139), ('normal', 1140), ('ns', 1141), ('nuts', 1142), ('ny', 1143), ('o', 1144), ('offer', 1145), ('open', 1146), ('opened', 1147), ('overwhelming', 1148), ('pace', 1149), ('paid', 1150), ('panic', 1151), ('pasta', 1152), ('penis', 1153), ('pissed', 1154), ('places', 1155), ('plus', 1156), ('pm', 1157), ('pop', 1158), ('position', 1159), ('pr', 1160), ('prayer', 1161), ('prepared', 1162), ('progress', 1163), ('proud', 1164), ('psychology', 1165), ('public', 1166), ('pull', 1167), ('puts', 1168), ('realised', 1169), ('reason', 1170), ('recent', 1171), ('refuse', 1172), ('relapse', 1173), ('respect', 1174), ('responsible', 1175), ('result', 1176), ('rethink', 1177), ('rip', 1178), ('rueful', 1179), ('sacrifice', 1180), ('says', 1181), ('seasons', 1182), ('seats', 1183), ('seems', 1184), ('sense', 1185), ('shame', 1186), ('shirt', 1187), ('shut', 1188), ('silver', 1189), ('simply', 1190), ('sing', 1191), ('small', 1192), ('smiling', 1193), ('snapchat', 1194), ('somebody', 1195), ('source', 1196), ('speak', 1197), ('spending', 1198), ('spirit', 1199), ('steps', 1200), ('stomach', 1201), ('store', 1202), ('stress', 1203), ('strongly', 1204), ('style', 1205), ('sucker', 1206), ('sudden', 1207), ('sunshine', 1208), ('taste', 1209), ('teaching', 1210), ('teeth', 1211), ('terence', 1212), ('test', 1213), ('th', 1214), ('themselves', 1215), ('thinking', 1216), ('tiny', 1217), ('town', 1218), ('track', 1219), ('train', 1220), ('trigger', 1221), ('truly', 1222), ('tulsa', 1223), ('twain', 1224), ('tweets', 1225), ('unfortunately', 1226), ('united', 1227), ('university', 1228), ('upside', 1229), ('usual', 1230), ('v', 1231), ('val', 1232), ('voice', 1233), ('voices', 1234), ('walker', 1235), ('ways', 1236), ('weak', 1237), ('wear', 1238), ('wednesday', 1239), ('weird', 1240), ('went', 1241), ('west', 1242), ('wins', 1243), ('winter', 1244), ('wondering', 1245), ('worked', 1246), ('worthless', 1247), ('write', 1248), ('written', 1249), ('yall', 1250), ('yours', 1251), ('yrs', 1252), ('|', 1253), ('»', 1254), ('%$', 1255), ('*\\\\', 1256), ('2017', 1257), ('4', 1258), (':((((((((((((((((((((((((', 1259), (':clinking_beer_mugs:', 1260), (':confused_face:', 1261), (':enraged_face:', 1262), (':fire:', 1263), (':flushed_face:', 1264), (':frowning_face:', 1265), (':grinning_face_with_sweat:', 1266), (':guitar:', 1267), (':headphone:', 1268), (':lipstick:', 1269), (':musical_score:', 1270), (':red_heart:', 1271), (':slightly_frowning_face:', 1272), (':smiling_face:', 1273), (':smiling_face_with_halo:', 1274), (':snowflake:', 1275), (':snowman:', 1276), (':squinting_face_with_tongue:', 1277), (':thinking_face:', 1278), (':umbrella_with_rain_drops:', 1279), (':water_pistol:', 1280), (':wind_face:', 1281), ('<', 1282), ('<censored>', 1283), ('<emphasis>', 1284), ('^', 1285), ('absolute', 1286), ('accept', 1287), ('acceptable', 1288), ('accidentally', 1289), ('accomplishments', 1290), ('acrylic', 1291), ('act', 1292), ('action', 1293), ('add', 1294), ('adulting', 1295), ('adults', 1296), ('adventure', 1297), ('affection', 1298), ('afterellen', 1299), ('agent', 1300), ('agin', 1301), ('ain', 1302), ('aired', 1303), ('airline', 1304), ('airport', 1305), ('along', 1306), ('alot', 1307), ('alt', 1308), ('amazing', 1309), ('america', 1310), ('amnd', 1311), ('angelina', 1312), ('animals', 1313), ('announcement', 1314), ('anyway', 1315), ('app', 1316), ('apparently', 1317), ('appear', 1318), ('appendix', 1319), ('apples', 1320), ('appreciated', 1321), ('appropriate', 1322), ('apps', 1323), ('armour', 1324), ('arrive', 1325), ('arriving', 1326), ('artist', 1327), ('associative', 1328), ('assume', 1329), ('astros', 1330), ('atm', 1331), ('attendance', 1332), ('attention', 1333), ('attitude', 1334), ('attracted', 1335), ('author', 1336), ('autumnal', 1337), ('ave', 1338), ('aw', 1339), ('award', 1340), ('aware', 1341), ('backyard', 1342), ('bag', 1343), ('bailed', 1344), ('bake', 1345), ('balance', 1346), ('ball', 1347), ('bands', 1348), ('barb', 1349), ('barely', 1350), ('bathing', 1351), ('bathroom', 1352), ('batman', 1353), ('battery', 1354), ('battling', 1355), ('bay', 1356), ('bcuz', 1357), ('beer', 1358), ('begun', 1359), ('bell', 1360), ('bet', 1361), ('biggest', 1362), ('bill', 1363), ('bitch', 1364), ('bitter', 1365), ('blanchardstown', 1366), ('blend', 1367), ('blew', 1368), ('blm', 1369), ('bloody', 1370), ('bluesfest', 1371), ('board', 1372), ('bojack', 1373), ('bold', 1374), ('boots', 1375), ('borrow', 1376), ('bot', 1377), ('bots', 1378), ('bottle', 1379), ('bought', 1380), ('bournemouth', 1381), ('bout', 1382), ('box', 1383), ('brave', 1384), ('bremen', 1385), ('brilliant', 1386), ('british', 1387), ('brother', 1388), ('browns', 1389), ('brutality', 1390), ('buchanan', 1391), ('buddha', 1392), ('burden', 1393), ('bus', 1394), ('busy', 1395), ('buying', 1396), ('byron', 1397), ('calling', 1398), ('camden', 1399), ('campaigning', 1400), ('cancel', 1401), ('candace', 1402), ('capture', 1403), ('cards', 1404), ('careful', 1405), ('carrick', 1406), ('catch', 1407), ('catching', 1408), ('cats', 1409), ('cc', 1410), ('celebrate', 1411), ('challenges', 1412), ('chance', 1413), ('chances', 1414), ('character', 1415), ('charger', 1416), ('charging', 1417), ('charlie', 1418), ('cheap', 1419), ('cheese', 1420), ('chill', 1421), ('china', 1422), ('chuckle', 1423), ('church', 1424), ('churchill', 1425), ('circumstances', 1426), ('classic', 1427), ('classical', 1428), ('clowns', 1429), ('cmon', 1430), ('co', 1431), ('comment', 1432), ('comparing', 1433), ('compelling', 1434), ('complain', 1435), ('complained', 1436), ('complaining', 1437), ('con', 1438), ('conceded', 1439), ('concert', 1440), ('conclusion', 1441), ('conditioned', 1442), ('condolences', 1443), ('confident', 1444), ('considering', 1445), ('constantly', 1446), ('constitutional', 1447), ('contention', 1448), ('contest', 1449), ('continue', 1450), ('control', 1451), ('cookies', 1452), ('cop', 1453), ('corbyn', 1454), ('cost', 1455), ('couldnt', 1456), ('county', 1457), ('couples', 1458), ('cousins', 1459), ('crazy', 1460), ('crimes', 1461), ('criticize', 1462), ('cubs', 1463), ('cuddles', 1464), ('cum', 1465), ('cutting', 1466), ('cuz', 1467), ('daily', 1468), ('damage', 1469), ('damaged', 1470), ('dance', 1471), ('darkest', 1472), ('dashboard', 1473), ('dating', 1474), ('daughter', 1475), ('dayof', 1476), ('deal', 1477), ('decision', 1478), ('decisions', 1479), ('defending', 1480), ('delete', 1481), ('depth', 1482), ('despite', 1483), ('despondency', 1484), ('dim', 1485), ('disciplined', 1486), ('discussion', 1487), ('disgusting', 1488), ('disney', 1489), ('display', 1490), ('distraught', 1491), ('divided', 1492), ('doe', 1493), ('dom', 1494), ('dove', 1495), ('drama', 1496), ('dreamer', 1497), ('drink', 1498), ('driving', 1499), ('drowning', 1500), ('dumb', 1501), ('duvet', 1502), ('dying', 1503), ('dylan', 1504), ('easily', 1505), ('editing', 1506), ('edwards', 1507), ('eh', 1508), ('elite', 1509), ('em', 1510), ('encouragement', 1511), ('encouraging', 1512), ('endorsing', 1513), ('ends', 1514), ('engagement', 1515), ('enjoyed', 1516), ('epic', 1517), ('equipment', 1518), ('escape', 1519), ('eternal', 1520), ('eval', 1521), ('eventually', 1522), ('ex', 1523), ('expects', 1524), ('explain', 1525), ('exterior', 1526), ('f', 1527), ('faced', 1528), ('facing', 1529), ('factor', 1530), ('failed', 1531), ('faith', 1532), ('fallen', 1533), ('fanbase', 1534), ('father', 1535), ('fault', 1536), ('fav', 1537), ('favorite', 1538), ('favourite', 1539), ('feared', 1540), ('fed', 1541), ('feelings', 1542), ('feet', 1543), ('ff', 1544), ('figuring', 1545), ('filmed', 1546), ('finally', 1547), ('financial', 1548), ('finish', 1549), ('fire', 1550), ('fitness', 1551), ('fix', 1552), ('flaws', 1553), ('florida', 1554), ('flt', 1555), ('fog', 1556), ('foh', 1557), ('foot', 1558), ('formed', 1559), ('fought', 1560), ('fox', 1561), ('fred', 1562), ('freshers', 1563), ('frozen', 1564), ('frustrating', 1565), ('fucked', 1566), ('fuckin', 1567), ('furry', 1568), ('g', 1569), ('gaa', 1570), ('gain', 1571), ('gamer', 1572), ('garbage', 1573), ('garcia', 1574), ('genius', 1575), ('george', 1576), ('georges', 1577), ('german', 1578), ('gilmour', 1579), ('giving', 1580), ('gloves', 1581), ('goodness', 1582), ('gop', 1583), ('gotta', 1584), ('gove', 1585), ('government', 1586), ('govt', 1587), ('gpa', 1588), ('grass', 1589), ('grave', 1590), ('gray', 1591), ('grief', 1592), ('grown', 1593), ('gud', 1594), ('gun', 1595), ('gutted', 1596), ('gym', 1597), ('haiku', 1598), ('haircuts', 1599), ('halloween', 1600), ('ham', 1601), ('handled', 1602), ('hangover', 1603), ('happening', 1604), ('harm', 1605), ('harris', 1606), ('havent', 1607), ('heading', 1608), ('healthy', 1609), ('hearted', 1610), ('hearts', 1611), ('heat', 1612), ('heated', 1613), ('helicopter', 1614), ('hello', 1615), ('helped', 1616), ('hermione', 1617), ('highlights', 1618), ('hijacked', 1619), ('hiring', 1620), ('hitting', 1621), ('hmm', 1622), ('holding', 1623), ('hole', 1624), ('honey', 1625), ('hopefully', 1626), ('hoping', 1627), ('horribly', 1628), ('hosting', 1629), ('however', 1630), ('hubby', 1631), ('huge', 1632), ('huh', 1633), ('humans', 1634), ('humorous', 1635), ('hungover', 1636), ('hurts', 1637), ('hype', 1638), ('ibiza', 1639), ('ideas', 1640), ('idiot', 1641), ('ignore', 1642), ('ima', 1643), ('immense', 1644), ('important', 1645), ('including', 1646), ('individuals', 1647), ('inform', 1648), ('initially', 1649), ('injury', 1650), ('inner', 1651), ('innocent', 1652), ('inspiration', 1653), ('instagram', 1654), ('instead', 1655), ('instinct', 1656), ('insurance', 1657), ('interview', 1658), ('invest', 1659), ('investigated', 1660), ('island', 1661), ('isnt', 1662), ('italy', 1663), ('itching', 1664), ('itself', 1665), ('jaime', 1666), ('jam', 1667), ('japanese', 1668), ('jay', 1669), ('jc', 1670), ('jeezus', 1671), ('jesus', 1672), ('joe', 1673), ('john', 1674), ('join', 1675), ('jose', 1676), ('joyful', 1677), ('jump', 1678), ('kara', 1679), ('karma', 1680), ('key', 1681), ('kick', 1682), ('kicking', 1683), ('kids', 1684), ('killed', 1685), ('klassic', 1686), ('knocks', 1687), ('kpop', 1688), ('kudos', 1689), ('lab', 1690), ('land', 1691), ('laughter', 1692), ('laurel', 1693), ('layover', 1694), ('lbs', 1695), ('leads', 1696), ('lesson', 1697), ('lessons', 1698), ('lethal', 1699), ('lets', 1700), ('lewis', 1701), ('lgbt', 1702), ('liars', 1703), ('lied', 1704), ('lifetime', 1705), ('lil', 1706), ('lines', 1707), ('link', 1708), ('lisa', 1709), ('living', 1710), ('london', 1711), ('lonley', 1712), ('loud', 1713), ('lovers', 1714), ('loves', 1715), ('loving', 1716), ('mall', 1717), ('manager', 1718), ('managing', 1719), ('march', 1720), ('marks', 1721), ('mask', 1722), ('master', 1723), ('matches', 1724), ('mates', 1725), ('matt', 1726), ('max', 1727), ('mdw', 1728), ('meditation', 1729), ('mercy', 1730), ('messing', 1731), ('middle', 1732), ('migraine', 1733), ('mike', 1734), ('mile', 1735), ('million', 1736), ('min', 1737), ('minivan', 1738), ('mondays', 1739), ('moon', 1740), ('motivation', 1741), ('mournful', 1742), ('mouth', 1743), ('mr', 1744), ('ms', 1745), ('mt', 1746), ('murder', 1747), ('musicians', 1748), ('nails', 1749), ('naked', 1750), ('narrowed', 1751), ('nation', 1752), ('nature', 1753), ('nbut', 1754), ('ndoesn', 1755), ('nearly', 1756), ('negative', 1757), ('neighborhood', 1758), ('nephews', 1759), ('ness', 1760), ('net', 1761), ('newton', 1762), ('nfrom', 1763), ('nhe', 1764), ('nif', 1765), ('niggas', 1766), ('nin', 1767), ('nj', 1768), ('nobody', 1769), ('non', 1770), ('nope', 1771), ('northampton', 1772), ('note', 1773), ('noticed', 1774), ('nots', 1775), ('notts', 1776), ('novel', 1777), ('nowhere', 1778), ('nso', 1779), ('nstill', 1780), ('nthey', 1781), ('nto', 1782), ('nu', 1783), ('nut', 1784), ('nwe', 1785), ('nwhat', 1786), ('objective', 1787), ('oct', 1788), ('october', 1789), ('offended', 1790), ('office', 1791), ('officers', 1792), ('officially', 1793), ('older', 1794), ('omg', 1795), ('onthe', 1796), ('opinion', 1797), ('opposite', 1798), ('optimism', 1799), ('origin', 1800), ('otherwise', 1801), ('outing', 1802), ('owned', 1803), ('pack', 1804), ('painful', 1805), ('pair', 1806), ('pants', 1807), ('pardon', 1808), ('parties', 1809), ('pas', 1810), ('passed', 1811), ('passion', 1812), ('pathetic', 1813), ('patti', 1814), ('pawn', 1815), ('pensive', 1816), ('per', 1817), ('perhaps', 1818), ('pet', 1819), ('physically', 1820), ('piano', 1821), ('picture', 1822), ('pills', 1823), ('pink', 1824), ('piss', 1825), ('pitt', 1826), ('pity', 1827), ('placed', 1828), ('planned', 1829), ('player', 1830), ('plays', 1831), ('plenty', 1832), ('plotting', 1833), ('pls', 1834), ('poetry', 1835), ('pointed', 1836), ('policies', 1837), ('policy', 1838), ('political', 1839), ('politics', 1840), ('pond', 1841), ('pops', 1842), ('porn', 1843), ('possess', 1844), ('prayers', 1845), ('premiere', 1846), ('preschool', 1847), ('present', 1848), ('pretend', 1849), ('pride', 1850), ('probz', 1851), ('process', 1852), ('products', 1853), ('programme', 1854), ('providing', 1855), ('puns', 1856), ('purchased', 1857), ('purely', 1858), ('purple', 1859), ('purpose', 1860), ('pushing', 1861), ('putting', 1862), ('quartet', 1863), ('quiet', 1864), ('racist', 1865), ('rain', 1866), ('rays', 1867), ('reading', 1868), ('ready', 1869), ('reality', 1870), ('reasoning', 1871), ('record', 1872), ('reds', 1873), ('refuge', 1874), ('rejoice', 1875), ('relating', 1876), ('relevant', 1877), ('remain', 1878), ('remains', 1879), ('remembering', 1880), ('reminded', 1881), ('remus', 1882), ('reply', 1883), ('reporter', 1884), ('reports', 1885), ('responds', 1886), ('response', 1887), ('resting', 1888), ('rich', 1889), ('rid', 1890), ('riding', 1891), ('ries', 1892), ('rights', 1893), ('ring', 1894), ('rm', 1895), ('rocknroll', 1896), ('rocks', 1897), ('roy', 1898), ('rt', 1899), ('ruin', 1900), ('rules', 1901), ('rush', 1902), ('rut', 1903), ('sadder', 1904), ('saddest', 1905), ('saddness', 1906), ('safely', 1907), ('sage', 1908), ('sales', 1909), ('sausage', 1910), ('sav', 1911), ('savvy', 1912), ('scam', 1913), ('scared', 1914), ('scary', 1915), ('scfc', 1916), ('scores', 1917), ('scuffs', 1918), ('secret', 1919), ('seemed', 1920), ('selasi', 1921), ('selfie', 1922), ('sell', 1923), ('severe', 1924), ('sex', 1925), ('shakespeare', 1926), ('shape', 1927), ('sharing', 1928), ('shaytan', 1929), ('shilling', 1930), ('shines', 1931), ('shitty', 1932), ('shoot', 1933), ('shooting', 1934), ('shootings', 1935), ('shopping', 1936), ('shoutout', 1937), ('showing', 1938), ('sign', 1939), ('singing', 1940), ('sister', 1941), ('six', 1942), ('skin', 1943), ('skype', 1944), ('slc', 1945), ('slept', 1946), ('slice', 1947), ('slightly', 1948), ('slough', 1949), ('society', 1950), ('sofa', 1951), ('soldiers', 1952), ('solid', 1953), ('solos', 1954), ('someday', 1955), ('somehow', 1956), ('somewhat', 1957), ('sons', 1958), ('soothe', 1959), ('sopranos', 1960), ('south', 1961), ('spain', 1962), ('speaking', 1963), ('speeds', 1964), ('spencer', 1965), ('spend', 1966), ('spends', 1967), ('spent', 1968), ('spring', 1969), ('stadium', 1970), ('stage', 1971), ('staid', 1972), ('stamped', 1973), ('stans', 1974), ('star', 1975), ('starving', 1976), ('states', 1977), ('station', 1978), ('steak', 1979), ('stephen', 1980), ('stood', 1981), ('stories', 1982), ('storm', 1983), ('straight', 1984), ('study', 1985), ('stumbling', 1986), ('substance', 1987), ('success', 1988), ('successful', 1989), ('sue', 1990), ('suggestions', 1991), ('suicidal', 1992), ('suicide', 1993), ('suits', 1994), ('sulky', 1995), ('sullen', 1996), ('sun', 1997), ('sunderland', 1998), ('sunny', 1999), ('superb', 2000), ('supplies', 2001), ('supporters', 2002), ('suppose', 2003), ('supposed', 2004), ('surely', 2005), ('suri', 2006), ('survivor', 2007), ('swallows', 2008), ('swear', 2009), ('swifts', 2010), ('switched', 2011), ('system', 2012), ('tag', 2013), ('talked', 2014), ('tat', 2015), ('tate', 2016), ('teach', 2017), ('teacher', 2018), ('teams', 2019), ('teens', 2020), ('tempered', 2021), ('terencecutcher', 2022), ('term', 2023), ('termianted', 2024), ('terms', 2025), ('terrance', 2026), ('texans', 2027), ('therapy', 2028), ('thief', 2029), ('thistle', 2030), ('thoughts', 2031), ('thousands', 2032), ('three', 2033), ('threw', 2034), ('throat', 2035), ('thursday', 2036), ('till', 2037), ('tip', 2038), ('tones', 2039), ('touch', 2040), ('tour', 2041), ('trap', 2042), ('travel', 2043), ('tree', 2044), ('trees', 2045), ('trip', 2046), ('troubled', 2047), ('troyler', 2048), ('tru', 2049), ('trumpie', 2050), ('tumblr', 2051), ('turnd', 2052), ('turned', 2053), ('turning', 2054), ('ty', 2055), ('type', 2056), ('ultra', 2057), ('un', 2058), ('unacceptable', 2059), ('unbelievably', 2060), ('uncle', 2061), ('underneath', 2062), ('understands', 2063), ('uni', 2064), ('unique', 2065), ('uplift', 2066), ('upsetting', 2067), ('usa', 2068), ('user', 2069), ('usually', 2070), ('vacation', 2071), ('valid', 2072), ('vin', 2073), ('violation', 2074), ('visited', 2075), ('viv', 2076), ('wales', 2077), ('wallet', 2078), ('wasted', 2079), ('water', 2080), ('waters', 2081), ('wayward', 2082), ('weapon', 2083), ('web', 2084), ('website', 2085), ('welcome', 2086), ('wheres', 2087), ('whilst', 2088), ('whipping', 2089), ('whiskey', 2090), ('whisper', 2091), ('wife', 2092), ('william', 2093), ('wine', 2094), ('winston', 2095), ('wood', 2096), ('woods', 2097), ('worried', 2098), ('worries', 2099), ('worry', 2100), ('wtf', 2101), ('xbox', 2102), ('y', 2103), ('yay', 2104), ('yellow', 2105), ('yield', 2106), ('z', 2107), ('zero', 2108), ('zones', 2109), ('ͫ', 2110), ('՟', 2111), ('ິ', 2112), ('–', 2113), ('$fogo', 2114), ('12', 2115), ('16', 2116), ('35', 2117), ('40', 2118), ('6', 2119), ('6200', 2120), ('7', 2121), ('8', 2122), ('815', 2123), (\":'d\", 2124), (':-)\\\\', 2125), (':airplane:', 2126), (':alien:', 2127), (':anchor:', 2128), (':angry_face_with_horns:', 2129), (':anxious_face_with_sweat:', 2130), (':black_circle:', 2131), (':blue_heart:', 2132), (':bottle_with_popping_cork:', 2133), (':collision:', 2134), (':cow_face:', 2135), (':crying_cat:', 2136), (':dark_skin_tone:', 2137), (':dashing_away:', 2138), (':dog_face:', 2139), (':doughnut:', 2140), (':evergreen_tree:', 2141), (':face_with_thermometer:', 2142), (':first_quarter_moon:', 2143), (':flexed_biceps:', 2144), (':fog:', 2145), (':folded_hands:', 2146), (':goat:', 2147), (':green_apple:', 2148), (':hot_beverage:', 2149), (':hot_springs:', 2150), (':medium_skin_tone:', 2151), (':milky_way:', 2152), (':moai:', 2153), (':musical_note:', 2154), (':oncoming_fist:', 2155), (':party_popper:', 2156), (':pencil:', 2157), (':pensive_face:', 2158), (':person_tipping_hand:', 2159), (':pig_face:', 2160), (':police_car_light:', 2161), (':raising_hands:', 2162), (':red_apple:', 2163), (':registered:', 2164), (':revolving_hearts:', 2165), (':ring:', 2166), (':rose:', 2167), (':smiling_face_with_horns:', 2168), (':smiling_face_with_open_hands:', 2169), (':smiling_face_with_sunglasses:', 2170), (':smirking_face:', 2171), (':sun:', 2172), (':thought_balloon:', 2173), (':thumbs_up:', 2174), (':tired_face:', 2175), (':victory_hand:', 2176), (':water_wave:', 2177), (':winking_face_with_tongue:', 2178), (':woman’s_boot:', 2179), (':yellow_heart:', 2180), (':yin_yang:', 2181), ('<laugh>', 2182), ('<phone>', 2183), ('>:(', 2184), ('_', 2185), ('__', 2186), ('_her', 2187), ('a3', 2188), ('aameen', 2189), ('abhors', 2190), ('abortion', 2191), ('above', 2192), ('absence', 2193), ('absurd', 2194), ('abt', 2195), ('abuse', 2196), ('abuser', 2197), ('abysmal', 2198), ('ac', 2199), ('academics', 2200), ('accepted', 2201), ('accepting', 2202), ('accidents', 2203), ('accurately', 2204), ('acting', 2205), ('actions', 2206), ('adam', 2207), ('adapts', 2208), ('addressed', 2209), ('adjusts', 2210), ('adoption', 2211), ('adrenaline', 2212), ('adrian', 2213), ('ads', 2214), ('adversity', 2215), ('afaik', 2216), ('afam', 2217), ('afew', 2218), ('affair', 2219), ('affairs', 2220), ('affects', 2221), ('afford', 2222), ('african', 2223), ('afrikaans', 2224), ('aged', 2225), ('aggravates', 2226), ('aggravation', 2227), ('agitate', 2228), ('ahid', 2229), ('ahut', 2230), ('aid', 2231), ('airbnb', 2232), ('ajvksk', 2233), ('ak47', 2234), ('akame', 2235), ('al', 2236), ('alan', 2237), ('alarm', 2238), ('alex', 2239), ('alicia', 2240), ('alive', 2241), ('allah', 2242), ('alliance', 2243), ('allowed', 2244), ('alphanumeric', 2245), ('amaity', 2246), ('amateur', 2247), ('amateurs', 2248), ('amazes', 2249), ('amazon', 2250), ('ambe', 2251), ('ambitions', 2252), ('amen', 2253), ('americans', 2254), ('amid', 2255), ('amiss', 2256), ('analysis', 2257), ('analyzing', 2258), ('anatomy', 2259), ('ancestry', 2260), ('ancient', 2261), ('andis', 2262), ('angela', 2263), ('angeles', 2264), ('angie', 2265), ('angry', 2266), ('animal', 2267), ('animation', 2268), ('anime', 2269), ('announce', 2270), ('annoy', 2271), ('annoyed', 2272), ('answer', 2273), ('anthony', 2274), ('anti', 2275), ('aosth', 2276), ('apartment', 2277), ('apes', 2278), ('apocalypse', 2279), ('apologising', 2280), ('appalling', 2281), ('appauling', 2282), ('appealing', 2283), ('applications', 2284), ('appreciate', 2285), ('approved', 2286), ('approximately', 2287), ('aqw', 2288), ('arch', 2289), ('architecture', 2290), ('arising', 2291), ('arming', 2292), ('army', 2293), ('arsene', 2294), ('arthur', 2295), ('article', 2296), ('artists', 2297), ('arwuevfqv', 2298), ('asian', 2299), ('aside', 2300), ('asking', 2301), ('asparagus', 2302), ('aspiring', 2303), ('assistance', 2304), ('atleast', 2305), ('atmosphere', 2306), ('attack', 2307), ('attacked', 2308), ('attempt', 2309), ('attempting', 2310), ('attending', 2311), ('attestative', 2312), ('attic', 2313), ('attract', 2314), ('audio', 2315), ('aunt', 2316), ('australia', 2317), ('authors', 2318), ('auto', 2319), ('autograph', 2320), ('available', 2321), ('avoided', 2322), ('awaits', 2323), ('awarded', 2324), ('awesome', 2325), ('aww', 2326), ('azi', 2327), ('b.b.', 2328), ('ba6426', 2329), ('baal', 2330), ('back2back', 2331), ('background', 2332), ('backgrounds', 2333), ('backing', 2334), ('backward', 2335), ('badge', 2336), ('badger', 2337), ('badu', 2338), ('bags', 2339), ('bain', 2340), ('bait', 2341), ('baker', 2342), ('bakes', 2343), ('balanced', 2344), ('balls', 2345), ('balotelli', 2346), ('ban', 2347), ('banner', 2348), ('bantz', 2349), ('baptist', 2350), ('barcelos', 2351), ('bare', 2352), ('barish', 2353), ('baron', 2354), ('barroso', 2355), ('basic', 2356), ('basically', 2357), ('bass', 2358), ('bastard', 2359), ('bat', 2360), ('battered', 2361), ('battleground', 2362), ('battles', 2363), ('bayern', 2364), ('bb', 2365), ('bb18', 2366), ('bbc', 2367), ('bbq', 2368), ('beacon', 2369), ('beans', 2370), ('beaten', 2371), ('beating', 2372), ('beautyof', 2373), ('bedrest', 2374), ('beef', 2375), ('begging', 2376), ('begin', 2377), ('beginning', 2378), ('behavior', 2379), ('beings', 2380), ('belly', 2381), ('belongings', 2382), ('ben', 2383), ('benadryl', 2384), ('bench', 2385), ('benching', 2386), ('beneath', 2387), ('berl', 2388), ('berry', 2389), ('besides', 2390), ('bey', 2391), ('bidded', 2392), ('bigger', 2393), ('bigotry', 2394), ('bikes', 2395), ('binge', 2396), ('bint', 2397), ('bio', 2398), ('biology', 2399), ('bipolar', 2400), ('birmingham', 2401), ('birth', 2402), ('bits', 2403), ('biz', 2404), ('bk', 2405), ('blackandwhite', 2406), ('blackout', 2407), ('blake', 2408), ('blankets', 2409), ('bless', 2410), ('blessings', 2411), ('blitz', 2412), ('blockade', 2413), ('blond', 2414), ('bloods', 2415), ('bls', 2416), ('bluff', 2417), ('bma', 2418), ('bojan', 2419), ('bomb', 2420), ('booked', 2421), ('boosted', 2422), ('booth', 2423), ('borderline', 2424), ('boredom', 2425), ('boshan', 2426), ('boss', 2427), ('boston', 2428), ('bowl', 2429), ('bows', 2430), ('boycott', 2431), ('bpd', 2432), ('bra', 2433), ('bradford', 2434), ('brainwashed', 2435), ('brand', 2436), ('branded', 2437), ('branding', 2438), ('brandon', 2439), ('brands', 2440), ('bravehearted', 2441), ('brawndo', 2442), ('brazy', 2443), ('brb', 2444), ('breadrins', 2445), ('breakfast', 2446), ('breasts', 2447), ('breath', 2448), ('brendon', 2449), ('brians', 2450), ('bribe', 2451), ('bridge', 2452), ('bridget', 2453), ('brighten', 2454), ('brighter', 2455), ('brightest', 2456), ('bringing', 2457), ('brithday', 2458), ('brodcast', 2459), ('bronzy', 2460), ('bruce', 2461), ('brussels', 2462), ('bs', 2463), ('bsg', 2464), ('bt', 2465), ('bts', 2466), ('bub', 2467), ('buddhism', 2468), ('buddy', 2469), ('buffy', 2470), ('buggered', 2471), ('build', 2472), ('building', 2473), ('bullet', 2474), ('bully', 2475), ('bun', 2476), ('bureaucrat', 2477), ('burns', 2478), ('burst', 2479), ('burton', 2480), ('butler', 2481), ('butt', 2482), ('button', 2483), ('buzzing', 2484), ('bye', 2485), ('c.c.', 2486), ('cab', 2487), ('caballero', 2488), ('cafe', 2489), ('cali', 2490), ('california', 2491), ('calm', 2492), ('camera', 2493), ('camp', 2494), ('campaignin', 2495), ('campground', 2496), ('campus', 2497), ('canadian', 2498), ('canceled', 2499), ('cancelled', 2500), ('cancer', 2501), ('candies', 2502), ('canola', 2503), ('cap', 2504), ('cape', 2505), ('capita', 2506), ('capo', 2507), ('captain', 2508), ('cardiff', 2509), ('carleton', 2510), ('carnival', 2511), ('carriages', 2512), ('cars', 2513), ('cash', 2514), ('cashew', 2515), ('casket', 2516), ('cassablancas', 2517), ('catchy', 2518), ('catfish', 2519), ('caused', 2520), ('cave', 2521), ('ccm', 2522), ('celebrated', 2523), ('celebrities', 2524), ('cell', 2525), ('center', 2526), ('century', 2527), ('cereal', 2528), ('certain', 2529), ('certainly', 2530), ('cf', 2531), ('chafford', 2532), ('chains', 2533), ('chairperson', 2534), ('chalk', 2535), ('challenge', 2536), ('champion', 2537), ('changing', 2538), ('chaotic', 2539), ('chapman', 2540), ('charge', 2541), ('chavs', 2542), ('cheat', 2543), ('cheated', 2544), ('cheaters', 2545), ('checking', 2546), ('cheeks', 2547), ('chem', 2548), ('chestnut', 2549), ('chick', 2550), ('chicken', 2551), ('childood', 2552), ('children', 2553), ('chillin', 2554), ('chilly', 2555), ('chinks', 2556), ('chip', 2557), ('chipotle', 2558), ('chiropractor', 2559), ('chirpy', 2560), ('chocolate', 2561), ('choice', 2562), ('choose', 2563), ('christ', 2564), ('cibulskis', 2565), ('cigar', 2566), ('circle', 2567), ('circles', 2568), ('circumstantial', 2569), ('cisco', 2570), ('citizen', 2571), ('civil', 2572), ('claims', 2573), ('clan', 2574), ('cleaning', 2575), ('clearing', 2576), ('clearly', 2577), ('cleaved', 2578), ('clemvs', 2579), ('clergy', 2580), ('cleveland', 2581), ('click', 2582), ('clients', 2583), ('clips', 2584), ('closed', 2585), ('cloud', 2586), ('cloudy', 2587), ('cloved', 2588), ('club', 2589), ('clueless', 2590), ('cm', 2591), ('cn', 2592), ('coat', 2593), ('code', 2594), ('coefficient', 2595), ('cognitive', 2596), ('cohen', 2597), ('coincidence', 2598), ('coins', 2599), ('colada', 2600), ('colder', 2601), ('collect', 2602), ('coloring', 2603), ('colors', 2604), ('colourful', 2605), ('comeback', 2606), ('comfortable', 2607), ('comforted', 2608), ('comments', 2609), ('committing', 2610), ('common', 2611), ('community', 2612), ('commute', 2613), ('comparisons', 2614), ('compassion', 2615), ('competition', 2616), ('completed', 2617), ('completing', 2618), ('complications', 2619), ('compliment', 2620), ('compliments', 2621), ('concern', 2622), ('concerned', 2623), ('concerts', 2624), ('concessions', 2625), ('conciousness', 2626), ('concluded', 2627), ('concorde', 2628), ('condemning', 2629), ('conductor', 2630), ('cones', 2631), ('conflate', 2632), ('conflict', 2633), ('conflicting', 2634), ('confusion', 2635), ('congratulations', 2636), ('congress', 2637), ('coniferous', 2638), ('connectivity', 2639), ('conscious', 2640), ('consenting', 2641), ('consistent', 2642), ('constipated', 2643), ('constipation', 2644), ('consumer', 2645), ('consumerism', 2646), ('contactless', 2647), ('contacts', 2648), ('contained', 2649), ('content', 2650), ('contestants', 2651), ('context', 2652), ('continual', 2653), ('continued', 2654), ('continues', 2655), ('contributing', 2656), ('conundrum', 2657), ('convenient', 2658), ('conversation', 2659), ('convo', 2660), ('cooked', 2661), ('cooped', 2662), ('cooties', 2663), ('cope', 2664), ('core246', 2665), ('corinthian', 2666), ('corn', 2667), ('corner', 2668), ('corpses', 2669), ('costs', 2670), ('counterfeit', 2671), ('couple', 2672), ('courageous', 2673), ('court', 2674), ('courting', 2675), ('covering', 2676), ('coworker', 2677), ('coys', 2678), ('coz', 2679), ('cpfc', 2680), ('cptsd', 2681), ('crackhead', 2682), ('craig', 2683), ('cramps', 2684), ('crap', 2685), ('crash', 2686), ('crashed', 2687), ('crb', 2688), ('cream', 2689), ('creamy', 2690), ('created', 2691), ('creative', 2692), ('creativity', 2693), ('credible', 2694), ('credit', 2695), ('creek', 2696), ('creeping', 2697), ('creepy', 2698), ('crew', 2699), ('cries', 2700), ('criminalized', 2701), ('criminals', 2702), ('cringe', 2703), ('crippling', 2704), ('crisp', 2705), ('critical', 2706), ('critters', 2707), ('crock', 2708), ('crooked', 2709), ('crossed', 2710), ('crucial', 2711), ('cruther', 2712), ('crystal', 2713), ('cs', 2714), ('ctid', 2715), ('cube', 2716), ('cucumbers', 2717), ('cuddled', 2718), ('cuddling', 2719), ('cudi', 2720), ('cue', 2721), ('cules', 2722), ('cull', 2723), ('cultural', 2724), ('culture', 2725), ('cunts', 2726), ('cupcake', 2727), ('cured', 2728), ('curling', 2729), ('curly', 2730), ('currently', 2731), ('curry', 2732), ('cutie', 2733), ('dada', 2734), ('dahl', 2735), ('dalai', 2736), ('dame', 2737), ('dami', 2738), ('dancing', 2739), ('dandelion', 2740), ('danishes', 2741), ('dare', 2742), ('darius', 2743), ('darker', 2744), ('darkness', 2745), ('dat', 2746), ('dates', 2747), ('davina', 2748), ('dayjob', 2749), ('dayz', 2750), ('dbs', 2751), ('dc', 2752), ('de', 2753), ('deadlines', 2754), ('dealer', 2755), ('debacle', 2756), ('debate', 2757), ('debating', 2758), ('decent', 2759), ('deception', 2760), ('decide', 2761), ('decided', 2762), ('deciduous', 2763), ('decoded', 2764), ('deed', 2765), ('deepening', 2766), ('def', 2767), ('defeat', 2768), ('defeated', 2769), ('defeats', 2770), ('defences', 2771), ('deficit', 2772), ('defining', 2773), ('degree', 2774), ('delayed', 2775), ('delight', 2776), ('delivery', 2777), ('demetria', 2778), ('democratic', 2779), ('demons', 2780), ('dented', 2781), ('dentist', 2782), ('denver', 2783), ('department', 2784), ('departure', 2785), ('depay', 2786), ('dependent', 2787), ('depending', 2788), ('depravity', 2789), ('desert', 2790), ('deserved', 2791), ('deserves', 2792), ('desication', 2793), ('design', 2794), ('designs', 2795), ('desire', 2796), ('desk', 2797), ('despicable', 2798), ('destroy', 2799), ('destroyed', 2800), ('detachment', 2801), ('detailed', 2802), ('deteriorada', 2803), ('determine', 2804), ('determined', 2805), ('devastated', 2806), ('developing', 2807), ('devils', 2808), ('dewayne', 2809), ('df', 2810), ('dfs', 2811), ('dharma', 2812), ('dhona', 2813), ('diagnosed', 2814), ('dick', 2815), ('didn', 2816), ('didnt', 2817), ('diego', 2818), ('diesel', 2819), ('differences', 2820), ('difficult', 2821), ('difficulties', 2822), ('digesting', 2823), ('digging', 2824), ('din', 2825), ('dinner', 2826), ('dire', 2827), ('direct', 2828), ('director', 2829), ('dirty', 2830), ('dis', 2831), ('disabled', 2832), ('disagree', 2833), ('disappeared', 2834), ('disappointing', 2835), ('disappointment', 2836), ('disaster', 2837), ('disasters', 2838), ('discouraging', 2839), ('discovered', 2840), ('disease', 2841), ('disgusted', 2842), ('dishearten', 2843), ('disheartening', 2844), ('dislike', 2845), ('dislikes', 2846), ('dismissal', 2847), ('disneyworld', 2848), ('dispirit', 2849), ('diss', 2850), ('dissapear', 2851), ('dissociating', 2852), ('distant', 2853), ('dive', 2854), ('divine', 2855), ('divorce', 2856), ('django', 2857), ('dms', 2858), ('documentary', 2859), ('dodges', 2860), ('doese', 2861), ('doesn', 2862), ('doesnt', 2863), ('doings', 2864), ('dollar', 2865), ('donate', 2866), ('donating', 2867), ('donation', 2868), ('donatists', 2869), ('donegal', 2870), ('dons', 2871), ('donte', 2872), ('donuts', 2873), ('doorway', 2874), ('doubt', 2875), ('downhearted', 2876), ('download', 2877), ('downstairs', 2878), ('downtrodden', 2879), ('dpt', 2880), ('drag', 2881), ('drake', 2882), ('drank', 2883), ('draw', 2884), ('drawing', 2885), ('dress', 2886), ('dressed', 2887), ('dried', 2888), ('drinkin', 2889), ('drinking', 2890), ('drinks', 2891), ('driscoll', 2892), ('driver', 2893), ('drivers', 2894), ('drooping', 2895), ('drugged', 2896), ('drugs', 2897), ('dsp', 2898), ('dt', 2899), ('dubai', 2900), ('dublin', 2901), ('dud', 2902), ('dumbass', 2903), ('dunno', 2904), ('dutch', 2905), ('dutty', 2906), ('dv', 2907), ('dwelling', 2908), ('dye', 2909), ('dylon', 2910), ('dyslexia', 2911), ('ea', 2912), ('eachother', 2913), ('eagles', 2914), ('earn', 2915), ('earning', 2916), ('earphones', 2917), ('earpiece', 2918), ('earrings', 2919), ('easier', 2920), ('econ', 2921), ('economy', 2922), ('edge', 2923), ('edge_could', 2924), ('edit', 2925), ('edited', 2926), ('edmonds', 2927), ('effective', 2928), ('egg', 2929), ('eix', 2930), ('ele', 2931), ('elected', 2932), ('election', 2933), ('elections', 2934), ('eleven', 2935), ('eliza', 2936), ('elliot', 2937), ('elphaba', 2938), ('elphie', 2939), ('elton', 2940), ('email', 2941), ('embarrassed', 2942), ('embodiment', 2943), ('embrace', 2944), ('emirates', 2945), ('emo', 2946), ('emoji', 2947), ('emojis', 2948), ('emoticon', 2949), ('emotion', 2950), ('emotional', 2951), ('emotionally', 2952), ('empath', 2953), ('empire', 2954), ('encroaching', 2955), ('ending', 2956), ('endless', 2957), ('endure', 2958), ('energy', 2959), ('engineer', 2960), ('enjoy', 2961), ('enlightenment', 2962), ('ent', 2963), ('entering', 2964), ('entertained', 2965), ('entertainers', 2966), ('entire', 2967), ('entree', 2968), ('environment', 2969), ('enzyme', 2970), ('ep', 2971), ('equally', 2972), ('equals', 2973), ('equivalent', 2974), ('era', 2975), ('ernest', 2976), ('err', 2977), ('erykah', 2978), ('es', 2979), ('escalate', 2980), ('escapism', 2981), ('especially', 2982), ('essay', 2983), ('establishment', 2984), ('estate', 2985), ('ethnic', 2986), ('eu', 2987), ('eun', 2988), ('evaluation', 2989), ('evangelical', 2990), ('events', 2991), ('everyday', 2992), ('everythin', 2993), ('ew', 2994), ('exact', 2995), ('exam', 2996), ('example', 2997), ('except', 2998), ('exciting', 2999), ('excuse', 3000), ('existent', 3001), ('existing', 3002), ('exists', 3003), ('expectation', 3004), ('expectations', 3005), ('expediency', 3006), ('expensive', 3007), ('explanation', 3008), ('explosion', 3009), ('exposed', 3010), ('exposing', 3011), ('express', 3012), ('expressed', 3013), ('extradition', 3014), ('eye', 3015), ('eyepiece', 3016), ('fa', 3017), ('fab', 3018), ('faces', 3019), ('factory', 3020), ('faculties', 3021), ('faded', 3022), ('fails', 3023), ('fair', 3024), ('faithful', 3025), ('fakes', 3026), ('fame', 3027), ('families', 3028), ('famous', 3029), ('fancam', 3030), ('fancy', 3031), ('fanfiction', 3032), ('fantasies', 3033), ('fantastic', 3034), ('fantasy', 3035), ('fart', 3036), ('fashion', 3037), ('fashionable', 3038), ('fat', 3039), ('faux', 3040), ('fearless', 3041), ('federer', 3042), ('feeding', 3043), ('feelin', 3044), ('feelz', 3045), ('feic', 3046), ('fell', 3047), ('fellaini', 3048), ('fellow', 3049), ('female', 3050), ('females', 3051), ('feminists', 3052), ('fever', 3053), ('fi', 3054), ('fibromyalgia', 3055), ('fic', 3056), ('fiddle', 3057), ('fight', 3058), ('fighter', 3059), ('figure', 3060), ('filter', 3061), ('filthy', 3062), ('finale', 3063), ('fines', 3064), ('fink', 3065), ('finland', 3066), ('finovate', 3067), ('firm', 3068), ('fish', 3069), ('fit', 3070), ('five', 3071), ('fl', 3072), ('flair', 3073), ('flares', 3074), ('flat', 3075), ('flavoured', 3076), ('flee', 3077), ('flip', 3078), ('flippant', 3079), ('flirt', 3080), ('floofel', 3081), ('flops', 3082), ('floyd', 3083), ('flu', 3084), ('fly', 3085), ('flybe', 3086), ('flying', 3087), ('fmr', 3088), ('fo', 3089), ('foaming', 3090), ('focus', 3091), ('foggy', 3092), ('folding', 3093), ('folks', 3094), ('following', 3095), ('fond', 3096), ('font', 3097), ('fools', 3098), ('footage', 3099), ('footy', 3100), ('forced', 3101), ('forcing', 3102), ('fordone', 3103), ('forehead', 3104), ('forge', 3105), ('forgive', 3106), ('forgiveness', 3107), ('forgot', 3108), ('forks', 3109), ('forrester', 3110), ('fort', 3111), ('fortune', 3112), ('foster', 3113), ('four', 3114), ('fraction', 3115), ('frame', 3116), ('franchise', 3117), ('franglaise', 3118), ('fraudsters', 3119), ('freaking', 3120), ('freedom', 3121), ('frequency', 3122), ('friedman', 3123), ('frogs', 3124), ('fruits', 3125), ('frustrated', 3126), ('frustration', 3127), ('fryers', 3128), ('ftw', 3129), ('fucken', 3130), ('fuel', 3131), ('fukc', 3132), ('fulford', 3133), ('fully', 3134), ('fumes', 3135), ('function', 3136), ('functional', 3137), ('fundamentals', 3138), ('fundraisers', 3139), ('funds', 3140), ('funereal', 3141), ('furious', 3142), ('further', 3143), ('fuse', 3144), ('fyi', 3145), ('gabe', 3146), ('gabriel', 3147), ('gaga', 3148), ('gaining', 3149), ('gal', 3150), ('gallery', 3151), ('gameing', 3152), ('gardevoir', 3153), ('gate', 3154), ('gateshead', 3155), ('gateway', 3156), ('gatherp', 3157), ('gavc', 3158), ('gave', 3159), ('gazed', 3160), ('gd', 3161), ('gear', 3162), ('geelong', 3163), ('gems', 3164), ('gen', 3165), ('generally', 3166), ('generation', 3167), ('generic', 3168), ('generosity', 3169), ('gents', 3170), ('genuinely', 3171), ('ghanaians', 3172), ('ghost', 3173), ('gif', 3174), ('gig', 3175), ('giggle', 3176), ('gini', 3177), ('gladly', 3178), ('glands', 3179), ('glasgow', 3180), ('glasses', 3181), ('global', 3182), ('glorifying', 3183), ('glow', 3184), ('glowy', 3185), ('glued', 3186), ('glum', 3187), ('goals', 3188), ('gobsmacked', 3189), ('golden', 3190), ('goldman', 3191), ('gomez', 3192), ('goods', 3193), ('goofy', 3194), ('google', 3195), ('gordon', 3196), ('gosh', 3197), ('gossip', 3198), ('gotham', 3199), ('gothic', 3200), ('gp', 3201), ('gps', 3202), ('grace', 3203), ('grade', 3204), ('graduate', 3205), ('graham', 3206), ('grandad', 3207), ('grandmother', 3208), ('grapefruit', 3209), ('gratitude', 3210), ('greatness', 3211), ('greener', 3212), ('greenville', 3213), ('greenwich', 3214), ('greys', 3215), ('grid', 3216), ('grilled', 3217), ('grimmerie', 3218), ('grind', 3219), ('grinding', 3220), ('gripping', 3221), ('gritty', 3222), ('groceries', 3223), ('groom', 3224), ('groove', 3225), ('grouch', 3226), ('growth', 3227), ('grumpy', 3228), ('gt', 3229), ('gtfo', 3230), ('guanxi', 3231), ('guard', 3232), ('guilty', 3233), ('gulking', 3234), ('gundlach', 3235), ('guru', 3236), ('habits', 3237), ('habitual', 3238), ('hacked', 3239), ('hackney', 3240), ('haddrell', 3241), ('hahahaha', 3242), ('hail', 3243), ('hakyeon', 3244), ('halfway', 3245), ('halifax', 3246), ('hammer', 3247), ('handed', 3248), ('handful', 3249), ('handle', 3250), ('hang', 3251), ('hangovers', 3252), ('hannah', 3253), ('happens', 3254), ('happily', 3255), ('harambe', 3256), ('hardy', 3257), ('hartramsey', 3258), ('harvard', 3259), ('harvey', 3260), ('hashtag', 3261), ('hateful', 3262), ('hates', 3263), ('hatred', 3264), ('hav', 3265), ('havnt', 3266), ('hchat', 3267), ('header', 3268), ('headlines', 3269), ('heads', 3270), ('heals', 3271), ('heartbeat', 3272), ('heartbreak', 3273), ('heaven', 3274), ('heckler', 3275), ('heels', 3276), ('hemingway', 3277), ('hemp', 3278), ('henderson', 3279), ('hennessey', 3280), ('henny', 3281), ('hentai', 3282), ('hereford', 3283), ('hero', 3284), ('heroes', 3285), ('hestia', 3286), ('hid', 3287), ('hide', 3288), ('hideki', 3289), ('hiding', 3290), ('higginbotham', 3291), ('highlight', 3292), ('highlighter', 3293), ('hijack', 3294), ('hilarity', 3295), ('hill', 3296), ('hippie', 3297), ('hireling', 3298), ('hispan', 3299), ('historic', 3300), ('historically', 3301), ('hmu', 3302), ('hockey', 3303), ('hoco', 3304), ('hoe', 3305), ('holders', 3306), ('holidays', 3307), ('holy', 3308), ('hominids', 3309), ('honest', 3310), ('honor', 3311), ('hood', 3312), ('hoody', 3313), ('hoop', 3314), ('hopeful', 3315), ('hopes', 3316), ('horrible', 3317), ('horrid', 3318), ('horse', 3319), ('horseman', 3320), ('hospital', 3321), ('hostel', 3322), ('hostels', 3323), ('hotel', 3324), ('houses', 3325), ('housing', 3326), ('houston', 3327), ('howland', 3328), ('hrc', 3329), ('hs', 3330), ('html5', 3331), ('hues', 3332), ('huff', 3333), ('huffing', 3334), ('hufflepuff', 3335), ('hug', 3336), ('humanity', 3337), ('hundred', 3338), ('hungry', 3339), ('hurry', 3340), ('hv', 3341), ('hyatt', 3342), ('hydra', 3343), ('hyped', 3344), ('hyper', 3345), ('hysterical', 3346), ('iced', 3347), ('idealize', 3348), ('identical', 3349), ('identify', 3350), ('ideologies', 3351), ('idgi', 3352), ('idiocracytoday', 3353), ('idiosyncrasies', 3354), ('idiots', 3355), ('ig', 3356), ('igda', 3357), ('ignite', 3358), ('ignored', 3359), ('ignoring', 3360), ('ih', 3361), ('ihave', 3362), ('ii', 3363), ('illness', 3364), ('imafiancée', 3365), ('images', 3366), ('imediately', 3367), ('immoral', 3368), ('immune', 3369), ('imo', 3370), ('impact', 3371), ('importance', 3372), ('impossible', 3373), ('impress', 3374), ('in2days', 3375), ('inadequacy', 3376), ('inappropriate', 3377), ('inasmuch', 3378), ('inclined', 3379), ('included', 3380), ('income', 3381), ('indeed', 3382), ('indian', 3383), ('indictment', 3384), ('indie', 3385), ('indignity', 3386), ('indy', 3387), ('inej', 3388), ('inexpensive', 3389), ('infections', 3390), ('inflicted', 3391), ('information', 3392), ('ingrained', 3393), ('initial', 3394), ('initiate', 3395), ('initiative', 3396), ('insanely', 3397), ('insensitive', 3398), ('insightful', 3399), ('inspire', 3400), ('insta', 3401), ('instructors', 3402), ('instrument', 3403), ('instrumentals', 3404), ('instruments', 3405), ('intelligent', 3406), ('intend', 3407), ('intensely', 3408), ('interested', 3409), ('interface', 3410), ('intermediaries', 3411), ('intervation', 3412), ('intervention', 3413), ('intuition', 3414), ('involuntarily', 3415), ('involved', 3416), ('involvement', 3417), ('ion', 3418), ('ios', 3419), ('ios10', 3420), ('iphone', 3421), ('ir', 3422), ('iraq', 3423), ('irish', 3424), ('irl', 3425), ('ironing', 3426), ('irrefutable', 3427), ('isaiah', 3428), ('ish', 3429), ('isle', 3430), ('isolated', 3431), ('italian', 3432), ('ivory', 3433), ('jaehwan', 3434), ('jams', 3435), ('jan', 3436), ('jauregui', 3437), ('jb', 3438), ('jenner', 3439), ('jennifer', 3440), ('jericho', 3441), ('jersey', 3442), ('jest', 3443), ('jfk', 3444), ('jimi', 3445), ('jk', 3446), ('joel', 3447), ('joint', 3448), ('joker', 3449), ('joking', 3450), ('jolie', 3451), ('jonathan', 3452), ('jones', 3453), ('jr', 3454), ('jst', 3455), ('jt', 3456), ('judge', 3457), ('judgement', 3458), ('judging', 3459), ('judo', 3460), ('judy', 3461), ('julian', 3462), ('jumia', 3463), ('jumin', 3464), ('jumps', 3465), ('junior', 3466), ('juniors', 3467), ('jury', 3468), ('juspopmolly', 3469), ('justlet', 3470), ('jut', 3471), ('juvia', 3472), ('kai', 3473), ('kal', 3474), ('kali', 3475), ('kardashian', 3476), ('kart', 3477), ('kate', 3478), ('katznelson', 3479), ('kay', 3480), ('kaz', 3481), ('keeping', 3482), ('kei', 3483), ('keller', 3484), ('kennen', 3485), ('kentphonic', 3486), ('kept', 3487), ('kerry', 3488), ('keyboard', 3489), ('keys', 3490), ('kickass', 3491), ('kicked', 3492), ('kickin', 3493), ('kickoff', 3494), ('kid', 3495), ('kidding', 3496), ('killing', 3497), ('kills', 3498), ('kin', 3499), ('kin26mins', 3500), ('kingdom', 3501), ('kit', 3502), ('kitten', 3503), ('kittens', 3504), ('knee', 3505), ('knees', 3506), ('knives', 3507), ('knocking', 3508), ('knox', 3509), ('krab', 3510), ('krusty', 3511), ('kuo', 3512), ('l1', 3513), ('la', 3514), ('labor', 3515), ('lacing', 3516), ('lacking', 3517), ('ladies', 3518), ('lads', 3519), ('lady', 3520), ('lake', 3521), ('lakes', 3522), ('lam', 3523), ('lama', 3524), ('lame', 3525), ('lament', 3526), ('lamentable', 3527), ('lamination', 3528), ('lamp', 3529), ('lampoon', 3530), ('lana', 3531), ('landscape', 3532), ('language', 3533), ('lans', 3534), ('larh', 3535), ('lasted', 3536), ('lasting', 3537), ('lasts', 3538), ('laughs', 3539), ('lauren', 3540), ('lawsons', 3541), ('laying', 3542), ('lbvfs', 3543), ('ldf16', 3544), ('le', 3545), ('lead', 3546), ('league', 3547), ('leeds', 3548), ('leftover', 3549), ('legacy', 3550), ('legally', 3551), ('leggings', 3552), ('legs', 3553), ('lemme', 3554), ('leo', 3555), ('leonard', 3556), ('leopard', 3557), ('lest', 3558), ('levied', 3559), ('lia', 3560), ('liam', 3561), ('liberal', 3562), ('license', 3563), ('lies', 3564), ('lifeisnotafairytale', 3565), ('lifers', 3566), ('lifes', 3567), ('lift', 3568), ('lighting', 3569), ('liked', 3570), ('likely', 3571), ('lily', 3572), ('limb', 3573), ('lime', 3574), ('limits', 3575), ('linc', 3576), ('linear', 3577), ('lions', 3578), ('lippies', 3579), ('litany', 3580), ('lived', 3581), ('lizards', 3582), ('lo', 3583), ('loathing', 3584), ('local', 3585), ('lock', 3586), ('locke', 3587), ('lodd', 3588), ('loft', 3589), ('logged', 3590), ('logging', 3591), ('loneliness', 3592), ('loner', 3593), ('lonesome', 3594), ('longing', 3595), ('loon', 3596), ('looney', 3597), ('loose', 3598), ('los', 3599), ('loses', 3600), ('lou', 3601), ('louis', 3602), ('lounge', 3603), ('lower', 3604), ('lowered', 3605), ('loyal', 3606), ('lpco', 3607), ('luca', 3608), ('luggage', 3609), ('lunches', 3610), ('lure', 3611), ('lust', 3612), ('luv', 3613), ('lyft', 3614), ('lyrics', 3615), ('m.d.', 3616), ('mac', 3617), ('madden', 3618), ('madridistas', 3619), ('magic', 3620), ('magical', 3621), ('mah', 3622), ('mailed', 3623), ('maine', 3624), ('majority', 3625), ('majors', 3626), ('makeup', 3627), ('males', 3628), ('malnutrition', 3629), ('mama', 3630), ('mamoru', 3631), ('manage', 3632), ('managed', 3633), ('management', 3634), ('managers', 3635), ('manitoba', 3636), ('manufacturer', 3637), ('maps', 3638), ('marcus', 3639), ('marley', 3640), ('marriage', 3641), ('mart', 3642), ('mary', 3643), ('masculine', 3644), ('massage', 3645), ('mastering', 3646), ('material', 3647), ('maternal', 3648), ('matters', 3649), ('maturity', 3650), ('maughm', 3651), ('maw', 3652), ('maximum', 3653), ('mayor', 3654), ('mc', 3655), ('mcdowell', 3656), ('mcfc', 3657), ('mcnair', 3658), ('md', 3659), ('meaningful', 3660), ('meat', 3661), ('mechanicals', 3662), ('medallion', 3663), ('meden', 3664), ('medication', 3665), ('mediocrity', 3666), ('mediumpurple', 3667), ('meet', 3668), ('mega', 3669), ('megaproject', 3670), ('meghan', 3671), ('mel', 3672), ('melbourne', 3673), ('memers', 3674), ('memes', 3675), ('memorized', 3676), ('mentally', 3677), ('mention', 3678), ('mer', 3679), ('merely', 3680), ('messageboard', 3681), ('messin', 3682), ('metropolitan', 3683), ('mets', 3684), ('mexico', 3685), ('mgla', 3686), ('mgs', 3687), ('mi', 3688), ('midfield', 3689), ('midfielders', 3690), ('midweek', 3691), ('mighty', 3692), ('mikeand', 3693), ('miley', 3694), ('mindfulness', 3695), ('minds', 3696), ('mindset', 3697), ('minehead', 3698), ('mini', 3699), ('minimally', 3700), ('minty', 3701), ('minus', 3702), ('minw', 3703), ('mio', 3704), ('mirror', 3705), ('mis', 3706), ('mischief', 3707), ('misinformation', 3708), ('mission', 3709), ('mississippi', 3710), ('mistaken', 3711), ('mistakes', 3712), ('misty', 3713), ('mix', 3714), ('mixing', 3715), ('mixtape', 3716), ('mixture', 3717), ('mlc', 3718), ('mo', 3719), ('mobile', 3720), ('mockingjay', 3721), ('mode', 3722), ('model', 3723), ('modifier', 3724), ('mohr', 3725), ('moira', 3726), ('molly', 3727), ('momentarily', 3728), ('mon', 3729), ('moncrief', 3730), ('monitor', 3731), ('mono', 3732), ('monsta', 3733), ('montreal', 3734), ('mooncup', 3735), ('moonshine', 3736), ('moor', 3737), ('morality', 3738), ('morons', 3739), ('morrow', 3740), ('mortgage', 3741), ('mostly', 3742), ('mouths', 3743), ('movember', 3744), ('movements', 3745), ('movie', 3746), ('mp', 3747), ('mrs', 3748), ('msm', 3749), ('mucho', 3750), ('mum', 3751), ('muni', 3752), ('murdered', 3753), ('murking', 3754), ('muscled', 3755), ('muscles', 3756), ('musically', 3757), ('musician', 3758), ('musings', 3759), ('mutch', 3760), ('mv', 3761), ('mystery', 3762), ('n*eats', 3763), ('n*slight', 3764), ('n1', 3765), ('n2', 3766), ('n3', 3767), ('n4', 3768), ('nachos', 3769), ('nahh', 3770), ('nail', 3771), ('nalso', 3772), ('names', 3773), ('namjoon', 3774), ('nan', 3775), ('nand', 3776), ('nani', 3777), ('naoto', 3778), ('narcolepsy', 3779), ('nare', 3780), ('national', 3781), ('native', 3782), ('naturally', 3783), ('nazr', 3784), ('nbogum', 3785), ('nboth', 3786), ('nbreath', 3787), ('nchurch', 3788), ('ncollege', 3789), ('nconfidence', 3790), ('ndeveloping', 3791), ('ndream', 3792), ('ndreams', 3793), ('neach', 3794), ('neals', 3795), ('neck', 3796), ('needa', 3797), ('neg', 3798), ('negativity', 3799), ('nehemiah', 3800), ('neighbor', 3801), ('neighbors', 3802), ('nell', 3803), ('nephew', 3804), ('nerd', 3805), ('nerves', 3806), ('nevery', 3807), ('newcastle', 3808), ('nfeeling', 3809), ('nforget', 3810), ('nfreedom', 3811), ('nfri', 3812), ('nfriend', 3813), ('ngerry', 3814), ('nget', 3815), ('nhappy', 3816), ('nhar', 3817), ('nhear', 3818), ('nhearing', 3819), ('nheheh', 3820), ('nhopefully', 3821), ('nicholson', 3822), ('nick', 3823), ('nicole', 3824), ('nigella', 3825), ('nightlife', 3826), ('nikerun', 3827), ('nim', 3828), ('nimble', 3829), ('nit', 3830), ('niv', 3831), ('nlast', 3832), ('nlife', 3833), ('nlikewise', 3834), ('nlong', 3835), ('nmade', 3836), ('nmy', 3837), ('nne', 3838), ('nnext', 3839), ('nngh', 3840), ('nnothing', 3841), ('nnow', 3842), ('noah', 3843), ('nod', 3844), ('noel', 3845), ('noh', 3846), ('noises', 3847), ('noob', 3848), ('norffampton', 3849), ('normally', 3850), ('north', 3851), ('notation', 3852), ('notch', 3853), ('note7', 3854), ('notebook', 3855), ('notebooks', 3856), ('nothings', 3857), ('notice', 3858), ('notif', 3859), ('nova', 3860), ('novelists', 3861), ('nowadays', 3862), ('np', 3863), ('nplay', 3864), ('nroad', 3865), ('nropes', 3866), ('nsaddened', 3867), ('nscent', 3868), ('nspunky', 3869), ('nsure', 3870), ('nsw', 3871), ('nt', 3872), ('nthere', 3873), ('nthis', 3874), ('nthose', 3875), ('ntomorrow', 3876), ('ntough', 3877), ('ntrap', 3878), ('nude', 3879), ('numb', 3880), ('number', 3881), ('numbers', 3882), ('numbness', 3883), ('nutella', 3884), ('nutter', 3885), ('nwhilst', 3886), ('nwith', 3887), ('nwo', 3888), ('nxx', 3889), ('nyall', 3890), ('nyet', 3891), ('oakley', 3892), ('oaths', 3893), ('obama', 3894), ('obese', 3895), ('observant', 3896), ('obsessed', 3897), ('occassionally', 3898), ('oceanic', 3899), ('oddish', 3900), ('oelrichs', 3901), ('offense', 3902), ('offensive', 3903), ('offered', 3904), ('official', 3905), ('offline', 3906), ('ohh', 3907), ('oldest', 3908), ('oldham', 3909), ('ole', 3910), ('oligodendroglioma', 3911), ('ollafest', 3912), ('olympia', 3913), ('ongoing', 3914), ('onto', 3915), ('opening', 3916), ('openness', 3917), ('opens', 3918), ('oppa', 3919), ('opportunities', 3920), ('oppose', 3921), ('opposition', 3922), ('oppressive', 3923), ('optimistic', 3924), ('options', 3925), ('ordered', 3926), ('org', 3927), ('organise', 3928), ('origins', 3929), ('orion', 3930), ('ornaments', 3931), ('osler', 3932), ('otis', 3933), ('ott', 3934), ('ottawa', 3935), ('ourselves', 3936), ('outage', 3937), ('output', 3938), ('overcome', 3939), ('overdue', 3940), ('overlooks', 3941), ('overseas', 3942), ('overwhelmed', 3943), ('ovth', 3944), ('owe', 3945), ('owl', 3946), ('owner', 3947), ('owning', 3948), ('oxford', 3949), ('oxidase', 3950), ('oxygen', 3951), ('ozzie', 3952), ('packed', 3953), ('packing', 3954), ('paddy', 3955), ('paella', 3956), ('pages', 3957), ('painted', 3958), ('painting', 3959), ('pajamas', 3960), ('paltry', 3961), ('pan', 3962), ('pandora', 3963), ('papa', 3964), ('paper', 3965), ('parallels', 3966), ('parcc', 3967), ('parking', 3968), ('parks', 3969), ('parted', 3970), ('particular', 3971), ('passage', 3972), ('passengers', 3973), ('passerby', 3974), ('passing', 3975), ('pastels', 3976), ('patches', 3977), ('patchy', 3978), ('patience', 3979), ('patient', 3980), ('patronus', 3981), ('paws', 3982), ('pax', 3983), ('paycheck', 3984), ('payet', 3985), ('paying', 3986), ('pdf', 3987), ('pe', 3988), ('peanut', 3989), ('peckham', 3990), ('pedal', 3991), ('pedro', 3992), ('peel', 3993), ('pencil', 3994), ('perched', 3995), ('perching', 3996), ('performance', 3997), ('period', 3998), ('permanent', 3999), ('permanently', 4000), ('perpetuating', 4001), ('perseverance', 4002), ('personal', 4003), ('perspective', 4004), ('pesto', 4005), ('pete', 4006), ('pfft', 4007), ('phenomena', 4008), ('philanthropy', 4009), ('phones', 4010), ('photography', 4011), ('photos', 4012), ('phrase', 4013), ('pick', 4014), ('pickering', 4015), ('picking', 4016), ('picnic', 4017), ('pie', 4018), ('piece', 4019), ('pierce', 4020), ('pig', 4021), ('pile', 4022), ('pinpoint', 4023), ('pisses', 4024), ('pitched', 4025), ('pjs', 4026), ('pk', 4027), ('plague', 4028), ('plan', 4029), ('plantation', 4030), ('plaster', 4031), ('plastic', 4032), ('plate', 4033), ('plath', 4034), ('plato', 4035), ('playoffs', 4036), ('pleased', 4037), ('plot', 4038), ('ploy', 4039), ('plugin', 4040), ('plumbers', 4041), ('plummet', 4042), ('podcast', 4043), ('podcasting', 4044), ('poem', 4045), ('points', 4046), ('poland', 4047), ('poli', 4048), ('policing', 4049), ('polite', 4050), ('politicans', 4051), ('poll', 4052), ('pollinate', 4053), ('polls', 4054), ('polluted', 4055), ('polyphenol', 4056), ('pondering', 4057), ('ponytail', 4058), ('pool', 4059), ('poop', 4060), ('popped', 4061), ('pores', 4062), ('pork', 4063), ('porkistan', 4064), ('porridge', 4065), ('portrayed', 4066), ('possession', 4067), ('possibly', 4068), ('poster', 4069), ('posting', 4070), ('postponed', 4071), ('pot', 4072), ('pound', 4073), ('pouring', 4074), ('pouting', 4075), ('powerful', 4076), ('practically', 4077), ('practices', 4078), ('pray', 4079), ('predator', 4080), ('predictable', 4081), ('prefect', 4082), ('preggy', 4083), ('premieres', 4084), ('premium', 4085), ('preparation', 4086), ('preposterous', 4087), ('preppy', 4088), ('prescribe', 4089), ('presence', 4090), ('presents', 4091), ('presidency', 4092), ('presidential', 4093), ('pressure', 4094), ('pretending', 4095), ('pretentious', 4096), ('prettier', 4097), ('prevelant', 4098), ('prevention', 4099), ('price', 4100), ('prices', 4101), ('prick', 4102), ('primary', 4103), ('prime', 4104), ('principles', 4105), ('prioritise', 4106), ('private', 4107), ('pro', 4108), ('prob', 4109), ('problems', 4110), ('probs', 4111), ('processor', 4112), ('produced', 4113), ('producer', 4114), ('profile', 4115), ('profit', 4116), ('programs', 4117), ('project', 4118), ('projection', 4119), ('promised', 4120), ('promo', 4121), ('promoter', 4122), ('promoting', 4123), ('pronunciation', 4124), ('propaganda', 4125), ('properly', 4126), ('proposal', 4127), ('protect', 4128), ('protected', 4129), ('protects', 4130), ('protein', 4131), ('prouder', 4132), ('provide', 4133), ('provoking', 4134), ('psychiatrist', 4135), ('psychological', 4136), ('publisher', 4137), ('pulled', 4138), ('pulling', 4139), ('punch', 4140), ('punish', 4141), ('pupper', 4142), ('purge', 4143), ('purges', 4144), ('purposes', 4145), ('pursuits', 4146), ('pussies', 4147), ('pussy', 4148), ('q', 4149), ('quality', 4150), ('questioned', 4151), ('quick', 4152), ('quickly', 4153), ('raabe', 4154), ('raabin', 4155), ('rafael', 4156), ('rainy', 4157), ('raise', 4158), ('raising', 4159), ('rake', 4160), ('rally', 4161), ('ram', 4162), ('rancour', 4163), ('randomly', 4164), ('range', 4165), ('rants', 4166), ('rape', 4167), ('raped', 4168), ('rapper', 4169), ('rappers', 4170), ('rarest', 4171), ('rashford', 4172), ('rat', 4173), ('ratings', 4174), ('ratio', 4175), ('rave', 4176), ('ravenclaw', 4177), ('raw', 4178), ('reached', 4179), ('reaching', 4180), ('reacts', 4181), ('readership', 4182), ('realising', 4183), ('realist', 4184), ('realizing', 4185), ('reaper', 4186), ('rebel', 4187), ('rebuild', 4188), ('recently', 4189), ('reception', 4190), ('recession', 4191), ('reckons', 4192), ('recognised', 4193), ('recognize', 4194), ('recommendations', 4195), ('recording', 4196), ('recruitment', 4197), ('redbus', 4198), ('redding', 4199), ('redhead', 4200), ('redstate', 4201), ('reed', 4202), ('refer', 4203), ('reference', 4204), ('reflects', 4205), ('refugees', 4206), ('refund', 4207), ('regarding', 4208), ('regardless', 4209), ('register', 4210), ('rehabilitated', 4211), ('reincarnation', 4212), ('rejection', 4213), ('relatable', 4214), ('relationship', 4215), ('relax', 4216), ('relaxed', 4217), ('released', 4218), ('relentlessly', 4219), ('relieve', 4220), ('relieves', 4221), ('religion', 4222), ('relocate', 4223), ('remainer', 4224), ('remembrances', 4225), ('reminder', 4226), ('reminds', 4227), ('remo', 4228), ('removal', 4229), ('repeat', 4230), ('repeating', 4231), ('replace', 4232), ('replacements', 4233), ('replying', 4234), ('repub', 4235), ('request', 4236), ('required', 4237), ('requirements', 4238), ('resentment', 4239), ('reshuffle', 4240), ('resolute', 4241), ('responsibility', 4242), ('restaurant', 4243), ('restless', 4244), ('restroom', 4245), ('retired', 4246), ('retirement', 4247), ('return', 4248), ('returned', 4249), ('returner', 4250), ('reusable', 4251), ('reveal', 4252), ('revival', 4253), ('reward', 4254), ('rewarded', 4255), ('rex', 4256), ('rhetoric', 4257), ('rhythm', 4258), ('rice', 4259), ('richards', 4260), ('ricketts', 4261), ('ride', 4262), ('rider', 4263), ('rides', 4264), ('ridge', 4265), ('ridiculous', 4266), ('righteous', 4267), ('rihanna', 4268), ('riots', 4269), ('risotto', 4270), ('rist', 4271), ('rita', 4272), ('rmanagement', 4273), ('rng', 4274), ('roach', 4275), ('roaches', 4276), ('roald', 4277), ('roast', 4278), ('rob', 4279), ('robbed', 4280), ('roberts', 4281), ('robot', 4282), ('rockandroll', 4283), ('rockin', 4284), ('rode', 4285), ('roger', 4286), ('rogerfederershop', 4287), ('roho', 4288), ('roll', 4289), ('rolling', 4290), ('romans', 4291), ('romero', 4292), ('ron', 4293), ('rona', 4294), ('rooneys', 4295), ('roots', 4296), ('rory', 4297), ('rough', 4298), ('round', 4299), ('rounding', 4300), ('row', 4301), ('royal', 4302), ('royals', 4303), ('rs', 4304), ('rucksack', 4305), ('rude', 4306), ('ruined', 4307), ('ruling', 4308), ('rung', 4309), ('runs', 4310), ('rupees', 4311), ('rvp', 4312), ('ryan', 4313), ('sa', 4314), ('sacks', 4315), ('saddened', 4316), ('safety', 4317), ('sail', 4318), ('sailed', 4319), ('sails', 4320), ('sake', 4321), ('salaams', 4322), ('salon', 4323), ('salvation', 4324), ('sam', 4325), ('san', 4326), ('sane', 4327), ('sanitizer', 4328), ('sarcasm', 4329), ('sauce', 4330), ('saves', 4331), ('saviour', 4332), ('sax', 4333), ('sc', 4334), ('scales', 4335), ('scandal', 4336), ('scanning', 4337), ('scar', 4338), ('scarred', 4339), ('scarves', 4340), ('scene', 4341), ('schalke', 4342), ('schedule', 4343), ('schkeuditz', 4344), ('science', 4345), ('scored', 4346), ('scorer', 4347), ('scott', 4348), ('scouts', 4349), ('scritch', 4350), ('scritching', 4351), ('sd', 4352), ('sdr', 4353), ('sea', 4354), ('search', 4355), ('seated', 4356), ('secretly', 4357), ('securing', 4358), ('seem', 4359), ('segregation', 4360), ('sejejs', 4361), ('selection', 4362), ('selena', 4363), ('selfies', 4364), ('semester', 4365), ('semi', 4366), ('sences', 4367), ('senile', 4368), ('senior', 4369), ('sent', 4370), ('sentence', 4371), ('sep', 4372), ('separation', 4373), ('sept', 4374), ('september2004', 4375), ('serves', 4376), ('set', 4377), ('severely', 4378), ('sexy', 4379), ('sg', 4380), ('shadows', 4381), ('shady', 4382), ('shaking', 4383), ('shambles', 4384), ('shambolic', 4385), ('shameless', 4386), ('shantosh', 4387), ('share', 4388), ('shares', 4389), ('shawarma', 4390), ('shawty', 4391), ('shed', 4392), ('sheldon', 4393), ('shes', 4394), ('shift', 4395), ('shining', 4396), ('ship', 4397), ('shiro', 4398), ('shitpost', 4399), ('shits', 4400), ('shoals', 4401), ('shocked', 4402), ('shoe', 4403), ('shoes', 4404), ('shop', 4405), ('shorty', 4406), ('shoulder', 4407), ('shouting', 4408), ('showed', 4409), ('showers', 4410), ('showroom', 4411), ('shows', 4412), ('shrines', 4413), ('sickness', 4414), ('sightedness', 4415), ('silence', 4416), ('simper', 4417), ('simple', 4418), ('singer', 4419), ('singin', 4420), ('sinking', 4421), ('sins', 4422), ('sir', 4423), ('siri', 4424), ('sissified', 4425), ('sisters', 4426), ('sit', 4427), ('sitcom', 4428), ('sits', 4429), ('sitting', 4430), ('situations', 4431), ('sixth', 4432), ('siyahleka', 4433), ('size', 4434), ('skeng', 4435), ('skies', 4436), ('skilling', 4437), ('skim', 4438), ('skinned', 4439), ('skittles', 4440), ('slap', 4441), ('slave', 4442), ('sliding', 4443), ('slightest', 4444), ('sling', 4445), ('slip', 4446), ('slithers', 4447), ('slow', 4448), ('slowing', 4449), ('slowly', 4450), ('slowness', 4451), ('slugging', 4452), ('smfh', 4453), ('smoke', 4454), ('smokeys', 4455), ('smooth', 4456), ('smutty', 4457), ('snacks', 4458), ('snaps', 4459), ('snores', 4460), ('soar', 4461), ('sobering', 4462), ('socal', 4463), ('soda', 4464), ('softer', 4465), ('sold', 4466), ('solemnity', 4467), ('soles', 4468), ('solution', 4469), ('somerset', 4470), ('sonicsatam', 4471), ('sooked', 4472), ('sore', 4473), ('sorrow', 4474), ('soulless', 4475), ('sounding', 4476), ('soup', 4477), ('sour', 4478), ('southampton', 4479), ('southern', 4480), ('sow', 4481), ('sox', 4482), ('space', 4483), ('spacious', 4484), ('spark', 4485), ('sparkling', 4486), ('speaker', 4487), ('speakinh', 4488), ('special', 4489), ('specialist', 4490), ('species', 4491), ('specific', 4492), ('spectacles', 4493), ('speech', 4494), ('speeder', 4495), ('spektar', 4496), ('spices', 4497), ('spikes', 4498), ('spill', 4499), ('spin', 4500), ('spine', 4501), ('spinning', 4502), ('spiritual', 4503), ('spit', 4504), ('spoke', 4505), ('sponsored', 4506), ('sponsorship', 4507), ('spooky', 4508), ('spoony', 4509), ('sports', 4510), ('spot', 4511), ('spots', 4512), ('spray', 4513), ('sq', 4514), ('squash', 4515), ('srv', 4516), ('ssri', 4517), ('sta', 4518), ('stalin', 4519), ('stall', 4520), ('standard', 4521), ('standing', 4522), ('standout', 4523), ('stange', 4524), ('starburst', 4525), ('starved', 4526), ('statement', 4527), ('staten', 4528), ('status', 4529), ('staying', 4530), ('steaks', 4531), ('stealing', 4532), ('stefano', 4533), ('stepping', 4534), ('stetson', 4535), ('steve', 4536), ('stg', 4537), ('sticks', 4538), ('stifle', 4539), ('stingrays', 4540), ('stinks', 4541), ('stitch', 4542), ('stohul', 4543), ('stoke', 4544), ('stolen', 4545), ('stone', 4546), ('stones', 4547), ('stores', 4548), ('stove', 4549), ('straightener', 4550), ('straightner', 4551), ('strategy', 4552), ('streak', 4553), ('street', 4554), ('strengthens', 4555), ('strengths', 4556), ('stressed', 4557), ('stressful', 4558), ('stricken', 4559), ('strive', 4560), ('struggle', 4561), ('struggling', 4562), ('stubborn', 4563), ('student', 4564), ('studs', 4565), ('studying', 4566), ('stunned', 4567), ('stunt', 4568), ('sub', 4569), ('subject', 4570), ('subminimum', 4571), ('substituted', 4572), ('suck', 4573), ('suffer', 4574), ('suffered', 4575), ('suffering', 4576), ('suitcase', 4577), ('sum', 4578), ('sunbed', 4579), ('sunglasses', 4580), ('sunlight', 4581), ('supernatural', 4582), ('supine', 4583), ('suplift', 4584), ('supporter', 4585), ('surfer', 4586), ('surprise', 4587), ('surprisingly', 4588), ('surrounded', 4589), ('survey', 4590), ('surviving', 4591), ('survivors', 4592), ('sustainability', 4593), ('swans', 4594), ('sweaters', 4595), ('swim', 4596), ('swinging', 4597), ('switch', 4598), ('sword', 4599), ('sybil', 4600), ('syfy', 4601), ('sylvia', 4602), ('sympathetic', 4603), ('synth', 4604), ('syria', 4605), ('syrian', 4606), ('syringes', 4607), ('tabitha', 4608), ('table', 4609), ('tabs', 4610), ('tad', 4611), ('tails', 4612), ('takeing', 4613), ('talents', 4614), ('talkin', 4615), ('talks', 4616), ('tall', 4617), ('tap', 4618), ('tape', 4619), ('tar', 4620), ('targeted', 4621), ('task', 4622), ('tattoo', 4623), ('taurus', 4624), ('tavern', 4625), ('taylor', 4626), ('tbh', 4627), ('tbptv', 4628), ('tc', 4629), ('teacherly', 4630), ('tear', 4631), ('technical', 4632), ('techno', 4633), ('teddy', 4634), ('tee', 4635), ('teen', 4636), ('teenager', 4637), ('telling', 4638), ('telltale', 4639), ('temperature', 4640), ('temporada', 4641), ('ten', 4642), ('tend', 4643), ('tenor', 4644), ('tension', 4645), ('terminal', 4646), ('terrible', 4647), ('terribly', 4648), ('terrified', 4649), ('terrifying', 4650), ('terror', 4651), ('terrorism', 4652), ('texas', 4653), ('tgif', 4654), ('tha', 4655), ('thankful', 4656), ('theft', 4657), ('therapist', 4658), ('therapists', 4659), ('therefore', 4660), ('theres', 4661), ('theyve', 4662), ('thier', 4663), ('thoroughly', 4664), ('thoughtful', 4665), ('threat', 4666), ('threaten', 4667), ('throw', 4668), ('throwing', 4669), ('thrown', 4670), ('ths', 4671), ('thunk', 4672), ('tickle', 4673), ('tidbit', 4674), ('tide', 4675), ('tie', 4676), ('tilli', 4677), ('tilting', 4678), ('tim', 4679), ('tinge', 4680), ('tips', 4681), ('tis', 4682), ('titanic', 4683), ('titles', 4684), ('tits', 4685), ('titter', 4686), ('tl', 4687), ('tld', 4688), ('to5', 4689), ('toad', 4690), ('toews', 4691), ('toilet', 4692), ('toilets', 4693), ('tokyo', 4694), ('tolerance', 4695), ('tomatoes', 4696), ('tone', 4697), ('tongue', 4698), ('tonne', 4699), ('toonie', 4700), ('topic', 4701), ('topped', 4702), ('tore', 4703), ('torn', 4704), ('total', 4705), ('totally', 4706), ('toughest', 4707), ('tourist', 4708), ('tournament', 4709), ('towards', 4710), ('towel', 4711), ('toys', 4712), ('tozer', 4713), ('tracks', 4714), ('tracy', 4715), ('traffic', 4716), ('tragedy', 4717), ('trails', 4718), ('trainee', 4719), ('trainer', 4720), ('training', 4721), ('tranquilizer', 4722), ('transaction', 4723), ('transcendent', 4724), ('transferring', 4725), ('transition', 4726), ('transparency', 4727), ('travelers', 4728), ('traveling', 4729), ('travis', 4730), ('treadlines', 4731), ('treatment', 4732), ('trend', 4733), ('trending', 4734), ('trial', 4735), ('tribe', 4736), ('tries', 4737), ('triggered', 4738), ('trilogy', 4739), ('trophy', 4740), ('truck', 4741), ('trumps', 4742), ('trumpweek', 4743), ('tryed', 4744), ('tshirt', 4745), ('tt', 4746), ('tub', 4747), ('tummy', 4748), ('tunbridge', 4749), ('tune', 4750), ('tunechi', 4751), ('tuned', 4752), ('turnout', 4753), ('turns', 4754), ('tvgirl', 4755), ('twats', 4756), ('twelve', 4757), ('twerking', 4758), ('twins', 4759), ('twisted', 4760), ('tyson', 4761), ('u.s.', 4762), ('u23', 4763), ('uber', 4764), ('ufc', 4765), ('ukrunchat', 4766), ('ul', 4767), ('ultimate', 4768), ('ultimately', 4769), ('unavoidable', 4770), ('unborn', 4771), ('unboxing', 4772), ('uncertainty', 4773), ('unchained', 4774), ('uncharted', 4775), ('uncontrollable', 4776), ('underground', 4777), ('underlie', 4778), ('underlying', 4779), ('undesirables', 4780), ('unfairly', 4781), ('unfollowing', 4782), ('unfortunate', 4783), ('unfulfilled', 4784), ('unide', 4785), ('uninteresting', 4786), ('unite', 4787), ('universal', 4788), ('unknown', 4789), ('unless', 4790), ('unmatched', 4791), ('unnerved', 4792), ('unopened', 4793), ('unproductive', 4794), ('unsettling', 4795), ('unto', 4796), ('untouchable', 4797), ('untypical', 4798), ('unverified', 4799), ('upcoming', 4800), ('update', 4801), ('updated', 4802), ('updating', 4803), ('upon', 4804), ('urgent', 4805), ('urgh', 4806), ('useless', 4807), ('uses', 4808), ('ux', 4809), ('vacuum', 4810), ('vaguely', 4811), ('value', 4812), ('vampires', 4813), ('van', 4814), ('varsity', 4815), ('vas', 4816), ('ve', 4817), ('vegas', 4818), ('vehicles', 4819), ('vein', 4820), ('venice', 4821), ('verb', 4822), ('verbs', 4823), ('verg', 4824), ('verge', 4825), ('verse', 4826), ('verses', 4827), ('version', 4828), ('vestiges', 4829), ('via', 4830), ('vickie', 4831), ('victims', 4832), ('vids', 4833), ('view', 4834), ('viewership', 4835), ('views', 4836), ('vigil', 4837), ('vigorous', 4838), ('vile', 4839), ('vileplume', 4840), ('village', 4841), ('villaseñor', 4842), ('vine', 4843), ('vintage', 4844), ('violence', 4845), ('violent', 4846), ('virgin', 4847), ('vision', 4848), ('visit', 4849), ('visiting', 4850), ('visitors', 4851), ('visits', 4852), ('vitamin', 4853), ('void', 4854), ('voidz', 4855), ('vols', 4856), ('vomit', 4857), ('voodoo', 4858), ('vot', 4859), ('voted', 4860), ('voters', 4861), ('voting', 4862), ('voucher', 4863), ('wage', 4864), ('wah', 4865), ('waitress', 4866), ('waits', 4867), ('wakeup', 4868), ('wal', 4869), ('walked', 4870), ('wallahi', 4871), ('wan', 4872), ('wana', 4873), ('ward', 4874), ('warn', 4875), ('warned', 4876), ('warning', 4877), ('washington', 4878), ('wasnt', 4879), ('waste', 4880), ('wasteland', 4881), ('watchin', 4882), ('waterloo', 4883), ('wave', 4884), ('weakness', 4885), ('weaknesses', 4886), ('wealthy', 4887), ('webstagram', 4888), ('wedding', 4889), ('weds', 4890), ('weekday', 4891), ('weep', 4892), ('weiner', 4893), ('wells', 4894), ('wembley', 4895), ('wenger', 4896), ('werder', 4897), ('wesper', 4898), ('wet', 4899), ('whats', 4900), ('wheel', 4901), ('wherever', 4902), ('whether', 4903), ('whines', 4904), ('whining', 4905), ('whoever', 4906), ('wi', 4907), ('wibbly', 4908), ('wight', 4909), ('wilderness', 4910), ('williams', 4911), ('willing', 4912), ('willows', 4913), ('winds', 4914), ('wing', 4915), ('wings', 4916), ('winner', 4917), ('winners', 4918), ('winning', 4919), ('winters', 4920), ('wipe', 4921), ('wire', 4922), ('wirral', 4923), ('wished', 4924), ('wishes', 4925), ('witches', 4926), ('witnesse', 4927), ('wls', 4928), ('wobble', 4929), ('wobbly', 4930), ('woken', 4931), ('wolf', 4932), ('wolves', 4933), ('wonderful', 4934), ('wonks', 4935), ('wonshik', 4936), ('wordpress', 4937), ('workplace', 4938), ('works', 4939), ('worn', 4940), ('worth', 4941), ('worthy', 4942), ('wound', 4943), ('wounds', 4944), ('wrapped', 4945), ('wraps', 4946), ('wreckin', 4947), ('wrecord', 4948), ('wrestling', 4949), ('wrinkles', 4950), ('writer', 4951), ('writers', 4952), ('wu', 4953), ('wut', 4954), ('wyrdljpyu', 4955), ('xbeujgb', 4956), ('xdd', 4957), ('xdddd', 4958), ('xl', 4959), ('xt', 4960), ('xx', 4961), ('yang', 4962), ('yard', 4963), ('yards', 4964), ('yawn', 4965), ('ye', 4966), ('yea', 4967), ('yearly', 4968), ('yearning', 4969), ('yell', 4970), ('yemen', 4971), ('yep', 4972), ('yeye', 4973), ('yo', 4974), ('york', 4975), ('youthful', 4976), ('youtube', 4977), ('youtubers', 4978), ('yters', 4979), ('yuck', 4980), ('yung', 4981), ('zen', 4982), ('´', 4983), ('،', 4984), ('ઘ', 4985), ('♪', 4986), ('アニメ', 4987), ('화양연화', 4988)])\n",
            "dict_items([('<unk>', 0), ('<pad>', 1), ('</hashtag>', 2), ('<hashtag>', 3), ('.', 4), ('<user>', 5), ('i', 6), ('the', 7), ('to', 8), ('a', 9), (',', 10), ('and', 11), (\"'\", 12), ('you', 13), ('not', 14), ('is', 15), ('of', 16), ('it', 17), ('!', 18), ('that', 19), ('my', 20), ('in', 21), ('s', 22), ('me', 23), ('<repeated>', 24), ('<number>', 25), ('on', 26), ('for', 27), ('?', 28), ('do', 29), ('</allcaps>', 30), ('<allcaps>', 31), ('are', 32), ('have', 33), ('-', 34), ('am', 35), ('just', 36), ('but', 37), ('with', 38), ('be', 39), ('like', 40), ('so', 41), ('at', 42), ('all', 43), ('this', 44), ('\\\\', 45), ('your', 46), ('no', 47), ('can', 48), ('he', 49), ('will', 50), ('up', 51), ('out', 52), ('if', 53), ('people', 54), ('when', 55), ('get', 56), ('was', 57), ('what', 58), ('anger', 59), ('angry', 60), ('they', 61), ('about', 62), ('as', 63), ('n', 64), ('we', 65), ('rage', 66), ('by', 67), ('one', 68), ('an', 69), ('bitter', 70), ('from', 71), ('who', 72), ('&', 73), ('his', 74), ('fuming', 75), ('why', 76), ('revenge', 77), ('how', 78), ('now', 79), ('she', 80), ('or', 81), ('snap', 82), ('offended', 83), (':face_with_tears_of_joy:', 84), ('her', 85), ('off', 86), ('u', 87), ('got', 88), ('there', 89), ('over', 90), ('outrage', 91), (':', 92), ('did', 93), ('has', 94), ('them', 95), ('<elongated>', 96), ('should', 97), ('think', 98), ('know', 99), ('because', 100), ('time', 101), ('would', 102), ('been', 103), ('/', 104), ('does', 105), ('lol', 106), ('never', 107), ('even', 108), ('more', 109), ('burning', 110), ('furious', 111), ('some', 112), ('someone', 113), ('sting', 114), ('their', 115), ('then', 116), ('today', 117), ('back', 118), ('raging', 119), ('really', 120), ('(', 121), ('love', 122), ('madden', 123), ('need', 124), ('want', 125), (')', 126), ('going', 127), ('man', 128), ('offend', 129), ('go', 130), ('him', 131), ('fury', 132), ('see', 133), ('best', 134), ('insult', 135), ('kik', 136), ('offense', 137), (':enraged_face:', 138), ('always', 139), ('ever', 140), ('life', 141), ('make', 142), ('our', 143), ('pout', 144), ('still', 145), ('being', 146), ('black', 147), ('boiling', 148), ('fucking', 149), ('hate', 150), ('into', 151), ('good', 152), ('had', 153), ('only', 154), ('right', 155), ('game', 156), ('oh', 157), ('relentless', 158), ('after', 159), ('burst', 160), ('day', 161), ('fuck', 162), ('irritate', 163), ('than', 164), ('us', 165), ('grudge', 166), ('work', 167), ('wrath', 168), (':face_with_rolling_eyes:', 169), ('let', 170), ('were', 171), (':loudly_crying_face:', 172), ('any', 173), ('frown', 174), ('im', 175), ('stop', 176), ('where', 177), ('feel', 178), ('something', 179), ('these', 180), ('too', 181), ('way', 182), ('hell', 183), ('its', 184), ('face', 185), ('fiery', 186), ('snapchat', 187), ('take', 188), (':face_with_steam_from_nose:', 189), ('getting', 190), ('give', 191), ('happy', 192), ('nothing', 193), ('old', 194), ('sorry', 195), ('t', 196), ('those', 197), ('away', 198), ('before', 199), ('big', 200), ('d', 201), ('look', 202), ('much', 203), ('new', 204), ('said', 205), ('say', 206), ('thing', 207), ('watch', 208), ('white', 209), (':frowning_face:', 210), ('against', 211), ('another', 212), ('come', 213), ('gonna', 214), ('here', 215), ('home', 216), ('keep', 217), ('little', 218), ('rabid', 219), ('resent', 220), ('shit', 221), ('again', 222), ('better', 223), ('every', 224), ('k', 225), ('mad', 226), ('made', 227), ('world', 228), ('#', 229), ('0', 230), ('damn', 231), ('doing', 232), ('down', 233), ('god', 234), ('tell', 235), ('2', 236), ('<happy>', 237), ('also', 238), ('bad', 239), ('believe', 240), ('everyone', 241), ('first', 242), ('live', 243), ('makes', 244), ('takes', 245), ('talk', 246), ('team', 247), ('wanna', 248), ('which', 249), ('worst', 250), ('*', 251), (';', 252), ('bb', 253), ('bc', 254), ('could', 255), ('done', 256), ('house', 257), ('huff', 258), ('literally', 259), ('might', 260), ('most', 261), ('next', 262), ('nicole', 263), ('ok', 264), ('own', 265), ('same', 266), ('through', 267), ('year', 268), ('+', 269), ('5', 270), ('<money>', 271), ('<time>', 272), ('bb18', 273), ('girl', 274), ('gone', 275), ('mind', 276), ('phone', 277), ('police', 278), ('put', 279), ('ugh', 280), ('very', 281), ('well', 282), ('blood', 283), ('days', 284), ('don', 285), ('eat', 286), ('fire', 287), ('guy', 288), ('guys', 289), ('hope', 290), ('making', 291), ('other', 292), ('paul', 293), ('players', 294), ('pretty', 295), ('provoke', 296), ('resentment', 297), ('tiff', 298), ('use', 299), ('w', 300), ('yeah', 301), ('actually', 302), ('already', 303), ('anything', 304), ('end', 305), ('found', 306), ('gets', 307), ('growl', 308), ('hard', 309), ('having', 310), ('hold', 311), ('incense', 312), ('joke', 313), ('long', 314), ('looking', 315), ('men', 316), ('ppl', 317), ('saying', 318), ('says', 319), ('self', 320), ('seriously', 321), ('since', 322), ('sometimes', 323), ('wish', 324), ('word', 325), ('<censored>', 326), ('anyone', 327), ('both', 328), ('brother', 329), ('customer', 330), ('die', 331), ('follow', 332), ('kind', 333), ('ll', 334), ('lot', 335), ('must', 336), ('pay', 337), ('person', 338), ('playing', 339), ('power', 340), ('quote', 341), ('sad', 342), ('season', 343), ('service', 344), ('shot', 345), ('show', 346), ('soon', 347), ('straight', 348), ('things', 349), ('true', 350), ('trump', 351), (':light_skin_tone:', 352), (':smiling_face_with_heart-eyes:', 353), ('asked', 354), ('baby', 355), ('change', 356), ('dm', 357), ('either', 358), ('forever', 359), ('fret', 360), ('gbbo', 361), ('gotta', 362), ('half', 363), ('hour', 364), ('kill', 365), ('killed', 366), ('last', 367), ('lmao', 368), ('looks', 369), ('media', 370), ('myself', 371), ('order', 372), ('play', 373), ('red', 374), ('taking', 375), ('tantrums', 376), ('tears', 377), ('thanks', 378), ('tired', 379), ('ur', 380), ('using', 381), ('wait', 382), ('whole', 383), ('without', 384), ('words', 385), ('wow', 386), ('1', 387), (':angry_face:', 388), (':fire:', 389), (':upside-down_face:', 390), (':weary_face:', 391), ('aggravate', 392), ('bored', 393), ('called', 394), ('candice', 395), ('charlotte', 396), ('cops', 397), ('cry', 398), ('dog', 399), ('everything', 400), ('find', 401), ('happened', 402), ('help', 403), ('holding', 404), ('irritation', 405), ('late', 406), ('laughing', 407), ('leave', 408), ('left', 409), ('lose', 410), ('many', 411), ('mean', 412), ('michelle', 413), ('sleep', 414), ('smh', 415), ('such', 416), ('tonight', 417), ('try', 418), ('turn', 419), ('twitter', 420), ('wasn', 421), ('woman', 422), ('', 423), (':crying_face:', 424), ('>', 425), ('absolutely', 426), ('animosity', 427), ('blacks', 428), ('call', 429), ('cause', 430), ('crying', 431), ('cut', 432), ('date', 433), ('died', 434), ('dude', 435), ('fast', 436), ('fume', 437), ('gave', 438), ('girls', 439), ('great', 440), ('haha', 441), ('human', 442), ('hurt', 443), ('job', 444), ('looting', 445), ('m', 446), ('minutes', 447), ('miss', 448), ('night', 449), ('party', 450), ('pick', 451), ('played', 452), ('road', 453), ('soul', 454), ('strong', 455), ('sure', 456), ('teams', 457), ('times', 458), ('took', 459), ('train', 460), ('trying', 461), ('two', 462), ('vs', 463), ('watching', 464), ('within', 465), ('worse', 466), ('wtf', 467), ('x', 468), ('years', 469), ('’', 470), ('3', 471), ('america', 472), ('annoyed', 473), ('ass', 474), ('between', 475), ('blm', 476), ('car', 477), ('city', 478), ('class', 479), ('cunt', 480), ('dear', 481), ('em', 482), ('fans', 483), ('finally', 484), ('fix', 485), ('forgot', 486), ('guess', 487), ('hands', 488), ('hours', 489), ('infuriate', 490), ('internet', 491), ('irate', 492), ('king', 493), ('least', 494), ('lost', 495), ('may', 496), ('month', 497), ('ni', 498), ('o', 499), ('pics', 500), ('please', 501), ('point', 502), ('posted', 503), ('pure', 504), ('re', 505), ('run', 506), ('smile', 507), ('speak', 508), ('stay', 509), ('talking', 510), ('thinking', 511), ('though', 512), ('thought', 513), ('towards', 514), ('tribute', 515), ('turned', 516), ('weak', 517), ('working', 518), ('writing', 519), (':expressionless_face:', 520), (':medium-dark_skin_tone:', 521), (':medium-light_skin_tone:', 522), ('account', 523), ('affront', 524), ('attention', 525), ('biggest', 526), ('bit', 527), ('bitch', 528), ('changing', 529), ('chat', 530), ('color', 531), ('comes', 532), ('dick', 533), ('ex', 534), ('fact', 535), ('fighting', 536), ('fuckin', 537), ('goes', 538), ('gold', 539), ('happen', 540), ('heart', 541), ('hillary', 542), ('hit', 543), ('innocent', 544), ('inside', 545), ('issues', 546), ('kid', 547), ('money', 548), ('moving', 549), ('needs', 550), ('once', 551), ('others', 552), ('peace', 553), ('perfect', 554), ('personal', 555), ('poor', 556), ('post', 557), ('protest', 558), ('racist', 559), ('real', 560), ('received', 561), ('second', 562), ('seeing', 563), ('shut', 564), ('song', 565), ('story', 566), ('stupid', 567), ('success', 568), ('tip', 569), ('tomorrow', 570), ('tv', 571), ('tweet', 572), ('under', 573), ('usa', 574), ('water', 575), ('while', 576), ('win', 577), ('women', 578), ('wonder', 579), ('yes', 580), ('18', 581), ('7', 582), (':double_exclamation_mark:', 583), (':grinning_squinting_face:', 584), (':sign_of_the_horns:', 585), (':sweat_droplets:', 586), (':unamused_face:', 587), ('<', 588), ('<percent>', 589), ('anyway', 590), ('apparently', 591), ('attempt', 592), ('beat', 593), ('bloody', 594), ('born', 595), ('bubble', 596), ('burns', 597), ('cannot', 598), ('cats', 599), ('club', 600), ('coach', 601), ('conflict', 602), ('conversation', 603), ('cool', 604), ('delivered', 605), ('destroy', 606), ('displeasure', 607), ('drunk', 608), ('early', 609), ('else', 610), ('enough', 611), ('exactly', 612), ('expect', 613), ('eyes', 614), ('fall', 615), ('fault', 616), ('few', 617), ('forget', 618), ('front', 619), ('fun', 620), ('future', 621), ('gym', 622), ('hahaha', 623), ('hair', 624), ('hangry', 625), ('hath', 626), ('hatred', 627), ('heard', 628), ('hey', 629), ('high', 630), ('hot', 631), ('hungry', 632), ('indignant', 633), ('l', 634), ('level', 635), ('lips', 636), ('listen', 637), ('lives', 638), ('living', 639), ('longer', 640), ('low', 641), ('luck', 642), ('machines', 643), ('match', 644), ('mate', 645), ('maybe', 646), ('move', 647), ('nudes', 648), ('online', 649), ('outside', 650), ('paid', 651), ('part', 652), ('pic', 653), ('pissed', 654), ('pre', 655), ('pro', 656), ('provocation', 657), ('quit', 658), ('r', 659), ('racism', 660), ('reading', 661), ('reason', 662), ('room', 663), ('ruffle', 664), ('scrum', 665), ('seen', 666), ('sense', 667), ('serious', 668), ('sick', 669), ('side', 670), ('sister', 671), ('sitting', 672), ('slow', 673), ('spent', 674), ('stars', 675), ('started', 676), ('support', 677), ('tho', 678), ('told', 679), ('trade', 680), ('trash', 681), ('tried', 682), ('truth', 683), ('tweets', 684), ('until', 685), ('waiting', 686), ('won', 687), ('wrong', 688), ('yesterday', 689), ('yourself', 690), ('4', 691), (':oncoming_fist:', 692), (':raising_hands:', 693), (':see-no-evil_monkey:', 694), (':thinking_face:', 695), ('<annoyed>', 696), ('across', 697), ('ages', 698), ('air', 699), ('angrily', 700), ('annoying', 701), ('app', 702), ('bed', 703), ('bee', 704), ('behind', 705), ('bet', 706), ('blame', 707), ('body', 708), ('business', 709), ('came', 710), ('challenge', 711), ('chance', 712), ('character', 713), ('chicago', 714), ('cock', 715), ('community', 716), ('cum', 717), ('decide', 718), ('despite', 719), ('different', 720), ('dont', 721), ('door', 722), ('each', 723), ('earth', 724), ('enjoy', 725), ('evolution', 726), ('faced', 727), ('fair', 728), ('far', 729), ('fear', 730), ('feels', 731), ('female', 732), ('fine', 733), ('folks', 734), ('form', 735), ('free', 736), ('fucked', 737), ('g', 738), ('gives', 739), ('hello', 740), ('hi', 741), ('holds', 742), ('huh', 743), ('idea', 744), ('imagine', 745), ('india', 746), ('indignation', 747), ('inflame', 748), ('injustice', 749), ('inner', 750), ('ive', 751), ('jesus', 752), ('jokes', 753), ('justice', 754), ('knows', 755), ('later', 756), ('laugh', 757), ('less', 758), ('lil', 759), ('line', 760), ('listening', 761), ('matter', 762), ('means', 763), ('message', 764), ('midnight', 765), ('nah', 766), ('negative', 767), ('nif', 768), ('nobody', 769), ('non', 770), ('opinion', 771), ('pain', 772), ('piss', 773), ('political', 774), ('politics', 775), ('probably', 776), ('question', 777), ('race', 778), ('rather', 779), ('reaction', 780), ('reality', 781), ('refund', 782), ('reply', 783), ('respect', 784), ('response', 785), ('rest', 786), ('rip', 787), ('ruins', 788), ('rush', 789), ('scene', 790), ('school', 791), ('scorned', 792), ('seems', 793), ('shitty', 794), ('shoes', 795), ('shooting', 796), ('sis', 797), ('smoke', 798), ('songs', 799), ('sort', 800), ('sound', 801), ('strike', 802), ('stuck', 803), ('suck', 804), ('sun', 805), ('suppose', 806), ('supposed', 807), ('swap', 808), ('swear', 809), ('th', 810), ('ticket', 811), ('tracking', 812), ('uber', 813), ('understand', 814), ('update', 815), ('upset', 816), ('used', 817), ('useless', 818), ('wanted', 819), ('wants', 820), ('wasp', 821), ('wasted', 822), ('week', 823), ('weeks', 824), ('weird', 825), ('went', 826), ('wisdom', 827), ('wks', 828), ('worry', 829), ('y', 830), ('yo', 831), ('young', 832), ('youtube', 833), ('~', 834), ('—', 835), ('”', 836), (':clapping_hands:', 837), (':face_with_medical_mask:', 838), (':grimacing_face:', 839), (':heart_with_arrow:', 840), (':smiling_face_with_open_hands:', 841), (':star:', 842), ('<laugh>', 843), ('=', 844), ('[', 845), (']', 846), ('act', 847), ('action', 848), ('af', 849), ('afl', 850), ('afternoon', 851), ('ago', 852), ('amazing', 853), ('animal', 854), ('answer', 855), ('anymore', 856), ('argh', 857), ('asian', 858), ('based', 859), ('become', 860), ('bees', 861), ('beyond', 862), ('bible', 863), ('bill', 864), ('blind', 865), ('block', 866), ('board', 867), ('boy', 868), ('boys', 869), ('break', 870), ('breaks', 871), ('bridges', 872), ('bridgette', 873), ('bring', 874), ('broncos', 875), ('brothers', 876), ('brown', 877), ('buzz', 878), ('buzzing', 879), ('cam', 880), ('cant', 881), ('career', 882), ('carpet', 883), ('celtic', 884), ('cheeks', 885), ('comedy', 886), ('coming', 887), ('comments', 888), ('concert', 889), ('congrats', 890), ('constant', 891), ('control', 892), ('country', 893), ('crazy', 894), ('cruel', 895), ('culture', 896), ('cup', 897), ('dad', 898), ('dark', 899), ('deal', 900), ('deep', 901), ('defense', 902), ('definitely', 903), ('desire', 904), ('dollars', 905), ('draw', 906), ('dream', 907), ('drink', 908), ('drop', 909), ('dumb', 910), ('e', 911), ('elway', 912), ('emo', 913), ('emotional', 914), ('emotions', 915), ('energy', 916), ('ep', 917), ('etc', 918), ('event', 919), ('everybody', 920), ('excited', 921), ('facebook', 922), ('fake', 923), ('family', 924), ('favorite', 925), ('favourite', 926), ('feelings', 927), ('felt', 928), ('fight', 929), ('football', 930), ('forward', 931), ('friend', 932), ('frustration', 933), ('funny', 934), ('general', 935), ('glad', 936), ('grateful', 937), ('growing', 938), ('gun', 939), ('haiku', 940), ('happens', 941), ('happiness', 942), ('held', 943), ('himself', 944), ('hockey', 945), ('hotel', 946), ('hurts', 947), ('icq', 948), ('id', 949), ('ignorance', 950), ('indian', 951), ('instagram', 952), ('interesting', 953), ('iphones', 954), ('jealous', 955), ('john', 956), ('johnson', 957), ('journey', 958), ('joy', 959), ('joyed', 960), ('jumped', 961), ('juz', 962), ('keeping', 963), ('keith', 964), ('kids', 965), ('kinda', 966), ('league', 967), ('learn', 968), ('lit', 969), ('lord', 970), ('losing', 971), ('machine', 972), ('mention', 973), ('mins', 974), ('missed', 975), ('morning', 976), ('movies', 977), ('murder', 978), ('nd', 979), ('news', 980), ('nice', 981), ('nose', 982), ('note', 983), ('number', 984), ('ones', 985), ('outta', 986), ('pait', 987), ('pakistan', 988), ('parking', 989), ('pen', 990), ('personally', 991), ('pieces', 992), ('pigs', 993), ('plenty', 994), ('politeness', 995), ('pop', 996), ('practice', 997), ('projection', 998), ('promising', 999), ('protests', 1000), ('putting', 1001), ('quotes', 1002), ('radio', 1003), ('ready', 1004), ('remember', 1005), ('remove', 1006), ('righteous', 1007), ('sadness', 1008), ('scared', 1009), ('scott', 1010), ('set', 1011), ('sets', 1012), ('sex', 1013), ('shoot', 1014), ('showing', 1015), ('sign', 1016), ('simple', 1017), ('singing', 1018), ('sizes', 1019), ('skin', 1020), ('sofas', 1021), ('spider', 1022), ('standing', 1023), ('star', 1024), ('start', 1025), ('starting', 1026), ('store', 1027), ('stuff', 1028), ('suffer', 1029), ('surprised', 1030), ('swallow', 1031), ('switch', 1032), ('system', 1033), ('telling', 1034), ('temper', 1035), ('terrible', 1036), ('text', 1037), ('thank', 1038), ('throw', 1039), ('toward', 1040), ('traffic', 1041), ('treat', 1042), ('tube', 1043), ('umbrage', 1044), ('values', 1045), ('vic', 1046), ('watched', 1047), ('website', 1048), ('winning', 1049), ('wished', 1050), ('workout', 1051), ('write', 1052), ('writer', 1053), ('writers', 1054), ('wwe', 1055), ('yet', 1056), ('yr', 1057), ('zero', 1058), ('°', 1059), ('“', 1060), ('•', 1061), ('…', 1062), ('┻', 1063), ('╯', 1064), ('*\\\\', 1065), ('6', 1066), (':face_blowing_a_kiss:', 1067), (':flushed_face:', 1068), (':grinning_face_with_sweat:', 1069), (':neutral_face:', 1070), (':person_tipping_hand:', 1071), (':smiling_face_with_smiling_eyes:', 1072), (':winking_face:', 1073), ('@', 1074), ('acc', 1075), ('actors', 1076), ('add', 1077), ('address', 1078), ('alarm', 1079), ('almost', 1080), ('alone', 1081), ('amazon', 1082), ('american', 1083), ('angelina', 1084), ('answers', 1085), ('anti', 1086), ('appearance', 1087), ('arms', 1088), ('around', 1089), ('arrest', 1090), ('ask', 1091), ('awesome', 1092), ('battle', 1093), ('beating', 1094), ('begin', 1095), ('beings', 1096), ('beneath', 1097), ('birthday', 1098), ('blog', 1099), ('bought', 1100), ('brad', 1101), ('brian', 1102), ('brings', 1103), ('british', 1104), ('broke', 1105), ('busy', 1106), ('buy', 1107), ('bye', 1108), ('calls', 1109), ('cancelled', 1110), ('cancer', 1111), ('canon', 1112), ('case', 1113), ('cat', 1114), ('changed', 1115), ('changes', 1116), ('chase', 1117), ('cheap', 1118), ('checked', 1119), ('child', 1120), ('children', 1121), ('chill', 1122), ('choice', 1123), ('chris', 1124), ('chuckle', 1125), ('church', 1126), ('clock', 1127), ('close', 1128), ('clothes', 1129), ('compared', 1130), ('con', 1131), ('congratulations', 1132), ('conscience', 1133), ('constantly', 1134), ('continue', 1135), ('cop', 1136), ('cover', 1137), ('cow', 1138), ('crap', 1139), ('crash', 1140), ('crashing', 1141), ('cross', 1142), ('customers', 1143), ('cute', 1144), ('cuz', 1145), ('damage', 1146), ('delete', 1147), ('destroyed', 1148), ('difference', 1149), ('dinner', 1150), ('disappointment', 1151), ('disgusted', 1152), ('dislike', 1153), ('disrespect', 1154), ('drag', 1155), ('driver', 1156), ('due', 1157), ('dying', 1158), ('easily', 1159), ('easy', 1160), ('eating', 1161), ('education', 1162), ('eh', 1163), ('email', 1164), ('emoji', 1165), ('enemy', 1166), ('episode', 1167), ('especially', 1168), ('everywhere', 1169), ('expected', 1170), ('expecting', 1171), ('expensive', 1172), ('explain', 1173), ('express', 1174), ('eye', 1175), ('fail', 1176), ('fairly', 1177), ('faster', 1178), ('faux', 1179), ('feathers', 1180), ('fed', 1181), ('feeling', 1182), ('fees', 1183), ('fifa', 1184), ('finding', 1185), ('finger', 1186), ('fingers', 1187), ('fit', 1188), ('flat', 1189), ('flew', 1190), ('flight', 1191), ('flu', 1192), ('food', 1193), ('foot', 1194), ('force', 1195), ('forgive', 1196), ('forgiveness', 1197), ('freezing', 1198), ('french', 1199), ('friends', 1200), ('frustrated', 1201), ('fuel', 1202), ('full', 1203), ('glasgow', 1204), ('goal', 1205), ('goals', 1206), ('govt', 1207), ('group', 1208), ('ha', 1209), ('hand', 1210), ('harder', 1211), ('hat', 1212), ('hating', 1213), ('head', 1214), ('heading', 1215), ('heat', 1216), ('heels', 1217), ('history', 1218), ('holiday', 1219), ('homie', 1220), ('honestly', 1221), ('honey', 1222), ('humans', 1223), ('ignore', 1224), ('illusion', 1225), ('instantly', 1226), ('instead', 1227), ('ios10', 1228), ('james', 1229), ('japanese', 1230), ('jared', 1231), ('jersey', 1232), ('jerseys', 1233), ('join', 1234), ('keeps', 1235), ('kendrick', 1236), ('kept', 1237), ('kicking', 1238), ('knew', 1239), ('la', 1240), ('lamont', 1241), ('leaves', 1242), ('letting', 1243), ('likes', 1244), ('lineman', 1245), ('lo', 1246), ('loose', 1247), ('loser', 1248), ('loses', 1249), ('lying', 1250), ('meant', 1251), ('meanwhile', 1252), ('meeting', 1253), ('middle', 1254), ('mistake', 1255), ('moment', 1256), ('mostly', 1257), ('moved', 1258), ('movie', 1259), ('mufc', 1260), ('multiple', 1261), ('nand', 1262), ('natalie', 1263), ('needed', 1264), ('nor', 1265), ('north', 1266), ('np', 1267), ('nwhen', 1268), ('obama', 1269), ('obviously', 1270), ('offensive', 1271), ('officer', 1272), ('often', 1273), ('okay', 1274), ('onto', 1275), ('open', 1276), ('opened', 1277), ('options', 1278), ('ordered', 1279), ('ours', 1280), ('outraged', 1281), ('p', 1282), ('parts', 1283), ('past', 1284), ('paying', 1285), ('perception', 1286), ('petty', 1287), ('piece', 1288), ('place', 1289), ('placed', 1290), ('plan', 1291), ('pleasure', 1292), ('points', 1293), ('poisonous', 1294), ('pokemon', 1295), ('posting', 1296), ('prank', 1297), ('pride', 1298), ('problem', 1299), ('problems', 1300), ('process', 1301), ('protesting', 1302), ('pulled', 1303), ('quad', 1304), ('quick', 1305), ('quiet', 1306), ('quite', 1307), ('raid', 1308), ('rain', 1309), ('rangers', 1310), ('rate', 1311), ('reaches', 1312), ('react', 1313), ('read', 1314), ('record', 1315), ('reform', 1316), ('rides', 1317), ('rooney', 1318), ('round', 1319), ('rt', 1320), ('rude', 1321), ('ruined', 1322), ('rules', 1323), ('safe', 1324), ('sam', 1325), ('satisfaction', 1326), ('saturday', 1327), ('score', 1328), ('scowl', 1329), ('seek', 1330), ('sentence', 1331), ('shall', 1332), ('shocking', 1333), ('signing', 1334), ('silver', 1335), ('sing', 1336), ('sit', 1337), ('situation', 1338), ('skittles', 1339), ('slowest', 1340), ('smells', 1341), ('snapped', 1342), ('social', 1343), ('socks', 1344), ('soft', 1345), ('somewhere', 1346), ('son', 1347), ('sounds', 1348), ('space', 1349), ('spend', 1350), ('spirit', 1351), ('st', 1352), ('storm', 1353), ('street', 1354), ('struggle', 1355), ('struggling', 1356), ('suggested', 1357), ('summer', 1358), ('swans', 1359), ('sweet', 1360), ('taken', 1361), ('terence', 1362), ('testing', 1363), ('tf', 1364), ('theme', 1365), ('themselves', 1366), ('thoughts', 1367), ('threat', 1368), ('thursday', 1369), ('till', 1370), ('tips', 1371), ('tits', 1372), ('together', 1373), ('tolerance', 1374), ('tom', 1375), ('top', 1376), ('town', 1377), ('travel', 1378), ('truly', 1379), ('tulsa', 1380), ('twenty', 1381), ('type', 1382), ('typical', 1383), ('uh', 1384), ('um', 1385), ('unarmed', 1386), ('unhappy', 1387), ('uploaded', 1388), ('upside', 1389), ('utd', 1390), ('v', 1391), ('version', 1392), ('violence', 1393), ('wanting', 1394), ('war', 1395), ('washed', 1396), ('weakest', 1397), ('weekend', 1398), ('welfare', 1399), ('whichever', 1400), ('whilst', 1401), ('whoever', 1402), ('wind', 1403), ('window', 1404), ('woke', 1405), ('worth', 1406), ('wouldnt', 1407), ('ya', 1408), ('yay', 1409), ('yikes', 1410), ('%', 1411), ('2016', 1412), (':anguished_face:', 1413), (':beaming_face_with_smiling_eyes:', 1414), (':bento_box:', 1415), (':blue_heart:', 1416), (':cheese_wedge:', 1417), (':disappointed_face:', 1418), (':ear:', 1419), (':eyes:', 1420), (':folded_hands:', 1421), (':ghost:', 1422), (':grinning_face_with_big_eyes:', 1423), (':leopard:', 1424), (':money_with_wings:', 1425), (':pizza:', 1426), (':red_heart:', 1427), (':rice_ball:', 1428), (':smiling_face:', 1429), (':spaghetti:', 1430), (':sparkles:', 1431), (':sparkling_heart:', 1432), (':victory_hand:', 1433), (':waving_hand:', 1434), (':winking_face_with_tongue:', 1435), (';-o', 1436), ('<emphasis>', 1437), ('<sad>', 1438), ('>:(', 1439), ('aaron', 1440), ('abc', 1441), ('abortion', 1442), ('absolute', 1443), ('abt', 1444), ('abusive', 1445), ('accent', 1446), ('acceptance', 1447), ('access', 1448), ('accessibility', 1449), ('accidently', 1450), ('achilles', 1451), ('acrid', 1452), ('acting', 1453), ('ad', 1454), ('adam', 1455), ('adder', 1456), ('adulting', 1457), ('adults', 1458), ('affective', 1459), ('afp', 1460), ('aggressive', 1461), ('agin', 1462), ('agree', 1463), ('akshay', 1464), ('al', 1465), ('alaska', 1466), ('alcoholic', 1467), ('alex', 1468), ('alive', 1469), ('alliance', 1470), ('allow', 1471), ('allowed', 1472), ('alternate', 1473), ('amen', 1474), ('amendment', 1475), ('americans', 1476), ('among', 1477), ('ampalaya', 1478), ('analogy', 1479), ('anatomy', 1480), ('ang', 1481), ('animals', 1482), ('anniversary', 1483), ('anonymously', 1484), ('anybody', 1485), ('apathy', 1486), ('appalling', 1487), ('appreciate', 1488), ('arch', 1489), ('army', 1490), ('arrange', 1491), ('arrives', 1492), ('arse', 1493), ('article', 1494), ('articles', 1495), ('arts', 1496), ('asap', 1497), ('asleep', 1498), ('aspect', 1499), ('assist', 1500), ('assistance', 1501), ('assumed', 1502), ('ate', 1503), ('atlanta', 1504), ('atm', 1505), ('atomic', 1506), ('atrocious', 1507), ('attachments', 1508), ('attack', 1509), ('attacking', 1510), ('automatic', 1511), ('average', 1512), ('aware', 1513), ('axis', 1514), ('azerbaijan', 1515), ('b', 1516), ('baggins', 1517), ('bake', 1518), ('bakes', 1519), ('bakewell', 1520), ('ball', 1521), ('bands', 1522), ('bankrupt', 1523), ('baptist', 1524), ('barbecue', 1525), ('barely', 1526), ('bargain', 1527), ('barracking', 1528), ('barred', 1529), ('barrier', 1530), ('base', 1531), ('bash', 1532), ('basically', 1533), ('bday', 1534), ('beans', 1535), ('beasts', 1536), ('beck', 1537), ('beer', 1538), ('behavior', 1539), ('benet', 1540), ('bes', 1541), ('bias', 1542), ('bigger', 1543), ('bitches', 1544), ('bitcoin', 1545), ('blaming', 1546), ('blinds', 1547), ('blissful', 1548), ('blocks', 1549), ('bloke', 1550), ('blooded', 1551), ('blue', 1552), ('bmth', 1553), ('bollocked', 1554), ('bomb', 1555), ('bombers', 1556), ('bon', 1557), ('bond', 1558), ('book', 1559), ('boost', 1560), ('bout', 1561), ('breaking', 1562), ('broadband', 1563), ('brock', 1564), ('brought', 1565), ('brutal', 1566), ('buck', 1567), ('buddy', 1568), ('buford', 1569), ('bugger', 1570), ('build', 1571), ('building', 1572), ('bull', 1573), ('bully', 1574), ('bump', 1575), ('burke', 1576), ('bus', 1577), ('buses', 1578), ('bushes', 1579), ('businesses', 1580), ('busted', 1581), ('butt', 1582), ('butter', 1583), ('button', 1584), ('bwahaha', 1585), ('c', 1586), ('cadres', 1587), ('callbacks', 1588), ('calming', 1589), ('canadian', 1590), ('cancel', 1591), ('canceled', 1592), ('cancelling', 1593), ('candace', 1594), ('candidates', 1595), ('canoodling', 1596), ('caption', 1597), ('card', 1598), ('cardiac', 1599), ('care', 1600), ('carlabtst15', 1601), ('carolina', 1602), ('casuals', 1603), ('category', 1604), ('causing', 1605), ('cbs', 1606), ('celebrity', 1607), ('centre', 1608), ('century', 1609), ('certain', 1610), ('channel', 1611), ('chaos', 1612), ('cheese', 1613), ('chicken', 1614), ('chocolate', 1615), ('chosen', 1616), ('circle', 1617), ('citizens', 1618), ('claimed', 1619), ('clap', 1620), ('cleaning', 1621), ('clear', 1622), ('clin', 1623), ('clips', 1624), ('clp', 1625), ('cmn', 1626), ('cmon', 1627), ('coaster', 1628), ('cocks', 1629), ('cod', 1630), ('code', 1631), ('colin', 1632), ('colleague', 1633), ('colors', 1634), ('combs', 1635), ('comment', 1636), ('comp', 1637), ('complete', 1638), ('compliment', 1639), ('condemn', 1640), ('conditioned', 1641), ('confimation', 1642), ('conflicted', 1643), ('confusion', 1644), ('consciousness', 1645), ('consequences', 1646), ('considered', 1647), ('contact', 1648), ('content', 1649), ('contracts', 1650), ('corbyn', 1651), ('corporate', 1652), ('correction', 1653), ('corrupt', 1654), ('countdown', 1655), ('couple', 1656), ('coupon', 1657), ('court', 1658), ('couscous', 1659), ('covey', 1660), ('coworker', 1661), ('crime', 1662), ('crossed', 1663), ('cruther', 1664), ('cult', 1665), ('cumtribute', 1666), ('current', 1667), ('cyran', 1668), ('daddy', 1669), ('daily', 1670), ('dam', 1671), ('dammit', 1672), ('dance', 1673), ('dancers', 1674), ('dared', 1675), ('dat', 1676), ('davis', 1677), ('dc', 1678), ('dead', 1679), ('decision', 1680), ('decorations', 1681), ('definition', 1682), ('delayed', 1683), ('delays', 1684), ('delhi', 1685), ('democrats', 1686), ('dentist', 1687), ('denver', 1688), ('departing', 1689), ('derby', 1690), ('deserve', 1691), ('destroyer', 1692), ('determine', 1693), ('devils', 1694), ('digging', 1695), ('ding', 1696), ('disappointing', 1697), ('discipline', 1698), ('disconnected', 1699), ('discover', 1700), ('disgraceful', 1701), ('distraught', 1702), ('div', 1703), ('diver', 1704), ('divide', 1705), ('doggies', 1706), ('dogs', 1707), ('dole', 1708), ('double', 1709), ('doubt', 1710), ('download', 1711), ('dragons', 1712), ('dreamboys', 1713), ('drill', 1714), ('drilling', 1715), ('drinking', 1716), ('driving', 1717), ('dropped', 1718), ('drops', 1719), ('drugged', 1720), ('dryer', 1721), ('during', 1722), ('earn', 1723), ('earnest', 1724), ('ease', 1725), ('easier', 1726), ('ef', 1727), ('effects', 1728), ('elect', 1729), ('elected', 1730), ('elephants', 1731), ('ended', 1732), ('engineer', 1733), ('engineers', 1734), ('epi', 1735), ('equipment', 1736), ('er', 1737), ('escape', 1738), ('espn', 1739), ('essentially', 1740), ('establishment', 1741), ('eta', 1742), ('ethan', 1743), ('ether', 1744), ('evenxxs', 1745), ('everytime', 1746), ('evil', 1747), ('exodus', 1748), ('expand', 1749), ('explore', 1750), ('extreme', 1751), ('ezy864', 1752), ('f*c', 1753), ('facts', 1754), ('fag', 1755), ('fails', 1756), ('faking', 1757), ('falling', 1758), ('fam', 1759), ('fan', 1760), ('fandom', 1761), ('fanfuckingtastic', 1762), ('faulty', 1763), ('fav', 1764), ('fave', 1765), ('faved', 1766), ('fb', 1767), ('fears', 1768), ('federal', 1769), ('feeds', 1770), ('feet', 1771), ('fellwinters', 1772), ('fibe', 1773), ('field', 1774), ('figure', 1775), ('fill', 1776), ('filling', 1777), ('finale', 1778), ('firmly', 1779), ('fist', 1780), ('fits', 1781), ('fixed', 1782), ('flamenco', 1783), ('flaunt', 1784), ('flint', 1785), ('floated', 1786), ('floor', 1787), ('flt', 1788), ('foaming', 1789), ('focus', 1790), ('focused', 1791), ('followers', 1792), ('follows', 1793), ('forgiving', 1794), ('forth', 1795), ('forthwith', 1796), ('fortune', 1797), ('foster', 1798), ('four', 1799), ('fp', 1800), ('fragment', 1801), ('francis', 1802), ('frank', 1803), ('freaking', 1804), ('fridge', 1805), ('friedrich', 1806), ('friendship', 1807), ('frisco', 1808), ('frustrates', 1809), ('frustrating', 1810), ('ftw', 1811), ('fu*k', 1812), ('fuckery', 1813), ('fumbled', 1814), ('further', 1815), ('gag', 1816), ('gaga', 1817), ('gameday', 1818), ('gaming', 1819), ('garda', 1820), ('garlic', 1821), ('gas', 1822), ('gate', 1823), ('gay', 1824), ('germantown', 1825), ('gf', 1826), ('gh', 1827), ('girlfriends', 1828), ('giving', 1829), ('goddamn', 1830), ('goin', 1831), ('golds24', 1832), ('golf', 1833), ('gon', 1834), ('goofballs', 1835), ('gosh', 1836), ('government', 1837), ('grabbed', 1838), ('graham', 1839), ('grapes', 1840), ('grasp', 1841), ('grass', 1842), ('greys', 1843), ('ground', 1844), ('grow', 1845), ('guarded', 1846), ('guilty', 1847), ('gurls', 1848), ('guts', 1849), ('gutted', 1850), ('gws', 1851), ('hacked', 1852), ('halp', 1853), ('ham', 1854), ('haney', 1855), ('hangs', 1856), ('happily', 1857), ('harm', 1858), ('hashtags', 1859), ('haters', 1860), ('hav', 1861), ('headache', 1862), ('hear', 1863), ('heartbreaking', 1864), ('heck', 1865), ('hegemonists', 1866), ('helpful', 1867), ('helping', 1868), ('hide', 1869), ('hilarious', 1870), ('hissy', 1871), ('hiya2247', 1872), ('hogging', 1873), ('homely', 1874), ('homework', 1875), ('honest', 1876), ('honesty', 1877), ('honor', 1878), ('hoopjunkie', 1879), ('horny', 1880), ('horror', 1881), ('hotter', 1882), ('houston', 1883), ('hp', 1884), ('hrc', 1885), ('huck', 1886), ('hufflepuff', 1887), ('huge', 1888), ('humour', 1889), ('husband', 1890), ('hustle', 1891), ('hypocrisy', 1892), ('ideologically', 1893), ('idiots', 1894), ('ignorant', 1895), ('ignoring', 1896), ('ima', 1897), ('imma', 1898), ('immobilize', 1899), ('imo', 1900), ('important', 1901), ('impossible', 1902), ('impressed', 1903), ('impressive', 1904), ('incite', 1905), ('including', 1906), ('incurable', 1907), ('indefensible', 1908), ('indignity', 1909), ('industry', 1910), ('infuriating', 1911), ('initiative', 1912), ('innovation', 1913), ('insecure', 1914), ('intelligent', 1915), ('inv', 1916), ('investigation', 1917), ('invite', 1918), ('iphone', 1919), ('irrelevant', 1920), ('jack', 1921), ('jags', 1922), ('jc', 1923), ('jeongguk', 1924), ('jerks', 1925), ('jimmy', 1926), ('joe', 1927), ('joined', 1928), ('jon', 1929), ('jongdae', 1930), ('joshua', 1931), ('junc', 1932), ('juror', 1933), ('justification', 1934), ('jv', 1935), ('kaepernick', 1936), ('kai', 1937), ('karev', 1938), ('karma', 1939), ('katya', 1940), ('keller', 1941), ('kendall', 1942), ('keys', 1943), ('kgb', 1944), ('kidding', 1945), ('kisses', 1946), ('kitchen', 1947), ('kkk', 1948), ('klitschko', 1949), ('knock', 1950), ('knowing', 1951), ('labor', 1952), ('lads', 1953), ('lady', 1954), ('lag', 1955), ('lake', 1956), ('lamar', 1957), ('landlord', 1958), ('laptops', 1959), ('laughter', 1960), ('law', 1961), ('laws', 1962), ('leads', 1963), ('leaving', 1964), ('legit', 1965), ('leic', 1966), ('leo', 1967), ('lets', 1968), ('letts', 1969), ('liberal', 1970), ('lid', 1971), ('lie', 1972), ('lies', 1973), ('likely', 1974), ('lily', 1975), ('lines', 1976), ('lingys', 1977), ('link', 1978), ('lip', 1979), ('list', 1980), ('lithuania', 1981), ('livid', 1982), ('loathing', 1983), ('log', 1984), ('logical', 1985), ('looney', 1986), ('loss', 1987), ('loved', 1988), ('loves', 1989), ('lowkey', 1990), ('loyalty', 1991), ('lucky', 1992), ('lunch', 1993), ('luv', 1994), ('mac', 1995), ('magazines', 1996), ('magic', 1997), ('main', 1998), ('maintain', 1999), ('makeup', 2000), ('male', 2001), ('mall', 2002), ('manchester', 2003), ('mark', 2004), ('marry', 2005), ('martial', 2006), ('mary', 2007), ('mass', 2008), ('massive', 2009), ('mates', 2010), ('meal', 2011), ('meals', 2012), ('med', 2013), ('meech', 2014), ('meet', 2015), ('mel', 2016), ('melbourne', 2017), ('members', 2018), ('meter', 2019), ('metropolitan', 2020), ('mighty', 2021), ('miles', 2022), ('min', 2023), ('minimum', 2024), ('minus', 2025), ('minuscule', 2026), ('minute', 2027), ('misdirected', 2028), ('miserable', 2029), ('misplacing', 2030), ('missus', 2031), ('mix', 2032), ('mixing', 2033), ('mobile', 2034), ('mode', 2035), ('model', 2036), ('molly', 2037), ('monica', 2038), ('montanashay_', 2039), ('months', 2040), ('moody', 2041), ('morgan', 2042), ('morn', 2043), ('mos', 2044), ('mother', 2045), ('motivational', 2046), ('motivations', 2047), ('mountain', 2048), ('mouth', 2049), ('mp', 2050), ('mum', 2051), ('muppet', 2052), ('murdered', 2053), ('music', 2054), ('muslim', 2055), ('na', 2056), ('nails', 2057), ('naked', 2058), ('name', 2059), ('named', 2060), ('names', 2061), ('nappy', 2062), ('national', 2063), ('nc', 2064), ('nearly', 2065), ('neighbours', 2066), ('neither', 2067), ('nerve', 2068), ('network', 2069), ('nffc', 2070), ('ngives', 2071), ('ngo', 2072), ('nhelen', 2073), ('nick', 2074), ('nietzsche', 2075), ('nigga', 2076), ('nimber', 2077), ('nj', 2078), ('nme', 2079), ('notorious', 2080), ('nstephen', 2081), ('nthey', 2082), ('numb', 2083), ('october', 2084), ('odds', 2085), ('offered', 2086), ('og', 2087), ('olive', 2088), ('olympic', 2089), ('olympics', 2090), ('onions', 2091), ('opportunistic', 2092), ('orton', 2093), ('otherwise', 2094), ('ourselves', 2095), ('outlet', 2096), ('package', 2097), ('panpiper', 2098), ('paper', 2099), ('parcel', 2100), ('parsnip', 2101), ('particular', 2102), ('passed', 2103), ('passenger', 2104), ('pastry', 2105), ('patience', 2106), ('patriots', 2107), ('patronus', 2108), ('pd', 2109), ('peak', 2110), ('peep', 2111), ('peg', 2112), ('personality', 2113), ('pete', 2114), ('ph', 2115), ('photo', 2116), ('physical', 2117), ('picked', 2118), ('picking', 2119), ('pictures', 2120), ('pill', 2121), ('pirate', 2122), ('pissing', 2123), ('pitched', 2124), ('pitches', 2125), ('pits', 2126), ('pizzas', 2127), ('plaits', 2128), ('plans', 2129), ('plastic', 2130), ('platform', 2131), ('player', 2132), ('pls', 2133), ('plus', 2134), ('poetry', 2135), ('poison', 2136), ('polite', 2137), ('politicians', 2138), ('popping', 2139), ('population', 2140), ('porn', 2141), ('positive', 2142), ('possibly', 2143), ('postcode', 2144), ('pot', 2145), ('potato', 2146), ('pottermore', 2147), ('potus', 2148), ('poured', 2149), ('practical', 2150), ('prayers', 2151), ('prayin', 2152), ('preached', 2153), ('pregnancy', 2154), ('president', 2155), ('pretending', 2156), ('previously', 2157), ('prevmed', 2158), ('prices', 2159), ('prick', 2160), ('priorities', 2161), ('priority', 2162), ('prison', 2163), ('prisoners', 2164), ('profile', 2165), ('promised', 2166), ('proper', 2167), ('properly', 2168), ('protection', 2169), ('prove', 2170), ('proving', 2171), ('prudence', 2172), ('psycho', 2173), ('public', 2174), ('pull', 2175), ('pushing', 2176), ('putin', 2177), ('queens', 2178), ('racists', 2179), ('radical', 2180), ('ramsey', 2181), ('random', 2182), ('ranks', 2183), ('rant', 2184), ('rapid', 2185), ('rappers', 2186), ('rare', 2187), ('rawr', 2188), ('realize', 2189), ('realized', 2190), ('realizing', 2191), ('recognized', 2192), ('recommended', 2193), ('refugees', 2194), ('regard', 2195), ('regretting', 2196), ('regularly', 2197), ('reinstate', 2198), ('relationships', 2199), ('relax', 2200), ('remind', 2201), ('renew', 2202), ('rent', 2203), ('replaced', 2204), ('replacement', 2205), ('reporting', 2206), ('rights', 2207), ('rimes', 2208), ('rises', 2209), ('rivalry', 2210), ('river', 2211), ('rn', 2212), ('robbing', 2213), ('rockys', 2214), ('roller', 2215), ('route', 2216), ('ru', 2217), ('ruin', 2218), ('rule', 2219), ('rumoured', 2220), ('sacked', 2221), ('salty', 2222), ('savage', 2223), ('save', 2224), ('saw', 2225), ('scorpio', 2226), ('scottish', 2227), ('screamer', 2228), ('screen', 2229), ('scuffs', 2230), ('se', 2231), ('seasons', 2232), ('seat', 2233), ('seats', 2234), ('seconds', 2235), ('seed', 2236), ('sees', 2237), ('send', 2238), ('sensing', 2239), ('sentimentality', 2240), ('series', 2241), ('sexist', 2242), ('sfv', 2243), ('sh*t', 2244), ('shady', 2245), ('shawty', 2246), ('shennan', 2247), ('shift', 2248), ('shire', 2249), ('shits', 2250), ('shonda', 2251), ('shootings', 2252), ('shoots', 2253), ('shouldof', 2254), ('showcase', 2255), ('silly', 2256), ('simon', 2257), ('simultaneously', 2258), ('sin', 2259), ('sippin', 2260), ('sir', 2261), ('sisters', 2262), ('siva', 2263), ('skepticism', 2264), ('skipping', 2265), ('slam', 2266), ('slider', 2267), ('slightly', 2268), ('slowly', 2269), ('slushie', 2270), ('smackdown', 2271), ('smarties', 2272), ('smrt', 2273), ('snake', 2274), ('snakes', 2275), ('snuggling', 2276), ('somebodies', 2277), ('somebody', 2278), ('sore', 2279), ('sorted', 2280), ('sorting', 2281), ('spam', 2282), ('speaking', 2283), ('speaks', 2284), ('special', 2285), ('specific', 2286), ('specifically', 2287), ('speed', 2288), ('spill', 2289), ('sprayed', 2290), ('sprint', 2291), ('sr1', 2292), ('srry', 2293), ('stalker', 2294), ('stand', 2295), ('standard', 2296), ('starts', 2297), ('state', 2298), ('statement', 2299), ('statements', 2300), ('status', 2301), ('std', 2302), ('steele', 2303), ('stehr', 2304), ('step', 2305), ('steve', 2306), ('sti', 2307), ('stonington', 2308), ('stores', 2309), ('stories', 2310), ('streatham', 2311), ('streets', 2312), ('stroppy', 2313), ('stuart', 2314), ('study', 2315), ('style', 2316), ('succeed', 2317), ('successfully', 2318), ('sucks', 2319), ('sudden', 2320), ('suddenly', 2321), ('sue', 2322), ('suggest', 2323), ('suggests', 2324), ('sulk', 2325), ('supports', 2326), ('sushi', 2327), ('sweating', 2328), ('swines', 2329), ('swore', 2330), ('swung', 2331), ('symbolizes', 2332), ('taffy', 2333), ('tart', 2334), ('taser', 2335), ('tasers', 2336), ('taste', 2337), ('tater', 2338), ('taught', 2339), ('tbh', 2340), ('tcare', 2341), ('td', 2342), ('teef', 2343), ('temperament', 2344), ('ten', 2345), ('ter', 2346), ('terror', 2347), ('test', 2348), ('thirsty', 2349), ('thnx', 2350), ('thoughtful', 2351), ('thousand', 2352), ('three', 2353), ('threesome', 2354), ('threw', 2355), ('throws', 2356), ('thurston', 2357), ('ticketing', 2358), ('tickets', 2359), ('tiddy', 2360), ('tiger', 2361), ('til', 2362), ('titans', 2363), ('tongue', 2364), ('toon', 2365), ('touch', 2366), ('tournament', 2367), ('tr', 2368), ('tragedy', 2369), ('trains', 2370), ('transgender', 2371), ('tries', 2372), ('triggered', 2373), ('truthful', 2374), ('tto', 2375), ('turning', 2376), ('turns', 2377), ('twain', 2378), ('twats', 2379), ('tweetin', 2380), ('twice', 2381), ('tx', 2382), ('types', 2383), ('ugly', 2384), ('ultimately', 2385), ('unable', 2386), ('unbelievable', 2387), ('uncontrollable', 2388), ('unfair', 2389), ('unfortunately', 2390), ('united', 2391), ('unjust', 2392), ('unless', 2393), ('upgrade', 2394), ('upon', 2395), ('uri', 2396), ('usual', 2397), ('vacation', 2398), ('vale', 2399), ('vandalism', 2400), ('vanity', 2401), ('vehicle', 2402), ('vengeful', 2403), ('verse', 2404), ('victim', 2405), ('victims', 2406), ('videos', 2407), ('viewed', 2408), ('vigil', 2409), ('violent', 2410), ('virgin', 2411), ('vision', 2412), ('vodka', 2413), ('vote', 2414), ('votedfor', 2415), ('vsvplou', 2416), ('waite', 2417), ('wake', 2418), ('wakes', 2419), ('walk', 2420), ('warner', 2421), ('warning', 2422), ('waste', 2423), ('wattch', 2424), ('ways', 2425), ('wear', 2426), ('wearing', 2427), ('web', 2428), ('wee', 2429), ('welcome', 2430), ('wen', 2431), ('wenger', 2432), ('wentworth', 2433), ('wgm', 2434), ('whatever', 2435), ('whays', 2436), ('whenever', 2437), ('wheres', 2438), ('wife', 2439), ('wigan', 2440), ('william', 2441), ('willing', 2442), ('wills', 2443), ('willynilly', 2444), ('winakor', 2445), ('winter', 2446), ('wipe', 2447), ('wise', 2448), ('withdrawals', 2449), ('wiz', 2450), ('woken', 2451), ('wondered', 2452), ('wondering', 2453), ('wont', 2454), ('works', 2455), ('wounds', 2456), ('wrestled', 2457), ('xbox', 2458), ('xs', 2459), ('yal', 2460), ('yankees', 2461), ('yas', 2462), ('yelled', 2463), ('yeller', 2464), ('yep', 2465), ('yhat', 2466), ('youraffair', 2467), ('yours', 2468), ('yuh', 2469), ('ͫ', 2470), ('՟', 2471), ('ິ', 2472), ('‘', 2473), ('━', 2474), ('□', 2475), ('︵', 2476), ('）', 2477), ('🇸', 2478), ('🇺', 2479), ('$', 2480), (':OK_hand:', 2481), (':P_button:', 2482), (':angry_face_with_horns:', 2483), (':bread:', 2484), (':cat_face:', 2485), (':confounded_face:', 2486), (':confused_face:', 2487), (':cooking:', 2488), (':dark_skin_tone:', 2489), (':downcast_face_with_sweat:', 2490), (':face_with_head-bandage:', 2491), (':full_moon_face:', 2492), (':heart_exclamation:', 2493), (':house_with_garden:', 2494), (':hundred_points:', 2495), (':kitchen_knife:', 2496), (':latin_cross:', 2497), (':lipstick:', 2498), (':litter_in_bin_sign:', 2499), (':medium_skin_tone:', 2500), (':middle_finger:', 2501), (':money-mouth_face:', 2502), (':new_moon_face:', 2503), (':pensive_face:', 2504), (':person_gesturing_NO:', 2505), (':police_car:', 2506), (':purple_heart:', 2507), (':raised_fist:', 2508), (':revolving_hearts:', 2509), (':rugby_football:', 2510), (':skull:', 2511), (':sleeping_face:', 2512), (':smiling_face_with_halo:', 2513), (':smiling_face_with_horns:', 2514), (':smirking_face:', 2515), (':soccer_ball:', 2516), (':thumbs_up:', 2517), (':tired_face:', 2518), (':trade_mark:', 2519), (':trophy:', 2520), (':two_hearts:', 2521), (':worried_face:', 2522), (';_;', 2523), ('<date>', 2524), ('<kiss>', 2525), ('<phone>', 2526), ('<wink>', 2527), ('>_<', 2528), ('_', 2529), ('a2', 2530), ('aa', 2531), ('aber', 2532), ('aboard', 2533), ('abomination', 2534), ('above', 2535), ('abroad', 2536), ('abundance', 2537), ('abusing', 2538), ('accepted', 2539), ('accidentally', 2540), ('accidetly', 2541), ('accountability', 2542), ('accused', 2543), ('accusing', 2544), ('acid', 2545), ('acknowledging', 2546), ('acount', 2547), ('actin', 2548), ('activated', 2549), ('actual', 2550), ('added', 2551), ('addictive', 2552), ('addicts', 2553), ('addressed', 2554), ('administration', 2555), ('admire', 2556), ('adored', 2557), ('adp', 2558), ('adrenaline', 2559), ('adrian', 2560), ('adult', 2561), ('advantage', 2562), ('affair', 2563), ('affected', 2564), ('afflict', 2565), ('afflicted', 2566), ('afghan', 2567), ('afr', 2568), ('afraid', 2569), ('africa', 2570), ('aggravated', 2571), ('aggravates', 2572), ('aggravation', 2573), ('agrees', 2574), ('ahahah', 2575), ('ahl', 2576), ('ahole', 2577), ('aid', 2578), ('aidy', 2579), ('ain', 2580), ('aint', 2581), ('ak47', 2582), ('alan', 2583), ('alarms', 2584), ('albert', 2585), ('albums', 2586), ('aldo', 2587), ('aleppo', 2588), ('alexandria', 2589), ('allah', 2590), ('allat', 2591), ('allergic', 2592), ('allo', 2593), ('alloa', 2594), ('allume', 2595), ('alwayss', 2596), ('amiss', 2597), ('amongst', 2598), ('amount', 2599), ('amy', 2600), ('amyah', 2601), ('ananya', 2602), ('andriaprebles', 2603), ('angela', 2604), ('angers', 2605), ('angle', 2606), ('anime', 2607), ('anja', 2608), ('ankle', 2609), ('announced', 2610), ('annoyance', 2611), ('anon', 2612), ('anorexia', 2613), ('antichrist', 2614), ('anxiety', 2615), ('apologies', 2616), ('apologist', 2617), ('apologists', 2618), ('apologizing', 2619), ('appealing', 2620), ('appear', 2621), ('applied', 2622), ('approaching', 2623), ('apps', 2624), ('archie', 2625), ('area', 2626), ('areas', 2627), ('argument', 2628), ('armed', 2629), ('armenian', 2630), ('aromatherapy', 2631), ('arranging', 2632), ('arrogance', 2633), ('arsed', 2634), ('artefact', 2635), ('asf', 2636), ('asians', 2637), ('aside', 2638), ('asking', 2639), ('assed', 2640), ('assessed', 2641), ('assets', 2642), ('assigned', 2643), ('associate', 2644), ('assume', 2645), ('assumption', 2646), ('assumptions', 2647), ('assure', 2648), ('assured', 2649), ('asylum', 2650), ('atl', 2651), ('attached', 2652), ('attacked', 2653), ('attacks', 2654), ('au', 2655), ('authentic', 2656), ('auto', 2657), ('autocorrect', 2658), ('automatically', 2659), ('available', 2660), ('avataar', 2661), ('avoid', 2662), ('awaiting', 2663), ('awareness', 2664), ('awh', 2665), ('awoke', 2666), ('aye', 2667), ('b***er', 2668), ('babes', 2669), ('background', 2670), ('backlash', 2671), ('backs', 2672), ('badly', 2673), ('bae', 2674), ('bag', 2675), ('baker', 2676), ('baku', 2677), ('balanced', 2678), ('ballet', 2679), ('ballmer', 2680), ('balls', 2681), ('balm', 2682), ('balor', 2683), ('band', 2684), ('bang', 2685), ('bank', 2686), ('barbed', 2687), ('barca', 2688), ('bare', 2689), ('baritone', 2690), ('barnes', 2691), ('baseball', 2692), ('baskets', 2693), ('bastard', 2694), ('bastards', 2695), ('batches', 2696), ('bathing', 2697), ('baton', 2698), ('batternburg', 2699), ('bayern', 2700), ('baylor', 2701), ('bbctw', 2702), ('beach', 2703), ('beard', 2704), ('bears', 2705), ('beautiful', 2706), ('beauty', 2707), ('becoming', 2708), ('becouse', 2709), ('beef', 2710), ('began', 2711), ('begging', 2712), ('behalf', 2713), ('behave', 2714), ('behaved', 2715), ('behaviour', 2716), ('belief', 2717), ('belittling', 2718), ('belt', 2719), ('ben', 2720), ('beside', 2721), ('besides', 2722), ('beware', 2723), ('beyoncé', 2724), ('bff', 2725), ('bigot', 2726), ('bigoted', 2727), ('bigotry', 2728), ('bigots', 2729), ('bikini', 2730), ('bil', 2731), ('bilbo', 2732), ('billed', 2733), ('bills', 2734), ('bin', 2735), ('bint', 2736), ('biological', 2737), ('bite', 2738), ('biting', 2739), ('bits', 2740), ('blames', 2741), ('bland', 2742), ('blanket', 2743), ('blessings', 2744), ('blizzard', 2745), ('blocked', 2746), ('bloods', 2747), ('blow', 2748), ('blowing', 2749), ('blush', 2750), ('bnqo', 2751), ('bob', 2752), ('bodega', 2753), ('bodies', 2754), ('bog', 2755), ('boltup', 2756), ('books', 2757), ('boots', 2758), ('booze', 2759), ('boring', 2760), ('bot', 2761), ('bottled', 2762), ('bound', 2763), ('boutta', 2764), ('bows', 2765), ('box', 2766), ('boxing', 2767), ('boycotting', 2768), ('bpharm4', 2769), ('bradley', 2770), ('brain', 2771), ('braininess', 2772), ('brand', 2773), ('brat', 2774), ('breath', 2775), ('brendan', 2776), ('bret', 2777), ('bribed', 2778), ('brietbart', 2779), ('brightly', 2780), ('brills', 2781), ('brissett', 2782), ('broadcast', 2783), ('broccoli', 2784), ('brodcast', 2785), ('broken', 2786), ('brotherhood', 2787), ('brush', 2788), ('brutally', 2789), ('bs', 2790), ('btw', 2791), ('bu', 2792), ('buckynat', 2793), ('bulb', 2794), ('bullet', 2795), ('bullies', 2796), ('bullshit', 2797), ('bundle', 2798), ('burdened', 2799), ('bureaucrat', 2800), ('burgers', 2801), ('burn', 2802), ('burned', 2803), ('burner', 2804), ('bussing', 2805), ('butterflies', 2806), ('butthurt', 2807), ('buying', 2808), ('byu', 2809), ('ca', 2810), ('cab', 2811), ('cabin', 2812), ('cactus', 2813), ('caffeine', 2814), ('cain', 2815), ('cal', 2816), ('calculated', 2817), ('caller', 2818), ('calling', 2819), ('calorie', 2820), ('cameras', 2821), ('campaign', 2822), ('cams', 2823), ('candidate', 2824), ('candles', 2825), ('capo', 2826), ('cards', 2827), ('carefull', 2828), ('carr', 2829), ('carrier', 2830), ('carrying', 2831), ('cars', 2832), ('cas', 2833), ('cases', 2834), ('castrated', 2835), ('caused', 2836), ('causes', 2837), ('cavalierly', 2838), ('cb', 2839), ('cba', 2840), ('cd', 2841), ('celebrate', 2842), ('cell', 2843), ('cena', 2844), ('centerpiece', 2845), ('ceo', 2846), ('ceremony', 2847), ('cf', 2848), ('chamber', 2849), ('chara', 2850), ('charge', 2851), ('chargers', 2852), ('charges', 2853), ('charging', 2854), ('charity', 2855), ('chart', 2856), ('chasing', 2857), ('cheats', 2858), ('check', 2859), ('checkin', 2860), ('checking', 2861), ('cheeky', 2862), ('chelsea', 2863), ('chemistry', 2864), ('chewing', 2865), ('chic', 2866), ('chick', 2867), ('childish', 2868), ('chilled', 2869), ('chills', 2870), ('chin', 2871), ('china', 2872), ('chipotle', 2873), ('choose', 2874), ('chop', 2875), ('chose', 2876), ('christ', 2877), ('christie', 2878), ('christmas', 2879), ('chronic', 2880), ('chuck', 2881), ('chus', 2882), ('cigarette', 2883), ('cigarettes', 2884), ('cinema', 2885), ('cities', 2886), ('clam', 2887), ('clarkeson', 2888), ('clash', 2889), ('classes', 2890), ('classic', 2891), ('classified', 2892), ('claws', 2893), ('clean', 2894), ('cleaners', 2895), ('cleanse', 2896), ('clearly', 2897), ('clenching', 2898), ('clicked', 2899), ('clients', 2900), ('clintwanda', 2901), ('closer', 2902), ('closest', 2903), ('clown', 2904), ('clowns', 2905), ('clt', 2906), ('clue', 2907), ('cmode', 2908), ('cnn', 2909), ('cobalt', 2910), ('coffee', 2911), ('coffees', 2912), ('cognitive', 2913), ('coincidentally', 2914), ('cold', 2915), ('collateral', 2916), ('college', 2917), ('comfort', 2918), ('comfortable', 2919), ('comic', 2920), ('commences', 2921), ('commentary', 2922), ('commentator', 2923), ('commercial', 2924), ('commiting', 2925), ('common', 2926), ('compact', 2927), ('comparing', 2928), ('competition', 2929), ('complain', 2930), ('complaining', 2931), ('completely', 2932), ('complicated', 2933), ('concept', 2934), ('concerning', 2935), ('concise', 2936), ('concur', 2937), ('conduct', 2938), ('cones', 2939), ('confident', 2940), ('confrontations', 2941), ('congress', 2942), ('congreve', 2943), ('connie', 2944), ('connivers', 2945), ('considers', 2946), ('conspiracy', 2947), ('constituents', 2948), ('constitution', 2949), ('contestants', 2950), ('controlled', 2951), ('controllers', 2952), ('controlling', 2953), ('controls', 2954), ('convenient', 2955), ('convo', 2956), ('cooked', 2957), ('cookies', 2958), ('cooking', 2959), ('cooks', 2960), ('cooperate', 2961), ('coordinators', 2962), ('cope', 2963), ('coping', 2964), ('copy', 2965), ('cordinator', 2966), ('core246', 2967), ('cork', 2968), ('coronary', 2969), ('corpse', 2970), ('correct', 2971), ('correctly', 2972), ('correlative', 2973), ('corrupted', 2974), ('corruption', 2975), ('cory', 2976), ('cos', 2977), ('costs', 2978), ('costume', 2979), ('counter', 2980), ('counting', 2981), ('coupons', 2982), ('course', 2983), ('cousin', 2984), ('coworkers', 2985), ('coz', 2986), ('craft', 2987), ('craigan', 2988), ('cramp', 2989), ('creams', 2990), ('created', 2991), ('creats', 2992), ('creature', 2993), ('creatures', 2994), ('creeds', 2995), ('cricket', 2996), ('crimes', 2997), ('crimewatch', 2998), ('criminalized', 2999), ('crocodile', 3000), ('crooked', 3001), ('crossfire', 3002), ('crota', 3003), ('crowded', 3004), ('crudely', 3005), ('cruise', 3006), ('cryptic', 3007), ('csa', 3008), ('csgo', 3009), ('cuck', 3010), ('cucks', 3011), ('cucumbers', 3012), ('cultures', 3013), ('cupboards', 3014), ('cure', 3015), ('curiosity', 3016), ('curious', 3017), ('curling', 3018), ('currently', 3019), ('curse', 3020), ('cursing', 3021), ('custom', 3022), ('cv', 3023), ('cybernats', 3024), ('da', 3025), ('dads', 3026), ('daft', 3027), ('dain', 3028), ('damning', 3029), ('damnit', 3030), ('dangerously', 3031), ('danny', 3032), ('darkness', 3033), ('darn', 3034), ('dating', 3035), ('daughter', 3036), ('dave', 3037), ('de', 3038), ('deaf', 3039), ('dealt', 3040), ('dean', 3041), ('death', 3042), ('deaths', 3043), ('debating', 3044), ('decade', 3045), ('decency', 3046), ('decent', 3047), ('decisions', 3048), ('decommissioned', 3049), ('deed', 3050), ('deeds', 3051), ('def', 3052), ('default', 3053), ('defeat', 3054), ('defence', 3055), ('defend', 3056), ('defending', 3057), ('defination', 3058), ('define', 3059), ('definitive', 3060), ('degree', 3061), ('deleted', 3062), ('deleting', 3063), ('delicious', 3064), ('delight', 3065), ('delivery', 3066), ('demo', 3067), ('demons', 3068), ('denial', 3069), ('deployment', 3070), ('depressed', 3071), ('deprived', 3072), ('deptch', 3073), ('deric', 3074), ('describe', 3075), ('desication', 3076), ('designed', 3077), ('desires', 3078), ('desolate', 3079), ('desperately', 3080), ('despise', 3081), ('destroying', 3082), ('detail', 3083), ('details', 3084), ('detector', 3085), ('detention', 3086), ('determined', 3087), ('detest', 3088), ('detox', 3089), ('develops', 3090), ('devil', 3091), ('devy', 3092), ('didnt', 3093), ('diesel', 3094), ('diet', 3095), ('differs', 3096), ('difficult', 3097), ('dignity', 3098), ('digs', 3099), ('dilated', 3100), ('dint', 3101), ('dip', 3102), ('direct', 3103), ('directing', 3104), ('directtv', 3105), ('dis', 3106), ('disagree', 3107), ('disciplines', 3108), ('discriminate', 3109), ('discussing', 3110), ('disdain', 3111), ('disease', 3112), ('disgrace', 3113), ('disingenuous', 3114), ('display', 3115), ('dissonance', 3116), ('district', 3117), ('divided', 3118), ('dividing', 3119), ('dj', 3120), ('django', 3121), ('dming', 3122), ('doctor', 3123), ('doesnt', 3124), ('doggie', 3125), ('doll', 3126), ('dollar', 3127), ('donal', 3128), ('donald', 3129), ('donalds', 3130), ('donations', 3131), ('dopey', 3132), ('dove', 3133), ('dr', 3134), ('dragoness', 3135), ('drank', 3136), ('draws', 3137), ('drift', 3138), ('drinks', 3139), ('dripping', 3140), ('drive', 3141), ('driven', 3142), ('drivers', 3143), ('drives', 3144), ('drone', 3145), ('drove', 3146), ('drug', 3147), ('drugs', 3148), ('dry', 3149), ('dt', 3150), ('duck', 3151), ('dudes', 3152), ('duff', 3153), ('dumbed', 3154), ('dump', 3155), ('dumped', 3156), ('dunk', 3157), ('durban', 3158), ('dust', 3159), ('dyke', 3160), ('ea', 3161), ('earl', 3162), ('east', 3163), ('eaten', 3164), ('echo', 3165), ('eclectic', 3166), ('ed', 3167), ('edge', 3168), ('eds', 3169), ('educate', 3170), ('effective', 3171), ('egyptian', 3172), ('eish', 3173), ('elbow', 3174), ('elec', 3175), ('election', 3176), ('elite', 3177), ('ellie', 3178), ('eloquence', 3179), ('elphaba', 3180), ('emailing', 3181), ('embarrassed', 3182), ('embarrassment', 3183), ('emerson', 3184), ('emotion', 3185), ('empty', 3186), ('enabler', 3187), ('encryption', 3188), ('ends', 3189), ('endulging', 3190), ('engaging', 3191), ('engineered', 3192), ('england', 3193), ('engulf', 3194), ('enjoyable', 3195), ('enrage', 3196), ('entertain', 3197), ('entertaining', 3198), ('entire', 3199), ('entitled', 3200), ('entity', 3201), ('epicenter', 3202), ('epipen', 3203), ('equality', 3204), ('equally', 3205), ('erica', 3206), ('es', 3207), ('escalate', 3208), ('escaping', 3209), ('esque', 3210), ('estate', 3211), ('eval', 3212), ('evanora', 3213), ('everton', 3214), ('evicted', 3215), ('evidence', 3216), ('ewu', 3217), ('exam', 3218), ('exasperate', 3219), ('exasperation', 3220), ('excel', 3221), ('except', 3222), ('excitement', 3223), ('excuse', 3224), ('exhausted', 3225), ('exist', 3226), ('existence', 3227), ('existent', 3228), ('existential', 3229), ('expac', 3230), ('expansion', 3231), ('expects', 3232), ('expedition', 3233), ('experience', 3234), ('expiring', 3235), ('exploding', 3236), ('explosion', 3237), ('expressed', 3238), ('expressing', 3239), ('extend', 3240), ('extra', 3241), ('extract', 3242), ('extraordinary', 3243), ('extremely', 3244), ('eyebrows', 3245), ('ez', 3246), ('f', 3247), ('f**kin', 3248), ('factors', 3249), ('faggy', 3250), ('faith', 3251), ('faithful', 3252), ('fakes', 3253), ('fallon', 3254), ('fame', 3255), ('fancie', 3256), ('fancypants', 3257), ('fantasy', 3258), ('farm', 3259), ('fatalism', 3260), ('fatalistic', 3261), ('father', 3262), ('fatigue', 3263), ('fattening', 3264), ('favorites', 3265), ('favour', 3266), ('fawn', 3267), ('fbi', 3268), ('fearing', 3269), ('fearsome', 3270), ('feed', 3271), ('feedback', 3272), ('feeding', 3273), ('feic', 3274), ('feigned', 3275), ('fein', 3276), ('fell', 3277), ('fellow', 3278), ('feminist', 3279), ('feodal', 3280), ('ferguson', 3281), ('ferocious', 3282), ('fever', 3283), ('fil', 3284), ('file', 3285), ('filled', 3286), ('fills', 3287), ('filmmakers', 3288), ('filmmaking', 3289), ('finals', 3290), ('finished', 3291), ('finnick', 3292), ('firefighting', 3293), ('firm', 3294), ('fishin', 3295), ('fists', 3296), ('five', 3297), ('flag', 3298), ('flags', 3299), ('flake', 3300), ('flames', 3301), ('flaming', 3302), ('flatmate', 3303), ('flaunting', 3304), ('flawlessly', 3305), ('flesh', 3306), ('flexing', 3307), ('flick', 3308), ('flinches', 3309), ('flirt', 3310), ('flow', 3311), ('fluffy', 3312), ('fly', 3313), ('foiled', 3314), ('folding', 3315), ('folk', 3316), ('fool', 3317), ('footwear', 3318), ('forces', 3319), ('forcing', 3320), ('forehead', 3321), ('forest', 3322), ('forgets', 3323), ('forgives', 3324), ('former', 3325), ('fortunate', 3326), ('foul', 3327), ('fouls', 3328), ('fox', 3329), ('frampton', 3330), ('franchise', 3331), ('freaks', 3332), ('fred', 3333), ('frequencies', 3334), ('friedman', 3335), ('fries', 3336), ('frightening', 3337), ('frog', 3338), ('frozen', 3339), ('fs', 3340), ('fucker', 3341), ('fuking', 3342), ('fulfilled', 3343), ('funniest', 3344), ('furrow', 3345), ('fwendz', 3346), ('gagged', 3347), ('gallen', 3348), ('gambino', 3349), ('gamedev', 3350), ('games', 3351), ('gang', 3352), ('gaps', 3353), ('garcia', 3354), ('gardevoir', 3355), ('gargle', 3356), ('gary', 3357), ('gasoline', 3358), ('gazillion', 3359), ('gazza', 3360), ('gc', 3361), ('gcse', 3362), ('ge', 3363), ('gear', 3364), ('geil', 3365), ('gender', 3366), ('generates', 3367), ('generation', 3368), ('generations', 3369), ('genes', 3370), ('gentleman', 3371), ('genuinely', 3372), ('gerard', 3373), ('germany', 3374), ('gestort', 3375), ('gettin', 3376), ('gg', 3377), ('ghandi', 3378), ('ghost', 3379), ('ghus', 3380), ('gifts', 3381), ('giggle', 3382), ('girly', 3383), ('giroud', 3384), ('given', 3385), ('glass', 3386), ('glitch', 3387), ('glory', 3388), ('glowing', 3389), ('glows', 3390), ('glutes', 3391), ('goddard', 3392), ('golden', 3393), ('gordon', 3394), ('gorgeous', 3395), ('gotham', 3396), ('gouging', 3397), ('gr', 3398), ('gra', 3399), ('gran', 3400), ('granted', 3401), ('grayson', 3402), ('greater', 3403), ('greatly', 3404), ('greedy', 3405), ('greenwich', 3406), ('griffin', 3407), ('griffith', 3408), ('grim', 3409), ('grimrail', 3410), ('grinding', 3411), ('grizzly', 3412), ('grocery', 3413), ('groovier', 3414), ('grope', 3415), ('growling', 3416), ('grown', 3417), ('grows', 3418), ('grumpy', 3419), ('guards', 3420), ('guest', 3421), ('guide', 3422), ('gum', 3423), ('gyimah', 3424), ('habit', 3425), ('hah', 3426), ('hahah', 3427), ('haired', 3428), ('hairline', 3429), ('hajime', 3430), ('halloween', 3431), ('hamstring', 3432), ('handed', 3433), ('handle', 3434), ('hang', 3435), ('happening', 3436), ('happond', 3437), ('harambe', 3438), ('harley', 3439), ('hated', 3440), ('hateful', 3441), ('havent', 3442), ('hayes', 3443), ('hbu', 3444), ('hc', 3445), ('hchat', 3446), ('heads', 3447), ('hearths', 3448), ('heatwaves', 3449), ('heaven', 3450), ('heavier', 3451), ('heel', 3452), ('heh', 3453), ('height', 3454), ('helium', 3455), ('helps', 3456), ('hence', 3457), ('hers', 3458), ('herself', 3459), ('hffhj', 3460), ('hialeshia', 3461), ('highs', 3462), ('hiit', 3463), ('hilarity', 3464), ('hiney', 3465), ('hips', 3466), ('hipster', 3467), ('hitler', 3468), ('hitting', 3469), ('hmm', 3470), ('hmu', 3471), ('ho', 3472), ('holdin', 3473), ('hollywood', 3474), ('holy', 3475), ('hometowns', 3476), ('hominem', 3477), ('honeymoon', 3478), ('hoop', 3479), ('hoped', 3480), ('horoscopes', 3481), ('horrible', 3482), ('host', 3483), ('howard', 3484), ('hr', 3485), ('hudcomedy', 3486), ('hue', 3487), ('huffed', 3488), ('huma', 3489), ('hungary', 3490), ('hurry', 3491), ('hut', 3492), ('hw', 3493), ('hydration', 3494), ('hypocrite', 3495), ('idc', 3496), ('ideas', 3497), ('identity', 3498), ('idiot', 3499), ('ignite', 3500), ('ignored', 3501), ('ikr', 3502), ('illegally', 3503), ('immaturity', 3504), ('immediately', 3505), ('immense', 3506), ('immodest', 3507), ('impact', 3508), ('impairment', 3509), ('implies', 3510), ('impotent', 3511), ('imprisonment', 3512), ('imran', 3513), ('inadvertently', 3514), ('inbreds', 3515), ('incensed', 3516), ('incentivise', 3517), ('inch', 3518), ('inches', 3519), ('incident', 3520), ('include', 3521), ('income', 3522), ('increasing', 3523), ('incurring', 3524), ('indeed', 3525), ('independant', 3526), ('indians', 3527), ('indira', 3528), ('industrial', 3529), ('inequality', 3530), ('inevitable', 3531), ('infected', 3532), ('inferno', 3533), ('info', 3534), ('inform', 3535), ('ingestion', 3536), ('initially', 3537), ('injury', 3538), ('inpouring', 3539), ('insist', 3540), ('insists', 3541), ('inspire', 3542), ('inspires', 3543), ('insta', 3544), ('intel', 3545), ('intelligence', 3546), ('intended', 3547), ('intending', 3548), ('intense', 3549), ('intensity', 3550), ('intention', 3551), ('intentional', 3552), ('intentionally', 3553), ('inter', 3554), ('international', 3555), ('interview', 3556), ('interviewed', 3557), ('introduce', 3558), ('invalid', 3559), ('investing', 3560), ('invigorate', 3561), ('involuntarily', 3562), ('involve', 3563), ('iranians', 3564), ('irish', 3565), ('irritated', 3566), ('irwin', 3567), ('ishoulddo', 3568), ('isi', 3569), ('islamic', 3570), ('island', 3571), ('isn', 3572), ('isolation', 3573), ('israel', 3574), ('israeli', 3575), ('ist', 3576), ('it1', 3577), ('itchy', 3578), ('itself', 3579), ('itunes', 3580), ('iver', 3581), ('jacket', 3582), ('jail', 3583), ('jamming', 3584), ('janice', 3585), ('janieck', 3586), ('jasper', 3587), ('jayme', 3588), ('jealousy', 3589), ('jeez', 3590), ('jelly', 3591), ('jenner', 3592), ('ji', 3593), ('jia', 3594), ('jk', 3595), ('joanna', 3596), ('jobs', 3597), ('joes', 3598), ('joey', 3599), ('jogging', 3600), ('johnathon', 3601), ('jojo', 3602), ('joker', 3603), ('jordy', 3604), ('jose', 3605), ('josh', 3606), ('journalism', 3607), ('jr', 3608), ('juan', 3609), ('judge', 3610), ('justin', 3611), ('justly', 3612), ('k17', 3613), ('kait', 3614), ('kali', 3615), ('kardashians', 3616), ('kashmir', 3617), ('kei', 3618), ('kenny', 3619), ('kessel', 3620), ('khan', 3621), ('killing', 3622), ('killings', 3623), ('kills', 3624), ('kissing', 3625), ('kitten', 3626), ('kitty', 3627), ('klan', 3628), ('kneel', 3629), ('knocked', 3630), ('knowingly', 3631), ('known', 3632), ('komisarjevsky', 3633), ('ku', 3634), ('kylexy', 3635), ('kyos', 3636), ('laboratory', 3637), ('labour', 3638), ('laden', 3639), ('ladies', 3640), ('lamest', 3641), ('land', 3642), ('landlords', 3643), ('landscape', 3644), ('language', 3645), ('large', 3646), ('lashes', 3647), ('lately', 3648), ('latest', 3649), ('lattes', 3650), ('launching', 3651), ('laura', 3652), ('laureliver', 3653), ('lawn', 3654), ('lbs', 3655), ('ldf16', 3656), ('ldquo', 3657), ('lead', 3658), ('leader', 3659), ('leading', 3660), ('leafy', 3661), ('learned', 3662), ('leather', 3663), ('led', 3664), ('leg', 3665), ('legitimate', 3666), ('legs', 3667), ('lerman', 3668), ('lest', 3669), ('letgo', 3670), ('lethal', 3671), ('letter', 3672), ('letterkenny', 3673), ('leveled', 3674), ('lgbtq', 3675), ('liar', 3676), ('liars', 3677), ('library', 3678), ('lif', 3679), ('lift', 3680), ('lightbulb', 3681), ('lights', 3682), ('liked', 3683), ('likewise', 3684), ('limbs', 3685), ('limit', 3686), ('lion', 3687), ('lions', 3688), ('lisa', 3689), ('listens', 3690), ('littlest', 3691), ('lizards', 3692), ('lizzy', 3693), ('lmaoo', 3694), ('load', 3695), ('loads', 3696), ('loki', 3697), ('lolololol', 3698), ('longterm', 3699), ('lookd', 3700), ('looked', 3701), ('lookin', 3702), ('lots', 3703), ('lou', 3704), ('louder', 3705), ('loudly', 3706), ('lovely', 3707), ('lover', 3708), ('loving', 3709), ('lt', 3710), ('luckily', 3711), ('lush', 3712), ('lvg', 3713), ('lvl', 3714), ('lyrics', 3715), ('m*lo', 3716), ('macom', 3717), ('maddie', 3718), ('magazine', 3719), ('maghrebi', 3720), ('magical', 3721), ('magnanimous', 3722), ('magnified', 3723), ('mainstream', 3724), ('major', 3725), ('majority', 3726), ('makin', 3727), ('manga', 3728), ('manner', 3729), ('manus', 3730), ('map', 3731), ('maps', 3732), ('marcus', 3733), ('maren', 3734), ('market', 3735), ('marlboro', 3736), ('married', 3737), ('mars', 3738), ('marshmallows', 3739), ('martel', 3740), ('masks', 3741), ('massage', 3742), ('math', 3743), ('matt', 3744), ('matthew', 3745), ('maturity', 3746), ('mauricio', 3747), ('max', 3748), ('mayfield', 3749), ('mbc', 3750), ('mcdanno', 3751), ('mcdonald', 3752), ('mchanzo', 3753), ('mcternan', 3754), ('meaning', 3755), ('mechanism', 3756), ('mechanisms', 3757), ('medicine', 3758), ('meditation', 3759), ('megan', 3760), ('melting', 3761), ('member', 3762), ('memes', 3763), ('mentions', 3764), ('mercy', 3765), ('merely', 3766), ('merry', 3767), ('mess', 3768), ('messaging', 3769), ('messing', 3770), ('method', 3771), ('mexican', 3772), ('mf', 3773), ('mfs', 3774), ('mia', 3775), ('miami', 3776), ('michael', 3777), ('migraines', 3778), ('millennials', 3779), ('milton', 3780), ('minding', 3781), ('mindless', 3782), ('mine', 3783), ('mini', 3784), ('minion', 3785), ('minority', 3786), ('mirror', 3787), ('misery', 3788), ('misled', 3789), ('misogyny', 3790), ('misplaced', 3791), ('missing', 3792), ('mistakes', 3793), ('mitty', 3794), ('mm', 3795), ('moan', 3796), ('moaning', 3797), ('models', 3798), ('modern', 3799), ('molna', 3800), ('mom', 3801), ('moments', 3802), ('momma', 3803), ('monalisa', 3804), ('monday', 3805), ('mooching', 3806), ('mood', 3807), ('moor', 3808), ('mopo', 3809), ('moral', 3810), ('morally', 3811), ('mordor', 3812), ('mosquitoes', 3813), ('motif', 3814), ('motivate', 3815), ('motm', 3816), ('mourning', 3817), ('mouse', 3818), ('moustache', 3819), ('mower', 3820), ('moyes', 3821), ('mr', 3822), ('msnbc', 3823), ('mulled', 3824), ('multi', 3825), ('multimillionaire', 3826), ('muscles', 3827), ('mute', 3828), ('mylan', 3829), ('n*an', 3830), ('n*plays', 3831), ('n*slight', 3832), ('naay', 3833), ('naidy', 3834), ('namely', 3835), ('naming', 3836), ('naoto', 3837), ('nap', 3838), ('narcissistic', 3839), ('nasty', 3840), ('nation', 3841), ('nationalists', 3842), ('nations', 3843), ('native', 3844), ('natural', 3845), ('nauru', 3846), ('nawaz', 3847), ('nby', 3848), ('ncfc', 3849), ('nchasing', 3850), ('ndn', 3851), ('ndoes', 3852), ('near', 3853), ('neighbor', 3854), ('neighborhood', 3855), ('neighborhoods', 3856), ('neill', 3857), ('nephew', 3858), ('nerfed', 3859), ('nerves', 3860), ('ness', 3861), ('net', 3862), ('netizen', 3863), ('neyeballs', 3864), ('nfl', 3865), ('nfrom', 3866), ('ngrasp', 3867), ('nhe', 3868), ('nheheh', 3869), ('nhi', 3870), ('nhl', 3871), ('nicholson', 3872), ('niggas', 3873), ('nights', 3874), ('nihilism', 3875), ('nin', 3876), ('ninja', 3877), ('nipples', 3878), ('nit', 3879), ('nje', 3880), ('nkenny', 3881), ('nletting', 3882), ('nlike', 3883), ('nliquid', 3884), ('nmade', 3885), ('nod', 3886), ('nodded', 3887), ('nominated', 3888), ('nomore', 3889), ('none', 3890), ('nonsensical', 3891), ('nope', 3892), ('normal', 3893), ('normalized', 3894), ('nostrils', 3895), ('notes', 3896), ('noting', 3897), ('nowadays', 3898), ('nowhere', 3899), ('nppl', 3900), ('ntf', 3901), ('nthank', 3902), ('nthat', 3903), ('nthe', 3904), ('ntime', 3905), ('nto', 3906), ('numbers', 3907), ('nurse', 3908), ('nutt', 3909), ('nwhere', 3910), ('nwhy', 3911), ('nwrdsmth', 3912), ('nyou', 3913), ('oakley', 3914), ('obnoxiously', 3915), ('obstruction', 3916), ('occurs', 3917), ('ocean', 3918), ('offer', 3919), ('offers', 3920), ('officers', 3921), ('officially', 3922), ('officials', 3923), ('older', 3924), ('olds', 3925), ('ole', 3926), ('omg', 3927), ('oncoming', 3928), ('oops', 3929), ('oow16', 3930), ('opening', 3931), ('opens', 3932), ('operating', 3933), ('opinions', 3934), ('opioid', 3935), ('opportunities', 3936), ('opt', 3937), ('optics', 3938), ('orange', 3939), ('oregon', 3940), ('originally', 3941), ('osu', 3942), ('ouch', 3943), ('outsiders', 3944), ('outweighs', 3945), ('overall', 3946), ('overdone', 3947), ('overpowered', 3948), ('overseer', 3949), ('overwhelmed', 3950), ('overwhelming', 3951), ('pac', 3952), ('pack', 3953), ('packaging', 3954), ('pacquaio', 3955), ('painting', 3956), ('pak', 3957), ('pamphlets', 3958), ('pander', 3959), ('pap', 3960), ('papa', 3961), ('papercuts', 3962), ('paradise', 3963), ('parenting', 3964), ('parentsof', 3965), ('parris', 3966), ('parrots', 3967), ('partner', 3968), ('passage', 3969), ('passion', 3970), ('patches', 3971), ('patent', 3972), ('paternalism', 3973), ('pathetic', 3974), ('pathetically', 3975), ('pathological', 3976), ('patient', 3977), ('patron', 3978), ('pawn', 3979), ('payet', 3980), ('peaked', 3981), ('peanut', 3982), ('pecan', 3983), ('peeped', 3984), ('pereg', 3985), ('pereira', 3986), ('perfection', 3987), ('perform', 3988), ('perfume', 3989), ('persecutes', 3990), ('persons', 3991), ('peter', 3992), ('petition', 3993), ('petits', 3994), ('pets', 3995), ('phantom', 3996), ('phil', 3997), ('philadelphia', 3998), ('philando', 3999), ('philosopher', 4000), ('phones', 4001), ('photos', 4002), ('physics', 4003), ('pi', 4004), ('picture', 4005), ('pig', 4006), ('pilot', 4007), ('pinching', 4008), ('pineapple', 4009), ('pipe', 4010), ('pipes', 4011), ('pitting', 4012), ('pizza', 4013), ('places', 4014), ('plague', 4015), ('plain', 4016), ('planet', 4017), ('playmaker', 4018), ('plays', 4019), ('plese', 4020), ('plymouth', 4021), ('plz', 4022), ('pm', 4023), ('poc', 4024), ('pochettino', 4025), ('pointe', 4026), ('pointed', 4027), ('policing', 4028), ('polis', 4029), ('politically', 4030), ('poorly', 4031), ('popular', 4032), ('porch', 4033), ('portal', 4034), ('portland', 4035), ('posing', 4036), ('position', 4037), ('positively', 4038), ('positivity', 4039), ('possible', 4040), ('potential', 4041), ('potentially', 4042), ('pouting', 4043), ('pow', 4044), ('practitioner', 4045), ('pray', 4046), ('prayerful', 4047), ('praying', 4048), ('preach', 4049), ('preaching', 4050), ('prebooked', 4051), ('precious', 4052), ('prejudice', 4053), ('prejudiced', 4054), ('preposterous', 4055), ('preschool', 4056), ('present', 4057), ('press', 4058), ('pressure', 4059), ('prevent', 4060), ('price', 4061), ('pricing', 4062), ('privileged', 4063), ('proceedings', 4064), ('professor', 4065), ('program', 4066), ('programme', 4067), ('programs', 4068), ('progressive', 4069), ('progressives', 4070), ('progrm', 4071), ('prom', 4072), ('prominent', 4073), ('promise', 4074), ('promoters', 4075), ('property', 4076), ('prosecution', 4077), ('protect', 4078), ('protein', 4079), ('proud', 4080), ('proverbs', 4081), ('provoked', 4082), ('ps4', 4083), ('psalms', 4084), ('psychological', 4085), ('psychowasp', 4086), ('pt', 4087), ('puck', 4088), ('puerto', 4089), ('puff', 4090), ('puffing', 4091), ('pulse', 4092), ('pumped', 4093), ('pumpkin', 4094), ('punching', 4095), ('punk', 4096), ('punt', 4097), ('purely', 4098), ('purpose', 4099), ('purposely', 4100), ('purposes', 4101), ('purse', 4102), ('pursuit', 4103), ('push', 4104), ('pussy', 4105), ('puzzle', 4106), ('q', 4107), ('qb', 4108), ('quaint', 4109), ('quanitfying', 4110), ('quatrains', 4111), ('queensland', 4112), ('quentin', 4113), ('questions', 4114), ('quran', 4115), ('r.i.p', 4116), ('races', 4117), ('racial', 4118), ('raids', 4119), ('rail', 4120), ('raised', 4121), ('ralph', 4122), ('ralso', 4123), ('ramos', 4124), ('ran', 4125), ('randomly', 4126), ('range', 4127), ('ranger', 4128), ('ranting', 4129), ('rap', 4130), ('rape', 4131), ('rapper', 4132), ('rarely', 4133), ('rational', 4134), ('rattatas', 4135), ('rattled', 4136), ('ravi', 4137), ('rawhide', 4138), ('rb', 4139), ('rd', 4140), ('rdquo', 4141), ('reach', 4142), ('realest', 4143), ('realise', 4144), ('realistic', 4145), ('reasonably', 4146), ('rebates', 4147), ('rebuke', 4148), ('receiving', 4149), ('recharged', 4150), ('recognize', 4151), ('redden', 4152), ('redzone', 4153), ('reeling', 4154), ('ref', 4155), ('refer', 4156), ('referred', 4157), ('reflects', 4158), ('refs', 4159), ('refunded', 4160), ('refuse', 4161), ('regarde', 4162), ('regardless', 4163), ('regret', 4164), ('reject', 4165), ('related', 4166), ('release', 4167), ('releases', 4168), ('relevant', 4169), ('reluctantly', 4170), ('remark', 4171), ('reminders', 4172), ('remix', 4173), ('remotely', 4174), ('removes', 4175), ('repeal', 4176), ('repeat', 4177), ('reporters', 4178), ('representation', 4179), ('republicans', 4180), ('requires', 4181), ('rescue', 4182), ('research', 4183), ('resettle', 4184), ('resort', 4185), ('resorted', 4186), ('respond', 4187), ('restaurant', 4188), ('restrained', 4189), ('rests', 4190), ('result', 4191), ('resulted', 4192), ('resultin', 4193), ('retaliation', 4194), ('return', 4195), ('rev', 4196), ('revealed', 4197), ('revelations', 4198), ('revells', 4199), ('reverb', 4200), ('reverse', 4201), ('reviews', 4202), ('revolution', 4203), ('reward', 4204), ('reyes', 4205), ('rhyme', 4206), ('rican', 4207), ('rid', 4208), ('ridden', 4209), ('ridiculous', 4210), ('rile', 4211), ('ringing', 4212), ('riots', 4213), ('rise', 4214), ('risk', 4215), ('riteing', 4216), ('rlly', 4217), ('rng', 4218), ('roach', 4219), ('roam', 4220), ('roast', 4221), ('robart', 4222), ('rock', 4223), ('rocket', 4224), ('rodgers', 4225), ('roght', 4226), ('roids', 4227), ('rojo', 4228), ('roll', 4229), ('rolling', 4230), ('romain', 4231), ('rooting', 4232), ('rose', 4233), ('rouge', 4234), ('rough', 4235), ('routes', 4236), ('rowdy', 4237), ('rowe', 4238), ('roy', 4239), ('royal', 4240), ('rs', 4241), ('rspca', 4242), ('rubberbath', 4243), ('rudely', 4244), ('rum', 4245), ('running', 4246), ('russia', 4247), ('rutgers', 4248), ('sabr', 4249), ('saddens', 4250), ('saddest', 4251), ('saddler', 4252), ('sadly', 4253), ('sagin', 4254), ('sailer', 4255), ('sake', 4256), ('salt', 4257), ('san', 4258), ('sanchez', 4259), ('sandra', 4260), ('sandwich', 4261), ('sane', 4262), ('sangria', 4263), ('sara', 4264), ('sasse', 4265), ('sat', 4266), ('satan', 4267), ('satellite', 4268), ('sauce', 4269), ('saucepan', 4270), ('saved', 4271), ('saving', 4272), ('scale', 4273), ('scar', 4274), ('scare', 4275), ('scatter', 4276), ('schedule', 4277), ('schneiderlin', 4278), ('scientific', 4279), ('scored', 4280), ('scores', 4281), ('scorning', 4282), ('scotland', 4283), ('scrap', 4284), ('scratch', 4285), ('scratched', 4286), ('screeching', 4287), ('screened', 4288), ('screenshotted', 4289), ('screw', 4290), ('screwed', 4291), ('scrubbed', 4292), ('scumbags', 4293), ('seam', 4294), ('secret', 4295), ('secs', 4296), ('seekers', 4297), ('seem', 4298), ('seep', 4299), ('selasi', 4300), ('selfie', 4301), ('selfs', 4302), ('sell', 4303), ('seller', 4304), ('semi', 4305), ('sending', 4306), ('sensible', 4307), ('sensitive', 4308), ('sensory', 4309), ('sent', 4310), ('separate', 4311), ('serve', 4312), ('server', 4313), ('session', 4314), ('sessions', 4315), ('settle', 4316), ('several', 4317), ('severe', 4318), ('sexuality', 4319), ('sexualizing', 4320), ('shadow', 4321), ('shadows', 4322), ('shaft', 4323), ('shaker', 4324), ('shakesphere', 4325), ('shame', 4326), ('shameful', 4327), ('sharif', 4328), ('sharing', 4329), ('sharpton', 4330), ('sheets', 4331), ('shelter', 4332), ('sheltered', 4333), ('sherri', 4334), ('shimmy', 4335), ('shine', 4336), ('ship', 4337), ('shirt', 4338), ('shitless', 4339), ('short', 4340), ('shoutdrive', 4341), ('shouts', 4342), ('showed', 4343), ('showmance', 4344), ('shown', 4345), ('shrug', 4346), ('sickle', 4347), ('sigh', 4348), ('sight', 4349), ('significantly', 4350), ('silence', 4351), ('silent', 4352), ('silicon', 4353), ('similar', 4354), ('simms', 4355), ('simper', 4356), ('simplified', 4357), ('sinatra', 4358), ('single', 4359), ('sings', 4360), ('sinn', 4361), ('sins', 4362), ('sip', 4363), ('sirli', 4364), ('situ', 4365), ('sixth', 4366), ('sixty', 4367), ('sj', 4368), ('sjw', 4369), ('skinned', 4370), ('slagging', 4371), ('slap', 4372), ('slightest', 4373), ('slim', 4374), ('slipper', 4375), ('slutfaceshlongnugget', 4376), ('small', 4377), ('smash', 4378), ('smell', 4379), ('smiles', 4380), ('smiling', 4381), ('smith', 4382), ('smokey', 4383), ('snacks', 4384), ('snarl', 4385), ('snarled', 4386), ('snitch', 4387), ('snorting', 4388), ('snow', 4389), ('snp', 4390), ('sobbing', 4391), ('sober', 4392), ('soccer', 4393), ('socialists', 4394), ('society', 4395), ('softball', 4396), ('softballs', 4397), ('sold', 4398), ('solo', 4399), ('solution', 4400), ('solve', 4401), ('somehow', 4402), ('sour', 4403), ('south', 4404), ('southern', 4405), ('spared', 4406), ('spark', 4407), ('sparkles', 4408), ('speaker', 4409), ('species', 4410), ('spectacle', 4411), ('speech', 4412), ('spills', 4413), ('spin', 4414), ('spine', 4415), ('spitting', 4416), ('splashed', 4417), ('spoiled', 4418), ('spoke', 4419), ('spoken', 4420), ('spontaneous', 4421), ('sports', 4422), ('spray', 4423), ('spurs', 4424), ('squashie', 4425), ('squat', 4426), ('squeezing', 4427), ('squid', 4428), ('squirm', 4429), ('src', 4430), ('stacked', 4431), ('stadium', 4432), ('stairs', 4433), ('stan', 4434), ('stans', 4435), ('starter', 4436), ('starved', 4437), ('stealing', 4438), ('steel', 4439), ('stefano', 4440), ('stem', 4441), ('stepped', 4442), ('stepping', 4443), ('stereotype', 4444), ('stfu', 4445), ('stick', 4446), ('stickers', 4447), ('sticks', 4448), ('stingrays', 4449), ('stirring', 4450), ('stk', 4451), ('stl', 4452), ('stock', 4453), ('stomach', 4454), ('stopping', 4455), ('stops', 4456), ('stored', 4457), ('strange', 4458), ('strawmen', 4459), ('stream', 4460), ('strength', 4461), ('stress', 4462), ('strides', 4463), ('strip', 4464), ('stripe', 4465), ('stronger', 4466), ('struggled', 4467), ('struggles', 4468), ('stub', 4469), ('student', 4470), ('stung', 4471), ('stupidity', 4472), ('stworldprobs4vs', 4473), ('styled', 4474), ('sua', 4475), ('subject', 4476), ('submit', 4477), ('subsist', 4478), ('subway', 4479), ('suffered', 4480), ('suicide', 4481), ('suit', 4482), ('suits', 4483), ('summar', 4484), ('sums', 4485), ('surah', 4486), ('surely', 4487), ('surfed', 4488), ('surge', 4489), ('survival', 4490), ('survivor', 4491), ('suspect', 4492), ('sutton', 4493), ('swallowing', 4494), ('sweaty', 4495), ('sweden', 4496), ('sweeter', 4497), ('swerve', 4498), ('sword', 4499), ('sympathy', 4500), ('synthetic', 4501), ('syria', 4502), ('systemic', 4503), ('table', 4504), ('taino', 4505), ('talkin', 4506), ('talks', 4507), ('tamed', 4508), ('tank', 4509), ('tantalizing', 4510), ('tapas', 4511), ('tape', 4512), ('tapes', 4513), ('target', 4514), ('targeted', 4515), ('tarts', 4516), ('tas', 4517), ('tastes', 4518), ('taurus', 4519), ('taxi', 4520), ('tay', 4521), ('tea', 4522), ('teacher', 4523), ('teaching', 4524), ('technician', 4525), ('tells', 4526), ('tenderhearted', 4527), ('tendonitis', 4528), ('tennessee', 4529), ('tensions', 4530), ('term', 4531), ('terms', 4532), ('terrific', 4533), ('terrifying', 4534), ('territorial', 4535), ('tes', 4536), ('testosterone', 4537), ('texans', 4538), ('texts', 4539), ('tgmbnquee', 4540), ('tha', 4541), ('thatmoment', 4542), ('theater', 4543), ('thee', 4544), ('theories', 4545), ('theory', 4546), ('theres', 4547), ('thick', 4548), ('thieves', 4549), ('thighs', 4550), ('thinks', 4551), ('thirst', 4552), ('thou', 4553), ('thread', 4554), ('threats', 4555), ('thrive', 4556), ('throat', 4557), ('throwback', 4558), ('throwing', 4559), ('thug', 4560), ('thugs', 4561), ('thumper', 4562), ('thunder', 4563), ('thursdays', 4564), ('thy', 4565), ('tiangong', 4566), ('tidying', 4567), ('tier', 4568), ('tierney', 4569), ('tilting', 4570), ('timid', 4571), ('tiny', 4572), ('tio', 4573), ('tirade', 4574), ('titter', 4575), ('tnf', 4576), ('toast', 4577), ('toasting', 4578), ('toddler', 4579), ('toe', 4580), ('toilet', 4581), ('tokens', 4582), ('tokyo', 4583), ('tomboy', 4584), ('tonic', 4585), ('tonys', 4586), ('torment', 4587), ('toronto', 4588), ('total', 4589), ('totally', 4590), ('touches', 4591), ('touchline', 4592), ('touchy', 4593), ('tough', 4594), ('tour', 4595), ('toussaint', 4596), ('trackdawgt', 4597), ('tractor', 4598), ('trailers', 4599), ('trainer', 4600), ('training', 4601), ('trample', 4602), ('transformed', 4603), ('transforming', 4604), ('transition', 4605), ('transmutation', 4606), ('trayvon', 4607), ('treated', 4608), ('trek', 4609), ('trick', 4610), ('trigger', 4611), ('trips', 4612), ('tristan', 4613), ('troll', 4614), ('trolled', 4615), ('trolling', 4616), ('trolls', 4617), ('trons', 4618), ('trophy', 4619), ('trouble', 4620), ('troubled', 4621), ('troye', 4622), ('trudeau', 4623), ('trumpism', 4624), ('trust', 4625), ('trusted', 4626), ('tryin', 4627), ('tub', 4628), ('tubers', 4629), ('tuh', 4630), ('tunes', 4631), ('turkey', 4632), ('turnovers', 4633), ('turtle', 4634), ('tweeted', 4635), ('twirl', 4636), ('ty', 4637), ('tyre', 4638), ('tyson', 4639), ('tøp', 4640), ('ugliest', 4641), ('ukip', 4642), ('ultra', 4643), ('ulzana', 4644), ('un', 4645), ('unattractive', 4646), ('unavoidable', 4647), ('unchained', 4648), ('uncle', 4649), ('uncomfortable', 4650), ('understandable', 4651), ('understood', 4652), ('undifferentiated', 4653), ('undress', 4654), ('unfairly', 4655), ('unfollow', 4656), ('unforgiving', 4657), ('uni', 4658), ('unimpressive', 4659), ('unite', 4660), ('university', 4661), ('unleashing', 4662), ('unpredictable', 4663), ('unprovoked', 4664), ('unreliable', 4665), ('unresolved', 4666), ('unwittingly', 4667), ('updates', 4668), ('usage', 4669), ('usc', 4670), ('useful', 4671), ('usually', 4672), ('utilize', 4673), ('uttered', 4674), ('valiant', 4675), ('valle', 4676), ('valley', 4677), ('vegans', 4678), ('veggie', 4679), ('vehs', 4680), ('vengeance', 4681), ('venom', 4682), ('venue', 4683), ('verification', 4684), ('verses', 4685), ('vespers', 4686), ('vessel', 4687), ('vets', 4688), ('via', 4689), ('vibe', 4690), ('victory', 4691), ('video', 4692), ('views', 4693), ('vigilant', 4694), ('vile', 4695), ('villa', 4696), ('vinyl', 4697), ('violations', 4698), ('viral', 4699), ('virtuoso', 4700), ('visceral', 4701), ('visit', 4702), ('visitor', 4703), ('vit', 4704), ('vita', 4705), ('voice', 4706), ('voiceover', 4707), ('voicing', 4708), ('void', 4709), ('voke', 4710), ('volleyball', 4711), ('voters', 4712), ('votes', 4713), ('voting', 4714), ('vulnerable', 4715), ('wae', 4716), ('waggled', 4717), ('waited', 4718), ('waldo', 4719), ('walking', 4720), ('wall', 4721), ('wallah', 4722), ('walter', 4723), ('warm', 4724), ('warrant', 4725), ('warren', 4726), ('warrior', 4727), ('wash', 4728), ('washington', 4729), ('wasnt', 4730), ('wasteland', 4731), ('watches', 4732), ('waterfall', 4733), ('wave', 4734), ('wcw', 4735), ('weapons', 4736), ('weary', 4737), ('webb', 4738), ('wedding', 4739), ('weekday', 4740), ('weight', 4741), ('weiner', 4742), ('weirdos', 4743), ('wellness', 4744), ('west', 4745), ('westminster', 4746), ('wet', 4747), ('whack', 4748), ('whatprank', 4749), ('whats', 4750), ('whether', 4751), ('whisper', 4752), ('whispers', 4753), ('whom', 4754), ('whore', 4755), ('whose', 4756), ('whyanewg', 4757), ('wi', 4758), ('wick', 4759), ('widely', 4760), ('wiener', 4761), ('wilt', 4762), ('windows', 4763), ('windy', 4764), ('wine', 4765), ('wingers', 4766), ('winner', 4767), ('wins', 4768), ('wire', 4769), ('wishes', 4770), ('withdraw', 4771), ('wnf', 4772), ('wolf', 4773), ('wolfpack', 4774), ('wore', 4775), ('worm', 4776), ('worried', 4777), ('worship', 4778), ('wots', 4779), ('wpt', 4780), ('wr', 4781), ('wrapped', 4782), ('wrathful', 4783), ('wreck', 4784), ('wrinkles', 4785), ('wrist', 4786), ('wrongdoing', 4787), ('wrongfully', 4788), ('wry', 4789), ('wukong', 4790), ('wvu', 4791), ('wycelv', 4792), ('xbox1', 4793), ('xlzjyhg', 4794), ('xmas', 4795), ('xo', 4796), ('yall', 4797), ('yalls', 4798), ('ydu', 4799), ('yea', 4800), ('yeahs', 4801), ('yell', 4802), ('yelling', 4803), ('yellow', 4804), ('yh', 4805), ('yourselves', 4806), ('youths', 4807), ('youtuber', 4808), ('yu', 4809), ('yuck', 4810), ('yummy', 4811), ('yuppie', 4812), ('zandu', 4813), ('zombie', 4814), ('zumba', 4815), ('{', 4816), ('}', 4817), ('óg', 4818), ('ઘ', 4819), ('–', 4820), ('―', 4821), ('←', 4822), ('→', 4823)])\n",
            "dict_items([('<unk>', 0), ('<pad>', 1), ('</hashtag>', 2), ('<hashtag>', 3), ('.', 4), ('<user>', 5), ('i', 6), ('the', 7), ('to', 8), (',', 9), ('a', 10), (\"'\", 11), ('is', 12), ('of', 13), ('and', 14), ('you', 15), ('not', 16), ('it', 17), ('!', 18), ('<repeated>', 19), ('in', 20), ('my', 21), ('s', 22), ('that', 23), ('<number>', 24), ('for', 25), ('?', 26), ('on', 27), ('have', 28), ('are', 29), ('am', 30), ('do', 31), ('me', 32), ('so', 33), ('\\\\', 34), ('</allcaps>', 35), ('<allcaps>', 36), ('this', 37), ('be', 38), ('-', 39), ('can', 40), ('your', 41), ('at', 42), ('but', 43), ('was', 44), ('with', 45), ('just', 46), ('all', 47), ('if', 48), ('like', 49), ('what', 50), ('about', 51), ('will', 52), ('n', 53), ('we', 54), ('no', 55), ('when', 56), ('up', 57), ('they', 58), ('get', 59), ('he', 60), ('from', 61), ('&', 62), ('an', 63), ('fear', 64), ('out', 65), ('anxiety', 66), ('now', 67), ('awful', 68), ('horror', 69), ('nightmare', 70), ('terrorism', 71), ('nervous', 72), ('start', 73), ('terrible', 74), ('would', 75), ('as', 76), ('or', 77), ('how', 78), ('people', 79), ('day', 80), ('know', 81), ('shocking', 82), ('there', 83), (':', 84), ('terror', 85), ('time', 86), ('going', 87), ('panic', 88), ('one', 89), ('why', 90), ('by', 91), ('afraid', 92), ('go', 93), ('bully', 94), ('/', 95), ('did', 96), ('horrible', 97), ('think', 98), ('been', 99), ('life', 100), ('more', 101), ('still', 102), ('being', 103), ('really', 104), ('never', 105), ('us', 106), ('make', 107), ('should', 108), ('who', 109), ('t', 110), ('their', 111), ('good', 112), ('her', 113), ('u', 114), ('worry', 115), ('has', 116), ('she', 117), ('only', 118), ('back', 119), ('bad', 120), ('want', 121), ('him', 122), ('some', 123), ('had', 124), ('his', 125), ('after', 126), ('off', 127), ('today', 128), ('our', 129), (':face_with_tears_of_joy:', 130), ('need', 131), ('see', 132), ('last', 133), ('much', 134), ('them', 135), ('d', 136), ('new', 137), ('trump', 138), ('too', 139), ('again', 140), ('does', 141), ('even', 142), ('right', 143), ('someone', 144), ('could', 145), ('first', 146), ('pakistan', 147), ('feel', 148), ('love', 149), ('then', 150), ('work', 151), ('because', 152), ('god', 153), ('shy', 154), ('awe', 155), ('down', 156), ('got', 157), ('over', 158), ('sleep', 159), ('way', 160), ('service', 161), ('shake', 162), ('than', 163), ('world', 164), ('alarm', 165), ('ever', 166), ('something', 167), ('tonight', 168), ('(', 169), ('fucking', 170), ('here', 171), (')', 172), ('shaking', 173), ('always', 174), ('any', 175), ('every', 176), ('im', 177), ('let', 178), ('night', 179), ('scare', 180), ('stop', 181), ('dreadful', 182), ('say', 183), ('things', 184), ('look', 185), ('most', 186), ('shit', 187), ('tomorrow', 188), (':weary_face:', 189), ('great', 190), ('old', 191), ('these', 192), ('through', 193), ('where', 194), ('<time>', 195), ('before', 196), ('concern', 197), ('despair', 198), ('its', 199), ('oh', 200), ('other', 201), ('worst', 202), ('having', 203), ('makes', 204), (':loudly_crying_face:', 205), ('another', 206), ('customer', 207), ('news', 208), ('same', 209), ('sure', 210), ('were', 211), ('better', 212), ('game', 213), ('give', 214), ('next', 215), ('remember', 216), ('watch', 217), ('week', 218), ('heart', 219), ('hope', 220), ('into', 221), ('movie', 222), ('please', 223), ('well', 224), ('’', 225), ('actually', 226), ('attack', 227), ('excited', 228), ('horrific', 229), ('those', 230), ('which', 231), ('best', 232), ('dread', 233), ('fire', 234), ('lol', 235), ('restless', 236), ('said', 237), ('team', 238), ('thanks', 239), ('thought', 240), ('war', 241), (':see-no-evil_monkey:', 242), ('<elongated>', 243), ('also', 244), ('big', 245), ('hate', 246), ('help', 247), ('india', 248), ('man', 249), ('nothing', 250), ('phone', 251), ('scared', 252), ('trying', 253), ('years', 254), ('*', 255), ('attacks', 256), ('away', 257), ('guys', 258), ('peace', 259), ('person', 260), ('state', 261), ('while', 262), ('anything', 263), ('come', 264), ('country', 265), ('depression', 266), ('job', 267), ('live', 268), ('morning', 269), ('says', 270), ('story', 271), ('test', 272), ('texans', 273), ('thank', 274), ('use', 275), ('w', 276), ('dream', 277), ('haunt', 278), ('horrid', 279), ('left', 280), ('long', 281), ('many', 282), ('may', 283), ('mean', 284), ('mind', 285), ('talk', 286), ('tell', 287), ('threaten', 288), ('very', 289), ('wait', 290), ('whole', 291), ('wish', 292), ('feeling', 293), ('find', 294), ('girl', 295), ('head', 296), ('made', 297), ('maybe', 298), ('minutes', 299), ('okay', 300), ('pak', 301), ('part', 302), ('play', 303), ('such', 304), ('take', 305), ('<money>', 306), ('against', 307), ('black', 308), ('doing', 309), ('free', 310), ('gbbo', 311), ('getting', 312), ('home', 313), ('problem', 314), ('social', 315), ('terrorist', 316), ('thing', 317), ('white', 318), ('win', 319), ('2', 320), (':flushed_face:', 321), (';', 322), ('believe', 323), ('else', 324), ('everyone', 325), ('experience', 326), ('fuck', 327), ('goes', 328), ('gonna', 329), ('guy', 330), ('half', 331), ('happy', 332), ('interview', 333), ('miss', 334), ('months', 335), ('own', 336), ('pretty', 337), ('quote', 338), ('school', 339), ('side', 340), ('two', 341), ('ur', 342), ('wanna', 343), ('wasn', 344), ('without', 345), ('+', 346), ('already', 347), ('american', 348), ('anyone', 349), ('around', 350), ('both', 351), ('car', 352), ('face', 353), ('feels', 354), ('film', 355), ('friday', 356), ('hands', 357), ('house', 358), ('leadership', 359), ('police', 360), ('re', 361), ('success', 362), ('times', 363), ('unga', 364), ('used', 365), ('watching', 366), ('worse', 367), ('yes', 368), ('call', 369), ('election', 370), ('end', 371), ('faith', 372), ('follow', 373), ('looking', 374), ('omg', 375), ('once', 376), ('problems', 377), ('ready', 378), ('sad', 379), ('saying', 380), ('scary', 381), ('show', 382), ('since', 383), ('sorry', 384), ('started', 385), ('terrific', 386), ('twitter', 387), ('vote', 388), ('working', 389), ('yet', 390), ('0', 391), ('2016', 392), ('<happy>', 393), ('alarming', 394), ('b', 395), ('contact', 396), ('family', 397), ('hesitate', 398), ('human', 399), ('intimidate', 400), ('little', 401), ('making', 402), ('might', 403), ('movies', 404), ('myself', 405), ('nice', 406), ('order', 407), ('past', 408), ('rather', 409), ('speech', 410), ('true', 411), ('turn', 412), ('wow', 413), ('year', 414), ('1', 415), ('3', 416), ('cry', 417), ('days', 418), ('death', 419), ('friends', 420), ('hours', 421), ('indian', 422), ('instead', 423), ('lies', 424), ('money', 425), ('needs', 426), ('ok', 427), ('playing', 428), ('talking', 429), ('tired', 430), ('tv', 431), ('yeah', 432), ('', 433), ('#', 434), (':smiling_face_with_smiling_eyes:', 435), ('absolutely', 436), ('ahs', 437), ('amazing', 438), ('bc', 439), ('charlotte', 440), ('comes', 441), ('discouraged', 442), ('enough', 443), ('fright', 444), ('full', 445), ('gets', 446), ('hair', 447), ('hard', 448), ('hell', 449), ('hey', 450), ('hour', 451), ('islam', 452), ('kill', 453), ('lady', 454), ('late', 455), ('laugh', 456), ('moment', 457), ('month', 458), ('music', 459), ('pay', 460), ('put', 461), ('real', 462), ('reason', 463), ('rooney', 464), ('sick', 465), ('step', 466), ('support', 467), ('together', 468), ('told', 469), ('words', 470), ('worried', 471), ('wrong', 472), ('yo', 473), ('“', 474), (':red_heart:', 475), ('>', 476), ('act', 477), ('almost', 478), ('america', 479), ('asked', 480), ('book', 481), ('care', 482), ('chance', 483), ('city', 484), ('class', 485), ('coming', 486), ('disabled', 487), ('done', 488), ('during', 489), ('fact', 490), ('fail', 491), ('front', 492), ('funny', 493), ('future', 494), ('hillary', 495), ('idea', 496), ('intimidated', 497), ('jump', 498), ('kids', 499), ('literally', 500), ('lose', 501), ('lost', 502), ('matter', 503), ('media', 504), ('nawaz', 505), ('o', 506), ('parents', 507), ('plus', 508), ('point', 509), ('poor', 510), ('president', 511), ('product', 512), ('run', 513), ('shudder', 514), ('though', 515), ('truth', 516), ('understand', 517), ('until', 518), ('weeks', 519), ('went', 520), ('within', 521), ('woman', 522), ('wonder', 523), ('writing', 524), ('yourself', 525), (':downcast_face_with_sweat:', 526), (':face_with_rolling_eyes:', 527), (':light_skin_tone:', 528), (':smiling_face_with_heart-eyes:', 529), ('<date>', 530), ('action', 531), ('ago', 532), ('alone', 533), ('americans', 534), ('article', 535), ('ask', 536), ('ass', 537), ('baby', 538), ('ball', 539), ('beautiful', 540), ('between', 541), ('beyond', 542), ('body', 543), ('business', 544), ('children', 545), ('completely', 546), ('crap', 547), ('crying', 548), ('dad', 549), ('deal', 550), ('dire', 551), ('dreams', 552), ('episode', 553), ('far', 554), ('favorite', 555), ('forward', 556), ('gave', 557), ('gone', 558), ('hand', 559), ('happen', 560), ('happened', 561), ('hold', 562), ('honestly', 563), ('imagine', 564), ('issue', 565), ('jesus', 566), ('joke', 567), ('lot', 568), ('meet', 569), ('men', 570), ('pizza', 571), ('post', 572), ('protest', 573), ('read', 574), ('rojo', 575), ('running', 576), ('season', 577), ('send', 578), ('set', 579), ('sharif', 580), ('soon', 581), ('super', 582), ('texas', 583), ('top', 584), ('tremendous', 585), ('try', 586), ('un', 587), ('under', 588), ('video', 589), ('vs', 590), ('waiting', 591), ('wake', 592), ('wanted', 593), ('wants', 594), ('weak', 595), ('windows', 596), (':grinning_face_with_sweat:', 597), ('<percent>', 598), ('ain', 599), ('anxious', 600), ('author', 601), ('avoid', 602), ('bit', 603), ('booked', 604), ('broken', 605), ('brown', 606), ('buy', 607), ('calling', 608), ('choose', 609), ('clown', 610), ('college', 611), ('debate', 612), ('discourage', 613), ('dog', 614), ('dont', 615), ('drive', 616), ('early', 617), ('etc', 618), ('evening', 619), ('everything', 620), ('express', 621), ('fan', 622), ('fearing', 623), ('feet', 624), ('friend', 625), ('handle', 626), ('hear', 627), ('important', 628), ('kashmir', 629), ('kid', 630), ('loving', 631), ('mad', 632), ('met', 633), ('mom', 634), ('muslims', 635), ('obama', 636), ('online', 637), ('pants', 638), ('park', 639), ('paul', 640), ('pick', 641), ('points', 642), ('power', 643), ('punch', 644), ('revolting', 645), ('room', 646), ('safe', 647), ('security', 648), ('seems', 649), ('seen', 650), ('sense', 651), ('sent', 652), ('seriously', 653), ('stuck', 654), ('sucks', 655), ('tech', 656), ('th', 657), ('thoughts', 658), ('tickets', 659), ('took', 660), ('usa', 661), ('woke', 662), ('women', 663), ('yesterday', 664), ('yr', 665), ('—', 666), (':confused_face:', 667), (':fearful_face:', 668), ('<sad>', 669), ('=', 670), ('afternoon', 671), ('bb', 672), ('beat', 673), ('boy', 674), ('bunch', 675), ('cats', 676), ('challenge', 677), ('change', 678), ('check', 679), ('child', 680), ('clear', 681), ('coffee', 682), ('comments', 683), ('complete', 684), ('courage', 685), ('cut', 686), ('cuz', 687), ('daunting', 688), ('dying', 689), ('emails', 690), ('exactly', 691), ('exist', 692), ('fans', 693), ('fashion', 694), ('father', 695), ('fearful', 696), ('finally', 697), ('forgot', 698), ('form', 699), ('frightened', 700), ('gas', 701), ('ghastly', 702), ('guess', 703), ('halloween', 704), ('happens', 705), ('heard', 706), ('high', 707), ('hit', 708), ('hot', 709), ('immediately', 710), ('internet', 711), ('intimidation', 712), ('john', 713), ('k', 714), ('knowing', 715), ('less', 716), ('line', 717), ('lives', 718), ('lonely', 719), ('looked', 720), ('looks', 721), ('low', 722), ('mine', 723), ('motivation', 724), ('mufc', 725), ('name', 726), ('ni', 727), ('pain', 728), ('probably', 729), ('project', 730), ('queue', 731), ('random', 732), ('realize', 733), ('roommate', 734), ('russia', 735), ('saturday', 736), ('saw', 737), ('self', 738), ('serious', 739), ('situation', 740), ('sometimes', 741), ('south', 742), ('st', 743), ('starting', 744), ('taking', 745), ('tells', 746), ('till', 747), ('towards', 748), ('tried', 749), ('turns', 750), ('united', 751), ('ve', 752), ('violence', 753), ('water', 754), ('wifi', 755), ('worth', 756), ('wtf', 757), ('”', 758), ('4', 759), ('5', 760), (':smiling_face_with_horns:', 761), ('agree', 762), ('air', 763), ('answer', 764), ('app', 765), ('become', 766), ('biggest', 767), ('blair', 768), ('box', 769), ('break', 770), ('breathless', 771), ('bt', 772), ('called', 773), ('card', 774), ('cares', 775), ('cause', 776), ('clinton', 777), ('close', 778), ('coaching', 779), ('company', 780), ('data', 781), ('dead', 782), ('deep', 783), ('dev', 784), ('distrust', 785), ('door', 786), ('dropped', 787), ('due', 788), ('exact', 789), ('extra', 790), ('factor', 791), ('failure', 792), ('fair', 793), ('fall', 794), ('false', 795), ('fest', 796), ('fight', 797), ('flight', 798), ('food', 799), ('foot', 800), ('forget', 801), ('freedom', 802), ('fun', 803), ('girls', 804), ('glad', 805), ('gotta', 806), ('gun', 807), ('gym', 808), ('haha', 809), ('hesitation', 810), ('history', 811), ('hoping', 812), ('huge', 813), ('involved', 814), ('jack', 815), ('keep', 816), ('kick', 817), ('kidding', 818), ('kind', 819), ('kinda', 820), ('latest', 821), ('leaving', 822), ('level', 823), ('living', 824), ('ll', 825), ('lmao', 826), ('local', 827), ('m', 828), ('mary', 829), ('michael', 830), ('mins', 831), ('mode', 832), ('mr', 833), ('na', 834), ('names', 835), ('network', 836), ('nope', 837), ('normal', 838), ('office', 839), ('open', 840), ('paid', 841), ('party', 842), ('penny', 843), ('played', 844), ('pop', 845), ('process', 846), ('promise', 847), ('question', 848), ('r', 849), ('racism', 850), ('radio', 851), ('rate', 852), ('rd', 853), ('reading', 854), ('refugee', 855), ('refugees', 856), ('reverse', 857), ('riots', 858), ('ruining', 859), ('sauce', 860), ('seeing', 861), ('shiver', 862), ('shower', 863), ('silence', 864), ('sleepless', 865), ('sleepy', 866), ('somebody', 867), ('sound', 868), ('stay', 869), ('stress', 870), ('supporting', 871), ('supposed', 872), ('sweet', 873), ('taken', 874), ('thinking', 875), ('tho', 876), ('three', 877), ('tnf', 878), ('total', 879), ('train', 880), ('trash', 881), ('tweet', 882), ('ugh', 883), ('update', 884), ('uri', 885), ('val', 886), ('view', 887), ('walk', 888), ('wall', 889), ('watched', 890), ('website', 891), ('wednesday', 892), ('word', 893), ('worrying', 894), ('ya', 895), ('~', 896), ('–', 897), ('%', 898), ('18', 899), ('8', 900), (':anxious_face_with_sweat:', 901), (':beaming_face_with_smiling_eyes:', 902), (':enraged_face:', 903), (':frowning_face:', 904), (':sad_but_relieved_face:', 905), (':two_hearts:', 906), (':upside-down_face:', 907), ('<emphasis>', 908), ('able', 909), ('absolute', 910), ('advice', 911), ('age', 912), ('airlines', 913), ('anywhere', 914), ('apart', 915), ('arc', 916), ('artist', 917), ('asking', 918), ('atmosphere', 919), ('bake', 920), ('bakewell', 921), ('bed', 922), ('beginning', 923), ('birthday', 924), ('blown', 925), ('bout', 926), ('bringing', 927), ('bs', 928), ('burning', 929), ('c', 930), ('calm', 931), ('campaign', 932), ('carpet', 933), ('carry', 934), ('case', 935), ('centre', 936), ('christ', 937), ('christmas', 938), ('clock', 939), ('cold', 940), ('comic', 941), ('control', 942), ('countries', 943), ('couple', 944), ('creepy', 945), ('cross', 946), ('danish', 947), ('defending', 948), ('democracy', 949), ('destroy', 950), ('different', 951), ('dr', 952), ('dry', 953), ('eat', 954), ('english', 955), ('epic', 956), ('everyday', 957), ('expected', 958), ('fam', 959), ('feature', 960), ('few', 961), ('fighting', 962), ('fill', 963), ('finish', 964), ('finished', 965), ('flat', 966), ('following', 967), ('football', 968), ('fraud', 969), ('freaked', 970), ('gaga', 971), ('ghost', 972), ('global', 973), ('goal', 974), ('gratitude', 975), ('greatest', 976), ('group', 977), ('harm', 978), ('himself', 979), ('hitler', 980), ('however', 981), ('hustle', 982), ('idk', 983), ('inspiring', 984), ('iphone', 985), ('issues', 986), ('keeps', 987), ('key', 988), ('lack', 989), ('leader', 990), ('learn', 991), ('leave', 992), ('listen', 993), ('loves', 994), ('luck', 995), ('lying', 996), ('majority', 997), ('means', 998), ('middle', 999), ('milk', 1000), ('monday', 1001), ('mother', 1002), ('mouth', 1003), ('muslim', 1004), ('must', 1005), ('nap', 1006), ('nd', 1007), ('nearly', 1008), ('nfl', 1009), ('nights', 1010), ('nso', 1011), ('ones', 1012), ('orange', 1013), ('outside', 1014), ('place', 1015), ('player', 1016), ('policy', 1017), ('pray', 1018), ('presidential', 1019), ('price', 1020), ('regular', 1021), ('research', 1022), ('respect', 1023), ('rest', 1024), ('revolution', 1025), ('rich', 1026), ('ride', 1027), ('rip', 1028), ('rn', 1029), ('role', 1030), ('sadly', 1031), ('save', 1032), ('scott', 1033), ('shoot', 1034), ('shows', 1035), ('single', 1036), ('sister', 1037), ('sit', 1038), ('sky', 1039), ('sleeping', 1040), ('smart', 1041), ('soft', 1042), ('song', 1043), ('source', 1044), ('spider', 1045), ('spoke', 1046), ('starts', 1047), ('stopped', 1048), ('stuff', 1049), ('swear', 1050), ('taste', 1051), ('tax', 1052), ('threat', 1053), ('touch', 1054), ('truly', 1055), ('turned', 1056), ('tweeting', 1057), ('twice', 1058), ('typical', 1059), ('usually', 1060), ('walking', 1061), ('x', 1062), ('zero', 1063), ('16', 1064), (':OK_hand:', 1065), (':broken_heart:', 1066), (':face_screaming_in_fear:', 1067), (':folded_hands:', 1068), (':grimacing_face:', 1069), (':grinning_squinting_face:', 1070), (':medium-light_skin_tone:', 1071), (':medium_skin_tone:', 1072), (':neutral_face:', 1073), (':peach:', 1074), (':thumbs_up:', 1075), ('<email>', 1076), ('above', 1077), ('address', 1078), ('afghanistan', 1079), ('airline', 1080), ('allow', 1081), ('along', 1082), ('amount', 1083), ('anger', 1084), ('anniversary', 1085), ('anymore', 1086), ('anyway', 1087), ('appearance', 1088), ('appointment', 1089), ('apprehend', 1090), ('archangel', 1091), ('armed', 1092), ('arms', 1093), ('army', 1094), ('arrive', 1095), ('asleep', 1096), ('assume', 1097), ('astounded', 1098), ('attention', 1099), ('awesome', 1100), ('bag', 1101), ('banter', 1102), ('bars', 1103), ('based', 1104), ('bbc', 1105), ('becoming', 1106), ('bees', 1107), ('behind', 1108), ('berry', 1109), ('bitch', 1110), ('blind', 1111), ('blocks', 1112), ('bloody', 1113), ('boggart', 1114), ('books', 1115), ('bottle', 1116), ('bring', 1117), ('bullying', 1118), ('cable', 1119), ('came', 1120), ('camps', 1121), ('candy', 1122), ('cant', 1123), ('catch', 1124), ('christianity', 1125), ('chronic', 1126), ('clearly', 1127), ('coach', 1128), ('comfortable', 1129), ('commercial', 1130), ('conversation', 1131), ('covering', 1132), ('cray', 1133), ('create', 1134), ('crime', 1135), ('crimes', 1136), ('crisis', 1137), ('crossed', 1138), ('current', 1139), ('damn', 1140), ('dangerous', 1141), ('dark', 1142), ('dating', 1143), ('dear', 1144), ('decent', 1145), ('decided', 1146), ('designer', 1147), ('didnt', 1148), ('distracting', 1149), ('divorce', 1150), ('dj', 1151), ('donald', 1152), ('either', 1153), ('email', 1154), ('embarrassing', 1155), ('entire', 1156), ('everybody', 1157), ('everywhere', 1158), ('ex', 1159), ('fa', 1160), ('facts', 1161), ('fake', 1162), ('fast', 1163), ('fat', 1164), ('favor', 1165), ('festival', 1166), ('ff', 1167), ('ffs', 1168), ('fiction', 1169), ('fine', 1170), ('firm', 1171), ('fish', 1172), ('flipping', 1173), ('focusing', 1174), ('followers', 1175), ('gathering', 1176), ('giving', 1177), ('goals', 1178), ('goat', 1179), ('gotten', 1180), ('green', 1181), ('groups', 1182), ('guns', 1183), ('hahaha', 1184), ('haunting', 1185), ('heads', 1186), ('hi', 1187), ('highlights', 1188), ('holy', 1189), ('hospital', 1190), ('houses', 1191), ('hurt', 1192), ('hut', 1193), ('ill', 1194), ('impression', 1195), ('insane', 1196), ('inspiration', 1197), ('international', 1198), ('invest', 1199), ('iraq', 1200), ('irony', 1201), ('itself', 1202), ('jersey', 1203), ('joy', 1204), ('keys', 1205), ('laser', 1206), ('laughs', 1207), ('learned', 1208), ('least', 1209), ('legs', 1210), ('letting', 1211), ('liar', 1212), ('likes', 1213), ('limited', 1214), ('lord', 1215), ('losing', 1216), ('lunch', 1217), ('manages', 1218), ('massiah', 1219), ('mcdavid', 1220), ('mental', 1221), ('mets', 1222), ('mindfulness', 1223), ('missing', 1224), ('mistake', 1225), ('monster', 1226), ('moses', 1227), ('move', 1228), ('moving', 1229), ('mum', 1230), ('n1peter', 1231), ('nails', 1232), ('nation', 1233), ('nicole', 1234), ('nor', 1235), ('note', 1236), ('nthrow', 1237), ('onto', 1238), ('onus', 1239), ('ordered', 1240), ('organization', 1241), ('others', 1242), ('overnight', 1243), ('painting', 1244), ('peter', 1245), ('pic', 1246), ('pitch', 1247), ('plan', 1248), ('players', 1249), ('polished', 1250), ('political', 1251), ('poop', 1252), ('positive', 1253), ('posted', 1254), ('prejudice', 1255), ('pressure', 1256), ('provider', 1257), ('public', 1258), ('pure', 1259), ('push', 1260), ('pussy', 1261), ('qb', 1262), ('questions', 1263), ('quite', 1264), ('race', 1265), ('rape', 1266), ('raw', 1267), ('refund', 1268), ('requires', 1269), ('results', 1270), ('return', 1271), ('ridiculous', 1272), ('rights', 1273), ('rioters', 1274), ('ripping', 1275), ('roanoke', 1276), ('rolls', 1277), ('round', 1278), ('rude', 1279), ('ruin', 1280), ('runs', 1281), ('sadness', 1282), ('sans', 1283), ('scroll', 1284), ('seek', 1285), ('seem', 1286), ('sensitive', 1287), ('sex', 1288), ('shame', 1289), ('ship', 1290), ('short', 1291), ('shot', 1292), ('showing', 1293), ('sir', 1294), ('sitting', 1295), ('smack', 1296), ('smell', 1297), ('smells', 1298), ('smile', 1299), ('soy', 1300), ('speak', 1301), ('sponsoring', 1302), ('spray', 1303), ('staff', 1304), ('statement', 1305), ('steal', 1306), ('stomach', 1307), ('straight', 1308), ('stupid', 1309), ('subway', 1310), ('suprised', 1311), ('survivor', 1312), ('table', 1313), ('talks', 1314), ('tamra', 1315), ('tea', 1316), ('terrifying', 1317), ('theism', 1318), ('therapist', 1319), ('third', 1320), ('timid', 1321), ('town', 1322), ('training', 1323), ('tremor', 1324), ('trust', 1325), ('tulsa', 1326), ('tweets', 1327), ('uber', 1328), ('uk', 1329), ('unbelievable', 1330), ('university', 1331), ('v', 1332), ('version', 1333), ('videos', 1334), ('visa', 1335), ('vista', 1336), ('walked', 1337), ('wash', 1338), ('wear', 1339), ('weather', 1340), ('weird', 1341), ('whisky', 1342), ('wife', 1343), ('wild', 1344), ('wishing', 1345), ('witch', 1346), ('writer', 1347), ('writers', 1348), ('young', 1349), ('ा', 1350), ('्', 1351), ('…', 1352), ('\\uf629', 1353), ('$', 1354), ('7', 1355), ('9', 1356), (':crying_face:', 1357), (':expressionless_face:', 1358), (':face_with_steam_from_nose:', 1359), (':face_without_mouth:', 1360), (':heart_with_arrow:', 1361), (':person_gesturing_NO:', 1362), (':sleeping_face:', 1363), (':thinking_face:', 1364), ('<', 1365), ('<censored>', 1366), ('<wink>', 1367), (']', 1368), ('^^', 1369), ('absurd', 1370), ('access', 1371), ('acting', 1372), ('adorable', 1373), ('af', 1374), ('agent', 1375), ('aid', 1376), ('aka', 1377), ('alive', 1378), ('angry', 1379), ('animal', 1380), ('announced', 1381), ('annoyed', 1382), ('annoys', 1383), ('anytime', 1384), ('appears', 1385), ('apprehension', 1386), ('artists', 1387), ('atlanta', 1388), ('attempt', 1389), ('awestruck', 1390), ('balochistan', 1391), ('ban', 1392), ('band', 1393), ('basket', 1394), ('bb18', 1395), ('became', 1396), ('beds', 1397), ('behavior', 1398), ('bill', 1399), ('blocked', 1400), ('boots', 1401), ('border', 1402), ('boys', 1403), ('brain', 1404), ('brave', 1405), ('breathe', 1406), ('british', 1407), ('bro', 1408), ('btw', 1409), ('bug', 1410), ('bullet', 1411), ('bus', 1412), ('bush', 1413), ('busy', 1414), ('butter', 1415), ('butterflies', 1416), ('caballero', 1417), ('cake', 1418), ('canada', 1419), ('candidate', 1420), ('cannot', 1421), ('career', 1422), ('carrick', 1423), ('ccot', 1424), ('celtic', 1425), ('certainly', 1426), ('charge', 1427), ('chill', 1428), ('chocolate', 1429), ('chris', 1430), ('christie', 1431), ('chuckles', 1432), ('citizens', 1433), ('community', 1434), ('con', 1435), ('concept', 1436), ('concerned', 1437), ('concerns', 1438), ('condition', 1439), ('connor', 1440), ('considered', 1441), ('continuously', 1442), ('convinced', 1443), ('cool', 1444), ('cops', 1445), ('cos', 1446), ('cover', 1447), ('coward', 1448), ('cower', 1449), ('coworkers', 1450), ('coy', 1451), ('crash', 1452), ('crazy', 1453), ('cried', 1454), ('crushes', 1455), ('cujo', 1456), ('cup', 1457), ('cute', 1458), ('cutting', 1459), ('cycle', 1460), ('daily', 1461), ('damp', 1462), ('danger', 1463), ('dealing', 1464), ('definitely', 1465), ('degree', 1466), ('delayed', 1467), ('deserves', 1468), ('despondency', 1469), ('details', 1470), ('die', 1471), ('discover', 1472), ('discuss', 1473), ('disgusted', 1474), ('doctor', 1475), ('documentary', 1476), ('don', 1477), ('double', 1478), ('download', 1479), ('driver', 1480), ('dumb', 1481), ('dunno', 1482), ('e', 1483), ('easy', 1484), ('elected', 1485), ('emoji', 1486), ('empire', 1487), ('energy', 1488), ('england', 1489), ('ep', 1490), ('epicentre', 1491), ('espresso', 1492), ('events', 1493), ('expect', 1494), ('eyes', 1495), ('f', 1496), ('famous', 1497), ('fashioned', 1498), ('feds', 1499), ('fellow', 1500), ('felt', 1501), ('fi', 1502), ('figuring', 1503), ('films', 1504), ('finale', 1505), ('fingers', 1506), ('fit', 1507), ('fix', 1508), ('flower', 1509), ('foam', 1510), ('follows', 1511), ('force', 1512), ('formidable', 1513), ('fr', 1514), ('freaking', 1515), ('freshman', 1516), ('frontier', 1517), ('fu', 1518), ('fuckin', 1519), ('gang', 1520), ('garbage', 1521), ('gate', 1522), ('gay', 1523), ('george', 1524), ('gives', 1525), ('google', 1526), ('government', 1527), ('grad', 1528), ('hahahah', 1529), ('hairy', 1530), ('hatred', 1531), ('haunted', 1532), ('health', 1533), ('heaven', 1534), ('helped', 1535), ('hide', 1536), ('hmm', 1537), ('holiday', 1538), ('holidays', 1539), ('hrc', 1540), ('humanity', 1541), ('hysteria', 1542), ('ice', 1543), ('icing', 1544), ('id', 1545), ('imagination', 1546), ('impact', 1547), ('impossible', 1548), ('including', 1549), ('indeed', 1550), ('indie', 1551), ('info', 1552), ('ing', 1553), ('injustice', 1554), ('inside', 1555), ('interested', 1556), ('jeans', 1557), ('jk', 1558), ('jobs', 1559), ('journey', 1560), ('junk', 1561), ('khan', 1562), ('killed', 1563), ('king', 1564), ('knew', 1565), ('l', 1566), ('la', 1567), ('lately', 1568), ('later', 1569), ('lead', 1570), ('league', 1571), ('learning', 1572), ('lets', 1573), ('lg', 1574), ('light', 1575), ('lips', 1576), ('listening', 1577), ('lived', 1578), ('london', 1579), ('longer', 1580), ('loosing', 1581), ('losers', 1582), ('lovely', 1583), ('main', 1584), ('manager', 1585), ('market', 1586), ('married', 1587), ('masses', 1588), ('match', 1589), ('math', 1590), ('matty', 1591), ('mea', 1592), ('meal', 1593), ('meds', 1594), ('melted', 1595), ('memory', 1596), ('mention', 1597), ('mild', 1598), ('million', 1599), ('minus', 1600), ('mob', 1601), ('mobile', 1602), ('momentum', 1603), ('mourinho', 1604), ('msm', 1605), ('murder', 1606), ('naked', 1607), ('narcissist', 1608), ('nbut', 1609), ('ne', 1610), ('near', 1611), ('nervousness', 1612), ('networking', 1613), ('nl', 1614), ('nobody', 1615), ('nthat', 1616), ('obviously', 1617), ('october', 1618), ('ohio', 1619), ('opened', 1620), ('opinion', 1621), ('original', 1622), ('overcoming', 1623), ('pablo', 1624), ('pal', 1625), ('paper', 1626), ('per', 1627), ('perfect', 1628), ('performance', 1629), ('pics', 1630), ('piece', 1631), ('plz', 1632), ('polish', 1633), ('posts', 1634), ('powerful', 1635), ('present', 1636), ('press', 1637), ('prime', 1638), ('probs', 1639), ('propaganda', 1640), ('protests', 1641), ('proud', 1642), ('prove', 1643), ('puns', 1644), ('quality', 1645), ('queen', 1646), ('quick', 1647), ('racing', 1648), ('racist', 1649), ('rap', 1650), ('rapper', 1651), ('reach', 1652), ('realising', 1653), ('reality', 1654), ('realized', 1655), ('received', 1656), ('regret', 1657), ('relationship', 1658), ('replaced', 1659), ('reply', 1660), ('resolved', 1661), ('respond', 1662), ('ridiculously', 1663), ('ring', 1664), ('riot', 1665), ('rt', 1666), ('sale', 1667), ('second', 1668), ('seconds', 1669), ('selection', 1670), ('several', 1671), ('shock', 1672), ('shooting', 1673), ('shut', 1674), ('signal', 1675), ('signed', 1676), ('silly', 1677), ('simple', 1678), ('size', 1679), ('slash', 1680), ('slogan', 1681), ('slow', 1682), ('small', 1683), ('smoke', 1684), ('society', 1685), ('sold', 1686), ('somewhere', 1687), ('sort', 1688), ('space', 1689), ('spooky', 1690), ('spread', 1691), ('stand', 1692), ('stanley', 1693), ('station', 1694), ('stepped', 1695), ('stressed', 1696), ('strong', 1697), ('struggle', 1698), ('students', 1699), ('study', 1700), ('suddenly', 1701), ('suggest', 1702), ('sunday', 1703), ('suspicion', 1704), ('sutton', 1705), ('sweat', 1706), ('syria', 1707), ('system', 1708), ('takes', 1709), ('tall', 1710), ('tan', 1711), ('tastes', 1712), ('tattoo', 1713), ('tears', 1714), ('term', 1715), ('terrify', 1716), ('terrorists', 1717), ('thinks', 1718), ('thousands', 1719), ('throat', 1720), ('thursday', 1721), ('ticket', 1722), ('tolerance', 1723), ('tough', 1724), ('track', 1725), ('treat', 1726), ('tryna', 1727), ('tuesday', 1728), ('ty', 1729), ('universal', 1730), ('unknown', 1731), ('upset', 1732), ('upside', 1733), ('useless', 1734), ('users', 1735), ('using', 1736), ('via', 1737), ('victim', 1738), ('vile', 1739), ('virgin', 1740), ('voices', 1741), ('voters', 1742), ('wakeup', 1743), ('wave', 1744), ('wch', 1745), ('wearing', 1746), ('werewolves', 1747), ('whatever', 1748), ('whats', 1749), ('whether', 1750), ('williams', 1751), ('wing', 1752), ('winning', 1753), ('wins', 1754), ('winter', 1755), ('won', 1756), ('works', 1757), ('wreck', 1758), ('write', 1759), ('written', 1760), ('y', 1761), ('yay', 1762), ('youre', 1763), ('»', 1764), ('―', 1765), ('‘', 1766), ('/:', 1767), ('10', 1768), ('20', 1769), ('500', 1770), ('6', 1771), ('90', 1772), (':angry_face:', 1773), (':clapper_board:', 1774), (':double_exclamation_mark:', 1775), (':eyes:', 1776), (':face_blowing_a_kiss:', 1777), (':face_with_open_mouth:', 1778), (':flexed_biceps:', 1779), (':frowning_face_with_open_mouth:', 1780), (':ghost:', 1781), (':grinning_face:', 1782), (':grinning_face_with_big_eyes:', 1783), (':kissing_face_with_smiling_eyes:', 1784), (':pensive_face:', 1785), (':performing_arts:', 1786), (':person_biking:', 1787), (':pile_of_poo:', 1788), (':sleepy_face:', 1789), (':slightly_smiling_face:', 1790), (':smiling_face_with_open_hands:', 1791), (':soccer_ball:', 1792), (':speak-no-evil_monkey:', 1793), (':spider:', 1794), (':victory_hand:', 1795), (':winking_face:', 1796), ('<annoyed>', 1797), ('<laugh>', 1798), ('<surprise>', 1799), ('@', 1800), ('[', 1801), ('^', 1802), ('aa', 1803), ('abdo', 1804), ('aberdeen', 1805), ('ability', 1806), ('abood', 1807), ('accept', 1808), ('acceptable', 1809), ('accepted', 1810), ('accounts', 1811), ('accrington', 1812), ('acknowledgement', 1813), ('across', 1814), ('active', 1815), ('activism', 1816), ('activity', 1817), ('actual', 1818), ('addair', 1819), ('addition', 1820), ('addressing', 1821), ('admin', 1822), ('admire', 1823), ('admitted', 1824), ('adrenaline', 1825), ('adulthood', 1826), ('advise', 1827), ('advisor', 1828), ('affair', 1829), ('afford', 1830), ('agenda', 1831), ('ages', 1832), ('aghast', 1833), ('ahmad', 1834), ('ahs6', 1835), ('ai', 1836), ('aiding', 1837), ('airbnb', 1838), ('airplane', 1839), ('aishwarya', 1840), ('alarmist', 1841), ('albums', 1842), ('alghul', 1843), ('align', 1844), ('allen', 1845), ('allows', 1846), ('almond', 1847), ('alternate', 1848), ('alternative', 1849), ('amal', 1850), ('amateurs', 1851), ('americano', 1852), ('amy', 1853), ('android', 1854), ('angelic', 1855), ('angels', 1856), ('animator', 1857), ('anime', 1858), ('answers', 1859), ('anthony', 1860), ('antisocial', 1861), ('anu', 1862), ('anybody', 1863), ('anychance', 1864), ('ap', 1865), ('apartment', 1866), ('apleasedont', 1867), ('apocalypse', 1868), ('appauling', 1869), ('appeal', 1870), ('appologies', 1871), ('appreciate', 1872), ('apprehensive', 1873), ('armageddon', 1874), ('arrivals', 1875), ('arrived', 1876), ('arriving', 1877), ('asap', 1878), ('assholes', 1879), ('assignment', 1880), ('assistance', 1881), ('assistant', 1882), ('atleast', 1883), ('atrocious', 1884), ('attended', 1885), ('attraction', 1886), ('attractive', 1887), ('australian', 1888), ('autocorrect', 1889), ('autoritaire', 1890), ('autumn', 1891), ('avanquest', 1892), ('average', 1893), ('averaging', 1894), ('awaits', 1895), ('awaken', 1896), ('awakening', 1897), ('awkward', 1898), ('bachchan', 1899), ('backward', 1900), ('balls', 1901), ('baluchistan', 1902), ('bands', 1903), ('barista', 1904), ('base', 1905), ('basement', 1906), ('basic', 1907), ('basketball', 1908), ('basshunter', 1909), ('bat', 1910), ('bayern', 1911), ('bbcan', 1912), ('beast', 1913), ('beatles', 1914), ('begins', 1915), ('behold', 1916), ('belichick', 1917), ('believed', 1918), ('bella', 1919), ('ben', 1920), ('bender', 1921), ('bi', 1922), ('biblemotivate', 1923), ('bike', 1924), ('biker', 1925), ('bilal', 1926), ('bills', 1927), ('bin', 1928), ('binary', 1929), ('biology', 1930), ('birth', 1931), ('bla', 1932), ('blame', 1933), ('blamed', 1934), ('blargh', 1935), ('ble', 1936), ('blessings', 1937), ('blinders', 1938), ('block', 1939), ('blowing', 1940), ('blvd', 1941), ('bomb', 1942), ('bombing', 1943), ('bored', 1944), ('bother', 1945), ('bought', 1946), ('bound', 1947), ('bowers', 1948), ('boxes', 1949), ('bp', 1950), ('brace', 1951), ('brad', 1952), ('bread', 1953), ('breakdown', 1954), ('breath', 1955), ('brendan', 1956), ('brendon', 1957), ('brendonati', 1958), ('bride', 1959), ('bridge', 1960), ('brien', 1961), ('brighton', 1962), ('brilliant', 1963), ('broadcasted', 1964), ('brock', 1965), ('broke', 1966), ('brother', 1967), ('bu', 1968), ('buble', 1969), ('buffy', 1970), ('buggy', 1971), ('building', 1972), ('bullied', 1973), ('bullshit', 1974), ('burhan', 1975), ('burners', 1976), ('buses', 1977), ('buying', 1978), ('cacking', 1979), ('cafe', 1980), ('cage', 1981), ('cages', 1982), ('calls', 1983), ('camera', 1984), ('cancel', 1985), ('candles', 1986), ('carr', 1987), ('carried', 1988), ('carries', 1989), ('cashier', 1990), ('cast', 1991), ('celeb', 1992), ('celebrities', 1993), ('cemetery', 1994), ('censored', 1995), ('censorship', 1996), ('center', 1997), ('ceo', 1998), ('chair', 1999), ('champagne', 2000), ('championships', 2001), ('champs', 2002), ('chances', 2003), ('changed', 2004), ('changing', 2005), ('chaos', 2006), ('characteristic', 2007), ('charged', 2008), ('chart', 2009), ('chase', 2010), ('chat', 2011), ('chavs', 2012), ('checked', 2013), ('cheese', 2014), ('chelsea', 2015), ('chemical', 2016), ('chick', 2017), ('chip', 2018), ('chips', 2019), ('choice', 2020), ('chrome', 2021), ('church', 2022), ('citizen', 2023), ('civilians', 2024), ('classes', 2025), ('cleveland', 2026), ('clients', 2027), ('clooney', 2028), ('clothes', 2029), ('cloud', 2030), ('clowns', 2031), ('clue', 2032), ('co', 2033), ('coached', 2034), ('coaches', 2035), ('cobblers', 2036), ('code', 2037), ('coding', 2038), ('coincidence', 2039), ('cole', 2040), ('colin', 2041), ('collect', 2042), ('com', 2043), ('combined', 2044), ('commercials', 2045), ('common', 2046), ('communication', 2047), ('compensation', 2048), ('complain', 2049), ('complaint', 2050), ('complex', 2051), ('computer', 2052), ('comrades', 2053), ('concessions', 2054), ('conditioner', 2055), ('confidence', 2056), ('confirm', 2057), ('congrats', 2058), ('connector', 2059), ('cons', 2060), ('considering', 2061), ('constructive', 2062), ('contacted', 2063), ('contagious', 2064), ('contributions', 2065), ('conversion', 2066), ('convince', 2067), ('cooked', 2068), ('cooper', 2069), ('coping', 2070), ('copy', 2071), ('corey', 2072), ('cosmos', 2073), ('costs', 2074), ('county', 2075), ('course', 2076), ('court', 2077), ('cowan', 2078), ('cowboys', 2079), ('crashes', 2080), ('created', 2081), ('credentials', 2082), ('credit', 2083), ('creeps', 2084), ('cringe', 2085), ('critical', 2086), ('criticism', 2087), ('criticize', 2088), ('crossover', 2089), ('crush', 2090), ('crutcher', 2091), ('cunt', 2092), ('curbing', 2093), ('cure', 2094), ('cursed', 2095), ('curtis', 2096), ('cyber', 2097), ('cyborg', 2098), ('dafron', 2099), ('daft', 2100), ('dalai', 2101), ('damnit', 2102), ('dance', 2103), ('dancing', 2104), ('dapl', 2105), ('dardanella', 2106), ('darkness', 2107), ('darth', 2108), ('dawson', 2109), ('daytime', 2110), ('dbacks', 2111), ('deadtime', 2112), ('deatheaters', 2113), ('decaf', 2114), ('deck', 2115), ('deet', 2116), ('defect', 2117), ('defender', 2118), ('defense', 2119), ('deflect', 2120), ('degrees', 2121), ('delivered', 2122), ('demons', 2123), ('denial', 2124), ('dentist', 2125), ('department', 2126), ('deplorable', 2127), ('dept', 2128), ('depth', 2129), ('depths', 2130), ('describe', 2131), ('deserved', 2132), ('design', 2133), ('designated', 2134), ('designed', 2135), ('destined', 2136), ('destroyed', 2137), ('determined', 2138), ('detox', 2139), ('devices', 2140), ('devil', 2141), ('devotee', 2142), ('dey', 2143), ('dictator', 2144), ('died', 2145), ('difference', 2146), ('dimension', 2147), ('directly', 2148), ('disappointing', 2149), ('disaster', 2150), ('discipleship', 2151), ('disco', 2152), ('dishes', 2153), ('dismal', 2154), ('display', 2155), ('distraction', 2156), ('diverted', 2157), ('division', 2158), ('dk', 2159), ('dlc', 2160), ('dm', 2161), ('dms', 2162), ('doctors', 2163), ('doesn', 2164), ('doesnt', 2165), ('dogs', 2166), ('doom', 2167), ('doors', 2168), ('doubt', 2169), ('dough', 2170), ('dps', 2171), ('dragged', 2172), ('drain', 2173), ('drained', 2174), ('drams', 2175), ('drinking', 2176), ('driving', 2177), ('drop', 2178), ('drove', 2179), ('drowning', 2180), ('drunk', 2181), ('dual', 2182), ('dufftown', 2183), ('dutch', 2184), ('dwork', 2185), ('dye', 2186), ('dynamic', 2187), ('dyson', 2188), ('e7', 2189), ('ea', 2190), ('each', 2191), ('easily', 2192), ('east', 2193), ('easyjet', 2194), ('eating', 2195), ('eats', 2196), ('echoes', 2197), ('editing', 2198), ('edition', 2199), ('edm', 2200), ('edt', 2201), ('effective', 2202), ('effer', 2203), ('eft', 2204), ('eh', 2205), ('ek', 2206), ('elder', 2207), ('electron', 2208), ('elephant', 2209), ('elouise', 2210), ('embracing', 2211), ('embrrssd', 2212), ('employees', 2213), ('employment', 2214), ('empty', 2215), ('ending', 2216), ('enjoying', 2217), ('entirely', 2218), ('eq', 2219), ('equestrian', 2220), ('equity', 2221), ('error', 2222), ('ethic', 2223), ('europe', 2224), ('eventually', 2225), ('evidently', 2226), ('except', 2227), ('excitement', 2228), ('excuse', 2229), ('excuses', 2230), ('exemplary', 2231), ('exhausted', 2232), ('exists', 2233), ('expat', 2234), ('expose', 2235), ('exposed', 2236), ('expression', 2237), ('exterior', 2238), ('eye', 2239), ('f1', 2240), ('facetime', 2241), ('facility', 2242), ('facing', 2243), ('factory', 2244), ('fails', 2245), ('faint', 2246), ('falling', 2247), ('fancy', 2248), ('fargo', 2249), ('fathom', 2250), ('fav', 2251), ('fb', 2252), ('fc', 2253), ('fears', 2254), ('febreeze', 2255), ('fed', 2256), ('feed', 2257), ('felon', 2258), ('ferriss', 2259), ('feud', 2260), ('fewer', 2261), ('fey', 2262), ('fiance', 2263), ('field', 2264), ('fights', 2265), ('files', 2266), ('final', 2267), ('finding', 2268), ('finland', 2269), ('fired', 2270), ('fla', 2271), ('flashbacks', 2272), ('flights', 2273), ('flirting', 2274), ('flourish', 2275), ('flutter', 2276), ('flying', 2277), ('fml', 2278), ('fnaf', 2279), ('focus', 2280), ('forces', 2281), ('forever', 2282), ('forgiveness', 2283), ('forming', 2284), ('forms', 2285), ('fosu', 2286), ('foundation', 2287), ('foundational', 2288), ('foursquare', 2289), ('fox', 2290), ('fr1005', 2291), ('freaky', 2292), ('freeway', 2293), ('fresh', 2294), ('freshers', 2295), ('fricking', 2296), ('frighten', 2297), ('frk', 2298), ('froch', 2299), ('fruit', 2300), ('fucked', 2301), ('fuels', 2302), ('fufill', 2303), ('fuller', 2304), ('funding', 2305), ('furniture', 2306), ('furthest', 2307), ('futuristic', 2308), ('gabriella', 2309), ('gad', 2310), ('gadot', 2311), ('gag', 2312), ('gahh', 2313), ('galore', 2314), ('gameface', 2315), ('gameplay', 2316), ('gamer', 2317), ('games', 2318), ('gear', 2319), ('generation', 2320), ('genius', 2321), ('genocide', 2322), ('georgia', 2323), ('germany', 2324), ('germs', 2325), ('giggle', 2326), ('ginger', 2327), ('girly', 2328), ('goodbye', 2329), ('goodmorning', 2330), ('gopats', 2331), ('gorre', 2332), ('gosh', 2333), ('gotham', 2334), ('govt', 2335), ('gp', 2336), ('grace', 2337), ('grande', 2338), ('grendel', 2339), ('grind', 2340), ('gross', 2341), ('ground', 2342), ('growthhacking', 2343), ('gryffindor', 2344), ('gt', 2345), ('gtfo', 2346), ('guise', 2347), ('ha', 2348), ('habit', 2349), ('habitual', 2350), ('hacks', 2351), ('haddon', 2352), ('hall', 2353), ('hangover', 2354), ('har', 2355), ('harassed', 2356), ('hardy', 2357), ('hart', 2358), ('hawking', 2359), ('header', 2360), ('healing', 2361), ('healthcare', 2362), ('hearing', 2363), ('heartbreaking', 2364), ('heather', 2365), ('held', 2366), ('helpful', 2367), ('helps', 2368), ('herbalife', 2369), ('herself', 2370), ('highly', 2371), ('hipp', 2372), ('hire', 2373), ('ho', 2374), ('hoax', 2375), ('hoco', 2376), ('holly', 2377), ('homefront', 2378), ('hometown', 2379), ('homosexuality', 2380), ('hooked', 2381), ('hopelessness', 2382), ('hopkins', 2383), ('hostile', 2384), ('howard', 2385), ('howl', 2386), ('hsm2', 2387), ('hug', 2388), ('humane', 2389), ('humanism', 2390), ('humans', 2391), ('hun', 2392), ('hung', 2393), ('hunger', 2394), ('hungry', 2395), ('huns', 2396), ('hunted', 2397), ('hunters', 2398), ('hurts', 2399), ('husband', 2400), ('hv', 2401), ('ibra', 2402), ('ideas', 2403), ('idina', 2404), ('ignorant', 2405), ('ignored', 2406), ('ihave', 2407), ('illusion', 2408), ('immediate', 2409), ('immigrant', 2410), ('impending', 2411), ('implantation', 2412), ('imposuture', 2413), ('inauthentic', 2414), ('incognito', 2415), ('independent', 2416), ('indians', 2417), ('individual', 2418), ('induce', 2419), ('induction', 2420), ('industry', 2421), ('inept', 2422), ('inferno', 2423), ('informed', 2424), ('inhaler', 2425), ('inner', 2426), ('inquiries', 2427), ('insect', 2428), ('insomnia', 2429), ('inspired', 2430), ('insult', 2431), ('intellectuelle', 2432), ('interesting', 2433), ('intimate', 2434), ('intuition', 2435), ('irrelevant', 2436), ('isis', 2437), ('islamic', 2438), ('island', 2439), ('isp', 2440), ('item', 2441), ('items', 2442), ('ivan', 2443), ('ive', 2444), ('jaded', 2445), ('jail', 2446), ('jerk', 2447), ('jesusisthesubject', 2448), ('jimmy', 2449), ('joey', 2450), ('johnstone', 2451), ('jorge', 2452), ('jose', 2453), ('joyce', 2454), ('jr', 2455), ('judge', 2456), ('jumanji', 2457), ('jumpstreet', 2458), ('justice', 2459), ('kapernick', 2460), ('kapoor', 2461), ('keeper', 2462), ('keeping', 2463), ('keith', 2464), ('kessler', 2465), ('kevin', 2466), ('kidney', 2467), ('killary', 2468), ('kin26mins', 2469), ('kirribilli', 2470), ('kissed', 2471), ('kit', 2472), ('kittens', 2473), ('knife', 2474), ('knowledge', 2475), ('knows', 2476), ('kratom', 2477), ('ks', 2478), ('kyle', 2479), ('label', 2480), ('labour', 2481), ('laden', 2482), ('lam', 2483), ('lama', 2484), ('lame', 2485), ('lamont', 2486), ('land', 2487), ('large', 2488), ('laughing', 2489), ('laughter', 2490), ('laura', 2491), ('law', 2492), ('laying', 2493), ('lazy', 2494), ('leaders', 2495), ('leads', 2496), ('leaks', 2497), ('lecturing', 2498), ('ledge', 2499), ('leg', 2500), ('legacy', 2501), ('lemme', 2502), ('lens', 2503), ('lessens', 2504), ('letdown', 2505), ('levels', 2506), ('liars', 2507), ('lib', 2508), ('libel', 2509), ('libeling', 2510), ('lifeguard', 2511), ('lifer', 2512), ('liked', 2513), ('lining', 2514), ('lit', 2515), ('litigator', 2516), ('livesmatter', 2517), ('lmfao', 2518), ('lo', 2519), ('loaf', 2520), ('locked', 2521), ('locks', 2522), ('loneliness', 2523), ('longing', 2524), ('loompa', 2525), ('loos', 2526), ('looting', 2527), ('loser', 2528), ('loughborough', 2529), ('loved', 2530), ('loyal', 2531), ('lsat', 2532), ('ltraffic', 2533), ('lundqvist', 2534), ('lure', 2535), ('luthansa', 2536), ('lysol', 2537), ('lérance', 2538), ('ma', 2539), ('madness', 2540), ('madrid', 2541), ('magpies', 2542), ('major', 2543), ('makeover', 2544), ('male', 2545), ('malik', 2546), ('mammal', 2547), ('managers', 2548), ('manufacture', 2549), ('map', 2550), ('marbles', 2551), ('mark', 2552), ('marshmallow', 2553), ('mass', 2554), ('massacre', 2555), ('massif', 2556), ('masters', 2557), ('matches', 2558), ('mate', 2559), ('matters', 2560), ('maxlevelz', 2561), ('mcds', 2562), ('meditation', 2563), ('meeting', 2564), ('melbourne', 2565), ('mello', 2566), ('meltdown', 2567), ('members', 2568), ('memorized', 2569), ('memphis', 2570), ('mentality', 2571), ('mercy', 2572), ('mesquite', 2573), ('messed', 2574), ('messy', 2575), ('meter', 2576), ('methods', 2577), ('mexico', 2578), ('meyer', 2579), ('mfa', 2580), ('mic', 2581), ('mid', 2582), ('migraine', 2583), ('miles', 2584), ('mill', 2585), ('mindset', 2586), ('mint', 2587), ('minute', 2588), ('miracle', 2589), ('misogynist', 2590), ('misogyny', 2591), ('missed', 2592), ('misses', 2593), ('missiles', 2594), ('mission', 2595), ('mixture', 2596), ('mlb', 2597), ('mm', 2598), ('mmt', 2599), ('model', 2600), ('momma', 2601), ('mongering', 2602), ('monkhill', 2603), ('mood', 2604), ('moon', 2605), ('moth', 2606), ('motlop', 2607), ('moto', 2608), ('moved', 2609), ('ms', 2610), ('mths', 2611), ('mtv', 2612), ('multitasking', 2613), ('multitude', 2614), ('murdered', 2615), ('murderer', 2616), ('murray', 2617), ('musicians', 2618), ('nam', 2619), ('nand', 2620), ('narley', 2621), ('nasty', 2622), ('national', 2623), ('nc', 2624), ('needed', 2625), ('negative', 2626), ('neighborhood', 2627), ('neighbours', 2628), ('neither', 2629), ('netflix', 2630), ('neurological', 2631), ('neveryone', 2632), ('nisn', 2633), ('nit', 2634), ('nkelly', 2635), ('nlikewise', 2636), ('nno', 2637), ('norms', 2638), ('northampton', 2639), ('nowadays', 2640), ('ns', 2641), ('nshe', 2642), ('nspeaking', 2643), ('nstephen', 2644), ('nt', 2645), ('nthis', 2646), ('ntoday', 2647), ('nuclear', 2648), ('number', 2649), ('nutrition', 2650), ('nv', 2651), ('nwe', 2652), ('ny', 2653), ('nyc', 2654), ('nyssa', 2655), ('obrien', 2656), ('odell', 2657), ('offensive', 2658), ('offered', 2659), ('offering', 2660), ('officer', 2661), ('official', 2662), ('oil', 2663), ('oligarchy', 2664), ('oompa', 2665), ('openly', 2666), ('opera', 2667), ('operation', 2668), ('opportunity', 2669), ('option', 2670), ('options', 2671), ('oregon', 2672), ('oriole', 2673), ('ortiz', 2674), ('osama', 2675), ('outfit', 2676), ('oven', 2677), ('overall', 2678), ('overcome', 2679), ('overseas', 2680), ('overtime', 2681), ('overweight', 2682), ('p2', 2683), ('package', 2684), ('page', 2685), ('pages', 2686), ('pakistani', 2687), ('palace', 2688), ('pale', 2689), ('pam', 2690), ('paranormal', 2691), ('parked', 2692), ('parking', 2693), ('partner', 2694), ('passenger', 2695), ('passing', 2696), ('pastor', 2697), ('patronus', 2698), ('pats', 2699), ('pave', 2700), ('payment', 2701), ('pcola', 2702), ('peanut', 2703), ('peppermint', 2704), ('perception', 2705), ('perilous', 2706), ('perils', 2707), ('permit', 2708), ('personnalité', 2709), ('pert', 2710), ('pervert', 2711), ('philosophy', 2712), ('photo', 2713), ('photography', 2714), ('photos', 2715), ('physically', 2716), ('picasso', 2717), ('picking', 2718), ('pie', 2719), ('pile', 2720), ('ping', 2721), ('pitched', 2722), ('plain', 2723), ('planet', 2724), ('planned', 2725), ('plug', 2726), ('plur', 2727), ('pockets', 2728), ('poet', 2729), ('pol', 2730), ('politics', 2731), ('polls', 2732), ('polygamy', 2733), ('pontefract', 2734), ('pool', 2735), ('poorest', 2736), ('population', 2737), ('position', 2738), ('possess', 2739), ('potatoes', 2740), ('potential', 2741), ('ppl', 2742), ('preaching', 2743), ('pregnant', 2744), ('preordered', 2745), ('presses', 2746), ('prevail', 2747), ('prices', 2748), ('primarily', 2749), ('princess', 2750), ('principle', 2751), ('prison', 2752), ('privacy', 2753), ('pro', 2754), ('probability', 2755), ('probz', 2756), ('producer', 2757), ('producers', 2758), ('productive', 2759), ('professionalism', 2760), ('profiling', 2761), ('profoundly', 2762), ('program', 2763), ('projection', 2764), ('promotes', 2765), ('proposed', 2766), ('prosecute', 2767), ('protection', 2768), ('protein', 2769), ('préjugés', 2770), ('ps', 2771), ('ps4pro', 2772), ('psychiatrist', 2773), ('psychology', 2774), ('psychopath', 2775), ('ptp', 2776), ('ptsd', 2777), ('publishing', 2778), ('puff', 2779), ('pull', 2780), ('pulls', 2781), ('pumped', 2782), ('pundit', 2783), ('punt', 2784), ('puppet', 2785), ('purgatory', 2786), ('putin', 2787), ('q', 2788), ('quarter', 2789), ('queues', 2790), ('quickie', 2791), ('quickly', 2792), ('quiet', 2793), ('quotes', 2794), ('radical', 2795), ('rage', 2796), ('raging', 2797), ('rahami', 2798), ('rai', 2799), ('ramping', 2800), ('ran', 2801), ('ranbir', 2802), ('rashford', 2803), ('reactivate', 2804), ('realistic', 2805), ('rearrange', 2806), ('reasonable', 2807), ('reassigned', 2808), ('recent', 2809), ('recently', 2810), ('recluse', 2811), ('recognizing', 2812), ('recommended', 2813), ('record', 2814), ('recording', 2815), ('redstate', 2816), ('reduce', 2817), ('reelected', 2818), ('ref', 2819), ('referees', 2820), ('referred', 2821), ('reflective', 2822), ('refs', 2823), ('refuse', 2824), ('refuses', 2825), ('regardless', 2826), ('regretting', 2827), ('reign', 2828), ('reilly', 2829), ('rejects', 2830), ('relief', 2831), ('religions', 2832), ('remains', 2833), ('remedy', 2834), ('remover', 2835), ('rent', 2836), ('rental', 2837), ('repainted', 2838), ('report', 2839), ('republicans', 2840), ('requested', 2841), ('requirements', 2842), ('resettlement', 2843), ('resident', 2844), ('responded', 2845), ('response', 2846), ('retarded', 2847), ('retire', 2848), ('retitled', 2849), ('returning', 2850), ('retweets', 2851), ('rev', 2852), ('reveal', 2853), ('revelation', 2854), ('revels', 2855), ('rh', 2856), ('rhobh', 2857), ('rider', 2858), ('ridicule', 2859), ('rigged', 2860), ('rioting', 2861), ('risk', 2862), ('rivertide', 2863), ('rlly', 2864), ('rns', 2865), ('robbery', 2866), ('robbie', 2867), ('robinson', 2868), ('robot', 2869), ('rock', 2870), ('rockefeller', 2871), ('rockies', 2872), ('roles', 2873), ('romero', 2874), ('rooting', 2875), ('roots', 2876), ('rp', 2877), ('rudeness', 2878), ('ruled', 2879), ('runway', 2880), ('rush', 2881), ('rushford', 2882), ('russell', 2883), ('russian', 2884), ('rusty', 2885), ('ryan', 2886), ('ryanair', 2887), ('s6', 2888), ('safety', 2889), ('saga', 2890), ('sales', 2891), ('sam', 2892), ('sanders', 2893), ('sat', 2894), ('sauced', 2895), ('sc', 2896), ('scariest', 2897), ('sci', 2898), ('scored', 2899), ('scotland', 2900), ('scream', 2901), ('screaming', 2902), ('screams', 2903), ('script', 2904), ('scripts', 2905), ('sdr', 2906), ('sea', 2907), ('search', 2908), ('searching', 2909), ('seaside', 2910), ('secondary', 2911), ('secretary', 2912), ('sect', 2913), ('seeds', 2914), ('semester', 2915), ('sending', 2916), ('senses', 2917), ('sensitivity', 2918), ('sensors', 2919), ('sentenced', 2920), ('seol', 2921), ('series', 2922), ('server', 2923), ('services', 2924), ('setting', 2925), ('sewage', 2926), ('sexism', 2927), ('sexuality', 2928), ('shall', 2929), ('shamefull', 2930), ('shampooing', 2931), ('share', 2932), ('sharia', 2933), ('shawty', 2934), ('shd', 2935), ('shes', 2936), ('shew', 2937), ('shift', 2938), ('shine', 2939), ('shitting', 2940), ('shook', 2941), ('shop', 2942), ('shots', 2943), ('shoulder', 2944), ('shoutout', 2945), ('showboating', 2946), ('showed', 2947), ('shriekfest', 2948), ('sickness', 2949), ('sight', 2950), ('sightedness', 2951), ('sign', 2952), ('sike', 2953), ('silent', 2954), ('silver', 2955), ('sim', 2956), ('similar', 2957), ('simply', 2958), ('sims', 2959), ('singing', 2960), ('sink', 2961), ('situations', 2962), ('skeptical', 2963), ('skill', 2964), ('skip', 2965), ('slacking', 2966), ('slagging', 2967), ('slide', 2968), ('slowly', 2969), ('smash', 2970), ('smelly', 2971), ('smh', 2972), ('smiled', 2973), ('smooth', 2974), ('sna', 2975), ('sneaking', 2976), ('soaking', 2977), ('soccer', 2978), ('software', 2979), ('solutions', 2980), ('solving', 2981), ('somehow', 2982), ('sooner', 2983), ('sorted', 2984), ('sounds', 2985), ('spain', 2986), ('spam', 2987), ('speaking', 2988), ('speechless', 2989), ('spell', 2990), ('spent', 2991), ('spidey', 2992), ('spirit', 2993), ('spirits', 2994), ('sporadic', 2995), ('sporadically', 2996), ('sport', 2997), ('sports', 2998), ('spot', 2999), ('spying', 3000), ('standstill', 3001), ('stayed', 3002), ('steak', 3003), ('steals', 3004), ('steamworks', 3005), ('steele', 3006), ('sticking', 3007), ('stinks', 3008), ('stolen', 3009), ('stood', 3010), ('store', 3011), ('stores', 3012), ('strange', 3013), ('strangers', 3014), ('street', 3015), ('stressful', 3016), ('strongly', 3017), ('strumbellas', 3018), ('student', 3019), ('style', 3020), ('submit', 3021), ('subtitles', 3022), ('sucker', 3023), ('sue', 3024), ('suffocating', 3025), ('sugar', 3026), ('suites', 3027), ('summit', 3028), ('supper', 3029), ('supporter', 3030), ('supporters', 3031), ('supports', 3032), ('surely', 3033), ('surprised', 3034), ('surprising', 3035), ('surprisingly', 3036), ('surrounded', 3037), ('sushi', 3038), ('suspension', 3039), ('sweden', 3040), ('swimmers', 3041), ('swooped', 3042), ('sworn', 3043), ('swung', 3044), ('sydney', 3045), ('synonymous', 3046), ('t_t', 3047), ('ta', 3048), ('tactics', 3049), ('tag', 3050), ('talented', 3051), ('talksport', 3052), ('tangled', 3053), ('tape', 3054), ('tapping', 3055), ('target', 3056), ('targeting', 3057), ('tart', 3058), ('tarts', 3059), ('task', 3060), ('tasting', 3061), ('tastings', 3062), ('taught', 3063), ('taxes', 3064), ('tbh', 3065), ('tcot', 3066), ('teach', 3067), ('teams', 3068), ('tear', 3069), ('technical', 3070), ('ted', 3071), ('teen', 3072), ('teles', 3073), ('tense', 3074), ('terms', 3075), ('terrorize', 3076), ('tesco', 3077), ('teufel', 3078), ('texting', 3079), ('thandie', 3080), ('thats', 3081), ('theatre', 3082), ('themed', 3083), ('therapy', 3084), ('thoroughly', 3085), ('thorpe', 3086), ('thread', 3087), ('throughout', 3088), ('throw', 3089), ('thru', 3090), ('tht', 3091), ('tie', 3092), ('tim', 3093), ('tina', 3094), ('tips', 3095), ('tna', 3096), ('toaster', 3097), ('tom', 3098), ('tool', 3099), ('topic', 3100), ('tour', 3101), ('tournaments', 3102), ('toxin', 3103), ('trailer', 3104), ('transition', 3105), ('treated', 3106), ('tremble', 3107), ('trends', 3108), ('tribute', 3109), ('tripadvisor', 3110), ('trips', 3111), ('trouble', 3112), ('troubled', 3113), ('troy', 3114), ('tsd', 3115), ('ttp', 3116), ('tube', 3117), ('tuber', 3118), ('turning', 3119), ('turnt', 3120), ('twitch', 3121), ('type', 3122), ('types', 3123), ('u.s.', 3124), ('ughh', 3125), ('ugly', 3126), ('unacceptable', 3127), ('undergraduate', 3128), ('understands', 3129), ('unequal', 3130), ('unfortunately', 3131), ('unhappy', 3132), ('uniform', 3133), ('unique', 3134), ('unity3d', 3135), ('unless', 3136), ('unlocks', 3137), ('unprofessional', 3138), ('unreasonable', 3139), ('unruly', 3140), ('unstoppable', 3141), ('unsubscribed', 3142), ('upcoming', 3143), ('uplift', 3144), ('upon', 3145), ('usapremiere', 3146), ('useful', 3147), ('uses', 3148), ('uvs', 3149), ('vader', 3150), ('value', 3151), ('vampires', 3152), ('vc', 3153), ('velocity', 3154), ('venkys', 3155), ('verses', 3156), ('vests', 3157), ('veterans', 3158), ('vetted', 3159), ('vibes', 3160), ('victims', 3161), ('violated', 3162), ('violations', 3163), ('vip', 3164), ('virtually', 3165), ('visits', 3166), ('vlogger', 3167), ('voice', 3168), ('void', 3169), ('volunteers', 3170), ('votes', 3171), ('waited', 3172), ('wallet', 3173), ('wani', 3174), ('wardrobe', 3175), ('warranty', 3176), ('warrior', 3177), ('warriors', 3178), ('washing', 3179), ('wasnt', 3180), ('waste', 3181), ('wat', 3182), ('waters', 3183), ('wax', 3184), ('weapon', 3185), ('weekend', 3186), ('weight', 3187), ('weiner', 3188), ('weirdest', 3189), ('welcome', 3190), ('wept', 3191), ('wet', 3192), ('whahibbi', 3193), ('whatt', 3194), ('wheelchair', 3195), ('whoa', 3196), ('wholly', 3197), ('wide', 3198), ('willing', 3199), ('wind', 3200), ('windy', 3201), ('wine', 3202), ('winners', 3203), ('woods', 3204), ('workers', 3205), ('worldwide', 3206), ('worries', 3207), ('worthless', 3208), ('wouldn', 3209), ('wrap', 3210), ('wwe', 3211), ('xx', 3212), ('yall', 3213), ('yanks', 3214), ('yelling', 3215), ('yep', 3216), ('yf', 3217), ('york', 3218), ('youtube', 3219), ('yuk', 3220), ('zika', 3221), ('zombie', 3222), ('zombies', 3223), ('त', 3224), ('ध_य', 3225), ('य_ब', 3226), ('र', 3227), ('सत', 3228), ('ो', 3229), ('☉', 3230), ('♪', 3231), ('$spx', 3232), (\":'o\", 3233), (':ZZZ:', 3234), (':backhand_index_pointing_left:', 3235), (':backhand_index_pointing_right:', 3236), (':backpack:', 3237), (':beer_mug:', 3238), (':books:', 3239), (':candle:', 3240), (':cat:', 3241), (':confounded_face:', 3242), (':dashing_away:', 3243), (':detective:', 3244), (':disappointed_face:', 3245), (':face_savoring_food:', 3246), (':face_with_head-bandage:', 3247), (':face_with_medical_mask:', 3248), (':female_sign:', 3249), (':film_projector:', 3250), (':front-facing_baby_chick:', 3251), (':globe_showing_Asia-Australia:', 3252), (':goblin:', 3253), (':grinning_face_with_smiling_eyes:', 3254), (':hammer:', 3255), (':headphone:', 3256), (':honeybee:', 3257), (':horse_face:', 3258), (':index_pointing_up:', 3259), (':kitchen_knife:', 3260), (':middle_finger:', 3261), (':musical_notes:', 3262), (':nut_and_bolt:', 3263), (':office_building:', 3264), (':passport_control:', 3265), (':person_raising_hand:', 3266), (':person_tipping_hand:', 3267), (':pizza:', 3268), (':poodle:', 3269), (':raising_hands:', 3270), (':red_exclamation_mark:', 3271), (':relieved_face:', 3272), (':robot:', 3273), (':skull:', 3274), (':small_airplane:', 3275), (':smiling_face_with_halo:', 3276), (':smirking_face:', 3277), (':snowman_without_snow:', 3278), (':squinting_face_with_tongue:', 3279), (':sweat_droplets:', 3280), (':thumbs_down:', 3281), (':tired_face:', 3282), (':top_hat:', 3283), (':tropical_drink:', 3284), (':tulip:', 3285), (':unamused_face:', 3286), (':unicorn:', 3287), (':water_pistol:', 3288), (':winking_face_with_tongue:', 3289), (':zipper-mouth_face:', 3290), (\";'p\", 3291), (';-;', 3292), ('=*', 3293), ('^^^', 3294), ('_', 3295), ('a79l', 3296), ('aarp', 3297), ('abhijit', 3298), ('abounds', 3299), ('abraham', 3300), ('abroad', 3301), ('abscess', 3302), ('abuser', 3303), ('abysmal', 3304), ('academic', 3305), ('acc', 3306), ('accident', 3307), ('accomplish', 3308), ('accomplishment', 3309), ('account', 3310), ('accountancy', 3311), ('accounting', 3312), ('accusd', 3313), ('accuse', 3314), ('achieve', 3315), ('activated', 3316), ('activists', 3317), ('actors', 3318), ('acts', 3319), ('acuff', 3320), ('acupuncture', 3321), ('add', 3322), ('addict', 3323), ('addresses', 3324), ('adhd', 3325), ('administration', 3326), ('admiring', 3327), ('admit', 3328), ('admitting', 3329), ('adopt', 3330), ('adopting', 3331), ('adult', 3332), ('advance', 3333), ('advantage', 3334), ('adventure', 3335), ('adviser', 3336), ('advocate', 3337), ('adweek', 3338), ('affairs', 3339), ('affect', 3340), ('afl', 3341), ('africa', 3342), ('afterlife', 3343), ('afterwards', 3344), ('aged', 3345), ('agents', 3346), ('agg', 3347), ('agreed', 3348), ('ah', 3349), ('ahead', 3350), ('ahve', 3351), ('airport', 3352), ('ajahnae', 3353), ('al', 3354), ('alarms', 3355), ('aleppo', 3356), ('alert', 3357), ('aliens', 3358), ('allah', 3359), ('allergic', 3360), ('alloa', 3361), ('ally', 3362), ('alongside', 3363), ('although', 3364), ('alton', 3365), ('amerika', 3366), ('amidst', 3367), ('amnd', 3368), ('anarchy', 3369), ('anatomy', 3370), ('anbu', 3371), ('ancestry', 3372), ('andis', 3373), ('andrew', 3374), ('angel', 3375), ('angelino', 3376), ('animals', 3377), ('aniston', 3378), ('anna', 3379), ('announce', 3380), ('annoy', 3381), ('annoying', 3382), ('anoint', 3383), ('anonymous', 3384), ('ansh', 3385), ('anti', 3386), ('anticipating', 3387), ('apologies', 3388), ('apology', 3389), ('apparently', 3390), ('appetizer', 3391), ('applaud', 3392), ('apple', 3393), ('apply', 3394), ('approach', 3395), ('approaching', 3396), ('appropriate', 3397), ('aps', 3398), ('aqeel', 3399), ('aquila', 3400), ('arab', 3401), ('areas', 3402), ('argentina', 3403), ('arguing', 3404), ('argument', 3405), ('arguments', 3406), ('ark', 3407), ('arm', 3408), ('arq', 3409), ('arrest', 3410), ('arrested', 3411), ('arrival', 3412), ('ashamed', 3413), ('aside', 3414), ('asp', 3415), ('aspak', 3416), ('aspiring', 3417), ('assad', 3418), ('assembly', 3419), ('asses', 3420), ('asset', 3421), ('assistants', 3422), ('assoc', 3423), ('ata', 3424), ('athabasca', 3425), ('athlete', 3426), ('athletic', 3427), ('atm', 3428), ('atop', 3429), ('atsu', 3430), ('att', 3431), ('attached', 3432), ('attempted', 3433), ('attitude', 3434), ('austin', 3435), ('australia', 3436), ('authority', 3437), ('avatar', 3438), ('avoided', 3439), ('avoiding', 3440), ('aw', 3441), ('awake', 3442), ('awareness', 3443), ('awhile', 3444), ('awkwardly', 3445), ('aye', 3446), ('az', 3447), ('b4', 3448), ('background', 3449), ('backpacks', 3450), ('bacon', 3451), ('badlaof', 3452), ('bae', 3453), ('bahamas', 3454), ('baio', 3455), ('bald', 3456), ('baldwin', 3457), ('baloch', 3458), ('banana', 3459), ('bangladesh', 3460), ('banking', 3461), ('bar', 3462), ('barack', 3463), ('bare', 3464), ('barely', 3465), ('barr', 3466), ('bases', 3467), ('bashing', 3468), ('basically', 3469), ('basis', 3470), ('bass', 3471), ('bathroom', 3472), ('battered', 3473), ('batteries', 3474), ('battlefield', 3475), ('bayer', 3476), ('bby', 3477), ('bcuz', 3478), ('beach', 3479), ('beak', 3480), ('bears', 3481), ('beating', 3482), ('beckett', 3483), ('becky', 3484), ('becz', 3485), ('bee', 3486), ('beeping', 3487), ('beer', 3488), ('began', 3489), ('beggars', 3490), ('begged', 3491), ('begging', 3492), ('beginnings', 3493), ('begun', 3494), ('behaviour', 3495), ('bejesus', 3496), ('belies', 3497), ('below', 3498), ('bench', 3499), ('benchmarks', 3500), ('bend', 3501), ('benghazi', 3502), ('bennett', 3503), ('bernabéu', 3504), ('bernards', 3505), ('bernie', 3506), ('berniebrocialists', 3507), ('beside', 3508), ('besides', 3509), ('bet', 3510), ('better2day', 3511), ('bewdley', 3512), ('bf', 3513), ('bias', 3514), ('biceps', 3515), ('bicycle', 3516), ('bidded', 3517), ('bigger', 3518), ('bigotry', 3519), ('binge', 3520), ('birthdays', 3521), ('birtherism', 3522), ('bitches', 3523), ('bite', 3524), ('biting', 3525), ('bits', 3526), ('bitter', 3527), ('blacks', 3528), ('blames', 3529), ('blancos', 3530), ('blasio', 3531), ('blast', 3532), ('blasting', 3533), ('blatant', 3534), ('bless', 3535), ('blew', 3536), ('blizzard', 3537), ('blood', 3538), ('blooded', 3539), ('blow', 3540), ('blowjobs', 3541), ('bnc', 3542), ('bob', 3543), ('bodies', 3544), ('bodybuilding', 3545), ('bombarding', 3546), ('bombed', 3547), ('bombings', 3548), ('bombs', 3549), ('bone', 3550), ('booming', 3551), ('boosts', 3552), ('booty', 3553), ('borderline', 3554), ('boring', 3555), ('born', 3556), ('boshan', 3557), ('boss', 3558), ('boston', 3559), ('bots', 3560), ('bottom', 3561), ('bowman', 3562), ('boyfriend', 3563), ('bozzelli', 3564), ('brb', 3565), ('breakaway', 3566), ('breaker', 3567), ('breaking', 3568), ('breaks', 3569), ('breathing', 3570), ('breaths', 3571), ('breeding', 3572), ('bres', 3573), ('brewer', 3574), ('brief', 3575), ('brissett', 3576), ('brooke', 3577), ('brotein', 3578), ('brothers', 3579), ('brought', 3580), ('brownie', 3581), ('brutality', 3582), ('bubledygook', 3583), ('bud', 3584), ('bugs', 3585), ('build', 3586), ('builds', 3587), ('bullies', 3588), ('bum', 3589), ('buried', 3590), ('burying', 3591), ('busier', 3592), ('bust', 3593), ('cal', 3594), ('calgary', 3595), ('calming', 3596), ('calves', 3597), ('camden', 3598), ('camouflage', 3599), ('camp', 3600), ('campus', 3601), ('canberra', 3602), ('canceled', 3603), ('candidates', 3604), ('cape', 3605), ('cappuccino', 3606), ('capture', 3607), ('carbon', 3608), ('cardinal', 3609), ('cardio', 3610), ('cards', 3611), ('carl', 3612), ('caroline', 3613), ('carpenter', 3614), ('carrie', 3615), ('cases', 3616), ('cat', 3617), ('catfish', 3618), ('caught', 3619), ('caused', 3620), ('causing', 3621), ('cbtworks', 3622), ('celebrate', 3623), ('celebrating', 3624), ('cept', 3625), ('cereal', 3626), ('certified', 3627), ('cfl', 3628), ('chait', 3629), ('changes', 3630), ('chappy', 3631), ('chapter', 3632), ('character', 3633), ('charcoal', 3634), ('charisma', 3635), ('charles', 3636), ('charlton', 3637), ('chasing', 3638), ('chattering', 3639), ('cheap', 3640), ('cheat', 3641), ('checking', 3642), ('cheeks', 3643), ('cheesy', 3644), ('chef', 3645), ('chem', 3646), ('cherry', 3647), ('cheryl', 3648), ('chew', 3649), ('chia', 3650), ('chicken', 3651), ('chief', 3652), ('chiefs', 3653), ('childhood', 3654), ('chilli', 3655), ('china', 3656), ('choices', 3657), ('cholroform', 3658), ('chomsky', 3659), ('choppers', 3660), ('christian', 3661), ('chuckle', 3662), ('circles', 3663), ('cisco', 3664), ('cities', 3665), ('citing', 3666), ('civil', 3667), ('claimed', 3668), ('classic', 3669), ('classmate', 3670), ('cleaning', 3671), ('cleared', 3672), ('cleaved', 3673), ('clenching', 3674), ('clergy', 3675), ('cliff', 3676), ('climate', 3677), ('closed', 3678), ('club', 3679), ('clutter', 3680), ('cn', 3681), ('cnn', 3682), ('cnt', 3683), ('cocks', 3684), ('coercion', 3685), ('coked', 3686), ('collects', 3687), ('colleen', 3688), ('color', 3689), ('colors', 3690), ('combat', 3691), ('combating', 3692), ('combination', 3693), ('comfort', 3694), ('comma', 3695), ('commissions', 3696), ('commit', 3697), ('commitments', 3698), ('communism', 3699), ('compare', 3700), ('compared', 3701), ('comparing', 3702), ('comparison', 3703), ('comparisons', 3704), ('competitors', 3705), ('complained', 3706), ('complaints', 3707), ('concede', 3708), ('concerts', 3709), ('conclusions', 3710), ('conditions', 3711), ('confesses', 3712), ('confiaejce', 3713), ('confident', 3714), ('confronted', 3715), ('confused', 3716), ('congis', 3717), ('congress', 3718), ('congressional', 3719), ('conscience', 3720), ('consciousness', 3721), ('consequence', 3722), ('conservative', 3723), ('consistency', 3724), ('consistently', 3725), ('constant', 3726), ('constitution', 3727), ('consumate', 3728), ('consumer', 3729), ('contained', 3730), ('contentment', 3731), ('continuation', 3732), ('continue', 3733), ('continues', 3734), ('contrasts', 3735), ('contrib', 3736), ('controls', 3737), ('controversies', 3738), ('conversations', 3739), ('convicts', 3740), ('convo', 3741), ('cookies', 3742), ('cooking', 3743), ('cooler', 3744), ('cooped', 3745), ('cooperation', 3746), ('cop', 3747), ('core', 3748), ('coren', 3749), ('cormack', 3750), ('corn', 3751), ('corner', 3752), ('correct', 3753), ('corrupt', 3754), ('cosplaying', 3755), ('countdown', 3756), ('counting', 3757), ('countless', 3758), ('courses', 3759), ('coventry', 3760), ('coverage', 3761), ('covered', 3762), ('cowardliness', 3763), ('cowardly', 3764), ('coyotes', 3765), ('coz', 3766), ('cpd', 3767), ('cpfc', 3768), ('crackheads', 3769), ('cracks', 3770), ('craigen', 3771), ('cramps', 3772), ('crappy', 3773), ('craving', 3774), ('crawl', 3775), ('crawling', 3776), ('crawls', 3777), ('creak', 3778), ('creaking', 3779), ('creating', 3780), ('creative', 3781), ('creator', 3782), ('credits', 3783), ('creepers', 3784), ('crew', 3785), ('criticise', 3786), ('critics', 3787), ('crocodile', 3788), ('crossing', 3789), ('crossword', 3790), ('crowds', 3791), ('crucial', 3792), ('cruelty', 3793), ('crust', 3794), ('ctt', 3795), ('cuddles', 3796), ('cue', 3797), ('culprits', 3798), ('cultures', 3799), ('cum', 3800), ('cunts', 3801), ('curious', 3802), ('currently', 3803), ('curtains', 3804), ('cuts', 3805), ('cycling', 3806), ('cynical', 3807), ('da', 3808), ('daddy', 3809), ('dads', 3810), ('dallas', 3811), ('damnnit', 3812), ('dandy', 3813), ('dann', 3814), ('darn', 3815), ('date', 3816), ('dates', 3817), ('david', 3818), ('davies', 3819), ('dayof', 3820), ('de', 3821), ('debates', 3822), ('deblasio', 3823), ('debo', 3824), ('debunking', 3825), ('decades', 3826), ('deceived', 3827), ('deception', 3828), ('decide', 3829), ('decision', 3830), ('declare', 3831), ('decline', 3832), ('decrying', 3833), ('dedicate', 3834), ('deepest', 3835), ('deeply', 3836), ('defensive', 3837), ('deffo', 3838), ('define', 3839), ('delete', 3840), ('deleted', 3841), ('delicious', 3842), ('delight', 3843), ('delusions', 3844), ('dem', 3845), ('demand', 3846), ('demanding', 3847), ('demands', 3848), ('demolition', 3849), ('demon', 3850), ('demonstrations', 3851), ('dems', 3852), ('dentists', 3853), ('departure', 3854), ('depay', 3855), ('depending', 3856), ('deranged', 3857), ('derbyshire', 3858), ('description', 3859), ('deselect', 3860), ('desire', 3861), ('desires', 3862), ('despite', 3863), ('destroys', 3864), ('detector', 3865), ('determine', 3866), ('device', 3867), ('df', 3868), ('diagnosis', 3869), ('dialing', 3870), ('dialogue', 3871), ('didn', 3872), ('diet', 3873), ('diets', 3874), ('differing', 3875), ('difficulties', 3876), ('diminishes', 3877), ('dinner', 3878), ('diplomat', 3879), ('directing', 3880), ('direction', 3881), ('directional', 3882), ('dirt', 3883), ('dirty', 3884), ('disapproves', 3885), ('disclosed', 3886), ('discriminatory', 3887), ('disease', 3888), ('disgrace', 3889), ('disguise', 3890), ('disgusting', 3891), ('dismay', 3892), ('dismayed', 3893), ('dispel', 3894), ('disputed', 3895), ('disruption', 3896), ('distinction', 3897), ('disturb', 3898), ('dit', 3899), ('diverse', 3900), ('divide', 3901), ('divided', 3902), ('divorcing', 3903), ('dl', 3904), ('dn', 3905), ('dna', 3906), ('dnt', 3907), ('dodger', 3908), ('doj', 3909), ('dollars', 3910), ('donation', 3911), ('donations', 3912), ('donatists', 3913), ('donnie', 3914), ('dontbuythesun', 3915), ('dor', 3916), ('downhill', 3917), ('downing', 3918), ('downstairs', 3919), ('draft', 3920), ('drafts', 3921), ('drama', 3922), ('draw', 3923), ('drawing', 3924), ('dreamt', 3925), ('drew', 3926), ('drill', 3927), ('drink', 3928), ('dropping', 3929), ('duck', 3930), ('dude', 3931), ('dudes', 3932), ('dug', 3933), ('dunk', 3934), ('durag', 3935), ('dv', 3936), ('dw', 3937), ('dwarfed', 3938), ('earhole', 3939), ('earth', 3940), ('earthlings', 3941), ('earthquake', 3942), ('eastpak', 3943), ('eaxh', 3944), ('echat', 3945), ('economic', 3946), ('ecosystem', 3947), ('ed', 3948), ('editorially', 3949), ('education', 3950), ('eeiry', 3951), ('effort', 3952), ('ekg', 3953), ('elbows', 3954), ('elders', 3955), ('electing', 3956), ('elementary', 3957), ('elite', 3958), ('elizabeth', 3959), ('ellie', 3960), ('emblem', 3961), ('emergency', 3962), ('emirates', 3963), ('emotionally', 3964), ('empathetic', 3965), ('emphasis', 3966), ('employee', 3967), ('empowers', 3968), ('emulate', 3969), ('encountered', 3970), ('encouragement', 3971), ('encouraging', 3972), ('encroachment', 3973), ('ended', 3974), ('endless', 3975), ('endure', 3976), ('energetic', 3977), ('enforcers', 3978), ('eng', 3979), ('engaged', 3980), ('engaging', 3981), ('engineers', 3982), ('enjoy', 3983), ('enjoyed', 3984), ('entre', 3985), ('entry', 3986), ('enveloped', 3987), ('environment', 3988), ('ep22', 3989), ('epistles', 3990), ('equality', 3991), ('equally', 3992), ('era', 3993), ('ere', 3994), ('eric', 3995), ('ert', 3996), ('es', 3997), ('escobar', 3998), ('especially', 3999), ('espouse', 4000), ('essay', 4001), ('essence', 4002), ('essential', 4003), ('esteem', 4004), ('estranged', 4005), ('et', 4006), ('etates', 4007), ('ethan', 4008), ('etihad', 4009), ('eu', 4010), ('euriechsa', 4011), ('european', 4012), ('eurythmics', 4013), ('evangelical', 4014), ('evans', 4015), ('everytime', 4016), ('evicted', 4017), ('evidence', 4018), ('evident', 4019), ('evrn', 4020), ('ew', 4021), ('ewr', 4022), ('exam', 4023), ('example', 4024), ('excite', 4025), ('exclusivity', 4026), ('exec', 4027), ('exemplifying', 4028), ('exercised', 4029), ('exhausting', 4030), ('existential', 4031), ('exit', 4032), ('exitaso', 4033), ('exited', 4034), ('exits', 4035), ('expansive', 4036), ('expecting', 4037), ('expects', 4038), ('experiences', 4039), ('experiencing', 4040), ('explain', 4041), ('exporting', 4042), ('expressed', 4043), ('extent', 4044), ('extract', 4045), ('extremism', 4046), ('extremist', 4047), ('fade', 4048), ('failed', 4049), ('failur', 4050), ('fairly', 4051), ('fallen', 4052), ('fame', 4053), ('fanbase', 4054), ('fandom', 4055), ('fantasies', 4056), ('farewell', 4057), ('farless', 4058), ('farther', 4059), ('farting', 4060), ('fascist', 4061), ('faux', 4062), ('fave', 4063), ('fazed', 4064), ('fckd', 4065), ('fdt', 4066), ('fe', 4067), ('feast', 4068), ('federal', 4069), ('fee', 4070), ('feelings', 4071), ('ferguson', 4072), ('festooned', 4073), ('fifa', 4074), ('file', 4075), ('filled', 4076), ('fills', 4077), ('filming', 4078), ('filmmaker', 4079), ('filters', 4080), ('fin', 4081), ('finals', 4082), ('financial', 4083), ('financing', 4084), ('finds', 4085), ('finger', 4086), ('fingernails', 4087), ('fitness', 4088), ('fixer', 4089), ('flag', 4090), ('flaked', 4091), ('flatmate', 4092), ('flatters', 4093), ('flavour', 4094), ('flaw', 4095), ('fleeing', 4096), ('flick', 4097), ('flicks', 4098), ('flinch', 4099), ('flip', 4100), ('floating', 4101), ('floor', 4102), ('florida', 4103), ('floss', 4104), ('fm', 4105), ('fmf', 4106), ('fmr', 4107), ('focal', 4108), ('followed', 4109), ('fomenting', 4110), ('fool', 4111), ('footballer', 4112), ('footy', 4113), ('forced', 4114), ('forcibly', 4115), ('forcing', 4116), ('foreign', 4117), ('foreigners', 4118), ('forest', 4119), ('forgive', 4120), ('forth', 4121), ('fortified', 4122), ('fossil', 4123), ('fought', 4124), ('found', 4125), ('foyer', 4126), ('fraction', 4127), ('franchise', 4128), ('franglaise', 4129), ('freekick', 4130), ('freeloader', 4131), ('friendship', 4132), ('fries', 4133), ('frisk', 4134), ('frm', 4135), ('frustration', 4136), ('fryers', 4137), ('fuc*ing', 4138), ('fueling', 4139), ('fully', 4140), ('fur', 4141), ('furries', 4142), ('further', 4143), ('futile', 4144), ('fwm', 4145), ('g', 4146), ('gaff', 4147), ('gained', 4148), ('gangster', 4149), ('garden', 4150), ('garments', 4151), ('garriott', 4152), ('gather', 4153), ('gawd', 4154), ('gb', 4155), ('gears', 4156), ('gender', 4157), ('genders', 4158), ('genre', 4159), ('gentlemen', 4160), ('gently', 4161), ('geo', 4162), ('geopolitics', 4163), ('gerrard', 4164), ('gimme', 4165), ('giuliani', 4166), ('given', 4167), ('glass', 4168), ('glasses', 4169), ('glizzy', 4170), ('globe', 4171), ('glossinger', 4172), ('glove', 4173), ('gohar', 4174), ('goin', 4175), ('goings', 4176), ('gold', 4177), ('golden', 4178), ('gonzaga', 4179), ('googling', 4180), ('goosebumping', 4181), ('gordon', 4182), ('gore', 4183), ('gothcore', 4184), ('gothic', 4185), ('governance', 4186), ('gpp', 4187), ('grades', 4188), ('graduate', 4189), ('grammar', 4190), ('grand', 4191), ('grandad', 4192), ('granger', 4193), ('greater', 4194), ('greatness', 4195), ('grim', 4196), ('grimy', 4197), ('grocery', 4198), ('grounds', 4199), ('growing', 4200), ('gta', 4201), ('guarantee', 4202), ('guard', 4203), ('guest', 4204), ('guiding', 4205), ('guilty', 4206), ('gump', 4207), ('gus', 4208), ('gusta', 4209), ('h3ll', 4210), ('habitation', 4211), ('habits', 4212), ('hahahaa', 4213), ('hahahaha', 4214), ('halfway', 4215), ('halifax', 4216), ('hallelujah', 4217), ('hallucination', 4218), ('ham', 4219), ('hamas', 4220), ('hammocks', 4221), ('handicap', 4222), ('handicapped', 4223), ('hang', 4224), ('hangers', 4225), ('hanging', 4226), ('happening', 4227), ('happiest', 4228), ('happiness', 4229), ('haqqani', 4230), ('harass', 4231), ('harboring', 4232), ('harder', 4233), ('hardship', 4234), ('harry', 4235), ('harsh', 4236), ('harvest', 4237), ('hasina', 4238), ('haters', 4239), ('hath', 4240), ('havens', 4241), ('havent', 4242), ('havin', 4243), ('hawk', 4244), ('haww', 4245), ('headache', 4246), ('headaches', 4247), ('headed', 4248), ('headers', 4249), ('heading', 4250), ('headline', 4251), ('headquarters', 4252), ('hearings', 4253), ('heath', 4254), ('heavy', 4255), ('heck', 4256), ('hedonists', 4257), ('heely', 4258), ('height', 4259), ('helicopter', 4260), ('hella', 4261), ('hello', 4262), ('helmann', 4263), ('helping', 4264), ('helpline', 4265), ('hen', 4266), ('hence', 4267), ('hennessey', 4268), ('herrera', 4269), ('hes', 4270), ('hesketh', 4271), ('hestia', 4272), ('hh', 4273), ('hiccuping', 4274), ('higher', 4275), ('highlighted', 4276), ('highs', 4277), ('highway', 4278), ('hiking', 4279), ('hilarious', 4280), ('hill', 4281), ('hillarys', 4282), ('hills', 4283), ('himani', 4284), ('hip', 4285), ('historic', 4286), ('historically', 4287), ('hitter', 4288), ('hives', 4289), ('hizbul', 4290), ('hodgson', 4291), ('hoe', 4292), ('hogs', 4293), ('holding', 4294), ('hole', 4295), ('holidog', 4296), ('hollywood', 4297), ('holmes', 4298), ('homily', 4299), ('honest', 4300), ('honor', 4301), ('hopeful', 4302), ('hopefully', 4303), ('horn', 4304), ('horrendous', 4305), ('horrors', 4306), ('host', 4307), ('hots', 4308), ('hounded', 4309), ('houston', 4310), ('hoys', 4311), ('hr', 4312), ('hrjdjd', 4313), ('hubby', 4314), ('huh', 4315), ('hunks', 4316), ('hurry', 4317), ('hurting', 4318), ('hustles', 4319), ('hve', 4320), ('hypocritical', 4321), ('hysterical', 4322), ('iam', 4323), ('ibiza', 4324), ('ideology', 4325), ('idiots', 4326), ('ie', 4327), ('ignorance', 4328), ('ignores', 4329), ('ignoring', 4330), ('ii', 4331), ('ijb', 4332), ('ik', 4333), ('ikaw', 4334), ('illiteracy', 4335), ('illness', 4336), ('image', 4337), ('imagined', 4338), ('imbeciles', 4339), ('imediately', 4340), ('immigration', 4341), ('immodest', 4342), ('implied', 4343), ('impose', 4344), ('improving', 4345), ('impulsive', 4346), ('inadvertently', 4347), ('inbound', 4348), ('incite', 4349), ('incomprehension', 4350), ('inconvenience', 4351), ('incorrect', 4352), ('incredibly', 4353), ('ind', 4354), ('index', 4355), ('indiansummer', 4356), ('indias', 4357), ('indicates', 4358), ('indifference', 4359), ('indoor', 4360), ('induced', 4361), ('indulges', 4362), ('inevitable', 4363), ('infidels', 4364), ('information', 4365), ('ingle', 4366), ('inha', 4367), ('inhuman', 4368), ('injure', 4369), ('injured', 4370), ('injuries', 4371), ('innocence', 4372), ('insensitive', 4373), ('insignificant', 4374), ('insomniacs', 4375), ('instestines', 4376), ('instills', 4377), ('instinct', 4378), ('instruments', 4379), ('insurance', 4380), ('intellectually', 4381), ('interference', 4382), ('internal', 4383), ('internship', 4384), ('intervention', 4385), ('interviews', 4386), ('intrigued', 4387), ('introduced', 4388), ('invade', 4389), ('investing', 4390), ('invitadated', 4391), ('involvement', 4392), ('involving', 4393), ('ios', 4394), ('ipca', 4395), ('iq', 4396), ('iran', 4397), ('iranian', 4398), ('iraqi', 4399), ('irl', 4400), ('iron', 4401), ('isa', 4402), ('ish', 4403), ('isi', 4404), ('islamist', 4405), ('islamophobic', 4406), ('islands', 4407), ('isn', 4408), ('isnt', 4409), ('israel', 4410), ('jacket', 4411), ('jacoby', 4412), ('jake', 4413), ('james', 4414), ('jamie', 4415), ('janecky', 4416), ('jb', 4417), ('jd', 4418), ('jeepers', 4419), ('jeff', 4420), ('jeffrey', 4421), ('jewish', 4422), ('jews', 4423), ('jihadist', 4424), ('jiho', 4425), ('jill', 4426), ('jim', 4427), ('jimmy_dore', 4428), ('joe', 4429), ('joined', 4430), ('joining', 4431), ('jon', 4432), ('josh', 4433), ('journ', 4434), ('journal', 4435), ('joyful', 4436), ('judgement', 4437), ('juliana', 4438), ('junky', 4439), ('jury', 4440), ('jus', 4441), ('justiceofpak', 4442), ('justified', 4443), ('justify', 4444), ('ka', 4445), ('kach', 4446), ('kal', 4447), ('kalil', 4448), ('kaniyah', 4449), ('kardashian', 4450), ('karma', 4451), ('katherine', 4452), ('kejriwal', 4453), ('kelvin', 4454), ('kenya', 4455), ('kernel', 4456), ('kerry', 4457), ('khalistaan', 4458), ('khloe', 4459), ('kicking', 4460), ('killing', 4461), ('kipling', 4462), ('kish', 4463), ('kitchen', 4464), ('kjartansson', 4465), ('kneeling', 4466), ('kno', 4467), ('knocked', 4468), ('known', 4469), ('knw', 4470), ('ko', 4471), ('korea', 4472), ('kulbhoshan', 4473), ('kulbhushan', 4474), ('kumar', 4475), ('kuo', 4476), ('kylie', 4477), ('lace', 4478), ('ladies', 4479), ('lakeside', 4480), ('lamented', 4481), ('landscape', 4482), ('language', 4483), ('laptop', 4484), ('larry', 4485), ('lasted', 4486), ('lay', 4487), ('layer', 4488), ('laziness', 4489), ('leather', 4490), ('leaves', 4491), ('leftover', 4492), ('legit', 4493), ('lehrer', 4494), ('lemon', 4495), ('leo', 4496), ('lesbian', 4497), ('letgo', 4498), ('lettin', 4499), ('lewis', 4500), ('li', 4501), ('librarians', 4502), ('lie', 4503), ('lifeis', 4504), ('lifting', 4505), ('likely', 4506), ('likewise', 4507), ('liking', 4508), ('lil', 4509), ('limbs', 4510), ('limits', 4511), ('lincoln', 4512), ('lines', 4513), ('links', 4514), ('liquor', 4515), ('liscence', 4516), ('list', 4517), ('listed', 4518), ('literary', 4519), ('litigious', 4520), ('lka', 4521), ('load', 4522), ('loads', 4523), ('loathing', 4524), ('loc', 4525), ('locator', 4526), ('lock', 4527), ('lockers', 4528), ('lolly', 4529), ('loner', 4530), ('lool', 4531), ('looping', 4532), ('loose', 4533), ('loot', 4534), ('los', 4535), ('lots', 4536), ('loud', 4537), ('louder', 4538), ('louth', 4539), ('lovingly', 4540), ('lovinleeds', 4541), ('lowest', 4542), ('lth', 4543), ('lunacy', 4544), ('lw', 4545), ('lydia', 4546), ('lyle', 4547), ('lynn', 4548), ('mac', 4549), ('maccabi', 4550), ('machine', 4551), ('mack', 4552), ('mackenzie', 4553), ('mackinnon', 4554), ('macon', 4555), ('madam', 4556), ('madame', 4557), ('mafia', 4558), ('maga', 4559), ('magazine', 4560), ('magnet', 4561), ('magnitude', 4562), ('maharashtra', 4563), ('mails', 4564), ('malaysia', 4565), ('malaysian', 4566), ('malice', 4567), ('malins', 4568), ('mamasitas', 4569), ('management', 4570), ('manakgupta', 4571), ('manchester', 4572), ('mandate', 4573), ('mandela', 4574), ('mania', 4575), ('manifest', 4576), ('mankind', 4577), ('manned', 4578), ('manpower', 4579), ('mans', 4580), ('march', 4581), ('marcos', 4582), ('marcus', 4583), ('mariah', 4584), ('marketing', 4585), ('markets', 4586), ('marks', 4587), ('marley', 4588), ('marnie', 4589), ('marriages', 4590), ('marshall', 4591), ('martyrs', 4592), ('maself', 4593), ('mask', 4594), ('masks', 4595), ('massive', 4596), ('mat', 4597), ('mates', 4598), ('matoma', 4599), ('matt', 4600), ('matthew', 4601), ('matthews', 4602), ('mattmilne76', 4603), ('mayor', 4604), ('mc', 4605), ('mccreadie', 4606), ('mcfc', 4607), ('mcintyre', 4608), ('md', 4609), ('meadow', 4610), ('meaindia', 4611), ('meals', 4612), ('meant', 4613), ('meantime', 4614), ('medal', 4615), ('medical', 4616), ('medicating', 4617), ('medication', 4618), ('mediocrity', 4619), ('megaphone', 4620), ('mel', 4621), ('mensa', 4622), ('mensah', 4623), ('mentally', 4624), ('merk', 4625), ('merlin', 4626), ('mesa', 4627), ('mess', 4628), ('message', 4629), ('messages', 4630), ('messenger', 4631), ('metal', 4632), ('mi', 4633), ('micha', 4634), ('michelle', 4635), ('midnight', 4636), ('midst', 4637), ('mike', 4638), ('mikeand', 4639), ('military', 4640), ('millennials', 4641), ('milo', 4642), ('min', 4643), ('mindblown', 4644), ('minds', 4645), ('minerva', 4646), ('minister', 4647), ('minorities', 4648), ('mirrors', 4649), ('misogynistic', 4650), ('misspelled', 4651), ('mistakes', 4652), ('mistrust', 4653), ('mix', 4654), ('mixing', 4655), ('mn', 4656), ('moan', 4657), ('modern', 4658), ('modest', 4659), ('modric', 4660), ('moisturizing', 4661), ('molly', 4662), ('mon', 4663), ('monitor', 4664), ('monoxide', 4665), ('moral', 4666), ('moreis', 4667), ('morgs', 4668), ('morgue', 4669), ('morons', 4670), ('morrisons', 4671), ('mos', 4672), ('moscow', 4673), ('motherland', 4674), ('motto', 4675), ('mouse', 4676), ('movement', 4677), ('mqm', 4678), ('msk', 4679), ('mugs', 4680), ('mumbai', 4681), ('munich', 4682), ('murders', 4683), ('muse', 4684), ('musician', 4685), ('mustache', 4686), ('mustard', 4687), ('mutch', 4688), ('mute', 4689), ('mutli', 4690), ('mutuals', 4691), ('muzzy', 4692), ('mwahaha', 4693), ('máxima', 4694), ('n*he', 4695), ('n*walks', 4696), ('nabandonment', 4697), ('nactually', 4698), ('nadia', 4699), ('nafter', 4700), ('nagainst', 4701), ('nah', 4702), ('nairobi', 4703), ('naive', 4704), ('nalso', 4705), ('naming', 4706), ('nana', 4707), ('nanyone', 4708), ('nas', 4709), ('nations', 4710), ('nature', 4711), ('nausea', 4712), ('nav', 4713), ('naya', 4714), ('nbecause', 4715), ('ncomment', 4716), ('ncould', 4717), ('ndidn', 4718), ('ndo', 4719), ('ndreams', 4720), ('needles', 4721), ('needy', 4722), ('negatives', 4723), ('negativity', 4724), ('negotiating', 4725), ('neighbourhood', 4726), ('nelson', 4727), ('neo', 4728), ('nerd', 4729), ('net', 4730), ('neutral', 4731), ('newly', 4732), ('nface', 4733), ('ngirls', 4734), ('ngo', 4735), ('nhad', 4736), ('nhappy', 4737), ('nhave', 4738), ('nhi', 4739), ('niall', 4740), ('nickels', 4741), ('nierstein', 4742), ('nigeria', 4743), ('nigga', 4744), ('niggas', 4745), ('nigt', 4746), ('nike', 4747), ('nikerun', 4748), ('nimagine', 4749), ('nindia', 4750), ('nite', 4751), ('nj', 4752), ('njust', 4753), ('nkindly', 4754), ('nliterally', 4755), ('nlove', 4756), ('nmatt', 4757), ('nme', 4758), ('nnow', 4759), ('nnurse', 4760), ('no1', 4761), ('noah', 4762), ('noam', 4763), ('nobler', 4764), ('noise', 4765), ('noises', 4766), ('nola', 4767), ('nominated', 4768), ('non', 4769), ('none', 4770), ('north', 4771), ('northamptons', 4772), ('noted', 4773), ('nothings', 4774), ('notice', 4775), ('noticed', 4776), ('notices', 4777), ('nougat', 4778), ('novel', 4779), ('november', 4780), ('np', 4781), ('nreally', 4782), ('nsame', 4783), ('nstop', 4784), ('nsuch', 4785), ('ntac', 4786), ('ntfcvmufc', 4787), ('nthe', 4788), ('nthese', 4789), ('nu', 4790), ('numbers', 4791), ('nurse', 4792), ('nutrient', 4793), ('nvr', 4794), ('nw', 4795), ('nwhat', 4796), ('nwhy', 4797), ('nyg', 4798), ('nyquil', 4799), ('nz', 4800), ('nzionists', 4801), ('oat', 4802), ('objective', 4803), ('objectively', 4804), ('obliged', 4805), ('oblivious', 4806), ('obstacles', 4807), ('obvs', 4808), ('octopus', 4809), ('odd', 4810), ('odi', 4811), ('offended', 4812), ('offer', 4813), ('offerings', 4814), ('offers', 4815), ('officaly', 4816), ('officialy', 4817), ('offline', 4818), ('often', 4819), ('ohh', 4820), ('okgoogle', 4821), ('oligarchs', 4822), ('olympia', 4823), ('ominous', 4824), ('omit', 4825), ('ongoing', 4826), ('ons', 4827), ('op', 4828), ('opening', 4829), ('opens', 4830), ('opponents', 4831), ('opposite', 4832), ('oprah', 4833), ('ops', 4834), ('optimist', 4835), ('ordering', 4836), ('orwellian', 4837), ('osha', 4838), ('ot', 4839), ('otherwise', 4840), ('ou', 4841), ('ourselves', 4842), ('outfits', 4843), ('outlet', 4844), ('outrage', 4845), ('overcomes', 4846), ('overwhelmed', 4847), ('overwhelming', 4848), ('owe', 4849), ('owen', 4850), ('owners', 4851), ('ownership', 4852), ('pac', 4853), ('pace', 4854), ('packed', 4855), ('paint', 4856), ('pakis', 4857), ('palestinian', 4858), ('palm', 4859), ('palms', 4860), ('panama', 4861), ('pangs', 4862), ('paradox', 4863), ('parallels', 4864), ('paranoia', 4865), ('parcel', 4866), ('pardon', 4867), ('parks', 4868), ('partially', 4869), ('participating', 4870), ('particular', 4871), ('partisan', 4872), ('partners', 4873), ('pas', 4874), ('passed', 4875), ('passion', 4876), ('passions', 4877), ('passport', 4878), ('pastries', 4879), ('pastry', 4880), ('patched', 4881), ('path', 4882), ('paths', 4883), ('patriot', 4884), ('patriotact', 4885), ('patterns', 4886), ('pauline', 4887), ('pause', 4888), ('paw', 4889), ('paying', 4890), ('pb2', 4891), ('pecking', 4892), ('pedicure', 4893), ('peeps', 4894), ('peers', 4895), ('peiyophobilia', 4896), ('pelting', 4897), ('penalty', 4898), ('pence', 4899), ('peoples', 4900), ('perceive', 4901), ('percentage', 4902), ('perform', 4903), ('permanent', 4904), ('permission', 4905), ('persecution', 4906), ('personal', 4907), ('persuasion', 4908), ('petrify', 4909), ('petulant', 4910), ('pfft', 4911), ('phantom', 4912), ('phedophiliac', 4913), ('phew', 4914), ('phos', 4915), ('phr', 4916), ('physiatrists', 4917), ('physical', 4918), ('picked', 4919), ('pickup', 4920), ('picture', 4921), ('pimp', 4922), ('pines', 4923), ('piss', 4924), ('pisses', 4925), ('pitt', 4926), ('pk', 4927), ('places', 4928), ('plans', 4929), ('platform', 4930), ('plato', 4931), ('plays', 4932), ('pleased', 4933), ('pleasure', 4934), ('pledge', 4935), ('plenty', 4936), ('pm', 4937), ('po', 4938), ('pointed', 4939), ('pok', 4940), ('poking', 4941), ('polictical', 4942), ('pompous', 4943), ('pomte', 4944), ('pope', 4945), ('popufurs', 4946), ('populace', 4947), ('portentous', 4948), ('portion', 4949), ('poses', 4950), ('posh', 4951), ('positioning', 4952), ('positives', 4953), ('posting', 4954), ('postrump', 4955), ('potus', 4956), ('pounding', 4957), ('poundstretcher', 4958), ('pov', 4959), ('poverty', 4960), ('powell', 4961), ('pr', 4962), ('prayerful', 4963), ('praying', 4964), ('pre', 4965), ('precaution', 4966), ('precious', 4967), ('precondition', 4968), ('premier', 4969), ('prepare', 4970), ('prepared', 4971), ('preparedness', 4972), ('preponderance', 4973), ('preschool', 4974), ('prescriptions', 4975), ('presenting', 4976), ('preset', 4977), ('pretending', 4978), ('pride', 4979), ('prisoned', 4980), ('proactive', 4981), ('prob', 4982), ('processed', 4983), ('procrasting', 4984), ('prodding', 4985), ('produces', 4986), ('prof', 4987), ('professions', 4988), ('professor', 4989), ('professors', 4990), ('profile', 4991), ('profit', 4992), ('projects', 4993), ('promote', 4994), ('proper', 4995), ('properly', 4996), ('property', 4997), ('proportion', 4998), ('proposes', 4999), ('protecting', 5000), ('protesters', 5001), ('protesting', 5002), ('protocols', 5003), ('pround', 5004), ('proved', 5005), ('proven', 5006), ('proverb', 5007), ('providers', 5008), ('provides', 5009), ('prudent', 5010), ('psyched', 5011), ('psycho', 5012), ('pts', 5013), ('publicly', 5014), ('puckish', 5015), ('puer', 5016), ('puke', 5017), ('pundits', 5018), ('punish', 5019), ('purchased', 5020), ('purge', 5021), ('purging', 5022), ('purpose', 5023), ('pushchair', 5024), ('pushed', 5025), ('pushing', 5026), ('putting', 5027), ('puzzle', 5028), ('qualifies', 5029), ('qualm', 5030), ('queensbury', 5031), ('quentin', 5032), ('queries', 5033), ('quiver', 5034), ('quiz', 5035), ('quran', 5036), ('r.i.p', 5037), ('rabid', 5038), ('races', 5039), ('racial', 5040), ('raid', 5041), ('raise', 5042), ('raised', 5043), ('raisingtheboss', 5044), ('ramble', 5045), ('ramos', 5046), ('rand', 5047), ('range', 5048), ('rare', 5049), ('rashid', 5050), ('rat', 5051), ('rates', 5052), ('ravis', 5053), ('react', 5054), ('realizes', 5055), ('realm', 5056), ('rear', 5057), ('reasoned', 5058), ('reasoning', 5059), ('reassuring', 5060), ('rebuild', 5061), ('recall', 5062), ('recognise', 5063), ('recorded', 5064), ('recruitment', 5065), ('red', 5066), ('rednecks', 5067), ('redundant', 5068), ('refer', 5069), ('reference', 5070), ('references', 5071), ('refusd', 5072), ('regards', 5073), ('regime', 5074), ('register', 5075), ('rein', 5076), ('reinstate', 5077), ('relate', 5078), ('relation', 5079), ('relax', 5080), ('release', 5081), ('releasing', 5082), ('relieve', 5083), ('relight', 5084), ('religious', 5085), ('rely', 5086), ('remain', 5087), ('remainder', 5088), ('remake', 5089), ('remarkable', 5090), ('remix', 5091), ('renee', 5092), ('renunciation', 5093), ('repeat', 5094), ('replace', 5095), ('replied', 5096), ('reporting', 5097), ('request', 5098), ('require', 5099), ('rescue', 5100), ('rescued', 5101), ('reserves', 5102), ('resist', 5103), ('resources', 5104), ('respectful', 5105), ('respecting', 5106), ('responsible', 5107), ('rested', 5108), ('result', 5109), ('resultin', 5110), ('returned', 5111), ('retweeted', 5112), ('reuters', 5113), ('review', 5114), ('reviews', 5115), ('revitalize', 5116), ('revive', 5117), ('revolves', 5118), ('rewards', 5119), ('rgds', 5120), ('rhe', 5121), ('rhino', 5122), ('rhow', 5123), ('ribbon', 5124), ('ricky', 5125), ('rid', 5126), ('righteous', 5127), ('ringing', 5128), ('ripe', 5129), ('rises', 5130), ('rising', 5131), ('risks', 5132), ('rlike', 5133), ('rly', 5134), ('rmavil', 5135), ('road', 5136), ('rob', 5137), ('rockland', 5138), ('rocky', 5139), ('roho', 5140), ('roll', 5141), ('rollover', 5142), ('ron', 5143), ('ronaldo', 5144), ('ronly', 5145), ('roof', 5146), ('rookie', 5147), ('rooms', 5148), ('rooneys', 5149), ('roosevelt', 5150), ('root', 5151), ('rootstock', 5152), ('rosa', 5153), ('rotherham', 5154), ('rough', 5155), ('route', 5156), ('roy', 5157), ('rpt', 5158), ('rubbish', 5159), ('rule', 5160), ('rules', 5161), ('russel', 5162), ('sacred', 5163), ('sacrificed', 5164), ('sadiq', 5165), ('safely', 5166), ('sai', 5167), ('sait', 5168), ('salty', 5169), ('samuel', 5170), ('sanctions', 5171), ('sang', 5172), ('sapient', 5173), ('sarwar', 5174), ('satellite', 5175), ('satisfaction', 5176), ('saudi', 5177), ('saved', 5178), ('sb', 5179), ('scaredy', 5180), ('scarier', 5181), ('scented', 5182), ('schalke', 5183), ('schools', 5184), ('scientist', 5185), ('scissors', 5186), ('scorer', 5187), ('scrapbook', 5188), ('scrolling', 5189), ('scrubbed', 5190), ('scrubs', 5191), ('scruple', 5192), ('se', 5193), ('seafood', 5194), ('seamlessly', 5195), ('secret', 5196), ('secretive', 5197), ('sede', 5198), ('selfish', 5199), ('sentence', 5200), ('separation', 5201), ('sept', 5202), ('sequels', 5203), ('serve', 5204), ('session', 5205), ('setback', 5206), ('sets', 5207), ('settle', 5208), ('severe', 5209), ('sexually', 5210), ('sexy', 5211), ('sh', 5212), ('shallowness', 5213), ('shambles', 5214), ('shameful', 5215), ('shameless', 5216), ('shaming', 5217), ('shattering', 5218), ('shaun', 5219), ('sheer', 5220), ('shehla', 5221), ('sheikh', 5222), ('shell', 5223), ('shelter', 5224), ('shet', 5225), ('shitlib', 5226), ('shitty', 5227), ('shoes', 5228), ('shootings', 5229), ('shouting', 5230), ('shove', 5231), ('shucks', 5232), ('shudders', 5233), ('siblings', 5234), ('sickening', 5235), ('siding', 5236), ('sif', 5237), ('sigh', 5238), ('sighted', 5239), ('significant', 5240), ('simon', 5241), ('sins', 5242), ('site', 5243), ('sites', 5244), ('sith', 5245), ('sits', 5246), ('sitter', 5247), ('sixth', 5248), ('skilled', 5249), ('skills', 5250), ('skinned', 5251), ('skydive', 5252), ('skyfall', 5253), ('slams', 5254), ('slightly', 5255), ('slip', 5256), ('slithering', 5257), ('slot', 5258), ('slump', 5259), ('smaller', 5260), ('sme', 5261), ('smirk', 5262), ('smithrud', 5263), ('snake', 5264), ('snapchat', 5265), ('sneezing', 5266), ('snot', 5267), ('snowden', 5268), ('sociopath', 5269), ('soda', 5270), ('softer', 5271), ('soggy', 5272), ('solid', 5273), ('solitary', 5274), ('soloist', 5275), ('solopreneur', 5276), ('solution', 5277), ('solve', 5278), ('someday', 5279), ('somethin', 5280), ('sometime', 5281), ('son', 5282), ('sounding', 5283), ('southampton', 5284), ('southend', 5285), ('spare', 5286), ('spark', 5287), ('speaks', 5288), ('specialise', 5289), ('specialist', 5290), ('specific', 5291), ('specter', 5292), ('speeds', 5293), ('spend', 5294), ('spine', 5295), ('spinoffs', 5296), ('spoils', 5297), ('spoken', 5298), ('sponsor', 5299), ('spy', 5300), ('squad', 5301), ('square', 5302), ('srry', 5303), ('srsly', 5304), ('ss', 5305), ('stability', 5306), ('stadium', 5307), ('stageschool', 5308), ('stall', 5309), ('stances', 5310), ('stanning', 5311), ('star', 5312), ('starbucks', 5313), ('stare', 5314), ('startups', 5315), ('states', 5316), ('status', 5317), ('stealing', 5318), ('steer', 5319), ('stein', 5320), ('steiner', 5321), ('stephen', 5322), ('steppingstone', 5323), ('stevie', 5324), ('stfu', 5325), ('stifled', 5326), ('stocks', 5327), ('stoked', 5328), ('stoking', 5329), ('stone', 5330), ('stopping', 5331), ('stranded', 5332), ('stranger', 5333), ('strategically', 5334), ('strategy', 5335), ('strawberry', 5336), ('streaming', 5337), ('strength', 5338), ('stretch', 5339), ('strive', 5340), ('stroked', 5341), ('strongest', 5342), ('strongwomen', 5343), ('studios', 5344), ('stunning', 5345), ('stupidity', 5346), ('submarine', 5347), ('succeed', 5348), ('succumb', 5349), ('suffered', 5350), ('suffering', 5351), ('suffern', 5352), ('suffers', 5353), ('suggestions', 5354), ('sugita', 5355), ('suicidal', 5356), ('suit', 5357), ('summer', 5358), ('summon', 5359), ('sunrise', 5360), ('superstitious', 5361), ('supported', 5362), ('supremacists', 5363), ('surfing', 5364), ('surging', 5365), ('surrendered', 5366), ('surround', 5367), ('suspect', 5368), ('suspected', 5369), ('suspend', 5370), ('suspended', 5371), ('swans', 5372), ('swayed', 5373), ('sweater', 5374), ('sweating', 5375), ('sweeper', 5376), ('sweetheart', 5377), ('swift', 5378), ('swingish', 5379), ('switched', 5380), ('swope', 5381), ('syed', 5382), ('symptom', 5383), ('syndrome', 5384), ('syrians', 5385), ('t20', 5386), ('t5ylw', 5387), ('tablets', 5388), ('tackling', 5389), ('tad', 5390), ('tagagising', 5391), ('tailster', 5392), ('talaga', 5393), ('talents', 5394), ('talked', 5395), ('taller', 5396), ('tamed', 5397), ('tamil', 5398), ('targett', 5399), ('tasked', 5400), ('tasks', 5401), ('tatinof', 5402), ('tatoos', 5403), ('tattooing', 5404), ('taylor', 5405), ('tbsp', 5406), ('tbt', 5407), ('td', 5408), ('te', 5409), ('teacher', 5410), ('teachers', 5411), ('teeth', 5412), ('tek', 5413), ('telford', 5414), ('telling', 5415), ('temperment', 5416), ('temporada', 5417), ('ten', 5418), ('terence', 5419), ('termination', 5420), ('terrence', 5421), ('territory', 5422), ('terrorizes', 5423), ('text', 5424), ('tfb', 5425), ('tfw', 5426), ('tg', 5427), ('thas', 5428), ('theme', 5429), ('themself', 5430), ('themselves', 5431), ('theological', 5432), ('theologically', 5433), ('theres', 5434), ('theyre', 5435), ('thick', 5436), ('thighs', 5437), ('thin', 5438), ('thisbhuge', 5439), ('thiza', 5440), ('thomas', 5441), ('thoreau', 5442), ('thoughtful', 5443), ('thousand', 5444), ('thriller', 5445), ('throwback', 5446), ('ths', 5447), ('thugs', 5448), ('thy', 5449), ('ticklish', 5450), ('tigers', 5451), ('til', 5452), ('timebreakers', 5453), ('timeless', 5454), ('tinder', 5455), ('tipo', 5456), ('tiptoeing', 5457), ('tires', 5458), ('tive', 5459), ('tmrw', 5460), ('tobi', 5461), ('toddler', 5462), ('toilet', 5463), ('tokyo', 5464), ('toldeo', 5465), ('toler', 5466), ('tongues', 5467), ('tools', 5468), ('tooth', 5469), ('top10', 5470), ('totally', 5471), ('touché', 5472), ('towels', 5473), ('towers', 5474), ('tracey', 5475), ('tracking', 5476), ('trade', 5477), ('traffic', 5478), ('tragedies', 5479), ('tragedy', 5480), ('trains', 5481), ('trans', 5482), ('transfer', 5483), ('transform', 5484), ('translate', 5485), ('transport', 5486), ('trap', 5487), ('trashy', 5488), ('travel', 5489), ('trc', 5490), ('tread', 5491), ('tree', 5492), ('trembling', 5493), ('tricks', 5494), ('tries', 5495), ('trivial', 5496), ('trix', 5497), ('trolls', 5498), ('trp', 5499), ('trumped', 5500), ('trumpeter', 5501), ('trumpone', 5502), ('trumppence', 5503), ('truncating', 5504), ('trvial', 5505), ('tryin', 5506), ('tulle', 5507), ('tummies', 5508), ('tunnels', 5509), ('tuppytupperware', 5510), ('twat', 5511), ('twinkie', 5512), ('typing', 5513), ('typo', 5514), ('typos', 5515), ('ufc', 5516), ('uggh', 5517), ('uhh', 5518), ('unafraid', 5519), ('uncertainty', 5520), ('unconditionally', 5521), ('uncontrollable', 5522), ('uncooperative', 5523), ('undergoing', 5524), ('unf', 5525), ('unfollowed', 5526), ('union', 5527), ('unit', 5528), ('universe', 5529), ('unlikely', 5530), ('unprepared', 5531), ('unreal', 5532), ('unregulated', 5533), ('unreliable', 5534), ('unrest', 5535), ('unroot', 5536), ('untouchable', 5537), ('untrusted', 5538), ('unuh', 5539), ('unzu', 5540), ('uofw', 5541), ('upping', 5542), ('ups', 5543), ('urge', 5544), ('urges', 5545), ('urghh', 5546), ('urself', 5547), ('usual', 5548), ('utd', 5549), ('uwu', 5550), ('vacant', 5551), ('vacation', 5552), ('values', 5553), ('vandalizing', 5554), ('vantage', 5555), ('vape', 5556), ('vapid', 5557), ('varies', 5558), ('vast', 5559), ('ventures', 5560), ('viallreal', 5561), ('vice', 5562), ('vicious', 5563), ('vidar', 5564), ('views', 5565), ('vigorous', 5566), ('vilifying', 5567), ('villa', 5568), ('vio', 5569), ('violent', 5570), ('viper', 5571), ('virgina', 5572), ('visible', 5573), ('visit', 5574), ('vitale', 5575), ('vitals', 5576), ('vivid', 5577), ('vodka', 5578), ('vox', 5579), ('voxbox', 5580), ('vulnerabilities', 5581), ('vulnerable', 5582), ('vwo', 5583), ('wah', 5584), ('wahhabism', 5585), ('waistband', 5586), ('waking', 5587), ('walker', 5588), ('walls', 5589), ('wanting', 5590), ('warmonger', 5591), ('warren', 5592), ('wasted', 5593), ('wayed', 5594), ('ways', 5595), ('wazza', 5596), ('weapons', 5597), ('wedding', 5598), ('wee', 5599), ('weekly', 5600), ('welp', 5601), ('west', 5602), ('westgate', 5603), ('wetter', 5604), ('whao', 5605), ('whatsapp', 5606), ('whatsoever', 5607), ('wheel', 5608), ('whenever', 5609), ('whilst', 5610), ('whispered', 5611), ('whn', 5612), ('whod', 5613), ('whoever', 5614), ('whos', 5615), ('wii', 5616), ('wince', 5617), ('window', 5618), ('winfrey', 5619), ('wisconsin', 5620), ('wise', 5621), ('wit', 5622), ('witness', 5623), ('wolves', 5624), ('wonders', 5625), ('wood', 5626), ('wordvof', 5627), ('worked', 5628), ('workplace', 5629), ('worms', 5630), ('worsening', 5631), ('worship', 5632), ('wouldve', 5633), ('wr', 5634), ('wrecord', 5635), ('wrote', 5636), ('wt', 5637), ('wtvr', 5638), ('ww1', 5639), ('ww3', 5640), ('xbox', 5641), ('xtremly', 5642), ('yadav', 5643), ('yadhav', 5644), ('yalla', 5645), ('yard', 5646), ('yards', 5647), ('yearning', 5648), ('yellow', 5649), ('yemen', 5650), ('yoga', 5651), ('yorkshire', 5652), ('younger', 5653), ('youngsters', 5654), ('younus', 5655), ('yourselves', 5656), ('yous', 5657), ('yr7', 5658), ('yup', 5659), ('zh', 5660), ('zidane', 5661), ('zilch', 5662), ('zionist', 5663), ('zipper', 5664), ('zone', 5665), ('{', 5666), ('}', 5667), ('¿', 5668), ('\\u200d', 5669), ('⊰', 5670), ('⊱', 5671), ('━', 5672), ('☊', 5673), ('\\uf3fc', 5674), ('\\uf62b', 5675), ('\\uf633', 5676), ('\\uf645', 5677), ('\\uf648', 5678), ('\\uf64a', 5679), ('\\U000fe358', 5680)])\n",
            "dict_items([('<unk>', 0), ('<pad>', 1), ('</hashtag>', 2), ('<hashtag>', 3), ('.', 4), ('<user>', 5), ('the', 6), ('i', 7), (',', 8), ('to', 9), (\"'\", 10), ('a', 11), ('and', 12), ('you', 13), ('is', 14), ('of', 15), ('it', 16), ('!', 17), ('not', 18), ('<repeated>', 19), ('in', 20), ('that', 21), ('s', 22), ('my', 23), ('for', 24), ('me', 25), ('<number>', 26), ('on', 27), ('are', 28), ('-', 29), ('so', 30), ('am', 31), ('but', 32), ('?', 33), ('have', 34), ('be', 35), ('\\\\', 36), ('do', 37), ('with', 38), ('your', 39), ('</allcaps>', 40), ('<allcaps>', 41), ('this', 42), ('just', 43), ('at', 44), ('will', 45), ('all', 46), ('can', 47), ('happy', 48), ('was', 49), ('up', 50), ('like', 51), ('they', 52), ('if', 53), ('we', 54), ('about', 55), ('n', 56), ('&', 57), ('he', 58), (':', 59), ('what', 60), ('when', 61), ('day', 62), ('love', 63), ('or', 64), ('no', 65), ('out', 66), ('by', 67), ('from', 68), ('good', 69), ('get', 70), ('know', 71), ('now', 72), ('as', 73), ('her', 74), ('u', 75), ('she', 76), ('life', 77), ('see', 78), ('go', 79), ('more', 80), ('people', 81), ('/', 82), ('time', 83), ('got', 84), ('has', 85), ('one', 86), ('an', 87), ('make', 88), ('always', 89), ('his', 90), ('smile', 91), (':face_with_tears_of_joy:', 92), ('been', 93), ('some', 94), ('hilarious', 95), ('how', 96), ('optimism', 97), ('because', 98), ('did', 99), ('going', 100), ('over', 101), ('them', 102), ('want', 103), ('being', 104), ('off', 105), ('why', 106), ('much', 107), ('new', 108), ('there', 109), ('way', 110), ('who', 111), ('would', 112), ('need', 113), ('their', 114), ('back', 115), ('feel', 116), ('our', 117), ('right', 118), ('every', 119), ('laughter', 120), ('never', 121), ('better', 122), ('breezy', 123), ('d', 124), ('give', 125), ('really', 126), ('something', 127), ('even', 128), ('today', 129), ('too', 130), ('into', 131), ('us', 132), ('well', 133), ('again', 134), ('best', 135), ('god', 136), ('little', 137), ('lol', 138), ('only', 139), ('still', 140), ('t', 141), ('then', 142), ('cheer', 143), ('days', 144), ('ever', 145), ('had', 146), ('let', 147), ('someone', 148), ('think', 149), ('work', 150), ('does', 151), ('find', 152), ('him', 153), ('said', 154), ('(', 155), (')', 156), ('*', 157), (';', 158), ('<elongated>', 159), ('<happy>', 160), ('here', 161), ('hope', 162), ('man', 163), ('say', 164), ('thank', 165), ('were', 166), ('after', 167), ('anger', 168), ('away', 169), ('lost', 170), ('sad', 171), ('should', 172), ('smiling', 173), ('beautiful', 174), ('everyone', 175), ('everything', 176), ('first', 177), ('glee', 178), ('hate', 179), ('live', 180), ('look', 181), ('most', 182), ('than', 183), ('thing', 184), ('wait', 185), ('where', 186), ('before', 187), ('birthday', 188), ('makes', 189), ('pleasing', 190), ('rejoice', 191), ('team', 192), ('watching', 193), ('world', 194), ('animated', 195), ('chirp', 196), ('fear', 197), ('hearty', 198), ('rejoicing', 199), ('sadness', 200), ('side', 201), ('such', 202), ('these', 203), ('things', 204), ('tonight', 205), ('cheerfully', 206), ('could', 207), ('delight', 208), ('fun', 209), ('gets', 210), ('having', 211), ('laugh', 212), ('long', 213), ('mind', 214), ('nice', 215), ('night', 216), ('playful', 217), ('sure', 218), ('very', 219), ('week', 220), ('year', 221), ('anxiety', 222), ('any', 223), ('cheerful', 224), ('exhilarating', 225), ('funny', 226), ('getting', 227), ('great', 228), ('hilarity', 229), ('its', 230), ('looking', 231), ('offended', 232), ('pout', 233), ('quote', 234), ('stop', 235), ('take', 236), ('thanks', 237), ('try', 238), ('watch', 239), ('+', 240), (':red_heart:', 241), (':smiling_face_with_heart-eyes:', 242), (':weary_face:', 243), ('<sad>', 244), ('<time>', 245), ('also', 246), ('blues', 247), ('bright', 248), ('cheering', 249), ('depression', 250), ('down', 251), ('heart', 252), ('im', 253), ('joy', 254), ('joyful', 255), ('last', 256), ('least', 257), ('looks', 258), ('many', 259), ('morning', 260), ('oh', 261), ('old', 262), ('same', 263), ('sorry', 264), ('sparkling', 265), ('1', 266), (':fire:', 267), (':frowning_face:', 268), (':light_skin_tone:', 269), ('amazing', 270), ('awful', 271), ('b', 272), ('bad', 273), ('cause', 274), ('change', 275), ('come', 276), ('elated', 277), ('end', 278), ('fucking', 279), ('horror', 280), ('hour', 281), ('jovial', 282), ('joyous', 283), ('keep', 284), ('laughing', 285), ('made', 286), ('may', 287), ('myself', 288), ('needs', 289), ('own', 290), ('sadly', 291), ('shit', 292), ('show', 293), ('social', 294), ('started', 295), ('strong', 296), ('terrible', 297), ('those', 298), ('trying', 299), ('use', 300), ('wanna', 301), ('2', 302), ('actually', 303), ('anything', 304), ('bitter', 305), ('black', 306), ('boy', 307), ('city', 308), ('comes', 309), ('dark', 310), ('follow', 311), ('gone', 312), ('guy', 313), ('hard', 314), ('help', 315), ('home', 316), ('lively', 317), ('m', 318), ('once', 319), ('other', 320), ('phone', 321), ('please', 322), ('re', 323), ('sleep', 324), ('sober', 325), ('tell', 326), ('thought', 327), ('tired', 328), ('tomorrow', 329), ('took', 330), ('trump', 331), ('ur', 332), ('which', 333), ('wish', 334), ('worst', 335), (':loudly_crying_face:', 336), (':smiling_face:', 337), (':sparkling_heart:', 338), ('ain', 339), ('angry', 340), ('around', 341), ('big', 342), ('bit', 343), ('body', 344), ('called', 345), ('depressing', 346), ('doing', 347), ('don', 348), ('done', 349), ('dont', 350), ('feeling', 351), ('gave', 352), ('heyday', 353), ('hungry', 354), ('hurt', 355), ('king', 356), ('levity', 357), ('means', 358), ('music', 359), ('must', 360), ('ni', 361), ('panic', 362), ('part', 363), ('person', 364), ('ppl', 365), ('pretty', 366), ('real', 367), ('revenge', 368), ('since', 369), ('snap', 370), ('start', 371), ('stay', 372), ('success', 373), ('talk', 374), ('twitter', 375), ('unhappy', 376), ('w', 377), ('water', 378), ('while', 379), ('word', 380), ('worry', 381), ('5', 382), (':enraged_face:', 383), ('<money>', 384), ('ago', 385), ('almost', 386), ('ass', 387), ('bb', 388), ('bot', 389), ('break', 390), ('bully', 391), ('cheery', 392), ('die', 393), ('died', 394), ('dreadful', 395), ('either', 396), ('else', 397), ('enough', 398), ('excited', 399), ('fall', 400), ('family', 401), ('found', 402), ('fuming', 403), ('gbbo', 404), ('girl', 405), ('girls', 406), ('glad', 407), ('gotta', 408), ('happiness', 409), ('hey', 410), ('high', 411), ('house', 412), ('irritate', 413), ('kill', 414), ('kinda', 415), ('lips', 416), ('lord', 417), ('mad', 418), ('mean', 419), ('media', 420), ('might', 421), ('miss', 422), ('moment', 423), ('name', 424), ('nervous', 425), ('next', 426), ('ok', 427), ('place', 428), ('point', 429), ('protest', 430), ('remember', 431), ('saying', 432), ('seems', 433), ('seen', 434), ('serious', 435), ('service', 436), ('shocking', 437), ('sometimes', 438), ('soul', 439), ('sunk', 440), ('taking', 441), ('talking', 442), ('terrorism', 443), ('thinking', 444), ('through', 445), ('times', 446), ('true', 447), ('truly', 448), ('understand', 449), ('video', 450), ('weak', 451), ('white', 452), ('whole', 453), ('words', 454), ('yet', 455), ('yo', 456), ('—', 457), ('’', 458), ('afternoon', 459), ('already', 460), ('another', 461), ('anymore', 462), ('anyone', 463), ('ask', 464), ('bill', 465), ('blessed', 466), ('bored', 467), ('charlotte', 468), ('choose', 469), ('class', 470), ('cut', 471), ('despair', 472), ('dream', 473), ('due', 474), ('during', 475), ('easy', 476), ('exhilaration', 477), ('face', 478), ('free', 479), ('friend', 480), ('fuck', 481), ('game', 482), ('gleeful', 483), ('goes', 484), ('gonna', 485), ('grim', 486), ('guys', 487), ('hear', 488), ('hell', 489), ('horrible', 490), ('jeep', 491), ('leads', 492), ('left', 493), ('listen', 494), ('lose', 495), ('lot', 496), ('lots', 497), ('making', 498), ('maybe', 499), ('milk', 500), ('money', 501), ('news', 502), ('nothing', 503), ('outrage', 504), ('pakistan', 505), ('past', 506), ('playing', 507), ('quite', 508), ('raging', 509), ('restless', 510), ('saw', 511), ('seeing', 512), ('self', 513), ('situation', 514), ('state', 515), ('super', 516), ('terror', 517), ('though', 518), ('wake', 519), ('woman', 520), ('wonder', 521), ('wonderful', 522), ('working', 523), ('wrath', 524), ('yeah', 525), ('young', 526), ('0', 527), (':heart_suit:', 528), (':see-no-evil_monkey:', 529), (':sign_of_the_horns:', 530), (':thinking_face:', 531), ('<censored>', 532), ('[', 533), ('accept', 534), ('against', 535), ('asked', 536), ('awareness', 537), ('awe', 538), ('baby', 539), ('bet', 540), ('call', 541), ('cheerfulness', 542), ('child', 543), ('club', 544), ('college', 545), ('cops', 546), ('country', 547), ('cry', 548), ('crying', 549), ('cute', 550), ('debate', 551), ('discourage', 552), ('double', 553), ('early', 554), ('experience', 555), ('extra', 556), ('frown', 557), ('full', 558), ('future', 559), ('gloomy', 560), ('goal', 561), ('guess', 562), ('half', 563), ('happened', 564), ('head', 565), ('hillary', 566), ('hit', 567), ('horrid', 568), ('indian', 569), ('k', 570), ('living', 571), ('ll', 572), ('lmao', 573), ('melancholy', 574), ('minutes', 575), ('mirth', 576), ('mom', 577), ('movie', 578), ('obviously', 579), ('offend', 580), ('okay', 581), ('omg', 582), ('outside', 583), ('peace', 584), ('player', 585), ('problem', 586), ('questions', 587), ('red', 588), ('rock', 589), ('room', 590), ('second', 591), ('sick', 592), ('sink', 593), ('sir', 594), ('sister', 595), ('starts', 596), ('story', 597), ('sweet', 598), ('tears', 599), ('tweet', 600), ('two', 601), ('until', 602), ('used', 603), ('v', 604), ('voice', 605), ('vote', 606), ('walk', 607), ('wanted', 608), ('welcome', 609), ('without', 610), ('years', 611), ('yes', 612), ('youtube', 613), ('', 614), ('#', 615), ('*\\\\', 616), ('4', 617), (':downcast_face_with_sweat:', 618), (':face_blowing_a_kiss:', 619), (':neutral_face:', 620), (':pensive_face:', 621), (':smiling_face_with_open_hands:', 622), (':smiling_face_with_smiling_eyes:', 623), ('<', 624), ('<percent>', 625), ('=', 626), ('>', 627), ('ages', 628), ('aggravate', 629), ('arms', 630), ('bc', 631), ('believe', 632), ('bitch', 633), ('blm', 634), ('boiling', 635), ('both', 636), ('breakfast', 637), ('bridget', 638), ('brother', 639), ('burning', 640), ('business', 641), ('cable', 642), ('cannot', 643), ('check', 644), ('chris', 645), ('church', 646), ('comedy', 647), ('coming', 648), ('cool', 649), ('damn', 650), ('deserve', 651), ('dire', 652), ('dismal', 653), ('draw', 654), ('drunk', 655), ('dull', 656), ('dying', 657), ('e', 658), ('events', 659), ('exact', 660), ('exactly', 661), ('expect', 662), ('express', 663), ('fans', 664), ('few', 665), ('fiery', 666), ('final', 667), ('fine', 668), ('finished', 669), ('flu', 670), ('friends', 671), ('front', 672), ('games', 673), ('goals', 674), ('green', 675), ('happen', 676), ('heard', 677), ('hold', 678), ('important', 679), ('innocent', 680), ('job', 681), ('join', 682), ('kind', 683), ('kindness', 684), ('knew', 685), ('knowing', 686), ('known', 687), ('knows', 688), ('late', 689), ('later', 690), ('leave', 691), ('less', 692), ('level', 693), ('light', 694), ('line', 695), ('literally', 696), ('lovely', 697), ('met', 698), ('mood', 699), ('mourn', 700), ('negative', 701), ('nhe', 702), ('nicole', 703), ('online', 704), ('order', 705), ('others', 706), ('pain', 707), ('perfect', 708), ('picture', 709), ('pleasure', 710), ('police', 711), ('power', 712), ('pre', 713), ('problems', 714), ('r', 715), ('radio', 716), ('respect', 717), ('save', 718), ('says', 719), ('school', 720), ('season', 721), ('seem', 722), ('send', 723), ('series', 724), ('seriously', 725), ('shake', 726), ('shame', 727), ('simply', 728), ('single', 729), ('smh', 730), ('st', 731), ('stayed', 732), ('sting', 733), ('straight', 734), ('swear', 735), ('texas', 736), ('th', 737), ('tour', 738), ('tried', 739), ('tv', 740), ('twice', 741), ('ty', 742), ('united', 743), ('vs', 744), ('wants', 745), ('war', 746), ('wasn', 747), ('watched', 748), ('wednesday', 749), ('weekend', 750), ('whatever', 751), ('whether', 752), ('wild', 753), ('william', 754), ('wine', 755), ('within', 756), ('won', 757), ('x', 758), ('yay', 759), ('yourself', 760), ('»', 761), ('…', 762), ('%', 763), ('18', 764), ('2016', 765), ('9', 766), (':blue_heart:', 767), (':confused_face:', 768), (':face_with_rolling_eyes:', 769), (':hundred_points:', 770), (':leaf_fluttering_in_wind:', 771), (':maple_leaf:', 772), (':slightly_smiling_face:', 773), (':thumbs_down:', 774), (':unamused_face:', 775), ('<annoyed>', 776), ('<date>', 777), ('<wink>', 778), (']', 779), ('ability', 780), ('able', 781), ('absolute', 782), ('absolutely', 783), ('across', 784), ('act', 785), ('acting', 786), ('aesthetically', 787), ('afraid', 788), ('air', 789), ('alarm', 790), ('along', 791), ('amendment', 792), ('america', 793), ('anthony', 794), ('attempt', 795), ('autumn', 796), ('average', 797), ('awesome', 798), ('bag', 799), ('banter', 800), ('begin', 801), ('behind', 802), ('between', 803), ('beyond', 804), ('birds', 805), ('blog', 806), ('blue', 807), ('boss', 808), ('bought', 809), ('boys', 810), ('brad', 811), ('breaks', 812), ('brilliant', 813), ('bring', 814), ('brings', 815), ('broke', 816), ('broken', 817), ('burns', 818), ('butter', 819), ('cake', 820), ('came', 821), ('care', 822), ('cats', 823), ('challenge', 824), ('changed', 825), ('changing', 826), ('clean', 827), ('clinton', 828), ('color', 829), ('comments', 830), ('company', 831), ('completely', 832), ('concern', 833), ('confused', 834), ('cup', 835), ('current', 836), ('customer', 837), ('cuz', 838), ('date', 839), ('dead', 840), ('death', 841), ('decide', 842), ('decision', 843), ('despite', 844), ('dick', 845), ('discouraged', 846), ('discover', 847), ('door', 848), ('dread', 849), ('drink', 850), ('dude', 851), ('each', 852), ('eat', 853), ('eating', 854), ('eh', 855), ('emails', 856), ('enjoy', 857), ('episode', 858), ('etc', 859), ('evil', 860), ('facebook', 861), ('fail', 862), ('failure', 863), ('fast', 864), ('fat', 865), ('feels', 866), ('felt', 867), ('fight', 868), ('film', 869), ('finally', 870), ('fire', 871), ('fitness', 872), ('fix', 873), ('food', 874), ('football', 875), ('forest', 876), ('forever', 877), ('forward', 878), ('four', 879), ('freedom', 880), ('fucked', 881), ('fuckin', 882), ('furious', 883), ('fury', 884), ('g', 885), ('gladness', 886), ('grand', 887), ('grateful', 888), ('grow', 889), ('grudge', 890), ('haha', 891), ('heaven', 892), ('held', 893), ('hesitate', 894), ('hi', 895), ('hockey', 896), ('honestly', 897), ('horrific', 898), ('hot', 899), ('huh', 900), ('human', 901), ('idea', 902), ('imagine', 903), ('inside', 904), ('instead', 905), ('irritation', 906), ('isis', 907), ('italian', 908), ('jack', 909), ('japanese', 910), ('joe', 911), ('john', 912), ('keeping', 913), ('kiss', 914), ('l', 915), ('language', 916), ('laughs', 917), ('laws', 918), ('learn', 919), ('liking', 920), ('lip', 921), ('listed', 922), ('liveliness', 923), ('lives', 924), ('looked', 925), ('loss', 926), ('loves', 927), ('loving', 928), ('madden', 929), ('meant', 930), ('meeting', 931), ('minute', 932), ('missed', 933), ('mon', 934), ('moon', 935), ('mouth', 936), ('moving', 937), ('mr', 938), ('names', 939), ('national', 940), ('nightmare', 941), ('non', 942), ('nut', 943), ('o', 944), ('obama', 945), ('ones', 946), ('opinion', 947), ('opportunity', 948), ('pak', 949), ('park', 950), ('party', 951), ('paul', 952), ('photo', 953), ('piece', 954), ('plan', 955), ('plenty', 956), ('pls', 957), ('points', 958), ('pour', 959), ('present', 960), ('pro', 961), ('probably', 962), ('pure', 963), ('put', 964), ('quiet', 965), ('race', 966), ('rage', 967), ('rap', 968), ('rather', 969), ('ready', 970), ('reality', 971), ('realize', 972), ('record', 973), ('relentless', 974), ('report', 975), ('resent', 976), ('rip', 977), ('rn', 978), ('run', 979), ('running', 980), ('scene', 981), ('seasons', 982), ('shaking', 983), ('share', 984), ('short', 985), ('shut', 986), ('shy', 987), ('sight', 988), ('somebody', 989), ('song', 990), ('soon', 991), ('sort', 992), ('sound', 993), ('south', 994), ('space', 995), ('speak', 996), ('speed', 997), ('spend', 998), ('spot', 999), ('sprightly', 1000), ('spry', 1001), ('stand', 1002), ('stomach', 1003), ('stuff', 1004), ('stupid', 1005), ('sutton', 1006), ('system', 1007), ('takes', 1008), ('teaching', 1009), ('test', 1010), ('tf', 1011), ('thenicebot', 1012), ('tho', 1013), ('thy', 1014), ('tom', 1015), ('train', 1016), ('trans', 1017), ('tremendous', 1018), ('truth', 1019), ('turned', 1020), ('tweets', 1021), ('ugh', 1022), ('useless', 1023), ('version', 1024), ('victory', 1025), ('view', 1026), ('waters', 1027), ('ways', 1028), ('weakness', 1029), ('weary', 1030), ('weather', 1031), ('weiner', 1032), ('went', 1033), ('whenever', 1034), ('win', 1035), ('winning', 1036), ('wins', 1037), ('women', 1038), ('worse', 1039), ('wow', 1040), ('ya', 1041), ('yell', 1042), ('yesterday', 1043), ('yoga', 1044), ('yr', 1045), ('•', 1046), ('16', 1047), ('69', 1048), (':balance_scale:', 1049), (':collision:', 1050), (':crying_face:', 1051), (':double_exclamation_mark:', 1052), (':fallen_leaf:', 1053), (':grinning_face_with_sweat:', 1054), (':medium-dark_skin_tone:', 1055), (':nerd_face:', 1056), (':party_popper:', 1057), (':person_tipping_hand:', 1058), (':raising_hands:', 1059), (':right_arrow:', 1060), (':smiling_face_with_horns:', 1061), (':sparkles:', 1062), (':two_hearts:', 1063), (':upside-down_face:', 1064), (':winking_face:', 1065), (':yellow_heart:', 1066), ('<laugh>', 1067), ('<surprise>', 1068), ('>_<', 1069), ('@', 1070), ('access', 1071), ('account', 1072), ('acquire', 1073), ('actors', 1074), ('add', 1075), ('addiction', 1076), ('af', 1077), ('affliction', 1078), ('age', 1079), ('agreed', 1080), ('airline', 1081), ('alive', 1082), ('alone', 1083), ('american', 1084), ('among', 1085), ('amount', 1086), ('angelou', 1087), ('animal', 1088), ('anna', 1089), ('annoyed', 1090), ('answer', 1091), ('answers', 1092), ('anxious', 1093), ('anybody', 1094), ('anyway', 1095), ('anywhere', 1096), ('apart', 1097), ('apologies', 1098), ('app', 1099), ('appreciate', 1100), ('arc', 1101), ('army', 1102), ('art', 1103), ('article', 1104), ('atmosphere', 1105), ('aw', 1106), ('awfy', 1107), ('awkward', 1108), ('aww', 1109), ('backing', 1110), ('ball', 1111), ('balloons', 1112), ('bars', 1113), ('bass', 1114), ('battleground', 1115), ('bbc', 1116), ('bear', 1117), ('beats', 1118), ('beauty', 1119), ('became', 1120), ('become', 1121), ('beer', 1122), ('bees', 1123), ('beneath', 1124), ('biggest', 1125), ('binge', 1126), ('birth', 1127), ('bitches', 1128), ('biz', 1129), ('blacks', 1130), ('blame', 1131), ('blamed', 1132), ('bldg', 1133), ('blend', 1134), ('blind', 1135), ('block', 1136), ('bloods', 1137), ('blow', 1138), ('bob', 1139), ('book', 1140), ('boots', 1141), ('born', 1142), ('bp', 1143), ('brain', 1144), ('brand', 1145), ('bringing', 1146), ('brock', 1147), ('brown', 1148), ('bunch', 1149), ('buoyant', 1150), ('burst', 1151), ('busy', 1152), ('button', 1153), ('buy', 1154), ('c', 1155), ('calm', 1156), ('canceled', 1157), ('candice', 1158), ('cant', 1159), ('car', 1160), ('celtic', 1161), ('century', 1162), ('chance', 1163), ('chances', 1164), ('channel', 1165), ('character', 1166), ('characters', 1167), ('charge', 1168), ('chasing', 1169), ('cheddar', 1170), ('cheerleaders', 1171), ('cheese', 1172), ('chills', 1173), ('china', 1174), ('christ', 1175), ('clear', 1176), ('clever', 1177), ('clients', 1178), ('close', 1179), ('clue', 1180), ('coach', 1181), ('cold', 1182), ('comment', 1183), ('community', 1184), ('congratulations', 1185), ('constant', 1186), ('content', 1187), ('contentwiththe', 1188), ('conversation', 1189), ('convinced', 1190), ('cook', 1191), ('cooped', 1192), ('copy', 1193), ('corrupt', 1194), ('costs', 1195), ('court', 1196), ('cover', 1197), ('coys', 1198), ('created', 1199), ('creative', 1200), ('crimes', 1201), ('crossover', 1202), ('crushes', 1203), ('dad', 1204), ('daily', 1205), ('dat', 1206), ('de', 1207), ('dean', 1208), ('definitely', 1209), ('dentist', 1210), ('depress', 1211), ('depressed', 1212), ('design', 1213), ('desolate', 1214), ('detox', 1215), ('development', 1216), ('devil', 1217), ('dex', 1218), ('different', 1219), ('difficulty', 1220), ('direct', 1221), ('direction', 1222), ('disallowed', 1223), ('disappointed', 1224), ('discipleship', 1225), ('disco', 1226), ('disheartened', 1227), ('displeasure', 1228), ('ditto', 1229), ('divorce', 1230), ('doesnt', 1231), ('dog', 1232), ('drop', 1233), ('dropped', 1234), ('dry', 1235), ('dt', 1236), ('dunno', 1237), ('dylan', 1238), ('ear', 1239), ('editing', 1240), ('efl', 1241), ('elate', 1242), ('embarrassed', 1243), ('embrace', 1244), ('embrrssd', 1245), ('emoji', 1246), ('emotion', 1247), ('english', 1248), ('entertaining', 1249), ('environment', 1250), ('equinox', 1251), ('escape', 1252), ('especially', 1253), ('etep', 1254), ('everyday', 1255), ('exist', 1256), ('explanation', 1257), ('expression', 1258), ('extremely', 1259), ('eye', 1260), ('f', 1261), ('faced', 1262), ('facing', 1263), ('fact', 1264), ('facts', 1265), ('fails', 1266), ('fallen', 1267), ('falling', 1268), ('fan', 1269), ('fantasy', 1270), ('far', 1271), ('faster', 1272), ('favourite', 1273), ('fed', 1274), ('feelings', 1275), ('female', 1276), ('ffs', 1277), ('fi', 1278), ('films', 1279), ('finds', 1280), ('finest', 1281), ('finish', 1282), ('fitted', 1283), ('flavors', 1284), ('flight', 1285), ('florida', 1286), ('folks', 1287), ('followback', 1288), ('followers', 1289), ('following', 1290), ('fond', 1291), ('form', 1292), ('former', 1293), ('forth', 1294), ('forté', 1295), ('friday', 1296), ('friedrich', 1297), ('frustrated', 1298), ('garbage', 1299), ('garda', 1300), ('gas', 1301), ('gate', 1302), ('gay', 1303), ('gen', 1304), ('german', 1305), ('ghost', 1306), ('gift', 1307), ('girlfriends', 1308), ('given', 1309), ('glorious', 1310), ('goin', 1311), ('gold', 1312), ('goodbye', 1313), ('gosh', 1314), ('govt', 1315), ('grass', 1316), ('gratefully', 1317), ('greater', 1318), ('greatest', 1319), ('greyhound', 1320), ('grieve', 1321), ('grindah', 1322), ('guitar', 1323), ('h', 1324), ('ha', 1325), ('habit', 1326), ('hahaha', 1327), ('hair', 1328), ('haircut', 1329), ('haitian', 1330), ('hamlet', 1331), ('hand', 1332), ('hang', 1333), ('hannah', 1334), ('harbor', 1335), ('haters', 1336), ('heads', 1337), ('health', 1338), ('hearts', 1339), ('hello', 1340), ('helping', 1341), ('hips', 1342), ('history', 1343), ('hopefully', 1344), ('hopelessness', 1345), ('hoping', 1346), ('hours', 1347), ('howard', 1348), ('huff', 1349), ('huge', 1350), ('hugs', 1351), ('hundred', 1352), ('idiot', 1353), ('ignorance', 1354), ('ignored', 1355), ('impractical', 1356), ('incredible', 1357), ('individual', 1358), ('info', 1359), ('initiative', 1360), ('insult', 1361), ('internet', 1362), ('invite', 1363), ('ish', 1364), ('island', 1365), ('issue', 1366), ('itching', 1367), ('jail', 1368), ('james', 1369), ('jaunty', 1370), ('jesus', 1371), ('jinyoung', 1372), ('johnson', 1373), ('joined', 1374), ('joke', 1375), ('jokers', 1376), ('jokes', 1377), ('jones', 1378), ('joshua', 1379), ('jubilant', 1380), ('justice', 1381), ('kerr', 1382), ('key', 1383), ('kidding', 1384), ('killing', 1385), ('klassic', 1386), ('kurt', 1387), ('lack', 1388), ('lads', 1389), ('lady', 1390), ('lately', 1391), ('laundry', 1392), ('leadership', 1393), ('leaned', 1394), ('learning', 1395), ('legit', 1396), ('lets', 1397), ('letting', 1398), ('lie', 1399), ('lies', 1400), ('lights', 1401), ('likely', 1402), ('lil', 1403), ('lion', 1404), ('listening', 1405), ('lived', 1406), ('lock', 1407), ('locks', 1408), ('london', 1409), ('longer', 1410), ('loose', 1411), ('looting', 1412), ('lords', 1413), ('losing', 1414), ('luv', 1415), ('lying', 1416), ('machine', 1417), ('manchester', 1418), ('mansfield', 1419), ('mark', 1420), ('master', 1421), ('mate', 1422), ('matt', 1423), ('matter', 1424), ('maya', 1425), ('mc', 1426), ('meds', 1427), ('meet', 1428), ('memories', 1429), ('men', 1430), ('mercy', 1431), ('messaged', 1432), ('messy', 1433), ('michael', 1434), ('michelle', 1435), ('midnight', 1436), ('midst', 1437), ('mil', 1438), ('mindfulness', 1439), ('minds', 1440), ('mine', 1441), ('mins', 1442), ('misery', 1443), ('missing', 1444), ('mistake', 1445), ('modern', 1446), ('modicum', 1447), ('month', 1448), ('morrow', 1449), ('mother', 1450), ('move', 1451), ('moved', 1452), ('movies', 1453), ('murky', 1454), ('musically', 1455), ('mustache', 1456), ('na', 1457), ('nand', 1458), ('nap', 1459), ('nascar', 1460), ('nature', 1461), ('nawaz', 1462), ('nd', 1463), ('ndoesn', 1464), ('nephew', 1465), ('nietzsche', 1466), ('niggas', 1467), ('nights', 1468), ('nmom', 1469), ('nno', 1470), ('nope', 1471), ('nose', 1472), ('notice', 1473), ('nstephen', 1474), ('nthere', 1475), ('numb', 1476), ('number', 1477), ('numbers', 1478), ('nyc', 1479), ('odd', 1480), ('odds', 1481), ('odell', 1482), ('offensive', 1483), ('offers', 1484), ('official', 1485), ('often', 1486), ('older', 1487), ('open', 1488), ('optimist', 1489), ('option', 1490), ('options', 1491), ('overcome', 1492), ('overtime', 1493), ('owners', 1494), ('pack', 1495), ('paid', 1496), ('painful', 1497), ('pait', 1498), ('pale', 1499), ('pants', 1500), ('parents', 1501), ('parks', 1502), ('pastor', 1503), ('pay', 1504), ('peanut', 1505), ('peg', 1506), ('per', 1507), ('perception', 1508), ('permissive', 1509), ('personal', 1510), ('pervert', 1511), ('pessimist', 1512), ('pete', 1513), ('phil', 1514), ('philipp', 1515), ('pine', 1516), ('pissed', 1517), ('pitt', 1518), ('pity', 1519), ('pizza', 1520), ('planned', 1521), ('play', 1522), ('players', 1523), ('plays', 1524), ('plein', 1525), ('plus', 1526), ('policy', 1527), ('politics', 1528), ('poll', 1529), ('poor', 1530), ('pop', 1531), ('portion', 1532), ('posi', 1533), ('positivity', 1534), ('post', 1535), ('posted', 1536), ('pot', 1537), ('praying', 1538), ('preparing', 1539), ('president', 1540), ('prices', 1541), ('princess', 1542), ('productivity', 1543), ('projection', 1544), ('proud', 1545), ('prussian', 1546), ('ps', 1547), ('public', 1548), ('pulls', 1549), ('q', 1550), ('quality', 1551), ('quit', 1552), ('quote_soup', 1553), ('quotes', 1554), ('racism', 1555), ('random', 1556), ('rape', 1557), ('rappers', 1558), ('raw', 1559), ('reading', 1560), ('reaper', 1561), ('reason', 1562), ('reasons', 1563), ('rec', 1564), ('recently', 1565), ('refs', 1566), ('refugees', 1567), ('relationships', 1568), ('relevant', 1569), ('relief', 1570), ('remind', 1571), ('requires', 1572), ('resentment', 1573), ('response', 1574), ('rest', 1575), ('return', 1576), ('revolution', 1577), ('rich', 1578), ('ride', 1579), ('ridiculously', 1580), ('robbery', 1581), ('roblox', 1582), ('rojo', 1583), ('rooney', 1584), ('rude', 1585), ('ruin', 1586), ('runs', 1587), ('russia', 1588), ('safe', 1589), ('sat', 1590), ('satisfy', 1591), ('saturday', 1592), ('scare', 1593), ('scared', 1594), ('sci', 1595), ('scripts', 1596), ('sedate', 1597), ('seek', 1598), ('sees', 1599), ('sending', 1600), ('sense', 1601), ('sent', 1602), ('sex', 1603), ('sexuality', 1604), ('sexually', 1605), ('shakespeare', 1606), ('shd', 1607), ('shooting', 1608), ('shootings', 1609), ('shot', 1610), ('showing', 1611), ('sign', 1612), ('signed', 1613), ('silly', 1614), ('similar', 1615), ('smack', 1616), ('smart', 1617), ('smell', 1618), ('snapchat', 1619), ('sniffle', 1620), ('snowgang', 1621), ('soft', 1622), ('sold', 1623), ('songs', 1624), ('sounds', 1625), ('source', 1626), ('speaking', 1627), ('speechless', 1628), ('spoken', 1629), ('sports', 1630), ('star', 1631), ('starting', 1632), ('statement', 1633), ('step', 1634), ('stitch', 1635), ('stock', 1636), ('store', 1637), ('strangers', 1638), ('stress', 1639), ('stretch', 1640), ('stuck', 1641), ('student', 1642), ('stunt', 1643), ('style', 1644), ('sucker', 1645), ('suggestions', 1646), ('sulk', 1647), ('summer', 1648), ('sunrise', 1649), ('superman', 1650), ('supporting', 1651), ('supposed', 1652), ('swans', 1653), ('swung', 1654), ('table', 1655), ('tag', 1656), ('talkin', 1657), ('tamra', 1658), ('taste', 1659), ('tax', 1660), ('tbh', 1661), ('teeth', 1662), ('tells', 1663), ('temperament', 1664), ('ten', 1665), ('tend', 1666), ('terms', 1667), ('terrific', 1668), ('terrorist', 1669), ('texans', 1670), ('thee', 1671), ('themselves', 1672), ('thin', 1673), ('thoughts', 1674), ('thousands', 1675), ('threaten', 1676), ('thru', 1677), ('thumbs', 1678), ('tiff', 1679), ('tigers', 1680), ('till', 1681), ('tips', 1682), ('together', 1683), ('told', 1684), ('tone', 1685), ('top', 1686), ('totally', 1687), ('training', 1688), ('tremor', 1689), ('tribute', 1690), ('triggered', 1691), ('troubled', 1692), ('troyler', 1693), ('trust', 1694), ('tulsa', 1695), ('turn', 1696), ('turning', 1697), ('twd', 1698), ('ugly', 1699), ('ukulele', 1700), ('un', 1701), ('uncertainty', 1702), ('universal', 1703), ('update', 1704), ('upset', 1705), ('using', 1706), ('val', 1707), ('value', 1708), ('vic', 1709), ('videos', 1710), ('violence', 1711), ('visit', 1712), ('waiting', 1713), ('wakes', 1714), ('warrior', 1715), ('wear', 1716), ('web', 1717), ('website', 1718), ('weeks', 1719), ('weird', 1720), ('whitegirlwednesday', 1721), ('wisdom', 1722), ('wise', 1723), ('woke', 1724), ('wondering', 1725), ('worth', 1726), ('write', 1727), ('written', 1728), ('wrong', 1729), ('wwe', 1730), ('yacht', 1731), ('yeg', 1732), ('yep', 1733), ('yikes', 1734), ('yrs', 1735), ('z', 1736), ('zero', 1737), ('~', 1738), ('°', 1739), ('–', 1740), ('―', 1741), ('“', 1742), ('”', 1743), ('┻', 1744), ('╯', 1745), ('☉', 1746), ('\\uf602', 1747), ('）', 1748), ('$', 1749), ('$$', 1750), ('$srpt', 1751), ('-_-', 1752), ('12', 1753), ('20', 1754), ('22', 1755), ('3', 1756), ('30', 1757), ('6', 1758), ('62', 1759), ('7', 1760), ('8', 1761), ('815', 1762), (':((((((((((((((((((((((((', 1763), (':OK_hand:', 1764), (':alien_monster:', 1765), (':anguished_face:', 1766), (':anxious_face_with_sweat:', 1767), (':atom_symbol:', 1768), (':baby_chick:', 1769), (':balloon:', 1770), (':beaming_face_with_smiling_eyes:', 1771), (':beer_mug:', 1772), (':bird:', 1773), (':blossom:', 1774), (':bow_and_arrow:', 1775), (':cheese_wedge:', 1776), (':clapping_hands:', 1777), (':clinking_beer_mugs:', 1778), (':confetti_ball:', 1779), (':crayon:', 1780), (':dark_skin_tone:', 1781), (':disappointed_face:', 1782), (':expressionless_face:', 1783), (':face_savoring_food:', 1784), (':face_screaming_in_fear:', 1785), (':face_with_steam_from_nose:', 1786), (':face_without_mouth:', 1787), (':first_quarter_moon:', 1788), (':flushed_face:', 1789), (':folded_hands:', 1790), (':front-facing_baby_chick:', 1791), (':grapes:', 1792), (':grinning_face_with_big_eyes:', 1793), (':heart_exclamation:', 1794), (':kaaba:', 1795), (':kissing_face_with_smiling_eyes:', 1796), (':left_arrow:', 1797), (':lemon:', 1798), (':medium-light_skin_tone:', 1799), (':medium_skin_tone:', 1800), (':money-mouth_face:', 1801), (':oncoming_fist:', 1802), (':pile_of_poo:', 1803), (':pizza:', 1804), (':purple_heart:', 1805), (':raised_hand:', 1806), (':red_circle:', 1807), (':relieved_face:', 1808), (':rose:', 1809), (':rosette:', 1810), (':rugby_football:', 1811), (':sheaf_of_rice:', 1812), (':skull:', 1813), (':sleepy_face:', 1814), (':smiling_face_with_halo:', 1815), (':smirking_face:', 1816), (':soccer_ball:', 1817), (':squinting_face_with_tongue:', 1818), (':thumbs_up:', 1819), (':trade_mark:', 1820), (':tropical_drink:', 1821), (':victory_hand:', 1822), (':white_circle:', 1823), (':wind_face:', 1824), (':wolf:', 1825), (':woman’s_clothes:', 1826), (':yin_yang:', 1827), (\";'(\\\\\", 1828), (';s', 1829), ('<emphasis>', 1830), ('<kiss>', 1831), ('<tong>', 1832), ('>:(', 1833), ('^', 1834), ('___', 1835), ('`', 1836), ('ab', 1837), ('abby', 1838), ('abood', 1839), ('above', 1840), ('abraham', 1841), ('abs', 1842), ('abt', 1843), ('abuse', 1844), ('abuser', 1845), ('abysmal', 1846), ('acc', 1847), ('acceptance', 1848), ('accepted', 1849), ('accidental', 1850), ('accommodate', 1851), ('accomplished', 1852), ('accomplishment', 1853), ('accounts', 1854), ('accrington', 1855), ('accurate', 1856), ('accurately', 1857), ('acknowledgement', 1858), ('acrid', 1859), ('action', 1860), ('activated', 1861), ('active', 1862), ('actively', 1863), ('activism', 1864), ('actresses', 1865), ('acts', 1866), ('ad', 1867), ('adcomm', 1868), ('adj', 1869), ('admin', 1870), ('administration', 1871), ('admits', 1872), ('admitted', 1873), ('admittedly', 1874), ('adopt', 1875), ('adopted', 1876), ('adorable', 1877), ('adoring', 1878), ('adrenaline', 1879), ('adressed', 1880), ('adrian', 1881), ('adverts', 1882), ('advise', 1883), ('advocate', 1884), ('afaik', 1885), ('afc', 1886), ('affect', 1887), ('afflicted', 1888), ('affront', 1889), ('afl', 1890), ('afp', 1891), ('afr', 1892), ('agin', 1893), ('ah', 1894), ('ahead', 1895), ('ahs6', 1896), ('aid', 1897), ('aiding', 1898), ('aidy', 1899), ('aint', 1900), ('airtel', 1901), ('ajahnae', 1902), ('aka', 1903), ('al', 1904), ('alan', 1905), ('alarming', 1906), ('album', 1907), ('aldub', 1908), ('aleppo', 1909), ('alert', 1910), ('alex', 1911), ('alicia', 1912), ('alloa', 1913), ('allow', 1914), ('aloha', 1915), ('alt', 1916), ('alternate', 1917), ('amaity', 1918), ('amal', 1919), ('ambitions', 1920), ('ampalaya', 1921), ('amystery', 1922), ('anatomy', 1923), ('andis', 1924), ('ang', 1925), ('angel', 1926), ('angelic', 1927), ('angelina', 1928), ('angers', 1929), ('angie', 1930), ('anniversary', 1931), ('announce', 1932), ('annoying', 1933), ('annoys', 1934), ('answered', 1935), ('anthem', 1936), ('anti', 1937), ('anticipating', 1938), ('anytime', 1939), ('apartment', 1940), ('apocalypse', 1941), ('apologize', 1942), ('appalling', 1943), ('apparently', 1944), ('appauling', 1945), ('appear', 1946), ('appearance', 1947), ('apples', 1948), ('applications', 1949), ('appreciation', 1950), ('apprehension', 1951), ('appropriate', 1952), ('approximately', 1953), ('apps', 1954), ('aps', 1955), ('aquila', 1956), ('arabia', 1957), ('archangel', 1958), ('archers', 1959), ('architecture', 1960), ('argh', 1961), ('argue', 1962), ('ari', 1963), ('ariel', 1964), ('ark', 1965), ('arrival', 1966), ('arriving', 1967), ('arrogance', 1968), ('arthur', 1969), ('asf', 1970), ('aside', 1971), ('asking', 1972), ('asks', 1973), ('asleep', 1974), ('aspak', 1975), ('aspire', 1976), ('assed', 1977), ('asshole', 1978), ('assigned', 1979), ('assume', 1980), ('assumed', 1981), ('assure', 1982), ('assured', 1983), ('astros', 1984), ('athlete', 1985), ('atkins', 1986), ('atm', 1987), ('atop', 1988), ('attachment', 1989), ('attack', 1990), ('attacked', 1991), ('attendance', 1992), ('attended', 1993), ('attracted', 1994), ('australia', 1995), ('automatic', 1996), ('autumnal', 1997), ('available', 1998), ('avataar', 1999), ('avatar', 2000), ('ave', 2001), ('avoided', 2002), ('awaiting', 2003), ('aware', 2004), ('awol', 2005), ('b4', 2006), ('b41', 2007), ('backbone', 2008), ('backed', 2009), ('backfire', 2010), ('background', 2011), ('backyard', 2012), ('badd', 2013), ('badger', 2014), ('badu', 2015), ('bahahahaha', 2016), ('bailey', 2017), ('bainton', 2018), ('bake', 2019), ('baker', 2020), ('bakes', 2021), ('balance', 2022), ('ballons', 2023), ('balls', 2024), ('balotelli', 2025), ('band', 2026), ('banging', 2027), ('banned', 2028), ('baptist', 2029), ('bar', 2030), ('barbaria', 2031), ('bare', 2032), ('barely', 2033), ('bashir', 2034), ('basic', 2035), ('basically', 2036), ('basket', 2037), ('baskets', 2038), ('basshunter', 2039), ('bat', 2040), ('bathroom', 2041), ('batman', 2042), ('battery', 2043), ('battle', 2044), ('bb18', 2045), ('bbva', 2046), ('bday', 2047), ('beating', 2048), ('beautyof', 2049), ('beef', 2050), ('beeping', 2051), ('beg', 2052), ('began', 2053), ('begging', 2054), ('beginning', 2055), ('bell', 2056), ('bellario', 2057), ('belly', 2058), ('ben', 2059), ('bench', 2060), ('bender', 2061), ('benefit', 2062), ('benet', 2063), ('benghazi', 2064), ('berniebrocialists', 2065), ('berry', 2066), ('besides', 2067), ('bey', 2068), ('beyoncé', 2069), ('bf', 2070), ('bffing', 2071), ('bible', 2072), ('bigger', 2073), ('bigotry', 2074), ('bilal', 2075), ('billie', 2076), ('bin', 2077), ('bird', 2078), ('birmingham', 2079), ('bitchass', 2080), ('bitcoin', 2081), ('bite', 2082), ('biweekly', 2083), ('bizarre', 2084), ('bizitalk', 2085), ('bjp', 2086), ('blaine', 2087), ('blair', 2088), ('blake', 2089), ('blames', 2090), ('blasio', 2091), ('bless', 2092), ('blessing', 2093), ('blew', 2094), ('blinding', 2095), ('bliss', 2096), ('blissful', 2097), ('blithe', 2098), ('blockade', 2099), ('blocking', 2100), ('blocks', 2101), ('blood', 2102), ('bloody', 2103), ('blossoming', 2104), ('blown', 2105), ('blushing', 2106), ('bodies', 2107), ('boggling', 2108), ('bojack', 2109), ('bolts', 2110), ('boobs', 2111), ('boosts', 2112), ('booth', 2113), ('bops', 2114), ('borrow', 2115), ('boshan', 2116), ('bout', 2117), ('bowels', 2118), ('box', 2119), ('boycotting', 2120), ('bozzelli', 2121), ('bpd', 2122), ('branch', 2123), ('bravado', 2124), ('brave', 2125), ('braved', 2126), ('brb', 2127), ('breakdown', 2128), ('breaking', 2129), ('breakups', 2130), ('brendan', 2131), ('brendon', 2132), ('brennan', 2133), ('brewing', 2134), ('brian', 2135), ('bribing', 2136), ('brighten', 2137), ('brims', 2138), ('british', 2139), ('broadband', 2140), ('broadcast', 2141), ('broccoli', 2142), ('brought', 2143), ('bruce', 2144), ('brush', 2145), ('brushing', 2146), ('brutality', 2147), ('bs', 2148), ('bsnl', 2149), ('bt', 2150), ('btw', 2151), ('bu', 2152), ('bub', 2153), ('bubble', 2154), ('buble', 2155), ('bugs', 2156), ('building', 2157), ('bulushi', 2158), ('bumping', 2159), ('burke', 2160), ('burton', 2161), ('bush', 2162), ('bushes', 2163), ('busquet', 2164), ('busty', 2165), ('butler', 2166), ('butt', 2167), ('butterflies', 2168), ('buzz', 2169), ('buzzin', 2170), ('buzzing', 2171), ('bye', 2172), ('ca', 2173), ('cab', 2174), ('caballero', 2175), ('cackle', 2176), ('caffeine', 2177), ('cage', 2178), ('cahracter', 2179), ('calls', 2180), ('calming', 2181), ('campaign', 2182), ('campaigning', 2183), ('camshaft', 2184), ('canada', 2185), ('canadian', 2186), ('cancel', 2187), ('cancelled', 2188), ('candace', 2189), ('candles', 2190), ('canning', 2191), ('capable', 2192), ('capita', 2193), ('capture', 2194), ('card', 2195), ('cards', 2196), ('caring', 2197), ('carriages', 2198), ('carrick', 2199), ('carries', 2200), ('cases', 2201), ('cashcow', 2202), ('cashew', 2203), ('cashier', 2204), ('cast', 2205), ('casuals', 2206), ('cat', 2207), ('catch', 2208), ('category', 2209), ('catering', 2210), ('caught', 2211), ('cavity', 2212), ('cbs', 2213), ('ccm', 2214), ('ccot', 2215), ('celebrate', 2216), ('celebrating', 2217), ('centre', 2218), ('certain', 2219), ('certainly', 2220), ('cf', 2221), ('chair', 2222), ('challenges', 2223), ('challenging', 2224), ('championship', 2225), ('changes', 2226), ('chaos', 2227), ('chapman', 2228), ('chapter', 2229), ('characteristic', 2230), ('charged', 2231), ('charger', 2232), ('charlie', 2233), ('charm', 2234), ('chases', 2235), ('chat', 2236), ('cheated', 2237), ('cheaters', 2238), ('checked', 2239), ('checkin', 2240), ('cheerios', 2241), ('cheeseburger', 2242), ('chelsea', 2243), ('cheryl', 2244), ('chewing', 2245), ('chick', 2246), ('chicken', 2247), ('childhood', 2248), ('childlike', 2249), ('children', 2250), ('chill', 2251), ('chirpy', 2252), ('chock', 2253), ('chocolate', 2254), ('chooses', 2255), ('chosen', 2256), ('christie', 2257), ('chronic', 2258), ('chuckin', 2259), ('chuckle', 2260), ('churchill', 2261), ('ciara', 2262), ('cider', 2263), ('cinematography', 2264), ('circle', 2265), ('circles', 2266), ('claimed', 2267), ('claims', 2268), ('claire', 2269), ('classic', 2270), ('claws', 2271), ('clearly', 2272), ('clergy', 2273), ('click', 2274), ('client', 2275), ('climb', 2276), ('climbed', 2277), ('clinging', 2278), ('clip', 2279), ('clips', 2280), ('clooney', 2281), ('closed', 2282), ('closest', 2283), ('clouded', 2284), ('clouds', 2285), ('cmon', 2286), ('cn', 2287), ('cnn', 2288), ('coaches', 2289), ('coaching', 2290), ('cocktails', 2291), ('coefficient', 2292), ('coffee', 2293), ('coffin', 2294), ('coincidence', 2295), ('coke', 2296), ('colin', 2297), ('collapses', 2298), ('colleague', 2299), ('collect', 2300), ('colleen', 2301), ('colors', 2302), ('combination', 2303), ('combined', 2304), ('comedic', 2305), ('comforting', 2306), ('comics', 2307), ('commercial', 2308), ('common', 2309), ('communism', 2310), ('communitysleepcoach', 2311), ('commute', 2312), ('comp', 2313), ('companies', 2314), ('companions', 2315), ('compelling', 2316), ('competitive', 2317), ('compiled', 2318), ('complete', 2319), ('compliment', 2320), ('comrades', 2321), ('con', 2322), ('concerned', 2323), ('concerning', 2324), ('concerns', 2325), ('concise', 2326), ('conclusion', 2327), ('confessions', 2328), ('confidence', 2329), ('confronted', 2330), ('confusion', 2331), ('congrats', 2332), ('congratulate', 2333), ('congress', 2334), ('connie', 2335), ('consistency', 2336), ('constitution', 2337), ('constraints', 2338), ('contact', 2339), ('contacted', 2340), ('contactless', 2341), ('contacts', 2342), ('contained', 2343), ('contestants', 2344), ('continue', 2345), ('continues', 2346), ('contract', 2347), ('contributing', 2348), ('contributions', 2349), ('controls', 2350), ('converting', 2351), ('convo', 2352), ('convos', 2353), ('cooking', 2354), ('coon', 2355), ('cooper', 2356), ('cooperate', 2357), ('cooperation', 2358), ('coping', 2359), ('corner', 2360), ('coronary', 2361), ('corpse', 2362), ('correction', 2363), ('corridor', 2364), ('cosmos', 2365), ('cost', 2366), ('coughing', 2367), ('countryside', 2368), ('couples', 2369), ('coupon', 2370), ('courage', 2371), ('cousin', 2372), ('covered', 2373), ('covering', 2374), ('covey', 2375), ('cowboys', 2376), ('coworker', 2377), ('cows', 2378), ('coz', 2379), ('cptsd', 2380), ('cqhshjsi', 2381), ('crack', 2382), ('craigan', 2383), ('craigen', 2384), ('crashed', 2385), ('crawling', 2386), ('crazy', 2387), ('crb', 2388), ('create', 2389), ('creatures', 2390), ('crickets', 2391), ('crock', 2392), ('cross', 2393), ('crossing', 2394), ('crowd', 2395), ('crowded', 2396), ('crucial', 2397), ('cruise', 2398), ('cruther', 2399), ('cst', 2400), ('cuck', 2401), ('cue', 2402), ('cum', 2403), ('cunt', 2404), ('cupcake', 2405), ('cure', 2406), ('curiosity', 2407), ('curly', 2408), ('curving', 2409), ('cutting', 2410), ('cv', 2411), ('dada', 2412), ('daddy', 2413), ('damning', 2414), ('damon', 2415), ('dandelion', 2416), ('dangerous', 2417), ('dangerously', 2418), ('dann', 2419), ('dardanella', 2420), ('dared', 2421), ('darn', 2422), ('darth', 2423), ('dashed', 2424), ('dating', 2425), ('daughter', 2426), ('daunting', 2427), ('dave', 2428), ('dayjob', 2429), ('dayof', 2430), ('dbs', 2431), ('deal', 2432), ('dealing', 2433), ('dear', 2434), ('deaths', 2435), ('debuts', 2436), ('decent', 2437), ('decided', 2438), ('declare', 2439), ('decommissioned', 2440), ('decorations', 2441), ('dedicated', 2442), ('def', 2443), ('defeated', 2444), ('defect', 2445), ('defences', 2446), ('defending', 2447), ('defiantly', 2448), ('defined', 2449), ('dehydrated', 2450), ('delete', 2451), ('deleted', 2452), ('delhi', 2453), ('delicious', 2454), ('delivered', 2455), ('dell', 2456), ('demi', 2457), ('demilterization', 2458), ('demise', 2459), ('demonstrating', 2460), ('dental', 2461), ('depay', 2462), ('dependent', 2463), ('depending', 2464), ('depict', 2465), ('describes', 2466), ('deserved', 2467), ('deserves', 2468), ('desication', 2469), ('desire', 2470), ('desires', 2471), ('despicable', 2472), ('destroy', 2473), ('destroys', 2474), ('destruction', 2475), ('detroit', 2476), ('deuces', 2477), ('devon', 2478), ('devotee', 2479), ('dharma', 2480), ('dialogue', 2481), ('didnt', 2482), ('diet', 2483), ('differently', 2484), ('difficulties', 2485), ('digging', 2486), ('diminish', 2487), ('diminishes', 2488), ('diner', 2489), ('dinner', 2490), ('diplomacy', 2491), ('diplomat', 2492), ('director', 2493), ('dirty', 2494), ('dis', 2495), ('disappointing', 2496), ('disappointment', 2497), ('disaster', 2498), ('disciplined', 2499), ('discomfort', 2500), ('discussion', 2501), ('disease', 2502), ('disgrace', 2503), ('dishes', 2504), ('dislikes', 2505), ('disney', 2506), ('display', 2507), ('disrespect', 2508), ('distant', 2509), ('distracted', 2510), ('district', 2511), ('dit', 2512), ('diverse', 2513), ('divide', 2514), ('django', 2515), ('dm', 2516), ('dming', 2517), ('doc', 2518), ('doctor', 2519), ('documentary', 2520), ('dolores', 2521), ('domestic', 2522), ('donalds', 2523), ('donation', 2524), ('donatists', 2525), ('dontbuythesun', 2526), ('donte', 2527), ('download', 2528), ('downtown', 2529), ('drain', 2530), ('drake', 2531), ('drama', 2532), ('dreamer', 2533), ('dreams', 2534), ('dressers', 2535), ('drinking', 2536), ('drinks', 2537), ('drive', 2538), ('driven', 2539), ('drug', 2540), ('drugged', 2541), ('drunken', 2542), ('dsp', 2543), ('dudes', 2544), ('duff', 2545), ('dumb', 2546), ('dump', 2547), ('dv', 2548), ('dwight', 2549), ('dynamic', 2550), ('dysfunctional', 2551), ('ea', 2552), ('earth', 2553), ('ease', 2554), ('eastpak', 2555), ('echat', 2556), ('echoed', 2557), ('echoes', 2558), ('eddy', 2559), ('edge', 2560), ('edinburgh', 2561), ('effective', 2562), ('egg', 2563), ('ek', 2564), ('elation', 2565), ('election', 2566), ('elegance', 2567), ('eliminate', 2568), ('elinor', 2569), ('elizabethan', 2570), ('elkracken', 2571), ('elouise', 2572), ('elton', 2573), ('em', 2574), ('email', 2575), ('emerson', 2576), ('emo', 2577), ('emoticons', 2578), ('emotionalhedge', 2579), ('emotionally', 2580), ('empty', 2581), ('endearing', 2582), ('ended', 2583), ('ending', 2584), ('endless', 2585), ('ends', 2586), ('energy', 2587), ('england', 2588), ('enhancment', 2589), ('enjoyable', 2590), ('enjoyment', 2591), ('enliven', 2592), ('enrique', 2593), ('ensue', 2594), ('ensure', 2595), ('entertainment', 2596), ('entire', 2597), ('entirely', 2598), ('entitled', 2599), ('entrepreneur', 2600), ('envy', 2601), ('ep', 2602), ('epic', 2603), ('epicurus', 2604), ('episodes', 2605), ('equality', 2606), ('equally', 2607), ('equity', 2608), ('ernest', 2609), ('ers', 2610), ('erykah', 2611), ('espn', 2612), ('esque', 2613), ('essay', 2614), ('et', 2615), ('ethan', 2616), ('eun', 2617), ('euriechsa', 2618), ('eva', 2619), ('eval', 2620), ('evaluation', 2621), ('evangelical', 2622), ('evanora', 2623), ('evans', 2624), ('evening', 2625), ('event', 2626), ('eventual', 2627), ('everone', 2628), ('everybody', 2629), ('everywhere', 2630), ('evidently', 2631), ('evolution', 2632), ('ex', 2633), ('excel', 2634), ('excite', 2635), ('excitement', 2636), ('excuse', 2637), ('exhausted', 2638), ('exhilarate', 2639), ('exited', 2640), ('exodus', 2641), ('expansive', 2642), ('expectation', 2643), ('expensive', 2644), ('experiences', 2645), ('explaining', 2646), ('explicits', 2647), ('expressed', 2648), ('expressing', 2649), ('extends', 2650), ('extension', 2651), ('exterior', 2652), ('extradition', 2653), ('eyebrows', 2654), ('eyelashes', 2655), ('failed', 2656), ('fake', 2657), ('fam', 2658), ('familiar', 2659), ('famous', 2660), ('fancy', 2661), ('fandom', 2662), ('farewell', 2663), ('farmhouse', 2664), ('farther', 2665), ('fashion', 2666), ('fatalism', 2667), ('fatalistic', 2668), ('father', 2669), ('fathom', 2670), ('fattening', 2671), ('fault', 2672), ('faux', 2673), ('fave', 2674), ('faved', 2675), ('favor', 2676), ('favorite', 2677), ('fb', 2678), ('fearful', 2679), ('fearing', 2680), ('fearless', 2681), ('fears', 2682), ('feature', 2683), ('features', 2684), ('feds', 2685), ('feed', 2686), ('feedback', 2687), ('feeds', 2688), ('feet', 2689), ('feic', 2690), ('fellaini', 2691), ('ferocious', 2692), ('festooned', 2693), ('fictional', 2694), ('fifa', 2695), ('fighter', 2696), ('fighting', 2697), ('figure', 2698), ('figured', 2699), ('file', 2700), ('filled', 2701), ('filling', 2702), ('finale', 2703), ('financial', 2704), ('finland', 2705), ('fireworks', 2706), ('firm', 2707), ('fish', 2708), ('fit', 2709), ('fits', 2710), ('five', 2711), ('fixed', 2712), ('fl', 2713), ('flag', 2714), ('flake', 2715), ('flakes', 2716), ('flames', 2717), ('flash', 2718), ('flawless', 2719), ('flaws', 2720), ('flexing', 2721), ('flint', 2722), ('flip', 2723), ('flirting', 2724), ('flops', 2725), ('flourish', 2726), ('flt', 2727), ('flying', 2728), ('fmr', 2729), ('fo', 2730), ('foaming', 2731), ('focus', 2732), ('focused', 2733), ('follows', 2734), ('footwear', 2735), ('forced', 2736), ('forces', 2737), ('forcing', 2738), ('foreign', 2739), ('forget', 2740), ('forgets', 2741), ('forgives', 2742), ('forgiving', 2743), ('forgot', 2744), ('forlorn', 2745), ('foundation', 2746), ('fox', 2747), ('fr', 2748), ('francoise', 2749), ('frank', 2750), ('freaked', 2751), ('freezer', 2752), ('french', 2753), ('frequency', 2754), ('fresh', 2755), ('freshly', 2756), ('freshman', 2757), ('fret', 2758), ('freudian', 2759), ('fri', 2760), ('fright', 2761), ('frivolity', 2762), ('frk', 2763), ('frog', 2764), ('frogs', 2765), ('frowning', 2766), ('frustrating', 2767), ('ft', 2768), ('fu*king', 2769), ('fuc*ing', 2770), ('fuckign', 2771), ('fuel', 2772), ('fume', 2773), ('fumes', 2774), ('function', 2775), ('fundraiser', 2776), ('fundraisers', 2777), ('funeral', 2778), ('funniest', 2779), ('funsville', 2780), ('furniture', 2781), ('further', 2782), ('gag', 2783), ('gain', 2784), ('gained', 2785), ('galaxy', 2786), ('gallery', 2787), ('galore', 2788), ('gamer', 2789), ('gaming', 2790), ('gangster', 2791), ('gaps', 2792), ('garageband', 2793), ('garden', 2794), ('gasoline', 2795), ('gathering', 2796), ('gatherp', 2797), ('gaudreau', 2798), ('gby', 2799), ('gear', 2800), ('genders', 2801), ('generally', 2802), ('generations', 2803), ('generic', 2804), ('genius', 2805), ('gently', 2806), ('genuinely', 2807), ('geo', 2808), ('geometry', 2809), ('george', 2810), ('germany', 2811), ('gesture', 2812), ('gets20', 2813), ('gf', 2814), ('gg', 2815), ('ghastly', 2816), ('gifs', 2817), ('giggle', 2818), ('giggles', 2819), ('gini', 2820), ('gipe', 2821), ('giuliani', 2822), ('gives', 2823), ('gladly', 2824), ('glass', 2825), ('glasses', 2826), ('gleesome', 2827), ('glen', 2828), ('gloom', 2829), ('glossed', 2830), ('gobbler', 2831), ('gobsmacked', 2832), ('golden', 2833), ('golf', 2834), ('gonzaga', 2835), ('goo', 2836), ('goodness', 2837), ('goofiest', 2838), ('goofy', 2839), ('google', 2840), ('goosebumps', 2841), ('gosling', 2842), ('gp', 2843), ('graced', 2844), ('graceful', 2845), ('grad', 2846), ('grade', 2847), ('graham', 2848), ('grandson', 2849), ('grapes', 2850), ('grasp', 2851), ('greengrass', 2852), ('greeted', 2853), ('grey', 2854), ('greys', 2855), ('grind', 2856), ('grinding', 2857), ('grinned', 2858), ('group', 2859), ('growing', 2860), ('growl', 2861), ('growls', 2862), ('grudgingly', 2863), ('grumbled', 2864), ('gt', 2865), ('gtfo', 2866), ('guanxi', 2867), ('gud', 2868), ('gunna', 2869), ('gunnysmith93', 2870), ('guts', 2871), ('gutted', 2872), ('h3ll', 2873), ('hacked', 2874), ('hah', 2875), ('hahahahahaha', 2876), ('hahahahahahaha', 2877), ('hairline', 2878), ('hairy', 2879), ('halfway', 2880), ('halifax', 2881), ('halsey', 2882), ('ham', 2883), ('handed', 2884), ('hands', 2885), ('handshake', 2886), ('hangry', 2887), ('happening', 2888), ('happens', 2889), ('happier', 2890), ('harass', 2891), ('hardwork', 2892), ('hardy', 2893), ('harking', 2894), ('harry', 2895), ('harwood', 2896), ('hat', 2897), ('hates', 2898), ('hath', 2899), ('hattrick', 2900), ('haunt', 2901), ('haunted', 2902), ('hawk', 2903), ('headed', 2904), ('header', 2905), ('healthy', 2906), ('heap', 2907), ('hearing', 2908), ('hears', 2909), ('heartbreak', 2910), ('heartbreaking', 2911), ('heather', 2912), ('heavy', 2913), ('heel', 2914), ('heels', 2915), ('height', 2916), ('helicopter', 2917), ('helpline', 2918), ('hemingway', 2919), ('hemp', 2920), ('hence', 2921), ('hereford', 2922), ('heres', 2923), ('hero', 2924), ('heroes', 2925), ('hesitantly', 2926), ('hesitation', 2927), ('hid', 2928), ('hide', 2929), ('highlight', 2930), ('highlights', 2931), ('hill', 2932), ('hiney', 2933), ('hip', 2934), ('hissy', 2935), ('historic', 2936), ('historically', 2937), ('hitler', 2938), ('hitting', 2939), ('hoco', 2940), ('hoe', 2941), ('holding', 2942), ('holidays', 2943), ('holmes', 2944), ('homecoming', 2945), ('homestyle', 2946), ('honest', 2947), ('honey', 2948), ('honks', 2949), ('honor', 2950), ('hood', 2951), ('hop', 2952), ('hoped', 2953), ('horseman', 2954), ('hospital', 2955), ('hostile', 2956), ('hotel', 2957), ('houses', 2958), ('houston', 2959), ('hovering', 2960), ('hovers', 2961), ('however', 2962), ('hrs', 2963), ('hsnhzmjsc', 2964), ('hubby', 2965), ('hubs', 2966), ('hufflepuff', 2967), ('hulu', 2968), ('humanism', 2969), ('humans', 2970), ('humbleness', 2971), ('hun', 2972), ('hunter', 2973), ('hurting', 2974), ('husband', 2975), ('hw', 2976), ('hydra', 2977), ('hypocrisy', 2978), ('hysterical', 2979), ('ian', 2980), ('icc', 2981), ('ice', 2982), ('ices', 2983), ('ideas', 2984), ('identify', 2985), ('idol', 2986), ('ie', 2987), ('ignorant', 2988), ('ignores', 2989), ('ignoring', 2990), ('illusion', 2991), ('ima', 2992), ('immense', 2993), ('immigrant', 2994), ('immodest', 2995), ('immoral', 2996), ('imo', 2997), ('impossible', 2998), ('impress', 2999), ('inappropriate', 3000), ('inc', 3001), ('incentivise', 3002), ('included', 3003), ('income', 3004), ('increasing', 3005), ('incurable', 3006), ('indeed', 3007), ('independant', 3008), ('india', 3009), ('indie', 3010), ('indignant', 3011), ('indignation', 3012), ('indignity', 3013), ('industrial', 3014), ('indymn', 3015), ('influenced', 3016), ('ing', 3017), ('inhaler', 3018), ('initial', 3019), ('injecting', 3020), ('injured', 3021), ('injustice', 3022), ('insecure', 3023), ('inspiration', 3024), ('inspire', 3025), ('inspired', 3026), ('inspires', 3027), ('instantly', 3028), ('instinct', 3029), ('instructor', 3030), ('insults', 3031), ('intelligence', 3032), ('intention', 3033), ('intentional', 3034), ('interests', 3035), ('intermit', 3036), ('international', 3037), ('interrupt', 3038), ('interview', 3039), ('intrigued', 3040), ('introducing', 3041), ('invaluable', 3042), ('investigated', 3043), ('invisible', 3044), ('inviting', 3045), ('involved', 3046), ('involvement', 3047), ('ios10', 3048), ('ipswich', 3049), ('ir', 3050), ('iraq', 3051), ('irate', 3052), ('ironman', 3053), ('irony', 3054), ('irrelevant', 3055), ('irruptive', 3056), ('issues', 3057), ('item', 3058), ('ive', 3059), ('izn', 3060), ('jaded', 3061), ('jammies', 3062), ('jams', 3063), ('jan', 3064), ('janecky', 3065), ('japes', 3066), ('jared', 3067), ('jay', 3068), ('jazz', 3069), ('jc', 3070), ('jd', 3071), ('jealousy', 3072), ('jean', 3073), ('jeans', 3074), ('jedi', 3075), ('jen', 3076), ('jennifer', 3077), ('jerk', 3078), ('jersey', 3079), ('jetlag', 3080), ('jimi', 3081), ('jin', 3082), ('jk', 3083), ('jkt', 3084), ('jkuvmvqxy', 3085), ('jobs', 3086), ('joey', 3087), ('johnny', 3088), ('joining', 3089), ('joker', 3090), ('jon', 3091), ('jonathan', 3092), ('jordy', 3093), ('jose', 3094), ('journey', 3095), ('joviality', 3096), ('joyce', 3097), ('joys', 3098), ('jr', 3099), ('jt', 3100), ('juan', 3101), ('juggle', 3102), ('julian', 3103), ('julie', 3104), ('jump', 3105), ('jumping', 3106), ('jungle', 3107), ('jus', 3108), ('juxtaposition', 3109), ('kach', 3110), ('kal', 3111), ('kanye', 3112), ('kap', 3113), ('kapernick', 3114), ('karma', 3115), ('kart', 3116), ('kashmires', 3117), ('katherine', 3118), ('keeper', 3119), ('keeps', 3120), ('kei', 3121), ('keifer', 3122), ('keller', 3123), ('kelvin', 3124), ('kemayoran', 3125), ('kept', 3126), ('kerry', 3127), ('keys', 3128), ('kg', 3129), ('kick', 3130), ('kicked', 3131), ('kicking', 3132), ('kidney', 3133), ('kids', 3134), ('killed', 3135), ('killgrave', 3136), ('kills', 3137), ('kip', 3138), ('kissing', 3139), ('kitchen', 3140), ('kitten', 3141), ('kjv', 3142), ('klitschko', 3143), ('kneeling', 3144), ('knvfkkjg', 3145), ('korgoth', 3146), ('kris', 3147), ('ksu', 3148), ('kudos', 3149), ('kyle', 3150), ('la', 3151), ('laboratory', 3152), ('labour', 3153), ('lackeys', 3154), ('laden', 3155), ('ladies', 3156), ('lane', 3157), ('lasts', 3158), ('latest', 3159), ('laudrup', 3160), ('laughed', 3161), ('laurel', 3162), ('lautner', 3163), ('lawsuit', 3164), ('leader', 3165), ('leafy', 3166), ('leaning', 3167), ('learned', 3168), ('learnt', 3169), ('leaves', 3170), ('leaving', 3171), ('lebanon', 3172), ('led', 3173), ('lemon', 3174), ('lengthening', 3175), ('leo', 3176), ('leon', 3177), ('leonard', 3178), ('lest', 3179), ('letgo', 3180), ('letter', 3181), ('letterkenny', 3182), ('letters', 3183), ('levels', 3184), ('lgbt', 3185), ('li', 3186), ('lia', 3187), ('liam', 3188), ('liar', 3189), ('liarthiefscoundrel', 3190), ('liberal', 3191), ('lifeisnotafairytale', 3192), ('lift', 3193), ('lighthearted', 3194), ('liked', 3195), ('likes', 3196), ('limit', 3197), ('lincoln', 3198), ('lipped', 3199), ('literal', 3200), ('liver', 3201), ('livid', 3202), ('lizards', 3203), ('llnp', 3204), ('lmboo', 3205), ('lmfao', 3206), ('lo', 3207), ('load', 3208), ('lolz', 3209), ('looney', 3210), ('loosen', 3211), ('loosing', 3212), ('losers', 3213), ('loses', 3214), ('losses', 3215), ('lott', 3216), ('loud', 3217), ('loudon', 3218), ('lovato', 3219), ('loved', 3220), ('loveet', 3221), ('lover', 3222), ('low', 3223), ('lowkey', 3224), ('lowly', 3225), ('lpco', 3226), ('lrt', 3227), ('lsat', 3228), ('lth', 3229), ('luck', 3230), ('lucky', 3231), ('luke', 3232), ('lust', 3233), ('luthansa', 3234), ('luther', 3235), ('ly', 3236), ('lydia', 3237), ('lyrics', 3238), ('ma', 3239), ('mackenzie', 3240), ('maddie', 3241), ('madhouse', 3242), ('magazines', 3243), ('magical', 3244), ('maine', 3245), ('major', 3246), ('mall', 3247), ('management', 3248), ('managing', 3249), ('manutd', 3250), ('marbles', 3251), ('march', 3252), ('marcus', 3253), ('mario', 3254), ('market', 3255), ('marmite', 3256), ('marriage', 3257), ('marry', 3258), ('mary', 3259), ('mask', 3260), ('mass', 3261), ('massive', 3262), ('mat', 3263), ('match', 3264), ('matilda', 3265), ('mattress', 3266), ('maturity', 3267), ('mcds', 3268), ('md', 3269), ('meal', 3270), ('meals', 3271), ('meanwhile', 3272), ('meetings', 3273), ('meets', 3274), ('mel', 3275), ('melancholic', 3276), ('melbjs', 3277), ('melted', 3278), ('melts', 3279), ('meme', 3280), ('memory', 3281), ('memphis', 3282), ('mendes', 3283), ('mention', 3284), ('mentioned', 3285), ('menus', 3286), ('meogys', 3287), ('merci', 3288), ('mercure', 3289), ('merriment', 3290), ('mesquite', 3291), ('mess', 3292), ('messages', 3293), ('messaging', 3294), ('messed', 3295), ('messenger', 3296), ('metro', 3297), ('metropolitan', 3298), ('mh', 3299), ('mic', 3300), ('midterm', 3301), ('mighty', 3302), ('mile', 3303), ('miley', 3304), ('milkmaid', 3305), ('milo', 3306), ('mindblown', 3307), ('minded', 3308), ('minding', 3309), ('mindset', 3310), ('mingo', 3311), ('mini', 3312), ('minimally', 3313), ('minivan', 3314), ('minority', 3315), ('mints', 3316), ('mischievous', 3317), ('misogyny', 3318), ('missiles', 3319), ('mississippi', 3320), ('mixture', 3321), ('mo', 3322), ('mode', 3323), ('momentum', 3324), ('moncrief', 3325), ('monica', 3326), ('monster', 3327), ('montalvo', 3328), ('months', 3329), ('moo', 3330), ('moons', 3331), ('mope', 3332), ('morally', 3333), ('mornings', 3334), ('morrisons', 3335), ('mostly', 3336), ('moth', 3337), ('motivation', 3338), ('moto', 3339), ('motteville', 3340), ('motto', 3341), ('mou', 3342), ('mourinho', 3343), ('mournful', 3344), ('moustache', 3345), ('moves', 3346), ('mpn', 3347), ('mrs', 3348), ('ms', 3349), ('msk', 3350), ('msnbc', 3351), ('mufc', 3352), ('mums', 3353), ('muppet', 3354), ('murdered', 3355), ('murking', 3356), ('musical', 3357), ('muslims', 3358), ('mustache_harbor', 3359), ('mustard', 3360), ('mute', 3361), ('muzzle', 3362), ('myspace', 3363), ('n*she', 3364), ('n*slight', 3365), ('n*walks', 3366), ('n10', 3367), ('naay', 3368), ('nah', 3369), ('naidy', 3370), ('naked', 3371), ('nalways', 3372), ('named', 3373), ('namely', 3374), ('naming', 3375), ('nana', 3376), ('nanny', 3377), ('nappy', 3378), ('narcissistic', 3379), ('narcoleptic', 3380), ('nation', 3381), ('nbaby', 3382), ('nblushing', 3383), ('nbogum', 3384), ('nbut', 3385), ('ncaa', 3386), ('nchurch', 3387), ('nconclusion', 3388), ('ndare', 3389), ('ndays', 3390), ('ndreams', 3391), ('near', 3392), ('nearly', 3393), ('necessary', 3394), ('necessity', 3395), ('needed', 3396), ('neighborhoods', 3397), ('neighbourhood', 3398), ('nell', 3399), ('nephalism', 3400), ('nephews', 3401), ('nerve', 3402), ('nerves', 3403), ('ness', 3404), ('network', 3405), ('nfrom', 3406), ('ngirls', 3407), ('nhappy', 3408), ('nheheh', 3409), ('nhelen', 3410), ('nhlnycsweepstakes', 3411), ('nibbles', 3412), ('nicholson', 3413), ('nick', 3414), ('nigga', 3415), ('nigger', 3416), ('nightmares', 3417), ('nihari', 3418), ('nin', 3419), ('ninto', 3420), ('nkelly', 3421), ('nmatt', 3422), ('nme', 3423), ('nnot', 3424), ('nnow', 3425), ('nnurse', 3426), ('noah', 3427), ('nobler', 3428), ('nol', 3429), ('nomore', 3430), ('none', 3431), ('nonsense', 3432), ('noodle', 3433), ('noodles', 3434), ('normal', 3435), ('normalized', 3436), ('normally', 3437), ('norms', 3438), ('north', 3439), ('note', 3440), ('notification', 3441), ('notts', 3442), ('nrl', 3443), ('nsfw', 3444), ('ntf', 3445), ('nthat', 3446), ('ntoday', 3447), ('nuclear', 3448), ('nuse', 3449), ('nutter', 3450), ('nwest', 3451), ('nwhat', 3452), ('nwill', 3453), ('nyall', 3454), ('nzephaniah', 3455), ('oakland', 3456), ('obnoxious', 3457), ('observation', 3458), ('obvious', 3459), ('occasion', 3460), ('occasionally', 3461), ('oceanic', 3462), ('od', 3463), ('offense', 3464), ('offered', 3465), ('offering', 3466), ('office', 3467), ('og', 3468), ('oi', 3469), ('oil', 3470), ('onion', 3471), ('onthe', 3472), ('onto', 3473), ('opened', 3474), ('opening', 3475), ('operating', 3476), ('operation', 3477), ('opinions', 3478), ('oppa', 3479), ('oppressive', 3480), ('optics', 3481), ('organ', 3482), ('organization', 3483), ('oriole', 3484), ('ornaments', 3485), ('ortiz', 3486), ('orton', 3487), ('osama', 3488), ('ostentatious', 3489), ('otis', 3490), ('ought', 3491), ('ounce', 3492), ('ourselves', 3493), ('outage', 3494), ('outlet', 3495), ('output', 3496), ('outta', 3497), ('outweighs', 3498), ('overheard', 3499), ('overthinking', 3500), ('overweight', 3501), ('overwhelming', 3502), ('owned', 3503), ('owner', 3504), ('p', 3505), ('p2', 3506), ('package', 3507), ('paint', 3508), ('painted', 3509), ('pair', 3510), ('pals', 3511), ('panda', 3512), ('pandora', 3513), ('paper', 3514), ('papercuts', 3515), ('parade', 3516), ('parcc', 3517), ('parking', 3518), ('parody', 3519), ('participants', 3520), ('particularly', 3521), ('partners', 3522), ('passed', 3523), ('passion', 3524), ('passionate', 3525), ('path', 3526), ('pathetically', 3527), ('patience', 3528), ('patients', 3529), ('patronus', 3530), ('patroons', 3531), ('pats', 3532), ('patterns', 3533), ('patton', 3534), ('pavel', 3535), ('pawpaw', 3536), ('payet', 3537), ('pays', 3538), ('peaceful', 3539), ('peacekeepers', 3540), ('pecan', 3541), ('pecking', 3542), ('pedal', 3543), ('peer', 3544), ('pens', 3545), ('pensive', 3546), ('perceived', 3547), ('perfection', 3548), ('performances', 3549), ('perhaps', 3550), ('personality', 3551), ('pessimism', 3552), ('peter', 3553), ('petty', 3554), ('philadelphia', 3555), ('phillips', 3556), ('photography', 3557), ('physiatrists', 3558), ('physically', 3559), ('physics', 3560), ('pick', 3561), ('picked', 3562), ('pickup', 3563), ('pics', 3564), ('pie', 3565), ('pile', 3566), ('pillow', 3567), ('pilot', 3568), ('piss', 3569), ('pitch', 3570), ('pitchers', 3571), ('pittsburgh', 3572), ('pixel', 3573), ('pizzas', 3574), ('pk', 3575), ('places', 3576), ('plague', 3577), ('played', 3578), ('pleased', 3579), ('plot', 3580), ('plotting', 3581), ('plugin', 3582), ('pm', 3583), ('po', 3584), ('pockets', 3585), ('poems', 3586), ('pole', 3587), ('policies', 3588), ('policing', 3589), ('politicans', 3590), ('poop', 3591), ('popped', 3592), ('popular', 3593), ('popularization', 3594), ('port', 3595), ('pos', 3596), ('pose', 3597), ('positions', 3598), ('positive', 3599), ('possibly', 3600), ('poster', 3601), ('potential', 3602), ('potter', 3603), ('pottermore', 3604), ('poured', 3605), ('pouring', 3606), ('pouting', 3607), ('powell', 3608), ('powerful', 3609), ('pp', 3610), ('practical', 3611), ('practically', 3612), ('practice', 3613), ('pray', 3614), ('prayed', 3615), ('prayer', 3616), ('prayers', 3617), ('preached', 3618), ('pregnancy', 3619), ('pregnant', 3620), ('prematured', 3621), ('premiere', 3622), ('premium', 3623), ('preordered', 3624), ('preparation', 3625), ('prepared', 3626), ('preponderance', 3627), ('preposterous', 3628), ('prepped', 3629), ('preschool', 3630), ('press', 3631), ('pretending', 3632), ('pretentious', 3633), ('previously', 3634), ('pricks', 3635), ('pride', 3636), ('priests', 3637), ('prince', 3638), ('principle', 3639), ('prison', 3640), ('private', 3641), ('privately', 3642), ('probz', 3643), ('produced', 3644), ('producers', 3645), ('produces', 3646), ('product', 3647), ('prof', 3648), ('profession', 3649), ('profile', 3650), ('profoundly', 3651), ('program', 3652), ('progressives', 3653), ('prom', 3654), ('promise', 3655), ('prosecute', 3656), ('protect', 3657), ('protesting', 3658), ('protests', 3659), ('proudly', 3660), ('pround', 3661), ('provide', 3662), ('provides', 3663), ('provoke', 3664), ('ps2', 3665), ('ps4pro', 3666), ('psa', 3667), ('psychological', 3668), ('psychowasp', 3669), ('publicly', 3670), ('publishing', 3671), ('puckish', 3672), ('puff', 3673), ('pull', 3674), ('pumping', 3675), ('pumpkin', 3676), ('pun', 3677), ('punch', 3678), ('punchline', 3679), ('pundit', 3680), ('punny', 3681), ('puns', 3682), ('puppies', 3683), ('purge', 3684), ('purple', 3685), ('purposes', 3686), ('pussy', 3687), ('putin', 3688), ('putting', 3689), ('qmiv', 3690), ('quarterback', 3691), ('queensbury', 3692), ('quentin', 3693), ('question', 3694), ('quickie', 3695), ('rabid', 3696), ('rachel', 3697), ('radical', 3698), ('raiders', 3699), ('rainbows', 3700), ('rainy', 3701), ('raise', 3702), ('rally', 3703), ('ralph', 3704), ('ramos', 3705), ('ramsey', 3706), ('ran', 3707), ('rancour', 3708), ('randomly', 3709), ('raper', 3710), ('rapids', 3711), ('rapper', 3712), ('rar', 3713), ('rasta', 3714), ('rat', 3715), ('rate', 3716), ('rays', 3717), ('rd', 3718), ('reach', 3719), ('reached', 3720), ('reaction', 3721), ('read', 3722), ('realise', 3723), ('realistic', 3724), ('realized', 3725), ('rear', 3726), ('reasoning', 3727), ('reciprocated', 3728), ('reclaimed', 3729), ('recommendations', 3730), ('recruiter', 3731), ('redding', 3732), ('reflection', 3733), ('reflects', 3734), ('reform', 3735), ('reformation', 3736), ('refreshing', 3737), ('refugee', 3738), ('refunded', 3739), ('refuses', 3740), ('regard', 3741), ('regarding', 3742), ('regardless', 3743), ('regenerated', 3744), ('register', 3745), ('regular', 3746), ('regularly', 3747), ('regulation', 3748), ('reid', 3749), ('reign', 3750), ('reinvigorated', 3751), ('rejects', 3752), ('relate', 3753), ('relating', 3754), ('relations', 3755), ('relax', 3756), ('relaxation', 3757), ('release', 3758), ('relive', 3759), ('remedy', 3760), ('reminder', 3761), ('reminds', 3762), ('remove', 3763), ('render', 3764), ('renown', 3765), ('rep', 3766), ('repeal', 3767), ('repentance', 3768), ('replace', 3769), ('replaced', 3770), ('replied', 3771), ('reporting', 3772), ('republicans', 3773), ('resettlement', 3774), ('resist', 3775), ('resolute', 3776), ('resolved', 3777), ('restaurant', 3778), ('resultin', 3779), ('retired', 3780), ('returned', 3781), ('returning', 3782), ('returns', 3783), ('retweet', 3784), ('retweeted', 3785), ('revealed', 3786), ('revelation', 3787), ('revelations', 3788), ('review', 3789), ('reviews', 3790), ('rewatching', 3791), ('rex', 3792), ('reyes', 3793), ('rhythm', 3794), ('rice', 3795), ('richness', 3796), ('rider', 3797), ('riding', 3798), ('rimes', 3799), ('riot', 3800), ('rioters', 3801), ('ripping', 3802), ('risky', 3803), ('road', 3804), ('roam', 3805), ('rob', 3806), ('robbing', 3807), ('robin', 3808), ('robocoq', 3809), ('rockin', 3810), ('rocknroll', 3811), ('rodgers', 3812), ('rohan', 3813), ('roho', 3814), ('roland', 3815), ('role', 3816), ('rolls', 3817), ('romain', 3818), ('romance', 3819), ('roof', 3820), ('rooneys', 3821), ('roosters', 3822), ('rope', 3823), ('rosa', 3824), ('rosy', 3825), ('rough', 3826), ('round', 3827), ('route', 3828), ('routine', 3829), ('rowdy', 3830), ('royals', 3831), ('rubbish', 3832), ('rugby', 3833), ('ruins', 3834), ('rule', 3835), ('rush', 3836), ('russell', 3837), ('russian', 3838), ('ruth', 3839), ('ryan', 3840), ('sable', 3841), ('saddest', 3842), ('saf', 3843), ('sait', 3844), ('salt', 3845), ('salty', 3846), ('san', 3847), ('sandwich', 3848), ('sanitizer', 3849), ('sasse', 3850), ('satan', 3851), ('satellite', 3852), ('satisfaction', 3853), ('satisfying', 3854), ('saucepan', 3855), ('saudi', 3856), ('saves', 3857), ('saving', 3858), ('savory', 3859), ('savvy', 3860), ('scale', 3861), ('scales', 3862), ('scaredy', 3863), ('scarf', 3864), ('scarring', 3865), ('scfc', 3866), ('schaaf', 3867), ('science', 3868), ('scoffed', 3869), ('score', 3870), ('scores', 3871), ('scorned', 3872), ('scorpio', 3873), ('scott', 3874), ('scra', 3875), ('screamer', 3876), ('screams', 3877), ('screened', 3878), ('screwed', 3879), ('search', 3880), ('searching', 3881), ('seat', 3882), ('seconds', 3883), ('secretary', 3884), ('secrets', 3885), ('secs', 3886), ('section', 3887), ('seed', 3888), ('sef', 3889), ('selfie', 3890), ('selfish', 3891), ('semores', 3892), ('sences', 3893), ('sentence', 3894), ('sentenced', 3895), ('seo', 3896), ('september2004', 3897), ('serve', 3898), ('set', 3899), ('setback', 3900), ('settled', 3901), ('several', 3902), ('shabazz', 3903), ('shade', 3904), ('shades', 3905), ('shadow', 3906), ('shadows', 3907), ('shady', 3908), ('shaft', 3909), ('shakespearean', 3910), ('shambolic', 3911), ('sharif', 3912), ('sharing', 3913), ('sharpton', 3914), ('shattering', 3915), ('shawarma', 3916), ('shawty', 3917), ('sheer', 3918), ('sheffield', 3919), ('sheldon', 3920), ('shelter', 3921), ('shennan', 3922), ('sherlock', 3923), ('shh', 3924), ('shield', 3925), ('shift', 3926), ('shimmy', 3927), ('shine', 3928), ('shines', 3929), ('shining', 3930), ('shiny', 3931), ('ship', 3932), ('shittalking', 3933), ('shitting', 3934), ('shitty', 3935), ('shoals', 3936), ('shonda', 3937), ('shook', 3938), ('shop', 3939), ('shopping', 3940), ('shorts', 3941), ('shorty', 3942), ('showed', 3943), ('shows', 3944), ('shrines', 3945), ('shudder', 3946), ('siblings', 3947), ('sigh', 3948), ('signs', 3949), ('silence', 3950), ('silver', 3951), ('simms', 3952), ('simon', 3953), ('simple', 3954), ('simultaneously', 3955), ('sinatra', 3956), ('sing', 3957), ('singin', 3958), ('singing', 3959), ('sioux', 3960), ('sis', 3961), ('sisters', 3962), ('sitcom', 3963), ('sites', 3964), ('sits', 3965), ('sitting', 3966), ('sixth', 3967), ('sixty', 3968), ('size', 3969), ('skeptical', 3970), ('skies', 3971), ('skipping', 3972), ('skittles', 3973), ('skull', 3974), ('sky', 3975), ('slacking', 3976), ('slap', 3977), ('sleepy', 3978), ('slice', 3979), ('slid', 3980), ('slightest', 3981), ('slip', 3982), ('slogan', 3983), ('slowest', 3984), ('slushie', 3985), ('smallest', 3986), ('smells', 3987), ('smiled', 3988), ('smiles', 3989), ('smirk', 3990), ('smokeys', 3991), ('smth', 3992), ('snacks', 3993), ('snarled', 3994), ('snuggle', 3995), ('soaked', 3996), ('sob', 3997), ('soccer', 3998), ('socialmedia', 3999), ('society', 4000), ('solange', 4001), ('solid', 4002), ('solo', 4003), ('solutions', 4004), ('solved', 4005), ('solving', 4006), ('somber', 4007), ('somewhere', 4008), ('son', 4009), ('soo', 4010), ('sooked', 4011), ('soothe', 4012), ('soros', 4013), ('sorted', 4014), ('soundtrack', 4015), ('soup', 4016), ('soy', 4017), ('spacious', 4018), ('spag', 4019), ('spain', 4020), ('spanish', 4021), ('speaker', 4022), ('speaks', 4023), ('special', 4024), ('species', 4025), ('specific', 4026), ('speech', 4027), ('spektar', 4028), ('spell', 4029), ('spent', 4030), ('spirit', 4031), ('spiritual', 4032), ('spoke', 4033), ('spread', 4034), ('spring', 4035), ('sprint', 4036), ('spy', 4037), ('sq', 4038), ('squirm', 4039), ('srv', 4040), ('stadium', 4041), ('stalker', 4042), ('stalkers', 4043), ('standing', 4044), ('stanley', 4045), ('stare', 4046), ('starving', 4047), ('station', 4048), ('statuses', 4049), ('std', 4050), ('steady', 4051), ('steal', 4052), ('steals', 4053), ('steam', 4054), ('steel', 4055), ('stefano', 4056), ('steps', 4057), ('stfu', 4058), ('stiff', 4059), ('stifle', 4060), ('stohul', 4061), ('stool', 4062), ('stopped', 4063), ('stories', 4064), ('storm', 4065), ('stormy', 4066), ('stream', 4067), ('strength', 4068), ('stressed', 4069), ('stricken', 4070), ('stringing', 4071), ('strongman', 4072), ('strongwomen', 4073), ('struck', 4074), ('struggling', 4075), ('strutting', 4076), ('stub', 4077), ('study', 4078), ('stunned', 4079), ('stupefied', 4080), ('stupendous', 4081), ('sub', 4082), ('subscribers', 4083), ('subsidised', 4084), ('subway', 4085), ('successful', 4086), ('successfully', 4087), ('suck', 4088), ('sucked', 4089), ('suds', 4090), ('sue', 4091), ('suffering', 4092), ('sugar', 4093), ('sugarboy', 4094), ('suggest', 4095), ('suicide', 4096), ('suit', 4097), ('sullen', 4098), ('summerof', 4099), ('summit', 4100), ('sun', 4101), ('sunday', 4102), ('sunny', 4103), ('sunshine', 4104), ('supper', 4105), ('support', 4106), ('surely', 4107), ('surfing', 4108), ('surprised', 4109), ('surprising', 4110), ('survive', 4111), ('survivor', 4112), ('sushi', 4113), ('suspect', 4114), ('swallows', 4115), ('swap', 4116), ('swapping', 4117), ('sweater', 4118), ('sweaty', 4119), ('sweeper', 4120), ('sweeping', 4121), ('sweetmeats', 4122), ('sweety', 4123), ('swift', 4124), ('swifts', 4125), ('swim', 4126), ('swingish', 4127), ('sword', 4128), ('sybil', 4129), ('sydney', 4130), ('sympathy', 4131), ('symptomatize', 4132), ('synth', 4133), ('syria', 4134), ('systemic', 4135), ('tablets', 4136), ('taht', 4137), ('talent', 4138), ('talented', 4139), ('talents', 4140), ('talked', 4141), ('talks', 4142), ('tame', 4143), ('tamu', 4144), ('tantrums', 4145), ('tat', 4146), ('tatenashi', 4147), ('tattoo', 4148), ('taught', 4149), ('taxed', 4150), ('tay', 4151), ('taylor', 4152), ('teach', 4153), ('tear', 4154), ('teases', 4155), ('teasing', 4156), ('tech', 4157), ('technics', 4158), ('techno', 4159), ('technology', 4160), ('ted', 4161), ('teef', 4162), ('telford', 4163), ('telling', 4164), ('telly', 4165), ('temper', 4166), ('tendonitis', 4167), ('tense', 4168), ('teppu', 4169), ('terence', 4170), ('terencecutcher', 4171), ('termianted', 4172), ('terrified', 4173), ('terrify', 4174), ('testing', 4175), ('text', 4176), ('texting', 4177), ('tgit', 4178), ('thankful', 4179), ('thankyou', 4180), ('thats', 4181), ('theater', 4182), ('theirs', 4183), ('theobald', 4184), ('theres', 4185), ('thinks', 4186), ('third', 4187), ('thoreau', 4188), ('thou', 4189), ('thoughtful', 4190), ('thread', 4191), ('threats', 4192), ('three', 4193), ('threesome', 4194), ('threw', 4195), ('thrilled', 4196), ('throat', 4197), ('throughout', 4198), ('throw', 4199), ('throwing', 4200), ('tht', 4201), ('thursday', 4202), ('tiangong', 4203), ('tiburonchamber', 4204), ('ticket', 4205), ('tide', 4206), ('tim', 4207), ('timid', 4208), ('tiny', 4209), ('tip', 4210), ('titan', 4211), ('tits', 4212), ('titter', 4213), ('tld', 4214), ('tnf', 4215), ('toad', 4216), ('toddler', 4217), ('toe', 4218), ('toews', 4219), ('tolerate', 4220), ('tonite', 4221), ('tony', 4222), ('toon', 4223), ('top10', 4224), ('total', 4225), ('totes', 4226), ('tough', 4227), ('toughest', 4228), ('toussaint', 4229), ('towards', 4230), ('towel', 4231), ('tracey', 4232), ('tracks', 4233), ('tracy', 4234), ('tradition', 4235), ('traditionalists', 4236), ('traffic', 4237), ('tragedy', 4238), ('treated', 4239), ('trees', 4240), ('trek', 4241), ('trends', 4242), ('trick', 4243), ('tries', 4244), ('trigger', 4245), ('tripping', 4246), ('trips', 4247), ('tristan', 4248), ('truck', 4249), ('trusted', 4250), ('trusting', 4251), ('tryin', 4252), ('tryna', 4253), ('tt', 4254), ('tumble', 4255), ('tumblr', 4256), ('tunes', 4257), ('turkish', 4258), ('turlock', 4259), ('turnovers', 4260), ('turns', 4261), ('tutoring', 4262), ('tweeted', 4263), ('tweetin', 4264), ('twelvemonth', 4265), ('twenty', 4266), ('twilight', 4267), ('twins', 4268), ('twirl', 4269), ('type', 4270), ('typical', 4271), ('u20', 4272), ('uk', 4273), ('ulysses', 4274), ('unable', 4275), ('unchained', 4276), ('under', 4277), ('underrated', 4278), ('understood', 4279), ('underwear', 4280), ('unfairly', 4281), ('unforgiving', 4282), ('unga', 4283), ('uninteresting', 4284), ('unknown', 4285), ('unlikely', 4286), ('uno', 4287), ('unreal', 4288), ('untill', 4289), ('untouchable', 4290), ('unwittingly', 4291), ('upgrade', 4292), ('uplift', 4293), ('upon', 4294), ('upper', 4295), ('upside', 4296), ('ure', 4297), ('username', 4298), ('usual', 4299), ('uttered', 4300), ('utterly', 4301), ('ux', 4302), ('vader', 4303), ('valerie', 4304), ('valid', 4305), ('valley', 4306), ('vampire', 4307), ('vampires', 4308), ('van', 4309), ('varsity', 4310), ('vas', 4311), ('ve', 4312), ('vegas', 4313), ('vehicles', 4314), ('verb', 4315), ('vernon', 4316), ('vestiges', 4317), ('vetted', 4318), ('via', 4319), ('vice', 4320), ('victim', 4321), ('vids', 4322), ('viewership', 4323), ('vigil', 4324), ('villaseñor', 4325), ('vincent', 4326), ('vine', 4327), ('violent', 4328), ('violently', 4329), ('vision', 4330), ('visitors', 4331), ('vitale', 4332), ('vitals', 4333), ('vitriolic', 4334), ('vivacious', 4335), ('vocab', 4336), ('voices', 4337), ('vols', 4338), ('voodoo', 4339), ('votes', 4340), ('vrlfeyrn', 4341), ('waking', 4342), ('waldo', 4343), ('walking', 4344), ('wallet', 4345), ('walterdale', 4346), ('waltz', 4347), ('ward', 4348), ('warm', 4349), ('warmth', 4350), ('warranty', 4351), ('washed', 4352), ('wave', 4353), ('waves', 4354), ('waving', 4355), ('wavy', 4356), ('wch', 4357), ('weans', 4358), ('wearing', 4359), ('weathers', 4360), ('webb', 4361), ('wee', 4362), ('weeksary', 4363), ('weirdest', 4364), ('wel', 4365), ('west', 4366), ('westminster', 4367), ('wet', 4368), ('wetter', 4369), ('wgj', 4370), ('whilst', 4371), ('whitfield', 4372), ('whoa', 4373), ('whoever', 4374), ('whoo', 4375), ('whore', 4376), ('whu', 4377), ('wicked', 4378), ('wide', 4379), ('widows', 4380), ('wiki', 4381), ('willynilly', 4382), ('windmill', 4383), ('windows', 4384), ('winds', 4385), ('wink', 4386), ('winston', 4387), ('wished', 4388), ('wishing', 4389), ('wit', 4390), ('withdraw', 4391), ('witness', 4392), ('wives', 4393), ('wiz', 4394), ('wobble', 4395), ('wonderstruck', 4396), ('wondrful', 4397), ('woods', 4398), ('wordpress', 4399), ('workout', 4400), ('works', 4401), ('worldwide', 4402), ('wormhole', 4403), ('worn', 4404), ('wps', 4405), ('wrecks', 4406), ('wrinkling', 4407), ('wsjnordics', 4408), ('wtf', 4409), ('x33', 4410), ('xbeujgb', 4411), ('xucqb', 4412), ('y', 4413), ('yahweh', 4414), ('yankees', 4415), ('yanks', 4416), ('ydu', 4417), ('yea', 4418), ('yeller', 4419), ('yemen', 4420), ('yesyesyes', 4421), ('yhat', 4422), ('yoosung', 4423), ('yorkshire', 4424), ('younger', 4425), ('yours', 4426), ('youthful', 4427), ('yt', 4428), ('yters', 4429), ('yup', 4430), ('yuppie', 4431), ('zen', 4432), ('\\x92', 4433), ('الخفجي', 4434), ('الوطني', 4435), ('اليوم', 4436), ('‘', 4437), ('∇', 4438), ('≦', 4439), ('≧', 4440), ('━', 4441), ('□', 4442), ('♪', 4443), ('\\uf499', 4444), ('\\uf631', 4445), ('︵', 4446), ('（', 4447), ('\\U000fe334', 4448)])\n",
            "dict_items([('<unk>', 0), ('<pad>', 1), ('</hashtag>', 2), ('<hashtag>', 3), ('.', 4), ('<user>', 5), ('the', 6), (',', 7), ('i', 8), ('to', 9), ('a', 10), (\"'\", 11), ('and', 12), ('you', 13), ('!', 14), ('is', 15), ('of', 16), ('it', 17), ('<repeated>', 18), ('s', 19), ('in', 20), ('that', 21), ('not', 22), ('be', 23), ('\\\\', 24), ('my', 25), ('so', 26), ('this', 27), ('for', 28), ('me', 29), ('<number>', 30), ('happy', 31), ('with', 32), ('on', 33), ('-', 34), ('am', 35), ('have', 36), ('are', 37), ('at', 38), ('your', 39), ('n', 40), ('but', 41), ('</allcaps>', 42), ('<allcaps>', 43), ('just', 44), ('will', 45), ('day', 46), ('love', 47), ('do', 48), ('up', 49), ('by', 50), ('&', 51), ('?', 52), ('smile', 53), ('was', 54), (':', 55), ('all', 56), ('can', 57), ('good', 58), ('he', 59), ('hilarious', 60), ('like', 61), ('as', 62), ('we', 63), ('when', 64), ('watch', 65), ('if', 66), ('optimism', 67), ('live', 68), ('amazing', 69), ('they', 70), ('about', 71), ('from', 72), ('laughter', 73), ('make', 74), ('more', 75), ('out', 76), ('what', 77), ('lively', 78), ('life', 79), ('time', 80), ('glee', 81), ('or', 82), ('her', 83), ('one', 84), ('she', 85), ('get', 86), ('musically', 87), ('broadcast', 88), ('his', 89), ('ly', 90), ('see', 91), ('/', 92), ('how', 93), ('know', 94), ('go', 95), ('now', 96), ('smiling', 97), ('always', 98), ('cheer', 99), ('much', 100), ('there', 101), ('an', 102), (':face_with_tears_of_joy:', 103), ('because', 104), ('has', 105), ('new', 106), ('u', 107), ('some', 108), ('today', 109), ('us', 110), ('who', 111), ('over', 112), ('people', 113), ('want', 114), ('did', 115), ('feel', 116), ('got', 117), ('our', 118), ('<happy>', 119), ('joyful', 120), ('no', 121), ('breezy', 122), ('delight', 123), ('rejoice', 124), ('great', 125), ('cheerful', 126), ('pleasing', 127), ('sparkling', 128), ('still', 129), ('animated', 130), ('cheering', 131), ('back', 132), ('hilarity', 133), ('need', 134), ('way', 135), ('*', 136), ('being', 137), ('best', 138), ('never', 139), ('why', 140), ('would', 141), ('elated', 142), ('had', 143), ('makes', 144), ('playful', 145), ('rejoicing', 146), ('bright', 147), ('exhilarating', 148), ('hearty', 149), ('cheery', 150), ('chirp', 151), ('lol', 152), ('than', 153), ('too', 154), ('well', 155), ('joyous', 156), ('man', 157), ('most', 158), ('thank', 159), ('their', 160), (';', 161), ('give', 162), ('them', 163), ('think', 164), (':red_heart:', 165), ('<elongated>', 166), ('better', 167), ('birthday', 168), ('cheerfully', 169), ('could', 170), ('him', 171), ('off', 172), ('been', 173), ('every', 174), ('everything', 175), ('levity', 176), ('little', 177), ('look', 178), ('night', 179), ('such', 180), ('team', 181), ('(', 182), ('going', 183), ('morning', 184), ('really', 185), ('something', 186), ('watching', 187), ('<time>', 188), ('funny', 189), ('someone', 190), ('then', 191), ('were', 192), ('ever', 193), ('everyone', 194), ('let', 195), ('world', 196), (')', 197), ('after', 198), ('beautiful', 199), ('quote', 200), ('right', 201), ('show', 202), ('t', 203), ('thanks', 204), ('come', 205), ('find', 206), ('fun', 207), ('god', 208), ('jovial', 209), ('keep', 210), ('very', 211), ('any', 212), ('even', 213), ('nice', 214), ('only', 215), ('week', 216), ('accept', 217), ('down', 218), ('exhilaration', 219), ('here', 220), ('into', 221), ('least', 222), ('say', 223), ('things', 224), ('days', 225), ('follow', 226), ('girl', 227), ('gleeful', 228), ('happiness', 229), ('heart', 230), ('heyday', 231), ('long', 232), ('may', 233), ('mirth', 234), ('said', 235), ('thing', 236), (':fire:', 237), (':smiling_face_with_heart-eyes:', 238), (':smiling_face_with_smiling_eyes:', 239), (':sparkling_heart:', 240), ('also', 241), ('bad', 242), ('face', 243), ('hope', 244), ('joy', 245), ('tomorrow', 246), ('victory', 247), ('wonderful', 248), ('work', 249), (':loudly_crying_face:', 250), ('<sad>', 251), ('again', 252), ('challenges', 253), ('d', 254), ('done', 255), ('getting', 256), ('hour', 257), ('its', 258), ('maybe', 259), ('mind', 260), ('side', 261), ('sure', 262), ('those', 263), ('water', 264), ('while', 265), ('yet', 266), ('before', 267), ('first', 268), ('future', 269), ('gbbo', 270), ('last', 271), ('looking', 272), ('looks', 273), ('nd', 274), ('through', 275), ('tonight', 276), ('big', 277), ('home', 278), ('laughing', 279), ('oh', 280), ('open', 281), ('since', 282), ('started', 283), ('stop', 284), ('tears', 285), ('tell', 286), ('’', 287), ('“', 288), ('change', 289), ('end', 290), ('feeling', 291), ('having', 292), ('laugh', 293), ('lord', 294), ('lots', 295), ('once', 296), ('others', 297), ('remember', 298), ('sweet', 299), ('wine', 300), ('x', 301), ('year', 302), ('young', 303), ('yourself', 304), ('—', 305), ('actually', 306), ('family', 307), ('friend', 308), ('lot', 309), ('made', 310), ('myself', 311), ('other', 312), ('person', 313), ('place', 314), ('same', 315), ('singing', 316), ('success', 317), ('thought', 318), ('try', 319), ('which', 320), ('~', 321), ('around', 322), ('bot', 323), ('does', 324), ('episode', 325), ('fucking', 326), ('gets', 327), ('hard', 328), ('head', 329), ('jeep', 330), ('miss', 331), ('next', 332), ('should', 333), ('took', 334), ('wait', 335), ('wanna', 336), ('+', 337), ('1', 338), ('2016', 339), (':smiling_face:', 340), ('bc', 341), ('bit', 342), ('blessed', 343), ('book', 344), ('break', 345), ('class', 346), ('dont', 347), ('far', 348), ('following', 349), ('free', 350), ('game', 351), ('gave', 352), ('glad', 353), ('grateful', 354), ('lost', 355), ('making', 356), ('many', 357), ('must', 358), ('news', 359), ('old', 360), ('own', 361), ('please', 362), ('re', 363), ('shit', 364), ('short', 365), ('strong', 366), ('take', 367), ('talk', 368), ('these', 369), ('together', 370), ('used', 371), ('wish', 372), ('”', 373), (':light_skin_tone:', 374), (':smiling_face_with_open_hands:', 375), ('against', 376), ('another', 377), ('baby', 378), ('balls', 379), ('boy', 380), ('call', 381), ('cheerfulness', 382), ('during', 383), ('each', 384), ('fans', 385), ('food', 386), ('half', 387), ('hey', 388), ('jaunty', 389), ('literally', 390), ('m', 391), ('media', 392), ('ok', 393), ('outside', 394), ('play', 395), ('r', 396), ('sad', 397), ('saw', 398), ('school', 399), ('seeing', 400), ('seen', 401), ('social', 402), ('spry', 403), ('stay', 404), ('tired', 405), ('trump', 406), ('twitter', 407), ('ur', 408), ('w', 409), ('wake', 410), ('watched', 411), ('welcome', 412), ('without', 413), (':face_blowing_a_kiss:', 414), ('ain', 415), ('ass', 416), ('awesome', 417), ('bet', 418), ('body', 419), ('both', 420), ('bring', 421), ('city', 422), ('enough', 423), ('excited', 424), ('fact', 425), ('fall', 426), ('friends', 427), ('fuck', 428), ('gen', 429), ('goals', 430), ('guys', 431), ('gym', 432), ('hate', 433), ('hear', 434), ('high', 435), ('im', 436), ('jubilant', 437), ('juggle', 438), ('living', 439), ('lucky', 440), ('means', 441), ('memories', 442), ('met', 443), ('mom', 444), ('moment', 445), ('nhe', 446), ('north', 447), ('nothing', 448), ('proud', 449), ('pun', 450), ('punny', 451), ('put', 452), ('race', 453), ('real', 454), ('sometimes', 455), ('taking', 456), ('top', 457), ('truly', 458), ('trying', 459), ('tweet', 460), ('use', 461), ('v', 462), ('voice', 463), ('wednesday', 464), ('where', 465), ('whole', 466), ('words', 467), ('working', 468), ('yeah', 469), ('years', 470), ('…', 471), ('<percent>', 472), ('ago', 473), ('almost', 474), ('anyone', 475), ('anything', 476), ('bday', 477), ('boss', 478), ('bridget', 479), ('buoyant', 480), ('cake', 481), ('crying', 482), ('cute', 483), ('death', 484), ('deserve', 485), ('doing', 486), ('door', 487), ('easy', 488), ('evening', 489), ('experience', 490), ('forward', 491), ('found', 492), ('hell', 493), ('help', 494), ('house', 495), ('job', 496), ('light', 497), ('lips', 498), ('listen', 499), ('listening', 500), ('lovely', 501), ('mean', 502), ('music', 503), ('ni', 504), ('part', 505), ('picture', 506), ('positive', 507), ('problems', 508), ('quite', 509), ('series', 510), ('sick', 511), ('soul', 512), ('start', 513), ('survivor', 514), ('talking', 515), ('tho', 516), ('though', 517), ('truth', 518), ('tv', 519), ('two', 520), ('until', 521), ('video', 522), ('woman', 523), ('word', 524), ('worst', 525), ('4', 526), (':blue_heart:', 527), (':party_popper:', 528), (':thinking_face:', 529), (':tongue:', 530), (':winking_face:', 531), ('[', 532), ('act', 533), ('aesthetically', 534), ('afternoon', 535), ('ages', 536), ('arms', 537), ('bill', 538), ('black', 539), ('bro', 540), ('called', 541), ('care', 542), ('chat', 543), ('comes', 544), ('coming', 545), ('cry', 546), ('damn', 547), ('debate', 548), ('different', 549), ('draw', 550), ('due', 551), ('eat', 552), ('enjoy', 553), ('eyes', 554), ('feels', 555), ('felt', 556), ('few', 557), ('film', 558), ('full', 559), ('gift', 560), ('goal', 561), ('goes', 562), ('important', 563), ('incredible', 564), ('jones', 565), ('key', 566), ('kids', 567), ('kind', 568), ('king', 569), ('kurt', 570), ('lets', 571), ('lmao', 572), ('mighty', 573), ('missed', 574), ('movie', 575), ('na', 576), ('past', 577), ('peace', 578), ('phone', 579), ('player', 580), ('playing', 581), ('point', 582), ('possible', 583), ('present', 584), ('problem', 585), ('promise', 586), ('read', 587), ('ready', 588), ('red', 589), ('saying', 590), ('season', 591), ('send', 592), ('set', 593), ('simply', 594), ('sleep', 595), ('somebody', 596), ('spirit', 597), ('spot', 598), ('st', 599), ('stuff', 600), ('super', 601), ('thy', 602), ('treat', 603), ('vs', 604), ('went', 605), ('white', 606), ('wonder', 607), ('yoga', 608), ('2', 609), ('5', 610), (':face_with_rolling_eyes:', 611), (':grinning_face_with_sweat:', 612), (':heart_suit:', 613), (':neutral_face:', 614), (':purple_heart:', 615), (':smirking_face:', 616), (':sparkles:', 617), (':sweat_droplets:', 618), ('<money>', 619), ('=', 620), (']', 621), ('age', 622), ('air', 623), ('along', 624), ('already', 625), ('american', 626), ('asked', 627), ('attempt', 628), ('away', 629), ('awful', 630), ('awkward', 631), ('b', 632), ('became', 633), ('bed', 634), ('begin', 635), ('bf', 636), ('biz', 637), ('blithe', 638), ('boys', 639), ('breakfast', 640), ('brilliant', 641), ('brought', 642), ('bunch', 643), ('business', 644), ('challenge', 645), ('charlotte', 646), ('check', 647), ('child', 648), ('children', 649), ('chris', 650), ('church', 651), ('club', 652), ('coffee', 653), ('confident', 654), ('content', 655), ('cool', 656), ('country', 657), ('coys', 658), ('cut', 659), ('cuz', 660), ('decide', 661), ('definitely', 662), ('don', 663), ('double', 664), ('dude', 665), ('early', 666), ('eating', 667), ('either', 668), ('endless', 669), ('everyday', 670), ('evil', 671), ('extra', 672), ('fight', 673), ('finally', 674), ('fine', 675), ('forgot', 676), ('four', 677), ('g', 678), ('george', 679), ('gladness', 680), ('gotta', 681), ('grand', 682), ('green', 683), ('gun', 684), ('hair', 685), ('happier', 686), ('history', 687), ('hopefully', 688), ('imagine', 689), ('individual', 690), ('instead', 691), ('joys', 692), ('kindness', 693), ('late', 694), ('less', 695), ('lie', 696), ('liveliness', 697), ('medicine', 698), ('meet', 699), ('merriment', 700), ('might', 701), ('minutes', 702), ('mood', 703), ('name', 704), ('nation', 705), ('nawaz', 706), ('needed', 707), ('needs', 708), ('negative', 709), ('okay', 710), ('omg', 711), ('pack', 712), ('patton', 713), ('pleasure', 714), ('positivity', 715), ('pure', 716), ('reason', 717), ('rest', 718), ('second', 719), ('secret', 720), ('seem', 721), ('self', 722), ('service', 723), ('share', 724), ('sharif', 725), ('sir', 726), ('sit', 727), ('smart', 728), ('smiles', 729), ('solving', 730), ('soon', 731), ('thursday', 732), ('totally', 733), ('true', 734), ('turn', 735), ('val', 736), ('wch', 737), ('within', 738), ('wolf', 739), ('wow', 740), ('ya', 741), ('yes', 742), ('yesterday', 743), ('yo', 744), ('', 745), ('#', 746), ('*\\\\', 747), (':balance_scale:', 748), (':bouquet:', 749), (':collision:', 750), (':double_exclamation_mark:', 751), (':fallen_leaf:', 752), (':grinning_squinting_face:', 753), (':hundred_points:', 754), (':maple_leaf:', 755), (':medium-light_skin_tone:', 756), (':raising_hands:', 757), (':squinting_face_with_tongue:', 758), (':zipper-mouth_face:', 759), ('<laugh>', 760), ('<wink>', 761), ('>', 762), ('able', 763), ('absolutely', 764), ('angelou', 765), ('angry', 766), ('anxiety', 767), ('apart', 768), ('ask', 769), ('attitude', 770), ('autumn', 771), ('bag', 772), ('bb', 773), ('beauty', 774), ('behind', 775), ('believe', 776), ('beyond', 777), ('birds', 778), ('blood', 779), ('books', 780), ('bored', 781), ('bout', 782), ('brown', 783), ('buck', 784), ('buy', 785), ('calm', 786), ('came', 787), ('cannot', 788), ('changing', 789), ('channel', 790), ('choir', 791), ('clean', 792), ('clear', 793), ('close', 794), ('college', 795), ('comedy', 796), ('company', 797), ('course', 798), ('create', 799), ('customer', 800), ('dance', 801), ('dancing', 802), ('decision', 803), ('dental', 804), ('describe', 805), ('die', 806), ('died', 807), ('direction', 808), ('disney', 809), ('dream', 810), ('drink', 811), ('dumb', 812), ('e', 813), ('else', 814), ('embrace', 815), ('events', 816), ('everybody', 817), ('everywhere', 818), ('expect', 819), ('fails', 820), ('failure', 821), ('fallen', 822), ('fan', 823), ('fat', 824), ('fear', 825), ('football', 826), ('forever', 827), ('girlfriends', 828), ('given', 829), ('gonna', 830), ('greatest', 831), ('greyhound', 832), ('hall', 833), ('happened', 834), ('health', 835), ('heard', 836), ('hearing', 837), ('hit', 838), ('holiday', 839), ('hoping', 840), ('hot', 841), ('impractical', 842), ('inspire', 843), ('isis', 844), ('italian', 845), ('jinyoung', 846), ('join', 847), ('jokers', 848), ('k', 849), ('keeping', 850), ('kinda', 851), ('known', 852), ('later', 853), ('learning', 854), ('leave', 855), ('letting', 856), ('likes', 857), ('lives', 858), ('london', 859), ('lose', 860), ('loved', 861), ('loves', 862), ('loving', 863), ('mad', 864), ('mansfield', 865), ('mark', 866), ('match', 867), ('maya', 868), ('men', 869), ('mercy', 870), ('midst', 871), ('minds', 872), ('mine', 873), ('mini', 874), ('minute', 875), ('misery', 876), ('modern', 877), ('month', 878), ('oil', 879), ('order', 880), ('party', 881), ('pastor', 882), ('phil', 883), ('players', 884), ('plays', 885), ('pre', 886), ('prince', 887), ('proverbs', 888), ('quinn', 889), ('quote_soup', 890), ('racism', 891), ('reality', 892), ('realize', 893), ('relief', 894), ('rojo', 895), ('room', 896), ('round', 897), ('sadness', 898), ('sam', 899), ('save', 900), ('says', 901), ('seems', 902), ('serious', 903), ('sharpened', 904), ('shower', 905), ('signed', 906), ('single', 907), ('situation', 908), ('snapchat', 909), ('song', 910), ('sound', 911), ('spent', 912), ('starts', 913), ('store', 914), ('story', 915), ('student', 916), ('support', 917), ('supposed', 918), ('talkin', 919), ('terrible', 920), ('thee', 921), ('thenicebot', 922), ('thoughts', 923), ('times', 924), ('tip', 925), ('told', 926), ('tour', 927), ('toward', 928), ('tried', 929), ('trust', 930), ('tumblr', 931), ('tweets', 932), ('uk', 933), ('ukulele', 934), ('uncle', 935), ('united', 936), ('upon', 937), ('using', 938), ('version', 939), ('wanted', 940), ('wasn', 941), ('weapon', 942), ('whatever', 943), ('whether', 944), ('william', 945), ('winning', 946), ('wisdom', 947), ('wise', 948), ('wishing', 949), ('worse', 950), ('worship', 951), ('writing', 952), ('wrong', 953), ('xmas', 954), ('$', 955), ('%', 956), ('0', 957), ('18', 958), ('7', 959), ('9', 960), (':OK_hand:', 961), (':clapping_hands:', 962), (':confused_face:', 963), (':crying_face:', 964), (':disappointed_face:', 965), (':flexed_biceps:', 966), (':folded_hands:', 967), (':grinning_face:', 968), (':grinning_face_with_smiling_eyes:', 969), (':heart_with_arrow:', 970), (':leaf_fluttering_in_wind:', 971), (':pensive_face:', 972), (':slightly_smiling_face:', 973), (':smiling_face_with_sunglasses:', 974), (':thumbs_up:', 975), (':weary_face:', 976), (':yellow_heart:', 977), ('<', 978), ('<tong>', 979), ('@', 980), ('above', 981), ('abt', 982), ('achieve', 983), ('across', 984), ('acting', 985), ('addiction', 986), ('af', 987), ('afraid', 988), ('alert', 989), ('alive', 990), ('allowed', 991), ('alright', 992), ('america', 993), ('anna', 994), ('answer', 995), ('anthony', 996), ('app', 997), ('appreciate', 998), ('asks', 999), ('babies', 1000), ('beer', 1001), ('beg', 1002), ('bitch', 1003), ('blake', 1004), ('blast', 1005), ('blessing', 1006), ('bob', 1007), ('bought', 1008), ('brave', 1009), ('bringing', 1010), ('brings', 1011), ('canceled', 1012), ('candles', 1013), ('cant', 1014), ('cats', 1015), ('cause', 1016), ('celebrity', 1017), ('chair', 1018), ('character', 1019), ('characters', 1020), ('cheerleaders', 1021), ('cheese', 1022), ('chelsea', 1023), ('chick', 1024), ('choice', 1025), ('choose', 1026), ('clever', 1027), ('clients', 1028), ('clouds', 1029), ('cluck', 1030), ('collect', 1031), ('comics', 1032), ('constantly', 1033), ('continue', 1034), ('convinced', 1035), ('cook', 1036), ('copy', 1037), ('coworker', 1038), ('currently', 1039), ('dad', 1040), ('daily', 1041), ('dead', 1042), ('dear', 1043), ('deserves', 1044), ('development', 1045), ('dirty', 1046), ('disappointed', 1047), ('dog', 1048), ('drinking', 1049), ('drinks', 1050), ('drug', 1051), ('dry', 1052), ('dying', 1053), ('entire', 1054), ('envy', 1055), ('experiences', 1056), ('eye', 1057), ('eyebrows', 1058), ('fantastic', 1059), ('fi', 1060), ('filled', 1061), ('final', 1062), ('finished', 1063), ('flying', 1064), ('focus', 1065), ('forget', 1066), ('friday', 1067), ('front', 1068), ('gained', 1069), ('games', 1070), ('gas', 1071), ('genuinely', 1072), ('gif', 1073), ('gifs', 1074), ('girls', 1075), ('gives', 1076), ('giving', 1077), ('glasses', 1078), ('gleek', 1079), ('glorious', 1080), ('gratefully', 1081), ('grind', 1082), ('grow', 1083), ('guy', 1084), ('habit', 1085), ('haha', 1086), ('hands', 1087), ('hang', 1088), ('happens', 1089), ('healthy', 1090), ('hearts', 1091), ('heaven', 1092), ('hello', 1093), ('hillary', 1094), ('hip', 1095), ('hockey', 1096), ('housing', 1097), ('human', 1098), ('hun', 1099), ('hungry', 1100), ('hurt', 1101), ('husband', 1102), ('idiot', 1103), ('incredibly', 1104), ('indeed', 1105), ('international', 1106), ('jesus', 1107), ('jobs', 1108), ('john', 1109), ('joviality', 1110), ('justice', 1111), ('kanye', 1112), ('keeps', 1113), ('kick', 1114), ('killed', 1115), ('kiss', 1116), ('ladies', 1117), ('lady', 1118), ('laughed', 1119), ('laughs', 1120), ('lautner', 1121), ('laws', 1122), ('leads', 1123), ('leaning', 1124), ('lethal', 1125), ('level', 1126), ('liking', 1127), ('lil', 1128), ('ll', 1129), ('lmfao', 1130), ('loud', 1131), ('luv', 1132), ('mc', 1133), ('meeting', 1134), ('method', 1135), ('miami', 1136), ('min', 1137), ('minded', 1138), ('modicum', 1139), ('money', 1140), ('moon', 1141), ('moonlight', 1142), ('mr', 1143), ('national', 1144), ('nephew', 1145), ('nit', 1146), ('nlet', 1147), ('nme', 1148), ('noir', 1149), ('noise', 1150), ('normal', 1151), ('numbers', 1152), ('ny', 1153), ('obama', 1154), ('occasion', 1155), ('odds', 1156), ('offers', 1157), ('office', 1158), ('often', 1159), ('ones', 1160), ('online', 1161), ('opening', 1162), ('otherwise', 1163), ('ought', 1164), ('overtime', 1165), ('p', 1166), ('pain', 1167), ('pak', 1168), ('pakistan', 1169), ('paper', 1170), ('paul', 1171), ('perfume', 1172), ('peter', 1173), ('photo', 1174), ('photographer', 1175), ('pie', 1176), ('piece', 1177), ('pile', 1178), ('pity', 1179), ('pizza', 1180), ('plus', 1181), ('policy', 1182), ('post', 1183), ('power', 1184), ('practice', 1185), ('praise', 1186), ('prayer', 1187), ('premiere', 1188), ('pretty', 1189), ('prices', 1190), ('productivity', 1191), ('provide', 1192), ('ps', 1193), ('questions', 1194), ('quotes', 1195), ('rape', 1196), ('raw', 1197), ('reading', 1198), ('recent', 1199), ('recipe', 1200), ('reflection', 1201), ('reminds', 1202), ('replace', 1203), ('respect', 1204), ('rewatching', 1205), ('ride', 1206), ('ring', 1207), ('rn', 1208), ('rock', 1209), ('romance', 1210), ('root', 1211), ('russian', 1212), ('ryan', 1213), ('scene', 1214), ('sci', 1215), ('score', 1216), ('seriously', 1217), ('sharing', 1218), ('shitty', 1219), ('showed', 1220), ('shows', 1221), ('shut', 1222), ('silence', 1223), ('simple', 1224), ('speak', 1225), ('speechless', 1226), ('spiritual', 1227), ('sports', 1228), ('sprightly', 1229), ('stand', 1230), ('state', 1231), ('strength', 1232), ('stretch', 1233), ('stupid', 1234), ('successful', 1235), ('sugar', 1236), ('surprise', 1237), ('swans', 1238), ('swear', 1239), ('sweden', 1240), ('swim', 1241), ('tag', 1242), ('taylor', 1243), ('technology', 1244), ('th', 1245), ('thankyou', 1246), ('thin', 1247), ('thinking', 1248), ('three', 1249), ('thumbs', 1250), ('tone', 1251), ('trans', 1252), ('under', 1253), ('ustinov', 1254), ('visit', 1255), ('vote', 1256), ('walking', 1257), ('ways', 1258), ('weakness', 1259), ('wearing', 1260), ('weather', 1261), ('weekend', 1262), ('weeks', 1263), ('weiner', 1264), ('whose', 1265), ('wicked', 1266), ('win', 1267), ('wink', 1268), ('wished', 1269), ('women', 1270), ('worry', 1271), ('ye', 1272), ('yeg', 1273), ('yep', 1274), ('youtube', 1275), ('yr', 1276), ('yrs', 1277), ('―', 1278), ('•', 1279), ('16', 1280), ('22', 1281), ('3', 1282), ('6', 1283), ('62', 1284), ('69', 1285), (':beaming_face_with_smiling_eyes:', 1286), (':beating_heart:', 1287), (':bow_and_arrow:', 1288), (':cherry_blossom:', 1289), (':confetti_ball:', 1290), (':crown:', 1291), (':ewe:', 1292), (':face_screaming_in_fear:', 1293), (':flushed_face:', 1294), (':green_heart:', 1295), (':headphone:', 1296), (':hushed_face:', 1297), (':pencil:', 1298), (':red_circle:', 1299), (':right_arrow:', 1300), (':skis:', 1301), (':smiling_face_with_halo:', 1302), (':smiling_face_with_horns:', 1303), (':soft_ice_cream:', 1304), (':two_hearts:', 1305), (':unamused_face:', 1306), (':white_circle:', 1307), (':winking_face_with_tongue:', 1308), (\";'(\\\\\", 1309), ('<annoyed>', 1310), ('<censored>', 1311), ('<date>', 1312), ('<emphasis>', 1313), ('<surprise>', 1314), ('>_<', 1315), ('__', 1316), ('`', 1317), ('a4', 1318), ('ab', 1319), ('ability', 1320), ('abs', 1321), ('absolute', 1322), ('accepted', 1323), ('access', 1324), ('accommodate', 1325), ('accounts', 1326), ('accurate', 1327), ('achievements', 1328), ('acquire', 1329), ('actions', 1330), ('actor', 1331), ('actors', 1332), ('actual', 1333), ('addicted', 1334), ('adorable', 1335), ('adult', 1336), ('advances', 1337), ('adverts', 1338), ('afghanistan', 1339), ('afl', 1340), ('aged', 1341), ('agreed', 1342), ('ahs', 1343), ('airtel', 1344), ('album', 1345), ('aldub', 1346), ('alike', 1347), ('aloha', 1348), ('alone', 1349), ('americans', 1350), ('among', 1351), ('amystery', 1352), ('angel', 1353), ('annoyed', 1354), ('answers', 1355), ('anti', 1356), ('anticipating', 1357), ('apples', 1358), ('appreciated', 1359), ('apprehension', 1360), ('appropriate', 1361), ('arc', 1362), ('armed', 1363), ('arrival', 1364), ('art', 1365), ('asking', 1366), ('assume', 1367), ('assured', 1368), ('atmosphere', 1369), ('au', 1370), ('auction', 1371), ('australia', 1372), ('avoid', 1373), ('award', 1374), ('awareness', 1375), ('awfy', 1376), ('aww', 1377), ('bachelorette', 1378), ('backed', 1379), ('bailey', 1380), ('ballons', 1381), ('balloons', 1382), ('baloch', 1383), ('band', 1384), ('banging', 1385), ('banned', 1386), ('banter', 1387), ('bar', 1388), ('barns', 1389), ('basic', 1390), ('basically', 1391), ('basket', 1392), ('bass', 1393), ('batman', 1394), ('battleground', 1395), ('bcoz', 1396), ('beat', 1397), ('beep', 1398), ('begins', 1399), ('biased', 1400), ('binge', 1401), ('bits', 1402), ('bizitalk', 1403), ('blackish', 1404), ('blacks', 1405), ('blaine', 1406), ('bldg', 1407), ('bless', 1408), ('blow', 1409), ('blue', 1410), ('bolts', 1411), ('bones', 1412), ('boostercourse', 1413), ('bottle', 1414), ('braces', 1415), ('brain', 1416), ('brand', 1417), ('braved', 1418), ('breaks', 1419), ('brewing', 1420), ('brexit', 1421), ('bri', 1422), ('broadband', 1423), ('broken', 1424), ('bs', 1425), ('burslem', 1426), ('busquet', 1427), ('c', 1428), ('cab', 1429), ('cable', 1430), ('calmness', 1431), ('canada', 1432), ('canal', 1433), ('cantor', 1434), ('car', 1435), ('cases', 1436), ('castlez', 1437), ('catch', 1438), ('caught', 1439), ('ceased', 1440), ('certainly', 1441), ('championship', 1442), ('chance', 1443), ('changed', 1444), ('chapter', 1445), ('charged', 1446), ('charles', 1447), ('charlie', 1448), ('chases', 1449), ('cheddar', 1450), ('cheeseburger', 1451), ('chewing', 1452), ('chills', 1453), ('chiropractor', 1454), ('chopper', 1455), ('christ', 1456), ('christmas', 1457), ('chuck', 1458), ('chuckin', 1459), ('ciara', 1460), ('cider', 1461), ('claire', 1462), ('classic', 1463), ('clayton', 1464), ('cletsgo', 1465), ('clip', 1466), ('closer', 1467), ('coffey1617', 1468), ('coincidence', 1469), ('cold', 1470), ('color', 1471), ('colors', 1472), ('comedic', 1473), ('comfortable', 1474), ('comment', 1475), ('commercial', 1476), ('common', 1477), ('compassion', 1478), ('compliment', 1479), ('concerned', 1480), ('confused', 1481), ('conjunction', 1482), ('contentwiththe', 1483), ('continues', 1484), ('contract', 1485), ('conversation', 1486), ('converting', 1487), ('cooked', 1488), ('cooking', 1489), ('cooped', 1490), ('correct', 1491), ('corridor', 1492), ('cost', 1493), ('coughing', 1494), ('council', 1495), ('couple', 1496), ('cover', 1497), ('cracks', 1498), ('creative', 1499), ('cried', 1500), ('cross', 1501), ('crushed', 1502), ('cuddles', 1503), ('cynicism', 1504), ('daddy', 1505), ('dancer', 1506), ('dark', 1507), ('dat', 1508), ('date', 1509), ('de', 1510), ('deal', 1511), ('dealing', 1512), ('dedicated', 1513), ('deeply', 1514), ('defined', 1515), ('dehydrated', 1516), ('delightful', 1517), ('delivered', 1518), ('despair', 1519), ('despite', 1520), ('destruction', 1521), ('detours', 1522), ('deuces', 1523), ('dex', 1524), ('dhoni', 1525), ('dick', 1526), ('differ', 1527), ('differently', 1528), ('digging', 1529), ('dinner', 1530), ('disallowed', 1531), ('discover', 1532), ('display', 1533), ('disposition', 1534), ('distracting', 1535), ('ditto', 1536), ('division', 1537), ('dm', 1538), ('doc', 1539), ('documentary', 1540), ('doesnt', 1541), ('dollar', 1542), ('donalds', 1543), ('donation', 1544), ('download', 1545), ('downtown', 1546), ('draft', 1547), ('drawing', 1548), ('dries', 1549), ('drive', 1550), ('drop', 1551), ('dropped', 1552), ('drove', 1553), ('dt', 1554), ('dudes', 1555), ('dwts', 1556), ('ear', 1557), ('earlier', 1558), ('economy', 1559), ('ed', 1560), ('edinburgh', 1561), ('editing', 1562), ('educate', 1563), ('education', 1564), ('effective', 1565), ('effects', 1566), ('efl', 1567), ('eh', 1568), ('elate', 1569), ('elkracken', 1570), ('email', 1571), ('en', 1572), ('encounter', 1573), ('enjoyment', 1574), ('enrique', 1575), ('entertaining', 1576), ('entrepreneur', 1577), ('environment', 1578), ('episodes', 1579), ('equally', 1580), ('especially', 1581), ('etc', 1582), ('etep', 1583), ('eventual', 1584), ('ex', 1585), ('excites', 1586), ('exciting', 1587), ('exhausted', 1588), ('expectant', 1589), ('expression', 1590), ('extends', 1591), ('extension', 1592), ('extremely', 1593), ('facebook', 1594), ('facing', 1595), ('fail', 1596), ('fake', 1597), ('farm365', 1598), ('fashionable', 1599), ('favourite', 1600), ('fellowship', 1601), ('fictional', 1602), ('file', 1603), ('films', 1604), ('finds', 1605), ('finest', 1606), ('finfin', 1607), ('finish', 1608), ('finishers', 1609), ('finn', 1610), ('fire', 1611), ('fireworks', 1612), ('fitness', 1613), ('fitted', 1614), ('flavored', 1615), ('flavors', 1616), ('flock', 1617), ('florida', 1618), ('folks', 1619), ('followback', 1620), ('followers', 1621), ('fond', 1622), ('foolish', 1623), ('foot', 1624), ('forced', 1625), ('form', 1626), ('forté', 1627), ('freaking', 1628), ('fresh', 1629), ('freshers', 1630), ('fri', 1631), ('friedrich', 1632), ('fscts', 1633), ('fulfilled', 1634), ('fundraiser', 1635), ('funeral', 1636), ('funnier', 1637), ('garoppolo', 1638), ('gay', 1639), ('genius', 1640), ('genocide', 1641), ('german', 1642), ('gesture', 1643), ('ghast', 1644), ('gifts', 1645), ('giggle', 1646), ('giggles', 1647), ('gipe', 1648), ('glen', 1649), ('glutting', 1650), ('goddamn', 1651), ('gods', 1652), ('gone', 1653), ('goodbye', 1654), ('gospel', 1655), ('gossip', 1656), ('grad', 1657), ('grass', 1658), ('grey', 1659), ('grindah', 1660), ('group', 1661), ('guess', 1662), ('gunnysmith93', 1663), ('guns', 1664), ('hahaha', 1665), ('haircut', 1666), ('haitian', 1667), ('hale', 1668), ('halfway', 1669), ('hamlet', 1670), ('handed', 1671), ('hannah', 1672), ('happen', 1673), ('happening', 1674), ('harbor', 1675), ('hardwork', 1676), ('harvest', 1677), ('haters', 1678), ('heavy', 1679), ('height', 1680), ('helpful', 1681), ('helping', 1682), ('helpline', 1683), ('hero', 1684), ('hi', 1685), ('highly', 1686), ('hihi', 1687), ('hillsboro', 1688), ('hips', 1689), ('hoe', 1690), ('hold', 1691), ('holding', 1692), ('holds', 1693), ('holidays', 1694), ('holmes', 1695), ('homecoming', 1696), ('homes', 1697), ('honest', 1698), ('honesty', 1699), ('honks', 1700), ('hood', 1701), ('hopeful', 1702), ('however', 1703), ('hubs', 1704), ('hugots', 1705), ('hugs', 1706), ('humanbeing', 1707), ('humbleness', 1708), ('hundred', 1709), ('hunter', 1710), ('hurts', 1711), ('hygge', 1712), ('hyper', 1713), ('ice', 1714), ('impressed', 1715), ('including', 1716), ('independence', 1717), ('independent', 1718), ('indian', 1719), ('inside', 1720), ('inspiration', 1721), ('insta', 1722), ('instructor', 1723), ('insults', 1724), ('interest', 1725), ('interests', 1726), ('interview', 1727), ('irony', 1728), ('israel', 1729), ('issues', 1730), ('itching', 1731), ('ive', 1732), ('izn', 1733), ('james', 1734), ('jasmine', 1735), ('jean', 1736), ('jen', 1737), ('jeremy', 1738), ('jh', 1739), ('jimmy', 1740), ('jin', 1741), ('jk', 1742), ('jkt', 1743), ('joe', 1744), ('joey', 1745), ('johnson', 1746), ('journey', 1747), ('julie', 1748), ('karma', 1749), ('kashmir', 1750), ('katherine', 1751), ('katie', 1752), ('kemayoran', 1753), ('kerr', 1754), ('kg', 1755), ('kicks', 1756), ('kid', 1757), ('kidding', 1758), ('kill', 1759), ('killgrave', 1760), ('killing', 1761), ('knew', 1762), ('knowing', 1763), ('kris', 1764), ('ksu', 1765), ('kwoir', 1766), ('kx4', 1767), ('l', 1768), ('la', 1769), ('lane', 1770), ('language', 1771), ('large', 1772), ('laundry', 1773), ('lazy', 1774), ('leadership', 1775), ('leaned', 1776), ('leaving', 1777), ('left', 1778), ('legit', 1779), ('legs', 1780), ('library', 1781), ('libtard', 1782), ('lights', 1783), ('liked', 1784), ('line', 1785), ('lion', 1786), ('listed', 1787), ('listened', 1788), ('lit', 1789), ('literal', 1790), ('lmboo', 1791), ('lolol', 1792), ('lololol', 1793), ('longer', 1794), ('looked', 1795), ('lopez', 1796), ('lords', 1797), ('lrt', 1798), ('luck', 1799), ('luke', 1800), ('ma', 1801), ('mae', 1802), ('magazines', 1803), ('magical', 1804), ('manchester', 1805), ('marmite', 1806), ('massali', 1807), ('master', 1808), ('mate', 1809), ('matilda', 1810), ('mccloud', 1811), ('meal', 1812), ('meant', 1813), ('meldonium', 1814), ('melts', 1815), ('meme', 1816), ('memphis', 1817), ('mercure', 1818), ('messaged', 1819), ('messages', 1820), ('messed', 1821), ('messenger', 1822), ('metro', 1823), ('midnight', 1824), ('mil', 1825), ('mila', 1826), ('millennials', 1827), ('milti', 1828), ('mindfulness', 1829), ('ministers', 1830), ('minority', 1831), ('mon', 1832), ('mornings', 1833), ('morrow', 1834), ('mostly', 1835), ('mother', 1836), ('motivation', 1837), ('mou', 1838), ('move', 1839), ('moved', 1840), ('mrs', 1841), ('msg', 1842), ('msnbc', 1843), ('mums', 1844), ('musical', 1845), ('mustache', 1846), ('mustard', 1847), ('nails', 1848), ('nascar', 1849), ('nconclusion', 1850), ('ndays', 1851), ('ndon', 1852), ('negro', 1853), ('neighbor', 1854), ('neighborhoods', 1855), ('nhf16', 1856), ('nietzsche', 1857), ('niggas', 1858), ('nightmares', 1859), ('nj', 1860), ('nmom', 1861), ('non', 1862), ('none', 1863), ('nose', 1864), ('nosing', 1865), ('notts', 1866), ('nrl', 1867), ('nso', 1868), ('number', 1869), ('nurse', 1870), ('nut', 1871), ('nyc', 1872), ('nzephaniah', 1873), ('o', 1874), ('odd', 1875), ('oi', 1876), ('older', 1877), ('onion', 1878), ('ooh', 1879), ('optimist', 1880), ('option', 1881), ('ortiz', 1882), ('ot', 1883), ('overdose', 1884), ('overflow', 1885), ('owner', 1886), ('pakis', 1887), ('pampered', 1888), ('panda', 1889), ('park', 1890), ('parody', 1891), ('passage', 1892), ('passed', 1893), ('pats', 1894), ('pause', 1895), ('pays', 1896), ('peaceful', 1897), ('peeps', 1898), ('pencil', 1899), ('perfect', 1900), ('perhaps', 1901), ('period', 1902), ('permissive', 1903), ('personal', 1904), ('phenomenal', 1905), ('philipp', 1906), ('photography', 1907), ('photos', 1908), ('physical', 1909), ('pig', 1910), ('pillow', 1911), ('pimp', 1912), ('piss', 1913), ('pitch', 1914), ('pitchers', 1915), ('places', 1916), ('planned', 1917), ('plein', 1918), ('plenty', 1919), ('pls', 1920), ('pm', 1921), ('podcasts', 1922), ('polish', 1923), ('poor', 1924), ('pop', 1925), ('portion', 1926), ('posi', 1927), ('possibilities', 1928), ('possibly', 1929), ('posted', 1930), ('posts', 1931), ('potential', 1932), ('pour', 1933), ('pouring', 1934), ('pov', 1935), ('ppl', 1936), ('practically', 1937), ('prayed', 1938), ('preparation', 1939), ('preparing', 1940), ('president', 1941), ('price', 1942), ('princess', 1943), ('priorities', 1944), ('privately', 1945), ('probably', 1946), ('procedures', 1947), ('proctor', 1948), ('prospect', 1949), ('protected', 1950), ('protests', 1951), ('proudly', 1952), ('prowess', 1953), ('prussian', 1954), ('public', 1955), ('pull', 1956), ('pumpkin', 1957), ('purpose', 1958), ('quality', 1959), ('quarterback', 1960), ('question', 1961), ('quiet', 1962), ('quit', 1963), ('quotient', 1964), ('racist', 1965), ('raising', 1966), ('rap', 1967), ('rather', 1968), ('razor', 1969), ('reached', 1970), ('realise', 1971), ('realized', 1972), ('reaper', 1973), ('reasons', 1974), ('rec', 1975), ('recently', 1976), ('reciprocated', 1977), ('recruiter', 1978), ('refreshing', 1979), ('regardless', 1980), ('reinforce', 1981), ('relate', 1982), ('relationship', 1983), ('relaxation', 1984), ('remains', 1985), ('remembered', 1986), ('reminder', 1987), ('reputation', 1988), ('research', 1989), ('reservations', 1990), ('respond', 1991), ('response', 1992), ('restoration', 1993), ('returns', 1994), ('retweeted', 1995), ('rhys', 1996), ('rian', 1997), ('ribena', 1998), ('rich', 1999), ('richness', 2000), ('riggs', 2001), ('rip', 2002), ('road', 2003), ('robin', 2004), ('robocoq', 2005), ('rohan', 2006), ('role', 2007), ('rollin', 2008), ('romans', 2009), ('roosters', 2010), ('route', 2011), ('routine', 2012), ('rugby', 2013), ('rule', 2014), ('running', 2015), ('runs', 2016), ('ruthann', 2017), ('salty', 2018), ('sanatana', 2019), ('satisfy', 2020), ('satisfying', 2021), ('saved', 2022), ('scares', 2023), ('scary', 2024), ('scenes', 2025), ('science', 2026), ('scum', 2027), ('seat', 2028), ('secrets', 2029), ('seed', 2030), ('selfie', 2031), ('sentence', 2032), ('session', 2033), ('sex', 2034), ('sexuality', 2035), ('shades', 2036), ('shakespeare', 2037), ('shaped', 2038), ('shawarma', 2039), ('sheffield', 2040), ('sherlock', 2041), ('shield', 2042), ('shift', 2043), ('shines', 2044), ('shirt', 2045), ('shop', 2046), ('shorts', 2047), ('sight', 2048), ('signs', 2049), ('silly', 2050), ('sis', 2051), ('sister', 2052), ('skies', 2053), ('skiving', 2054), ('skywalkers', 2055), ('sleeping', 2056), ('sloth', 2057), ('slowly', 2058), ('sniffle', 2059), ('snowgang', 2060), ('snuggle', 2061), ('songs', 2062), ('sorry', 2063), ('souls', 2064), ('south', 2065), ('spag', 2066), ('speaking', 2067), ('speech', 2068), ('speed', 2069), ('spend', 2070), ('spying', 2071), ('stands', 2072), ('stare', 2073), ('stars', 2074), ('starting', 2075), ('steve', 2076), ('stock', 2077), ('stomach', 2078), ('storm', 2079), ('straight', 2080), ('strangers', 2081), ('stressed', 2082), ('stronger', 2083), ('struck', 2084), ('struggles', 2085), ('students', 2086), ('stunt', 2087), ('styles', 2088), ('sub', 2089), ('subscribers', 2090), ('sucks', 2091), ('suds', 2092), ('sufficiently', 2093), ('summer', 2094), ('sun', 2095), ('sunbeds', 2096), ('sunny', 2097), ('sunrise', 2098), ('sunshine', 2099), ('superman', 2100), ('surfing', 2101), ('surprises', 2102), ('survived', 2103), ('swapping', 2104), ('swimming', 2105), ('switch', 2106), ('sydney', 2107), ('system', 2108), ('taken', 2109), ('talent', 2110), ('talented', 2111), ('talked', 2112), ('tamron', 2113), ('taste', 2114), ('tasted', 2115), ('tea', 2116), ('teams', 2117), ('teasing', 2118), ('teeth', 2119), ('telling', 2120), ('tells', 2121), ('temper', 2122), ('ten', 2123), ('tenacious', 2124), ('tend', 2125), ('terrified', 2126), ('test', 2127), ('testimonies', 2128), ('text', 2129), ('tf', 2130), ('tfw', 2131), ('tgit', 2132), ('thankful', 2133), ('themed', 2134), ('theobald', 2135), ('theory', 2136), ('therapy', 2137), ('thine', 2138), ('thinks', 2139), ('thoughtful', 2140), ('thousand', 2141), ('thru', 2142), ('thus', 2143), ('tigers', 2144), ('tiller', 2145), ('tips', 2146), ('toddler', 2147), ('tom', 2148), ('toronto', 2149), ('towne', 2150), ('train', 2151), ('training', 2152), ('tribez', 2153), ('truck', 2154), ('tumble', 2155), ('tumbling', 2156), ('turlock', 2157), ('turns', 2158), ('twd', 2159), ('twice', 2160), ('twilight', 2161), ('twitching', 2162), ('u20', 2163), ('ugh', 2164), ('un', 2165), ('understand', 2166), ('understood', 2167), ('underwear', 2168), ('unknown', 2169), ('untill', 2170), ('update', 2171), ('uses', 2172), ('utd', 2173), ('utterly', 2174), ('ve', 2175), ('waiting', 2176), ('walk', 2177), ('walterdale', 2178), ('wants', 2179), ('war', 2180), ('warfare', 2181), ('warmth', 2182), ('warrior', 2183), ('wasteful', 2184), ('wave', 2185), ('waves', 2186), ('wear', 2187), ('web', 2188), ('wee', 2189), ('weekly', 2190), ('weeksary', 2191), ('wetter', 2192), ('whistle', 2193), ('whitegirlwednesday', 2194), ('wild', 2195), ('wins', 2196), ('wit', 2197), ('wives', 2198), ('wobble', 2199), ('won', 2200), ('workout', 2201), ('workplace', 2202), ('works', 2203), ('worried', 2204), ('wrecks', 2205), ('written', 2206), ('wsjnordics', 2207), ('x33', 2208), ('xx', 2209), ('y', 2210), ('yacht', 2211), ('yankees', 2212), ('yawning', 2213), ('yay', 2214), ('yorkshire', 2215), ('yt', 2216), ('yup', 2217), ('yvr', 2218), ('z', 2219), ('zen', 2220), ('zoolander', 2221), ('»', 2222), ('الخفجي', 2223), ('♪', 2224), ('・', 2225), ('\\uf602', 2226), ('$$', 2227), ('$srpt', 2228), ('(*', 2229), ('-_-', 2230), ('10', 2231), ('20', 2232), ('30', 2233), ('8', 2234), (':Aquarius:', 2235), (':ZZZ:', 2236), (':alien_monster:', 2237), (':atom_symbol:', 2238), (':automobile:', 2239), (':baby_chick:', 2240), (':balloon:', 2241), (':bird:', 2242), (':blossom:', 2243), (':books:', 2244), (':cheese_wedge:', 2245), (':copyright:', 2246), (':crayon:', 2247), (':desktop_computer:', 2248), (':dog_face:', 2249), (':enraged_face:', 2250), (':expressionless_face:', 2251), (':face_with_open_mouth:', 2252), (':face_with_steam_from_nose:', 2253), (':fountain_pen:', 2254), (':front-facing_baby_chick:', 2255), (':grapes:', 2256), (':grinning_face_with_big_eyes:', 2257), (':growing_heart:', 2258), (':heart_exclamation:', 2259), (':hot_beverage:', 2260), (':kaaba:', 2261), (':laptop:', 2262), (':left_arrow:', 2263), (':lemon:', 2264), (':money-mouth_face:', 2265), (':nerd_face:', 2266), (':office_building:', 2267), (':open_book:', 2268), (':paw_prints:', 2269), (':person_lifting_weights:', 2270), (':person_raising_hand:', 2271), (':person_tipping_hand:', 2272), (':popcorn:', 2273), (':raised_hand:', 2274), (':relieved_face:', 2275), (':rosette:', 2276), (':rugby_football:', 2277), (':see-no-evil_monkey:', 2278), (':sheaf_of_rice:', 2279), (':skull:', 2280), (':slightly_frowning_face:', 2281), (':sport_utility_vehicle:', 2282), (':sun_behind_cloud:', 2283), (':taxi:', 2284), (':telephone_receiver:', 2285), (':television:', 2286), (':trade_mark:', 2287), (':victory_hand:', 2288), (':water_pistol:', 2289), (':wind_face:', 2290), (':wolf:', 2291), (':woman’s_clothes:', 2292), (':wrapped_gift:', 2293), (';s', 2294), ('<kiss>', 2295), ('^', 2296), ('^_^', 2297), ('___', 2298), ('_full_plus_by_gora', 2299), ('aaron', 2300), ('abby', 2301), ('abla', 2302), ('abuses', 2303), ('abysmal', 2304), ('academia', 2305), ('acapella', 2306), ('accepts', 2307), ('accidental', 2308), ('accomplished', 2309), ('account', 2310), ('actively', 2311), ('actresses', 2312), ('acts', 2313), ('adcomm', 2314), ('add', 2315), ('adding', 2316), ('addition', 2317), ('adj', 2318), ('administrators', 2319), ('admits', 2320), ('admittedly', 2321), ('adopted', 2322), ('adoring', 2323), ('adrenaline', 2324), ('adressed', 2325), ('adults', 2326), ('advance', 2327), ('adventures', 2328), ('advocate', 2329), ('afc', 2330), ('affection', 2331), ('afflicted', 2332), ('affliction', 2333), ('afford', 2334), ('affordable', 2335), ('aflame', 2336), ('agentsof', 2337), ('ah', 2338), ('ahead', 2339), ('ahh', 2340), ('aico', 2341), ('aimed', 2342), ('aint', 2343), ('aja', 2344), ('ajafue', 2345), ('akon', 2346), ('akt', 2347), ('alias', 2348), ('alleged', 2349), ('allow', 2350), ('alternate', 2351), ('amelie', 2352), ('amen', 2353), ('amendment', 2354), ('amidst', 2355), ('amirite', 2356), ('amnd', 2357), ('amount', 2358), ('amy', 2359), ('anfield', 2360), ('angelina', 2361), ('anger', 2362), ('angle', 2363), ('angles', 2364), ('animals', 2365), ('animation', 2366), ('anime', 2367), ('annotated', 2368), ('announce', 2369), ('annoying', 2370), ('annual', 2371), ('answered', 2372), ('anthem', 2373), ('anticipation', 2374), ('anxious', 2375), ('anyway', 2376), ('apartment', 2377), ('apj', 2378), ('apocalypse', 2379), ('apologize', 2380), ('apparently', 2381), ('applauding', 2382), ('apply', 2383), ('appreciation', 2384), ('apps', 2385), ('arabia', 2386), ('archers', 2387), ('aren', 2388), ('argue', 2389), ('ari', 2390), ('aria', 2391), ('ariel', 2392), ('arkham', 2393), ('aroma', 2394), ('arrive', 2395), ('arrogance', 2396), ('arround', 2397), ('arrowverse', 2398), ('arsenal', 2399), ('arthur', 2400), ('articles', 2401), ('arts', 2402), ('arty', 2403), ('ashland', 2404), ('aside', 2405), ('aspire', 2406), ('assault', 2407), ('asshole', 2408), ('assure', 2409), ('astros', 2410), ('asylum', 2411), ('athenian', 2412), ('atkins', 2413), ('atop', 2414), ('attachment', 2415), ('attack', 2416), ('attacked', 2417), ('attempted', 2418), ('attempts', 2419), ('attend', 2420), ('attendance', 2421), ('attended', 2422), ('attending', 2423), ('attention', 2424), ('attentive', 2425), ('audacity', 2426), ('audience', 2427), ('audio', 2428), ('aurora', 2429), ('auto', 2430), ('automatic', 2431), ('av', 2432), ('avatar', 2433), ('ave', 2434), ('average', 2435), ('avid', 2436), ('avoiding', 2437), ('aw', 2438), ('awol', 2439), ('b4', 2440), ('b41', 2441), ('babe', 2442), ('bachelor', 2443), ('backbone', 2444), ('backfire', 2445), ('backing', 2446), ('badd', 2447), ('badger', 2448), ('bahahahaha', 2449), ('bainton', 2450), ('bake', 2451), ('balance', 2452), ('balancing', 2453), ('ball', 2454), ('banger', 2455), ('banquet', 2456), ('baptist', 2457), ('barbaria', 2458), ('barca', 2459), ('bare', 2460), ('barren', 2461), ('bartolomeo', 2462), ('baseball', 2463), ('based', 2464), ('bashir', 2465), ('basset', 2466), ('battle', 2467), ('bazigaga', 2468), ('bbva', 2469), ('beaches', 2470), ('beaming', 2471), ('bear', 2472), ('beats', 2473), ('beaut', 2474), ('beautifully', 2475), ('becca', 2476), ('become', 2477), ('becomes', 2478), ('becoming', 2479), ('bedtime', 2480), ('beef', 2481), ('beginning', 2482), ('beings', 2483), ('bellario', 2484), ('bellboy', 2485), ('belle', 2486), ('bells', 2487), ('belly', 2488), ('benefit', 2489), ('benefits', 2490), ('benoist', 2491), ('bernblade', 2492), ('betelgeuse', 2493), ('between', 2494), ('bey', 2495), ('bezza', 2496), ('bffing', 2497), ('bible', 2498), ('bigger', 2499), ('billie', 2500), ('biraz', 2501), ('bird', 2502), ('birth', 2503), ('birthdays', 2504), ('bitchass', 2505), ('bitches', 2506), ('biweekly', 2507), ('bizarre', 2508), ('bjp', 2509), ('blaiming', 2510), ('blame', 2511), ('blend', 2512), ('blew', 2513), ('blind', 2514), ('blinding', 2515), ('bliss', 2516), ('block', 2517), ('blocked', 2518), ('blocking', 2519), ('bloody', 2520), ('blossoming', 2521), ('blowing', 2522), ('blues', 2523), ('blushing', 2524), ('bobs', 2525), ('bodies', 2526), ('boggling', 2527), ('bomb', 2528), ('bonnie', 2529), ('boobs', 2530), ('boots', 2531), ('bops', 2532), ('boro', 2533), ('bother', 2534), ('bots', 2535), ('bounce', 2536), ('bounty', 2537), ('bouquet', 2538), ('bow', 2539), ('bowels', 2540), ('bowls', 2541), ('boxers', 2542), ('boxing', 2543), ('bqavtja', 2544), ('brad', 2545), ('brag', 2546), ('branch', 2547), ('bravado', 2548), ('bravo', 2549), ('brb', 2550), ('bread', 2551), ('breaking', 2552), ('breakups', 2553), ('breathe', 2554), ('breathes', 2555), ('breeze', 2556), ('brennan', 2557), ('brian', 2558), ('bribe', 2559), ('bribing', 2560), ('bridesmaid', 2561), ('brightens', 2562), ('brims', 2563), ('british', 2564), ('broad', 2565), ('brock', 2566), ('broke', 2567), ('brothers', 2568), ('bruce', 2569), ('brushing', 2570), ('brussels', 2571), ('brutality', 2572), ('bsnl', 2573), ('bt', 2574), ('btw', 2575), ('buffn', 2576), ('bull', 2577), ('bulldogs', 2578), ('bulushi', 2579), ('bumping', 2580), ('bunk', 2581), ('bunkface', 2582), ('burgers', 2583), ('burn', 2584), ('burning', 2585), ('burp', 2586), ('bursting', 2587), ('bushes', 2588), ('bust', 2589), ('busty', 2590), ('busy', 2591), ('butt', 2592), ('button', 2593), ('buying', 2594), ('buzzin', 2595), ('bway', 2596), ('bye', 2597), ('c4', 2598), ('cackle', 2599), ('caffeine', 2600), ('cahracter', 2601), ('calling', 2602), ('calming', 2603), ('cam', 2604), ('camera', 2605), ('camshaft', 2606), ('canadian', 2607), ('cannabis', 2608), ('canning', 2609), ('capable', 2610), ('caps', 2611), ('captivating', 2612), ('caramel', 2613), ('carbonation', 2614), ('card', 2615), ('careful', 2616), ('caressing', 2617), ('caring', 2618), ('carnal', 2619), ('carolina', 2620), ('carrell', 2621), ('carrick', 2622), ('carry', 2623), ('cars', 2624), ('cashcow', 2625), ('cast', 2626), ('caste', 2627), ('cat', 2628), ('categ', 2629), ('catering', 2630), ('cave', 2631), ('cavity', 2632), ('cb', 2633), ('cease', 2634), ('celebrated', 2635), ('celebration', 2636), ('celtic', 2637), ('cencerned', 2638), ('certain', 2639), ('challenged', 2640), ('challenging', 2641), ('champ', 2642), ('chances', 2643), ('changes', 2644), ('charm', 2645), ('chasing', 2646), ('chaudhry', 2647), ('chd', 2648), ('cheat', 2649), ('cheated', 2650), ('cheerios', 2651), ('chests', 2652), ('chicago', 2653), ('chicken', 2654), ('chief', 2655), ('childlike', 2656), ('chill', 2657), ('chilly', 2658), ('chime', 2659), ('chimes', 2660), ('chirps', 2661), ('chock', 2662), ('choked', 2663), ('chokes', 2664), ('chooses', 2665), ('chosen', 2666), ('christian', 2667), ('chuckle', 2668), ('cigarette', 2669), ('cinematography', 2670), ('cities', 2671), ('civil', 2672), ('clapping', 2673), ('classics', 2674), ('click', 2675), ('client', 2676), ('climb', 2677), ('climbed', 2678), ('clinging', 2679), ('clinton', 2680), ('closed', 2681), ('closest', 2682), ('cloud', 2683), ('cloudy', 2684), ('clowns', 2685), ('cmmyd', 2686), ('co', 2687), ('coaches', 2688), ('cocktails', 2689), ('coffin', 2690), ('coke', 2691), ('cokie', 2692), ('collab', 2693), ('collapse', 2694), ('collapses', 2695), ('colts', 2696), ('combination', 2697), ('combined', 2698), ('comedies', 2699), ('comfort', 2700), ('comforting', 2701), ('commando', 2702), ('commentary', 2703), ('comments', 2704), ('community', 2705), ('communitysleepcoach', 2706), ('companies', 2707), ('companion', 2708), ('companions', 2709), ('compatibility', 2710), ('competitive', 2711), ('compilations', 2712), ('compiled', 2713), ('complete', 2714), ('completely', 2715), ('complimentary', 2716), ('compromise', 2717), ('concert', 2718), ('condolences', 2719), ('confessions', 2720), ('confidence', 2721), ('congratulate', 2722), ('congress', 2723), ('connie', 2724), ('constant', 2725), ('constraints', 2726), ('cont2', 2727), ('contestants', 2728), ('continually', 2729), ('control', 2730), ('controlled', 2731), ('converse', 2732), ('convo', 2733), ('convos', 2734), ('cookie', 2735), ('coon', 2736), ('coping', 2737), ('cops', 2738), ('coq', 2739), ('core', 2740), ('corner', 2741), ('corners', 2742), ('corridors', 2743), ('cos', 2744), ('costs', 2745), ('couch', 2746), ('counsel', 2747), ('counted', 2748), ('countenance', 2749), ('counterintuitively', 2750), ('countryside', 2751), ('court', 2752), ('covered', 2753), ('cowboys', 2754), ('cows', 2755), ('coz', 2756), ('cqhshjsi', 2757), ('crack', 2758), ('cracking', 2759), ('created', 2760), ('crew', 2761), ('crickets', 2762), ('criminal', 2763), ('crinkling', 2764), ('crofting', 2765), ('cronies', 2766), ('crossfit', 2767), ('crossing', 2768), ('crossover', 2769), ('crowd', 2770), ('crushes', 2771), ('cst', 2772), ('cultivate', 2773), ('cum', 2774), ('cup', 2775), ('curiosity', 2776), ('current', 2777), ('curtains', 2778), ('curve', 2779), ('curving', 2780), ('cushion', 2781), ('cutest', 2782), ('cx', 2783), ('cyprus', 2784), ('da', 2785), ('dabbed', 2786), ('dalai', 2787), ('damon', 2788), ('dan', 2789), ('darker', 2790), ('darkest', 2791), ('darn', 2792), ('darren', 2793), ('dashed', 2794), ('data', 2795), ('dates', 2796), ('dating', 2797), ('daughter', 2798), ('daughters', 2799), ('david', 2800), ('dawn', 2801), ('dayof', 2802), ('dbs', 2803), ('dc', 2804), ('dean', 2805), ('deaths', 2806), ('debuts', 2807), ('decent', 2808), ('decided', 2809), ('decomposing', 2810), ('deep', 2811), ('def', 2812), ('defeated', 2813), ('defending', 2814), ('defiantly', 2815), ('defo', 2816), ('degenerates', 2817), ('delegation', 2818), ('deleted', 2819), ('delicious', 2820), ('deliver', 2821), ('dell', 2822), ('demi', 2823), ('demilterization', 2824), ('demise', 2825), ('demon', 2826), ('demonstrating', 2827), ('denigrate', 2828), ('denny', 2829), ('dense', 2830), ('denser', 2831), ('dependent', 2832), ('depict', 2833), ('depravity', 2834), ('depressed', 2835), ('dept', 2836), ('depth', 2837), ('describes', 2838), ('desgusting', 2839), ('design', 2840), ('designs', 2841), ('dessert', 2842), ('details', 2843), ('determined', 2844), ('determining', 2845), ('detroit', 2846), ('devastating', 2847), ('developed', 2848), ('developers', 2849), ('deviant', 2850), ('devil', 2851), ('devoid', 2852), ('devon', 2853), ('devout', 2854), ('dg', 2855), ('dialogue', 2856), ('dies', 2857), ('dignified', 2858), ('dilemma', 2859), ('diminish', 2860), ('diner', 2861), ('dingle', 2862), ('diplomacy', 2863), ('diplomat', 2864), ('directions', 2865), ('director', 2866), ('disappearing', 2867), ('discomfort', 2868), ('discord', 2869), ('discovered', 2870), ('discuss', 2871), ('disgrace', 2872), ('dishes', 2873), ('disrespect', 2874), ('distance', 2875), ('distinguished', 2876), ('distracted', 2877), ('diversity', 2878), ('divided', 2879), ('django', 2880), ('dming', 2881), ('dn', 2882), ('doctor', 2883), ('docu', 2884), ('doings', 2885), ('dolores', 2886), ('domestic', 2887), ('doorjambs', 2888), ('doth', 2889), ('doubles', 2890), ('doubt', 2891), ('dprk', 2892), ('drake', 2893), ('dramatically', 2894), ('dreams', 2895), ('dress', 2896), ('dressers', 2897), ('dresses', 2898), ('driven', 2899), ('drivers', 2900), ('driving', 2901), ('drooling', 2902), ('drown', 2903), ('drunk', 2904), ('drunken', 2905), ('ducked', 2906), ('duha', 2907), ('dunno', 2908), ('dutch', 2909), ('duterte', 2910), ('dvlpt', 2911), ('dwight', 2912), ('dysfunctional', 2913), ('ealing', 2914), ('earth', 2915), ('eas', 2916), ('easily', 2917), ('echoed', 2918), ('echoes', 2919), ('economic', 2920), ('eddy', 2921), ('educational', 2922), ('efficient', 2923), ('egg', 2924), ('ego', 2925), ('egypt', 2926), ('el', 2927), ('elation', 2928), ('election', 2929), ('electro', 2930), ('elegance', 2931), ('elevated', 2932), ('elicits', 2933), ('eliminated', 2934), ('elinor', 2935), ('elites', 2936), ('elizabeth', 2937), ('elizabethan', 2938), ('elton', 2939), ('elyse', 2940), ('embarrassed', 2941), ('emerson', 2942), ('emoji', 2943), ('emoticons', 2944), ('emotion', 2945), ('emotional', 2946), ('emotionalhedge', 2947), ('emotions', 2948), ('empty', 2949), ('endearing', 2950), ('ended', 2951), ('endorsement', 2952), ('ends', 2953), ('engineering', 2954), ('engiybhekayo', 2955), ('english', 2956), ('enhanced', 2957), ('enhancment', 2958), ('enigmatic', 2959), ('enjoyable', 2960), ('enjoying', 2961), ('enliven', 2962), ('ensue', 2963), ('ensues', 2964), ('ensure', 2965), ('entertainment', 2966), ('enthusiastic', 2967), ('entice', 2968), ('entirely', 2969), ('epicurus', 2970), ('equestrian', 2971), ('equivalent', 2972), ('ers', 2973), ('escape', 2974), ('espinosa', 2975), ('essential', 2976), ('eternal', 2977), ('eternity', 2978), ('eva', 2979), ('eve', 2980), ('evenin', 2981), ('everone', 2982), ('everythings', 2983), ('evolution', 2984), ('evolve', 2985), ('evolvement', 2986), ('exact', 2987), ('exactment', 2988), ('excitement', 2989), ('exhilarate', 2990), ('exist', 2991), ('existence', 2992), ('expectation', 2993), ('expected', 2994), ('expecting', 2995), ('expense', 2996), ('expensive', 2997), ('explain', 2998), ('explaining', 2999), ('explanation', 3000), ('explicits', 3001), ('exponentially', 3002), ('expressed', 3003), ('extended', 3004), ('eyed', 3005), ('eyelashes', 3006), ('eyepatch', 3007), ('eyggjxml', 3008), ('f', 3009), ('fab', 3010), ('faced', 3011), ('facets', 3012), ('failed', 3013), ('faith', 3014), ('faithful', 3015), ('faking', 3016), ('familiar', 3017), ('families', 3018), ('famous', 3019), ('fancy', 3020), ('fanfiction', 3021), ('fanz', 3022), ('farmhouse', 3023), ('fashion', 3024), ('fast', 3025), ('faster', 3026), ('fault', 3027), ('fave', 3028), ('favor', 3029), ('favorite', 3030), ('fays', 3031), ('fcking', 3032), ('fee', 3033), ('feed', 3034), ('feelings', 3035), ('fees', 3036), ('feet', 3037), ('fellow', 3038), ('females', 3039), ('fence', 3040), ('ff', 3041), ('ffs', 3042), ('fg2a', 3043), ('fighting', 3044), ('fights', 3045), ('figuring', 3046), ('filming', 3047), ('finale', 3048), ('finger', 3049), ('fingers', 3050), ('fitlife', 3051), ('fix', 3052), ('flag', 3053), ('flailing', 3054), ('flakes', 3055), ('flash', 3056), ('flawless', 3057), ('flick', 3058), ('flirting', 3059), ('floor', 3060), ('flourishes', 3061), ('flower', 3062), ('flushed', 3063), ('fodder', 3064), ('folding', 3065), ('folk', 3066), ('followed', 3067), ('follower', 3068), ('font', 3069), ('fooled', 3070), ('force', 3071), ('forego', 3072), ('foreign', 3073), ('foreigners', 3074), ('forest', 3075), ('forgiving', 3076), ('former', 3077), ('forth', 3078), ('foundation', 3079), ('fox', 3080), ('fr', 3081), ('francoise', 3082), ('freedom', 3083), ('freezer', 3084), ('french', 3085), ('freshly', 3086), ('freshmen', 3087), ('freudian', 3088), ('fried', 3089), ('friendship', 3090), ('frighteningly', 3091), ('frisbee', 3092), ('frivolity', 3093), ('froa', 3094), ('frolicsome', 3095), ('frown', 3096), ('fructose', 3097), ('fruit', 3098), ('frustrate', 3099), ('frustrated', 3100), ('frybread', 3101), ('ft', 3102), ('fu*king', 3103), ('fucked', 3104), ('fuckign', 3105), ('fuckin', 3106), ('fueled', 3107), ('fulfilling', 3108), ('function', 3109), ('funerals', 3110), ('funniest', 3111), ('funsville', 3112), ('furniture', 3113), ('further', 3114), ('fut', 3115), ('fx', 3116), ('galaxy', 3117), ('gamay', 3118), ('gaming', 3119), ('gangster', 3120), ('garageband', 3121), ('garbage', 3122), ('garden', 3123), ('garrett', 3124), ('gasp', 3125), ('gaudreau', 3126), ('gby', 3127), ('gears', 3128), ('geek', 3129), ('geese', 3130), ('gemma', 3131), ('generally', 3132), ('generations', 3133), ('genetics', 3134), ('geniuses', 3135), ('gently', 3136), ('geo', 3137), ('geometry', 3138), ('gershwin', 3139), ('gets20', 3140), ('gf', 3141), ('ghost', 3142), ('gideon', 3143), ('gkn', 3144), ('glance', 3145), ('glanced', 3146), ('glasgow', 3147), ('glass', 3148), ('gleesome', 3149), ('gloria', 3150), ('gloriosa', 3151), ('glossed', 3152), ('glow', 3153), ('glowing', 3154), ('gluttony', 3155), ('gn', 3156), ('goats', 3157), ('gobbler', 3158), ('goggles', 3159), ('goldbergs', 3160), ('gona', 3161), ('goo', 3162), ('goofiest', 3163), ('goofy', 3164), ('google', 3165), ('goosebumps', 3166), ('gorgeous', 3167), ('gosh', 3168), ('gosling', 3169), ('gough', 3170), ('gov', 3171), ('grace', 3172), ('graced', 3173), ('graceful', 3174), ('grade', 3175), ('grandson', 3176), ('granny', 3177), ('grant', 3178), ('grate', 3179), ('gratin', 3180), ('gratitude', 3181), ('greed', 3182), ('greengrass', 3183), ('greet', 3184), ('greeted', 3185), ('grid', 3186), ('grief', 3187), ('grim', 3188), ('grin', 3189), ('grinned', 3190), ('grins', 3191), ('gripping', 3192), ('groans', 3193), ('growls', 3194), ('grudgingly', 3195), ('grumbled', 3196), ('grumpy', 3197), ('guber', 3198), ('gud', 3199), ('guesses', 3200), ('guffaw', 3201), ('guilt', 3202), ('guitar', 3203), ('gulps', 3204), ('gunna', 3205), ('gunners', 3206), ('gustin', 3207), ('guts', 3208), ('gutted', 3209), ('h', 3210), ('ha', 3211), ('hah', 3212), ('hahahaha', 3213), ('hahahahahaha', 3214), ('hahahahahahaha', 3215), ('haired', 3216), ('halloween', 3217), ('halsey', 3218), ('ham', 3219), ('hand', 3220), ('handmade', 3221), ('handshake', 3222), ('harbour', 3223), ('harder', 3224), ('harking', 3225), ('harley', 3226), ('harry', 3227), ('harwood', 3228), ('hashtag', 3229), ('hassle', 3230), ('hater', 3231), ('hates', 3232), ('hats', 3233), ('hattrick', 3234), ('havent', 3235), ('hawkings', 3236), ('headed', 3237), ('heads', 3238), ('heap', 3239), ('hears', 3240), ('heartfelt', 3241), ('heather', 3242), ('hedonic', 3243), ('heel', 3244), ('held', 3245), ('hella', 3246), ('helluva', 3247), ('helpless', 3248), ('helps', 3249), ('hemmings', 3250), ('hephxho', 3251), ('heres', 3252), ('hernandez', 3253), ('heroes', 3254), ('herself', 3255), ('hesitantly', 3256), ('hestia', 3257), ('hide', 3258), ('highlight', 3259), ('hilt', 3260), ('himself', 3261), ('hindsight', 3262), ('hire', 3263), ('hitting', 3264), ('hms', 3265), ('hobbit', 3266), ('hole', 3267), ('hollywood', 3268), ('homefolks', 3269), ('homestyle', 3270), ('honestly', 3271), ('honor', 3272), ('honorable', 3273), ('honored', 3274), ('honoured', 3275), ('hooked', 3276), ('hop', 3277), ('horny', 3278), ('horror', 3279), ('horse', 3280), ('hospitalized', 3281), ('hotel', 3282), ('hound', 3283), ('houston', 3284), ('hovering', 3285), ('hovers', 3286), ('howling', 3287), ('hrs', 3288), ('hsnhzmjsc', 3289), ('hssu', 3290), ('htt', 3291), ('hubby', 3292), ('hug', 3293), ('hugging', 3294), ('hughes', 3295), ('hulu', 3296), ('humbly', 3297), ('humor', 3298), ('humping', 3299), ('hurting', 3300), ('hysterical', 3301), ('ian', 3302), ('icc', 3303), ('iced', 3304), ('ices', 3305), ('icon', 3306), ('icy', 3307), ('id', 3308), ('idc', 3309), ('idea', 3310), ('ideal', 3311), ('identity', 3312), ('idol', 3313), ('idolize', 3314), ('ignorance', 3315), ('ignored', 3316), ('ignoring', 3317), ('ihwph', 3318), ('ill', 3319), ('ily', 3320), ('images', 3321), ('imaginative', 3322), ('imessage', 3323), ('immolation', 3324), ('imo', 3325), ('imply', 3326), ('impress', 3327), ('impressive', 3328), ('improving', 3329), ('inactive', 3330), ('inappropriate', 3331), ('inc', 3332), ('incetown', 3333), ('included', 3334), ('increasing', 3335), ('indymn', 3336), ('infants', 3337), ('infectious', 3338), ('infinite', 3339), ('inflatable', 3340), ('influence', 3341), ('influenced', 3342), ('info', 3343), ('ing', 3344), ('ingest', 3345), ('inglorious', 3346), ('inhuman', 3347), ('iniesta', 3348), ('injecting', 3349), ('inmates', 3350), ('inner', 3351), ('innovative', 3352), ('inquiries', 3353), ('insides', 3354), ('inspired', 3355), ('instagram', 3356), ('instagramers', 3357), ('installed', 3358), ('instantly', 3359), ('instructional', 3360), ('insurance', 3361), ('insure', 3362), ('intelligence', 3363), ('interested', 3364), ('intermit', 3365), ('internet', 3366), ('interpreting', 3367), ('interrogation', 3368), ('interrupt', 3369), ('interviews', 3370), ('introducing', 3371), ('invaluable', 3372), ('invisible', 3373), ('invite', 3374), ('inviting', 3375), ('involved', 3376), ('ios10', 3377), ('ipaglabanmo', 3378), ('iphone', 3379), ('ipswich', 3380), ('iraq', 3381), ('ironman', 3382), ('irrelevant', 3383), ('irruptive', 3384), ('isaiah', 3385), ('ish', 3386), ('ishqbaaz', 3387), ('itll', 3388), ('itnis', 3389), ('itself', 3390), ('itt', 3391), ('iwldpe', 3392), ('izzy', 3393), ('jade', 3394), ('jail', 3395), ('jammies', 3396), ('jammy', 3397), ('janak', 3398), ('japes', 3399), ('jasperino', 3400), ('jauntily', 3401), ('jawbone', 3402), ('jaws', 3403), ('jd', 3404), ('jedi', 3405), ('jelly', 3406), ('jerk', 3407), ('jerkass', 3408), ('jersey', 3409), ('jetlag', 3410), ('jewels', 3411), ('jigaboo', 3412), ('jkuvmvqxy', 3413), ('johnny', 3414), ('joined', 3415), ('joining', 3416), ('joke', 3417), ('jokes', 3418), ('jolie', 3419), ('jonza', 3420), ('joyce', 3421), ('jpod', 3422), ('jstor', 3423), ('judge', 3424), ('julian', 3425), ('jummah', 3426), ('jumping', 3427), ('jungle', 3428), ('jus', 3429), ('justin', 3430), ('juxtaposition', 3431), ('kaml', 3432), ('kap', 3433), ('kardashian', 3434), ('karl', 3435), ('kashmires', 3436), ('kayaking', 3437), ('kayaks', 3438), ('kda', 3439), ('keifer', 3440), ('kelvin', 3441), ('kenneth', 3442), ('kept', 3443), ('kerry', 3444), ('kicked', 3445), ('killer', 3446), ('kills', 3447), ('kip', 3448), ('kirby', 3449), ('kisses', 3450), ('kissing', 3451), ('kitten', 3452), ('kjv', 3453), ('knitting', 3454), ('knocks', 3455), ('knows', 3456), ('knvfkkjg', 3457), ('korgoth', 3458), ('kudos', 3459), ('kyle', 3460), ('kzfqr', 3461), ('labeled', 3462), ('lacan', 3463), ('lack', 3464), ('lackeys', 3465), ('lakis', 3466), ('lama', 3467), ('lame', 3468), ('lara', 3469), ('latest', 3470), ('laudrup', 3471), ('laurel', 3472), ('laurie', 3473), ('lawsuit', 3474), ('layout', 3475), ('le', 3476), ('lea', 3477), ('lead', 3478), ('leafy', 3479), ('leap', 3480), ('leaqiwy', 3481), ('learned', 3482), ('learnt', 3483), ('lebanon', 3484), ('led', 3485), ('lengthening', 3486), ('leon', 3487), ('lers', 3488), ('lesson', 3489), ('lest', 3490), ('letter', 3491), ('letters', 3492), ('levels', 3493), ('ley', 3494), ('lfc', 3495), ('li', 3496), ('liarthiefscoundrel', 3497), ('liberals', 3498), ('liberated', 3499), ('lick', 3500), ('lies', 3501), ('lifelong', 3502), ('lifetime', 3503), ('lighthearted', 3504), ('lightly', 3505), ('likefor', 3506), ('likely', 3507), ('lingua', 3508), ('link', 3509), ('lip', 3510), ('lipped', 3511), ('lipstick', 3512), ('lisavikingstad', 3513), ('lisse', 3514), ('literature', 3515), ('liver', 3516), ('liverpool', 3517), ('lkskqufke', 3518), ('llnp', 3519), ('lmaoo', 3520), ('lo', 3521), ('loading', 3522), ('lobby', 3523), ('lochte', 3524), ('lock', 3525), ('lof', 3526), ('logically', 3527), ('lolz', 3528), ('loosen', 3529), ('looting', 3530), ('lordderply', 3531), ('losers', 3532), ('losing', 3533), ('losses', 3534), ('lott', 3535), ('loudon', 3536), ('lovato', 3537), ('loveet', 3538), ('lovelier', 3539), ('lover', 3540), ('lowe', 3541), ('lowers', 3542), ('lowkey', 3543), ('lowly', 3544), ('lucha', 3545), ('lucina', 3546), ('luckii', 3547), ('lufc', 3548), ('luis', 3549), ('lunatic', 3550), ('lundqvist', 3551), ('lungs', 3552), ('lurk', 3553), ('luther', 3554), ('luxurious', 3555), ('lv', 3556), ('lways', 3557), ('lyre', 3558), ('mackinnon', 3559), ('mackinonns', 3560), ('macos', 3561), ('madhouse', 3562), ('madly', 3563), ('mail', 3564), ('maine', 3565), ('maintain', 3566), ('maker', 3567), ('maketh', 3568), ('maliciously', 3569), ('mam', 3570), ('manage', 3571), ('manager', 3572), ('managers', 3573), ('maniac', 3574), ('manner', 3575), ('manuellynch99', 3576), ('manutd', 3577), ('margaret', 3578), ('marilyn', 3579), ('mariners', 3580), ('mario', 3581), ('marks', 3582), ('martin', 3583), ('mary', 3584), ('mass', 3585), ('mastered', 3586), ('matches', 3587), ('materialistic', 3588), ('math', 3589), ('matrix', 3590), ('matt', 3591), ('matter', 3592), ('matters', 3593), ('mattress', 3594), ('mb', 3595), ('md', 3596), ('meaningful', 3597), ('meanwhile', 3598), ('meatloaf', 3599), ('mebbe', 3600), ('medically', 3601), ('meditative', 3602), ('meetings', 3603), ('meets', 3604), ('mel', 3605), ('melbjs', 3606), ('melissa', 3607), ('melt', 3608), ('memorable', 3609), ('mendes', 3610), ('menninger', 3611), ('mental', 3612), ('mention', 3613), ('mentioned', 3614), ('mentions', 3615), ('menus', 3616), ('meogys', 3617), ('merci', 3618), ('merrier', 3619), ('messaging', 3620), ('metal', 3621), ('metaphor', 3622), ('metaphysics', 3623), ('meters', 3624), ('meth', 3625), ('mh', 3626), ('michael', 3627), ('michelle', 3628), ('mick', 3629), ('microsoft_', 3630), ('mid', 3631), ('middlesbrough', 3632), ('midterm', 3633), ('midweek', 3634), ('mikey', 3635), ('mile', 3636), ('milkmaid', 3637), ('millenials', 3638), ('million', 3639), ('mingo', 3640), ('minimize', 3641), ('mins', 3642), ('mints', 3643), ('mirana', 3644), ('mirror', 3645), ('mischievous', 3646), ('miserable', 3647), ('mishle', 3648), ('misogyny', 3649), ('missing', 3650), ('mississippi', 3651), ('mistook', 3652), ('mitts', 3653), ('mitzvah', 3654), ('mix', 3655), ('mode', 3656), ('moderately', 3657), ('modi', 3658), ('moments', 3659), ('monday', 3660), ('monotonous', 3661), ('monroe', 3662), ('monster', 3663), ('monsters', 3664), ('montalvo', 3665), ('moo', 3666), ('moons', 3667), ('moron', 3668), ('motivational', 3669), ('motteville', 3670), ('mouchole', 3671), ('mountain', 3672), ('mourinho', 3673), ('mouth', 3674), ('movements', 3675), ('moves', 3676), ('movies', 3677), ('moving', 3678), ('mpn', 3679), ('mqnaocu', 3680), ('ms', 3681), ('msm', 3682), ('mt', 3683), ('mubarak', 3684), ('mucus', 3685), ('mug', 3686), ('murmuring', 3687), ('murphy', 3688), ('muslims', 3689), ('mustache_harbor', 3690), ('muzzle', 3691), ('myspace', 3692), ('n*she', 3693), ('n10', 3694), ('n311', 3695), ('nah', 3696), ('naivety', 3697), ('nalso', 3698), ('nalways', 3699), ('named', 3700), ('nand', 3701), ('nanny', 3702), ('narcoleptic', 3703), ('nare', 3704), ('narkham', 3705), ('nat', 3706), ('natong', 3707), ('naturally', 3708), ('nature', 3709), ('nbaby', 3710), ('nblushing', 3711), ('nbut', 3712), ('ncaa', 3713), ('ndare', 3714), ('near', 3715), ('nearly', 3716), ('neating', 3717), ('necessary', 3718), ('necessity', 3719), ('neck', 3720), ('neeze', 3721), ('nephalism', 3722), ('nephews', 3723), ('nerve', 3724), ('nerves', 3725), ('nervous', 3726), ('net', 3727), ('net_framework_4', 3728), ('newbies', 3729), ('nga', 3730), ('ngood', 3731), ('nharry', 3732), ('nheart', 3733), ('nhlnyc', 3734), ('nhlnycsweepstakes', 3735), ('nhobbit', 3736), ('nhow', 3737), ('nibbles', 3738), ('nicest', 3739), ('nick', 3740), ('nif', 3741), ('nigger', 3742), ('nights', 3743), ('nihari', 3744), ('nill', 3745), ('nin', 3746), ('ninaturner', 3747), ('ninto', 3748), ('nirvana', 3749), ('niv', 3750), ('nkindly', 3751), ('nmake', 3752), ('nmichael', 3753), ('nmore', 3754), ('nmummified', 3755), ('nmy', 3756), ('nnot', 3757), ('nnow', 3758), ('noah', 3759), ('nods', 3760), ('nof', 3761), ('nol', 3762), ('nominate', 3763), ('nonsense', 3764), ('noodle', 3765), ('noodles', 3766), ('nor', 3767), ('normally', 3768), ('normy', 3769), ('northampton', 3770), ('nostalgia', 3771), ('note', 3772), ('notebooks', 3773), ('nother', 3774), ('notice', 3775), ('notification', 3776), ('nov26', 3777), ('npraise', 3778), ('npsa113', 3779), ('npursing', 3780), ('nsam', 3781), ('nsfw', 3782), ('nshe', 3783), ('nshut', 3784), ('ntalk', 3785), ('nthank', 3786), ('nthat', 3787), ('nthere', 3788), ('nthey', 3789), ('nto', 3790), ('ntoday', 3791), ('ntomiho', 3792), ('nuclear', 3793), ('nuse', 3794), ('nwest', 3795), ('nwhat', 3796), ('nwho', 3797), ('nwill', 3798), ('nyes', 3799), ('nyou', 3800), ('nything', 3801), ('oakland', 3802), ('obc', 3803), ('obfuscated', 3804), ('object', 3805), ('oblivion', 3806), ('obnoxious', 3807), ('obscura', 3808), ('observant', 3809), ('observation', 3810), ('observing', 3811), ('obvious', 3812), ('obviously', 3813), ('occasional', 3814), ('occasionally', 3815), ('od', 3816), ('odell', 3817), ('offensive', 3818), ('offer', 3819), ('official', 3820), ('offline', 3821), ('ogre', 3822), ('oii', 3823), ('ointment', 3824), ('olives', 3825), ('omd', 3826), ('ondo', 3827), ('onision', 3828), ('onstage', 3829), ('onto', 3830), ('opened', 3831), ('openly', 3832), ('opinions', 3833), ('oppressor', 3834), ('optimistic', 3835), ('optimization', 3836), ('options', 3837), ('ordering', 3838), ('oregon', 3839), ('organ', 3840), ('orient', 3841), ('origins', 3842), ('os', 3843), ('oscillation', 3844), ('ostentatious', 3845), ('ounce', 3846), ('ours', 3847), ('ourselves', 3848), ('outage', 3849), ('outfit', 3850), ('outta', 3851), ('overheard', 3852), ('overthinking', 3853), ('ovet', 3854), ('owners', 3855), ('pa', 3856), ('package', 3857), ('pages', 3858), ('paid', 3859), ('paige', 3860), ('painful', 3861), ('paint', 3862), ('pal', 3863), ('pale', 3864), ('palm', 3865), ('pals', 3866), ('pampa', 3867), ('pandering', 3868), ('panel', 3869), ('panther', 3870), ('parade', 3871), ('paradigm', 3872), ('parched', 3873), ('parent', 3874), ('parents', 3875), ('parking', 3876), ('participants', 3877), ('particularly', 3878), ('parties', 3879), ('partners', 3880), ('passenger', 3881), ('passing', 3882), ('passionate', 3883), ('pastel', 3884), ('path', 3885), ('pathetic', 3886), ('patience', 3887), ('patient', 3888), ('patients', 3889), ('patron', 3890), ('patroons', 3891), ('paused', 3892), ('pavel', 3893), ('pawpaw', 3894), ('paws', 3895), ('pay', 3896), ('payet', 3897), ('peacebuilding', 3898), ('peacekeepers', 3899), ('peeping', 3900), ('peer', 3901), ('peers', 3902), ('peg', 3903), ('pens', 3904), ('per', 3905), ('perceived', 3906), ('performance', 3907), ('performances', 3908), ('personality', 3909), ('personally', 3910), ('pervert', 3911), ('pessimism', 3912), ('pet', 3913), ('peterson', 3914), ('phases', 3915), ('phenomanal', 3916), ('philippians', 3917), ('phillips', 3918), ('phones', 3919), ('photograph', 3920), ('php54', 3921), ('pic', 3922), ('pick', 3923), ('picked', 3924), ('pickup', 3925), ('pics', 3926), ('pictures', 3927), ('piers', 3928), ('pikxb', 3929), ('pilot', 3930), ('pinafore', 3931), ('pinch', 3932), ('pink', 3933), ('pinky', 3934), ('pinning', 3935), ('pinterest', 3936), ('pissed', 3937), ('pistols', 3938), ('pitt', 3939), ('pittsburgh', 3940), ('pixel', 3941), ('plan', 3942), ('planet', 3943), ('planning', 3944), ('plant', 3945), ('plants', 3946), ('plop', 3947), ('plot', 3948), ('plum', 3949), ('pno', 3950), ('po', 3951), ('pochama', 3952), ('pochettino', 3953), ('pocket', 3954), ('poems', 3955), ('poet', 3956), ('poetic', 3957), ('pointless', 3958), ('points', 3959), ('pole', 3960), ('political', 3961), ('poll', 3962), ('pool', 3963), ('popped', 3964), ('popular', 3965), ('popularization', 3966), ('port', 3967), ('porthole', 3968), ('portrait', 3969), ('pos', 3970), ('pose', 3971), ('positions', 3972), ('positives', 3973), ('pot', 3974), ('potter', 3975), ('poured', 3976), ('powerful', 3977), ('powerpoint', 3978), ('pp', 3979), ('pray', 3980), ('prayers', 3981), ('predictions', 3982), ('predicts', 3983), ('prefer', 3984), ('prematured', 3985), ('premiers', 3986), ('prepare', 3987), ('prepped', 3988), ('prescription', 3989), ('presentation', 3990), ('presentations', 3991), ('press', 3992), ('pressed', 3993), ('pressure', 3994), ('previous', 3995), ('priceless', 3996), ('prick', 3997), ('pricks', 3998), ('pride', 3999), ('priests', 4000), ('principle', 4001), ('prison', 4002), ('private', 4003), ('pro', 4004), ('produced', 4005), ('produces', 4006), ('productive', 4007), ('prof', 4008), ('profession', 4009), ('professional', 4010), ('professor', 4011), ('proficiency', 4012), ('profile', 4013), ('profit', 4014), ('profitable', 4015), ('prohibitionists', 4016), ('project', 4017), ('prolly', 4018), ('prom', 4019), ('pronouncing', 4020), ('propensity', 4021), ('proper', 4022), ('proprieties', 4023), ('protect', 4024), ('protesters', 4025), ('proven', 4026), ('provided', 4027), ('ps2', 4028), ('psa', 4029), ('psalm', 4030), ('psychology', 4031), ('pto', 4032), ('ptsd', 4033), ('publicly', 4034), ('pulls', 4035), ('pulpit', 4036), ('pummeling', 4037), ('pumping', 4038), ('punchline', 4039), ('punkin', 4040), ('puppies', 4041), ('pups', 4042), ('push', 4043), ('pussy', 4044), ('qmiv', 4045), ('queen', 4046), ('queendom', 4047), ('queens', 4048), ('quick', 4049), ('quickly', 4050), ('quoteoftheday', 4051), ('rachel', 4052), ('raiders', 4053), ('rain', 4054), ('rainbows', 4055), ('raining', 4056), ('rainy', 4057), ('raise', 4058), ('raised', 4059), ('ralph', 4060), ('random', 4061), ('randomly', 4062), ('range', 4063), ('raper', 4064), ('rapeseed', 4065), ('rapidly', 4066), ('rapids', 4067), ('rappers', 4068), ('rapping', 4069), ('rar', 4070), ('rare', 4071), ('rast', 4072), ('rasta', 4073), ('rat', 4074), ('ratio', 4075), ('rcmh', 4076), ('reaches', 4077), ('react', 4078), ('reactions', 4079), ('realisation', 4080), ('realising', 4081), ('realization', 4082), ('rebuild', 4083), ('receive', 4084), ('reckon', 4085), ('reclaimed', 4086), ('recognized', 4087), ('record', 4088), ('recorded', 4089), ('recording', 4090), ('redefine', 4091), ('reflect', 4092), ('reformation', 4093), ('refs', 4094), ('refuse', 4095), ('regard', 4096), ('regenerated', 4097), ('regimes', 4098), ('reginald', 4099), ('regular', 4100), ('regulation', 4101), ('regulus', 4102), ('rehoi', 4103), ('rei', 4104), ('reid', 4105), ('reinvigorated', 4106), ('rejoices', 4107), ('relations', 4108), ('relationships', 4109), ('relatives', 4110), ('relaxing', 4111), ('release', 4112), ('relentless', 4113), ('relentlessly', 4114), ('relevant', 4115), ('religious', 4116), ('relive', 4117), ('remain', 4118), ('remembering', 4119), ('remind', 4120), ('render', 4121), ('renew', 4122), ('renown', 4123), ('rep', 4124), ('reparations', 4125), ('repentance', 4126), ('replaced', 4127), ('reporting', 4128), ('require', 4129), ('researching', 4130), ('reservation', 4131), ('resident', 4132), ('respectful', 4133), ('responsibility', 4134), ('restaurant', 4135), ('retaliation', 4136), ('return', 4137), ('returned', 4138), ('retweet', 4139), ('reveal', 4140), ('revealed', 4141), ('revelation', 4142), ('review', 4143), ('revision', 4144), ('reynolds', 4145), ('rfpbax', 4146), ('rhythm', 4147), ('ri', 4148), ('riches', 4149), ('ridiculous', 4150), ('riding', 4151), ('rikona', 4152), ('rise', 4153), ('risk', 4154), ('risky', 4155), ('ritual', 4156), ('rivers', 4157), ('rob', 4158), ('robot', 4159), ('rodrigo', 4160), ('roland', 4161), ('rollerblades', 4162), ('romcoms', 4163), ('rope', 4164), ('rose', 4165), ('roses', 4166), ('rosy', 4167), ('roth', 4168), ('rub', 4169), ('rubbing', 4170), ('ruins', 4171), ('run', 4172), ('russia', 4173), ('ruth', 4174), ('rwanda', 4175), ('sable', 4176), ('sacrificed', 4177), ('sacrifices', 4178), ('saddened', 4179), ('saf', 4180), ('safe', 4181), ('sager', 4182), ('sales', 4183), ('salespeople', 4184), ('salon', 4185), ('salt', 4186), ('saltes', 4187), ('sanki', 4188), ('saps', 4189), ('sat', 4190), ('satan', 4191), ('satisfied', 4192), ('saturdaynightlive', 4193), ('saudi', 4194), ('saves', 4195), ('savory', 4196), ('scarf', 4197), ('scarring', 4198), ('sceen', 4199), ('scenery', 4200), ('scent', 4201), ('schaaf', 4202), ('schools', 4203), ('scoffed', 4204), ('scra', 4205), ('scratched', 4206), ('scream', 4207), ('screams', 4208), ('screen', 4209), ('scripted', 4210), ('scrum', 4211), ('searching', 4212), ('seasons', 4213), ('secretary', 4214), ('section', 4215), ('seek', 4216), ('seemed', 4217), ('sef', 4218), ('segment', 4219), ('selflessly', 4220), ('selling', 4221), ('semores', 4222), ('senator', 4223), ('sending', 4224), ('sends', 4225), ('senior', 4226), ('sense', 4227), ('sentient', 4228), ('seo', 4229), ('separates', 4230), ('servants', 4231), ('served', 4232), ('services', 4233), ('settled', 4234), ('sevylor', 4235), ('sexually', 4236), ('sg', 4237), ('shabazz', 4238), ('shade', 4239), ('shakespearean', 4240), ('shame', 4241), ('shanghais', 4242), ('shared', 4243), ('shark', 4244), ('shattering', 4245), ('shawty', 4246), ('sheet', 4247), ('shh', 4248), ('shining', 4249), ('shiny', 4250), ('shipped', 4251), ('shite', 4252), ('shittalking', 4253), ('shoes', 4254), ('shook', 4255), ('shooting', 4256), ('shopping', 4257), ('shorty', 4258), ('shot', 4259), ('shots', 4260), ('shoulders', 4261), ('shouting', 4262), ('shove', 4263), ('shown', 4264), ('shred', 4265), ('shy', 4266), ('siblings', 4267), ('sid', 4268), ('sidedly', 4269), ('sign', 4270), ('siguemeytesigo', 4271), ('silky', 4272), ('similar', 4273), ('simmons', 4274), ('simon', 4275), ('simultaneously', 4276), ('sincere', 4277), ('sing', 4278), ('sioux', 4279), ('sip', 4280), ('sipping', 4281), ('sisters', 4282), ('sites', 4283), ('sitting', 4284), ('six', 4285), ('sixth', 4286), ('skating', 4287), ('skein', 4288), ('skin', 4289), ('skinny', 4290), ('skull', 4291), ('sky', 4292), ('skydrive', 4293), ('slavery', 4294), ('slid', 4295), ('slide', 4296), ('slip', 4297), ('slutty', 4298), ('smack', 4299), ('small', 4300), ('smallest', 4301), ('smell', 4302), ('smelled', 4303), ('smells', 4304), ('smiled', 4305), ('smirk', 4306), ('smith', 4307), ('smithsonian', 4308), ('smoking', 4309), ('smoky', 4310), ('smoother', 4311), ('smth', 4312), ('snap', 4313), ('snow', 4314), ('soaked', 4315), ('sob', 4316), ('socialmedia', 4317), ('society', 4318), ('sofa', 4319), ('soft', 4320), ('solange', 4321), ('sold', 4322), ('soldier', 4323), ('solo', 4324), ('solutions', 4325), ('solved', 4326), ('someday', 4327), ('somewhere', 4328), ('soo', 4329), ('soothe', 4330), ('soros', 4331), ('sort', 4332), ('sorta', 4333), ('soundtrack', 4334), ('soup', 4335), ('sour', 4336), ('source', 4337), ('sox', 4338), ('soy', 4339), ('space', 4340), ('spanish', 4341), ('spar', 4342), ('sparkly', 4343), ('special', 4344), ('specific', 4345), ('spell', 4346), ('spice', 4347), ('spider', 4348), ('spin', 4349), ('spiritually', 4350), ('spit', 4351), ('spoken', 4352), ('spontaneous', 4353), ('sporting', 4354), ('sprouts', 4355), ('spy', 4356), ('sq', 4357), ('squad', 4358), ('squats', 4359), ('squeak', 4360), ('squealed', 4361), ('squirrel', 4362), ('stack', 4363), ('stadium', 4364), ('stairs', 4365), ('stalkers', 4366), ('starbucks', 4367), ('starving', 4368), ('statement', 4369), ('statuses', 4370), ('statutes', 4371), ('steady', 4372), ('steam', 4373), ('steer', 4374), ('step', 4375), ('stewards', 4376), ('sticking', 4377), ('stiff', 4378), ('stifle', 4379), ('stifled', 4380), ('stitch', 4381), ('stitches', 4382), ('stool', 4383), ('stoops', 4384), ('stopped', 4385), ('stories', 4386), ('storing', 4387), ('stormy', 4388), ('strange', 4389), ('strangely', 4390), ('stratagem', 4391), ('stream', 4392), ('street', 4393), ('stress', 4394), ('string', 4395), ('stringing', 4396), ('strongman', 4397), ('struggle', 4398), ('strutting', 4399), ('studio', 4400), ('studying', 4401), ('stupefied', 4402), ('stupendous', 4403), ('style', 4404), ('subscribe', 4405), ('subsidised', 4406), ('subway', 4407), ('succeed', 4408), ('suck', 4409), ('sucked', 4410), ('sue', 4411), ('suffering', 4412), ('suffocate', 4413), ('sugarboy', 4414), ('suggest', 4415), ('suggesting', 4416), ('suicidal', 4417), ('summerof', 4418), ('sunday', 4419), ('sunniness', 4420), ('sunset', 4421), ('supper', 4422), ('supported', 4423), ('supporters', 4424), ('suppose', 4425), ('surely', 4426), ('surprised', 4427), ('surrey', 4428), ('survive', 4429), ('sushi', 4430), ('suspect', 4431), ('swallow', 4432), ('swam', 4433), ('swan', 4434), ('sweater', 4435), ('sweaty', 4436), ('sweeping', 4437), ('sweepstakes', 4438), ('sweetmeats', 4439), ('sweetness', 4440), ('sweets', 4441), ('sweety', 4442), ('swinging', 4443), ('sylvia', 4444), ('symmetry', 4445), ('symptomatize', 4446), ('table', 4447), ('taco', 4448), ('tags', 4449), ('taguchi', 4450), ('taht', 4451), ('tail', 4452), ('takeru', 4453), ('takes', 4454), ('talkative', 4455), ('tambourines', 4456), ('tame', 4457), ('tamu', 4458), ('tapingday2', 4459), ('target', 4460), ('tatenashi', 4461), ('tattoos', 4462), ('taurus', 4463), ('tax', 4464), ('taxed', 4465), ('tbh', 4466), ('teach', 4467), ('teacher', 4468), ('teaching', 4469), ('tear', 4470), ('teases', 4471), ('technics', 4472), ('ted', 4473), ('telly', 4474), ('temperament', 4475), ('tenaciously', 4476), ('teppu', 4477), ('term', 4478), ('terrorism', 4479), ('texting', 4480), ('texture', 4481), ('thatcher', 4482), ('theaters', 4483), ('thehulk', 4484), ('theirs', 4485), ('themselves', 4486), ('thfc', 4487), ('thicker', 4488), ('thighs', 4489), ('thoroughly', 4490), ('thou', 4491), ('thread', 4492), ('threats', 4493), ('threesome', 4494), ('thrilled', 4495), ('throat', 4496), ('throughout', 4497), ('tht', 4498), ('thugs', 4499), ('tiburonchamber', 4500), ('ticket', 4501), ('tickets', 4502), ('tiger', 4503), ('tightly', 4504), ('tilting', 4505), ('timeless', 4506), ('timid', 4507), ('tinted', 4508), ('tiny', 4509), ('tipped', 4510), ('titan', 4511), ('title', 4512), ('titter', 4513), ('token', 4514), ('tolerate', 4515), ('tonite', 4516), ('tops', 4517), ('tore', 4518), ('totes', 4519), ('touching', 4520), ('tough', 4521), ('towel', 4522), ('tower', 4523), ('town', 4524), ('toy', 4525), ('trace', 4526), ('tradition', 4527), ('traditionalists', 4528), ('tragically', 4529), ('transcendent', 4530), ('transitioning', 4531), ('transmission', 4532), ('transpo', 4533), ('transportation', 4534), ('trashy', 4535), ('travelled', 4536), ('treated', 4537), ('trepidation', 4538), ('tribal', 4539), ('tribute', 4540), ('tricks', 4541), ('triggered', 4542), ('trip', 4543), ('tripping', 4544), ('trips', 4545), ('trondheim', 4546), ('troops', 4547), ('trusted', 4548), ('trusting', 4549), ('truthfm', 4550), ('tryna', 4551), ('tuhday', 4552), ('tune', 4553), ('turkish', 4554), ('turned', 4555), ('tutoring', 4556), ('twain', 4557), ('tweegram', 4558), ('tweeted', 4559), ('tweeting', 4560), ('twelvemonth', 4561), ('twin', 4562), ('ty', 4563), ('types', 4564), ('typical', 4565), ('tzxchldaq', 4566), ('u.k.', 4567), ('uff', 4568), ('ugly', 4569), ('ulysses', 4570), ('unable', 4571), ('unappetizing', 4572), ('unbelievably', 4573), ('uncertainty', 4574), ('unchained', 4575), ('unchecked', 4576), ('unconceivable', 4577), ('underground', 4578), ('underlord', 4579), ('underneath', 4580), ('underrated', 4581), ('undying', 4582), ('unending', 4583), ('unexpectedly', 4584), ('unfailingly', 4585), ('unintentional', 4586), ('unite', 4587), ('universal', 4588), ('unixware', 4589), ('unless', 4590), ('unlikely', 4591), ('unlimited', 4592), ('uno', 4593), ('unplug', 4594), ('unreal', 4595), ('unwelcome', 4596), ('upbeat', 4597), ('updated', 4598), ('uplifting', 4599), ('upper', 4600), ('uproarious', 4601), ('upset', 4602), ('upside', 4603), ('uranus', 4604), ('ure', 4605), ('useless', 4606), ('username', 4607), ('usual', 4608), ('usually', 4609), ('uudqujcia', 4610), ('vacation', 4611), ('valerie', 4612), ('vallely', 4613), ('valley', 4614), ('vals', 4615), ('value', 4616), ('vampire', 4617), ('various', 4618), ('varsity', 4619), ('vawyzad', 4620), ('veg', 4621), ('vegas', 4622), ('venezuela', 4623), ('vents', 4624), ('verified', 4625), ('vernon', 4626), ('versus', 4627), ('vest', 4628), ('vet', 4629), ('veteran', 4630), ('vibes', 4631), ('vice', 4632), ('victim', 4633), ('vids', 4634), ('view', 4635), ('viewers', 4636), ('vigorous', 4637), ('vile', 4638), ('village', 4639), ('vincent', 4640), ('violence', 4641), ('violently', 4642), ('visitors', 4643), ('visually', 4644), ('vitriolic', 4645), ('vivacious', 4646), ('vixx', 4647), ('vocab', 4648), ('volatility', 4649), ('vrlfeyrn', 4650), ('vulnerable', 4651), ('wagging', 4652), ('waist', 4653), ('waited', 4654), ('wakes', 4655), ('waldo', 4656), ('walked', 4657), ('wall', 4658), ('waltz', 4659), ('warblers', 4660), ('ward', 4661), ('warm', 4662), ('wasted', 4663), ('watering', 4664), ('waving', 4665), ('wavy', 4666), ('wawa', 4667), ('waxen', 4668), ('wcw', 4669), ('weak', 4670), ('wealth', 4671), ('weans', 4672), ('weary', 4673), ('weathers', 4674), ('webcast', 4675), ('webiversary', 4676), ('website', 4677), ('websites', 4678), ('weed', 4679), ('weighty', 4680), ('weird', 4681), ('wel', 4682), ('werent', 4683), ('wesszz', 4684), ('west', 4685), ('westbank', 4686), ('western', 4687), ('westminster', 4688), ('wet', 4689), ('wgj', 4690), ('whenever', 4691), ('whiney', 4692), ('whistling', 4693), ('whitfield', 4694), ('whoo', 4695), ('whu', 4696), ('wide', 4697), ('widows', 4698), ('wifl', 4699), ('wiki', 4700), ('willing', 4701), ('willingness', 4702), ('willxbe', 4703), ('wind', 4704), ('windermere', 4705), ('windmill', 4706), ('window', 4707), ('winds', 4708), ('winter', 4709), ('wiser', 4710), ('witches', 4711), ('witness', 4712), ('witout', 4713), ('wl', 4714), ('woah', 4715), ('woke', 4716), ('wondering', 4717), ('wonderstruck', 4718), ('wondrful', 4719), ('woods', 4720), ('woolwich', 4721), ('worker', 4722), ('wormhole', 4723), ('worth', 4724), ('worthy', 4725), ('wps', 4726), ('wrapped', 4727), ('wraps', 4728), ('wrath', 4729), ('wrinkling', 4730), ('writes', 4731), ('wtf', 4732), ('wyd', 4733), ('x16', 4734), ('x225', 4735), ('xhaka', 4736), ('xucqb', 4737), ('yahweh', 4738), ('yall', 4739), ('yapyap', 4740), ('yasezayoni', 4741), ('ydxef', 4742), ('yell', 4743), ('yelling', 4744), ('yells', 4745), ('yessir', 4746), ('yesyesyes', 4747), ('yiu', 4748), ('yoosung', 4749), ('younger', 4750), ('yourselves', 4751), ('youthful', 4752), ('youthfulness', 4753), ('yugioh', 4754), ('zales', 4755), ('zaza', 4756), ('zeph', 4757), ('zephaniah', 4758), ('zero', 4759), ('zeth', 4760), ('{', 4761), ('}', 4762), ('\\x92', 4763), ('ω', 4764), ('деньги', 4765), ('الوطني', 4766), ('اليوم', 4767), ('∇', 4768), ('≦', 4769), ('≧', 4770), ('➤', 4771), ('「', 4772), ('」', 4773), ('\\uf3fc', 4774), ('\\uf481', 4775), ('\\uf499', 4776), ('\\uf525', 4777), ('\\uf610', 4778), ('\\uf631', 4779), ('珞', 4780), ('（', 4781), ('）', 4782), ('🇦', 4783), ('🇨', 4784), ('🇳', 4785), ('🇴', 4786), ('\\U000fe334', 4787)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(list(field_tweet.vocab.stoi.items()))\n",
        "# print(field_tweet.vocab.vectors[field_tweet.vocab.stoi['the']])"
      ],
      "metadata": {
        "id": "V4fmUSnDCrzU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch_number, batch in enumerate(train_iterator):\n",
        "#   print(batch_number)\n",
        "#   print(batch.tweet)\n",
        "#   print(batch.intensity)"
      ],
      "metadata": {
        "id": "HqQWc-yYsQkN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TorchText treatment of Target Data"
      ],
      "metadata": {
        "id": "-IPH_hfSxxjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Target Data\n",
        "df_target = pd.read_csv(os.path.join(TARGET_DIR,'winomt_saunders_combined.csv'))\n",
        "print(df_target.head())\n",
        "target_file_name = 'target_csv.csv'\n",
        "target_csv_file = df_target.to_csv(str(target_file_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4RhbHIw0odH",
        "outputId": "b28ef105-ec60-49e7-d678-b3bea0be1f6e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             Tweet  Intensity Score\n",
            "0    the actor finished her work .                0\n",
            "1    the actor finished his work .                0\n",
            "2  the actuary finished her work .                0\n",
            "3  the actuary finished his work .                0\n",
            "4    the agent finished her work .                0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_target_dataset = {}\n",
        "for name in list_name:\n",
        "  # print(dict_fields[name])\n",
        "  dict_target_dataset [name] = TabularDataset( path = os.path.join(TARGET_DIR,'winomt_saunders_combined.csv'), \n",
        "                                              format = 'csv',\n",
        "                                              fields = dict_fields[name] )\n",
        "\n",
        "print(dict_target_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dy6lhcIwwb7",
        "outputId": "593f0788-bbb3-4d84-dccb-9d39b996d642"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'EI_sadness': <torchtext.legacy.data.dataset.TabularDataset object at 0x7f0844b45ed0>, 'EI_anger': <torchtext.legacy.data.dataset.TabularDataset object at 0x7f0844b3b150>, 'EI_fear': <torchtext.legacy.data.dataset.TabularDataset object at 0x7f0844b45890>, 'V': <torchtext.legacy.data.dataset.TabularDataset object at 0x7f0844b45f10>, 'EI_joy': <torchtext.legacy.data.dataset.TabularDataset object at 0x7f084359aa90>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# target_data = TabularDataset( path = os.path.join(TARGET_DIR,'winomt_saunders_combined.csv'), \n",
        "#                                               format = 'csv',\n",
        "#                                               fields = fields\n",
        "#                                           )\n",
        "# print(target_data[0].__dict__.keys())\n",
        "# print(target_data[0].__dict__.values())"
      ],
      "metadata": {
        "id": "wL8EgUfFgc4g"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, dataset in dict_target_dataset.items():\n",
        "  count=0\n",
        "  for example in dataset:\n",
        "    print(example.tweet, example.intensity)\n",
        "    count += 1\n",
        "    if count > 2:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxZytUW1zVKL",
        "outputId": "c455598a-ca81-45fc-9a56-53520a4ef774"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'actor', 'finished', 'her', 'work', '.'] 0\n",
            "['the', 'actor', 'finished', 'his', 'work', '.'] 0\n",
            "['the', 'actuary', 'finished', 'her', 'work', '.'] 0\n",
            "['the', 'actor', 'finished', 'her', 'work', '.'] 0\n",
            "['the', 'actor', 'finished', 'his', 'work', '.'] 0\n",
            "['the', 'actuary', 'finished', 'her', 'work', '.'] 0\n",
            "['the', 'actor', 'finished', 'her', 'work', '.'] 0\n",
            "['the', 'actor', 'finished', 'his', 'work', '.'] 0\n",
            "['the', 'actuary', 'finished', 'her', 'work', '.'] 0\n",
            "['the', 'actor', 'finished', 'her', 'work', '.'] 0\n",
            "['the', 'actor', 'finished', 'his', 'work', '.'] 0\n",
            "['the', 'actuary', 'finished', 'her', 'work', '.'] 0\n",
            "['the', 'actor', 'finished', 'her', 'work', '.'] 0\n",
            "['the', 'actor', 'finished', 'his', 'work', '.'] 0\n",
            "['the', 'actuary', 'finished', 'her', 'work', '.'] 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count = 0\n",
        "# for example in target_data:\n",
        "#   print(example.tweet, example.intensity)\n",
        "#   count += 1\n",
        "#   if count > 2:\n",
        "#     break"
      ],
      "metadata": {
        "id": "F8eLBUX33d9o"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_target_iterator = {}\n",
        "for name in list_name:\n",
        "  dict_target_iterator [name] = BucketIterator(dict_target_dataset[name], # given that there is only one dataset we are not using splits\n",
        "                                 batch_size= TARGET_BATCH_SIZE,\n",
        "                                 sort_key = lambda x: len(x.tweet),\n",
        "                                 sort_within_batch=True,\n",
        "                                 device = DEVICE,\n",
        "                                 repeat=True,\n",
        "                                 shuffle= True)\n",
        "\n",
        "print(dict_target_iterator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tNj30540-Bq",
        "outputId": "f5616fe1-1e73-4bed-9e0a-fbf6e3b75a72"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'EI_sadness': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f07fccd3cd0>, 'EI_anger': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f07fccd3d50>, 'EI_fear': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f07fccd3dd0>, 'V': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f07fccd3e50>, 'EI_joy': <torchtext.legacy.data.iterator.BucketIterator object at 0x7f07fccd3ed0>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# target_iterator = BucketIterator(target_data, # given that there is only one dataset we are not using splits\n",
        "#                                  batch_size= TARGET_BATCH_SIZE,\n",
        "#                                  sort_key = lambda x: len(x.tweet),\n",
        "#                                  sort_within_batch=True,\n",
        "#                                  device = DEVICE,\n",
        "#                                  repeat=True,\n",
        "#                                  shuffle= True)"
      ],
      "metadata": {
        "id": "uWpJ_-jxuzIQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next(iter(target_iterator))"
      ],
      "metadata": {
        "id": "FhBEp-LnZ5JR"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, iterator in dict_target_iterator.items():\n",
        "  count = 0\n",
        "  for batch in iterator:\n",
        "    print(name)\n",
        "    print(batch)\n",
        "    print (batch.tweet)\n",
        "    print (batch.intensity)\n",
        "    count += 1\n",
        "    break\n",
        "    if count > 2:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRGXKSZy1mXw",
        "outputId": "1f72090f-5d84-41b7-9bc0-0f7d726e12c2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EI_sadness\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 8]\n",
            "\t[.tweet]:[torch.cuda.LongTensor of size 8x50 (GPU 0)]\n",
            "\t[.intensity]:[torch.cuda.FloatTensor of size 8 (GPU 0)]\n",
            "tensor([[   7,    0,    0,    7, 1718,    9,    7,    0,   12,  396,   68,   62,\n",
            "            0,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7,    0,    0,    7,    0,   47,  292,  325,   12,  129,    0,   71,\n",
            "           82,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7, 3232,    0,    7,    0,   81,    7, 2702,  115,   97,   59,   27,\n",
            "            0,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7, 2018,    0,    7,  419,    9,    7,    0,   12,  404,    0,   85,\n",
            "          151,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7,    0,    0,    7,    0,   12, 1380,   82,   10,  132,  348,   23,\n",
            "          168,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7,    0, 1162,   10, 4477,   23,    7,    0,   12,  269,   82,    9,\n",
            "          701,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7, 1718,  396,    7,    0,    9,  353,  342,  115,   97,   16,  602,\n",
            "            0,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7,  442,  264,    7,    0,   10,  440, 2038,  115,   97,   59,  198,\n",
            "            0,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1]], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
            "EI_anger\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 8]\n",
            "\t[.tweet]:[torch.cuda.LongTensor of size 8x50 (GPU 0)]\n",
            "\t[.intensity]:[torch.cuda.FloatTensor of size 8 (GPU 0)]\n",
            "tensor([[   7,    0,    0,    7,    0,    8,    7,    0,   11,  354,   58,   49,\n",
            "         1264,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7,    0,    0,    7,    0,   42,  224,  419,   11,  108,    0,   62,\n",
            "           85,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7,    0, 4271,    7,    0,   71,    7,    0,  100,   80,   57,   26,\n",
            "            0,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7, 4523,    0,    7, 1338,    8,    7,    0,   11,  928, 4652,   67,\n",
            "          131,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7,    0,    0,    7,    0,   11, 1100,   85,    9,  204,  277,   27,\n",
            "          167,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7,    0,    0,    9,    0,   27,    7,    0,   11,  382,   85,    8,\n",
            "          337,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7,    0,  354,    7,    0,    8,  509,  406,  100,   80,   15, 1273,\n",
            "            0,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7,  330,  409,    7,    0,    9,  200,  569,  100,   80,   57, 1182,\n",
            "            0,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1]], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
            "EI_fear\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 8]\n",
            "\t[.tweet]:[torch.cuda.LongTensor of size 8x50 (GPU 0)]\n",
            "\t[.intensity]:[torch.cuda.FloatTensor of size 8 (GPU 0)]\n",
            "tensor([[   7,    0,    0,    7, 1585,    8,    7,    0,   14,  480,   50,   60,\n",
            "         2625,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7,    0,    0,    7, 3652,   42,  176,  412,   14,  142,    0,   51,\n",
            "          113,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7, 4203, 5178,    7,    0,   61,    7,    0,  152,  117,   44,   27,\n",
            "            0,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7, 5410,    0,    7,  740,    8,    7, 3652,   14, 1501,    0,   91,\n",
            "          122,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7, 3652,    0,    7,    0,   14, 1946,  113,   10,  137,  251,   25,\n",
            "          151,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7,    0, 4971,   10,    0,   25,    7,    0,   14,  290,  113,    8,\n",
            "          460,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7, 1585,  480,    7,    0,    8,  869,  455,  152,  117,   12, 4819,\n",
            "         3847,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7,  207,  280,    7,    0,   10,  245,    0,  152,  117,   44,  293,\n",
            "            0,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1]], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
            "V\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 8]\n",
            "\t[.tweet]:[torch.cuda.LongTensor of size 8x50 (GPU 0)]\n",
            "\t[.intensity]:[torch.cuda.FloatTensor of size 8 (GPU 0)]\n",
            "tensor([[   6,    0,    0,    6,    0,    9,    6,    0,   12,  536,   60,   58,\n",
            "         3396,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   6,    0,    0,    6,    0,   44,  119, 1696,   12,  128,    0,   55,\n",
            "           74,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   6,    0,    0,    6,    0,   68,    6,    0,   98,   76,   49,   27,\n",
            "            0,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   6,    0,    0,    6,  514,    9,    6,    0,   12,  867, 4279,   67,\n",
            "          153,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   6,    0,    0,    6,    0,   12,  809,   74,   11,  108,  321,   24,\n",
            "          150,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   6, 1191, 3626,   11, 4016,   24,    6,    0,   12,  185,   74,    9,\n",
            "         1504,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   6,    0,  536,    6,    0,    9,  372,  689,   98,   76,   14, 1486,\n",
            "            0,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   6,  837,  493,    6,    0,   11,  342, 4210,   98,   76,   49,  351,\n",
            "            0,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1]], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
            "EI_joy\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 8]\n",
            "\t[.tweet]:[torch.cuda.LongTensor of size 8x50 (GPU 0)]\n",
            "\t[.intensity]:[torch.cuda.FloatTensor of size 8 (GPU 0)]\n",
            "tensor([[   6,    0,    0,    6, 3572,    9,    6,    0,   12,  627,   77,   59,\n",
            "          707,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   6,    0,    0,    6, 2655,   38,  174,  735,   12,  213,    0,   71,\n",
            "           83,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   6,    0, 2022,    6,    0,   72,    6,    0,  104,   85,   54,   33,\n",
            "            0,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   6, 4468,    0,    6,  908,    9,    6, 2655,   12,  556, 2167,   50,\n",
            "          171,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   6, 2655,    0,    6,    0,   12, 1008,   83,   10,  106,  579,   28,\n",
            "          249,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   6, 1036,    0,   10, 4335,   28,    6,    0,   12,  335,   83,    9,\n",
            "         3896,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   6, 3572,  627,    6,    0,    9,  404,  694,  104,   85,   15, 1159,\n",
            "            0,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   6,  800, 1778,    6,    0,   10,  277,  925,  104,   85,   54,  291,\n",
            "            0,    4,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1]], device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count = 0\n",
        "# for batch in target_iterator:\n",
        "#   print(batch)\n",
        "#   print (batch.tweet)\n",
        "#   print (batch.intensity)\n",
        "#   count += 1\n",
        "#   if count > 2:\n",
        "#     break"
      ],
      "metadata": {
        "id": "pso0cqFZvQWA"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN 1d model"
      ],
      "metadata": {
        "id": "-h9cXmnyualc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Reversal layer"
      ],
      "metadata": {
        "id": "qyDfiHFNkZNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "\n",
        "class GradientReversalFn(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "        \n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None"
      ],
      "metadata": {
        "id": "2TdezuQokWiJ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN 1 D model\n",
        "Reference: A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification, Ye Zhang, Byron Wallace 2015\n",
        "\n",
        "Difference:\n",
        "\n",
        "use of embedding\n",
        "use of sigmoid function, as we are having a regression model not a classififer as the main task"
      ],
      "metadata": {
        "id": "J53qMJn4kfv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN1d(nn.Module):\n",
        "    def __init__(self, \n",
        "                 vocab_size, \n",
        "                 embedding_dim, \n",
        "                 n_filters, \n",
        "                 filter_sizes, \n",
        "                 output_dim, \n",
        "                 dropout, \n",
        "                 pad_idx\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        \n",
        "        #---------------------Feature Extractor Network----------------------#\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "\n",
        "        # Convolutional Network\n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv1d(in_channels = embedding_dim, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = fs)\n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        #---------------------Regression Network------------------------#\n",
        "        # Fully-connected layer and Dropout\n",
        "        self.regression = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(len(filter_sizes) * n_filters, len(filter_sizes) * n_filters // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(len(filter_sizes) * n_filters // 2, output_dim * 10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(output_dim * 10, output_dim)\n",
        "            # ,\n",
        "            # nn.Sigmoid()\n",
        "        )\n",
        "        # self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim * 10)\n",
        "        # self.fc2 = nn.Linear(output_dim * 10, output_dim)\n",
        "        # self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        #---------------------Domain Classifier Network------------------------#\n",
        "        # Fully-connected layer and Dropout\n",
        "        self.domain_classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(len(filter_sizes) * n_filters, len(filter_sizes) * n_filters // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(len(filter_sizes) * n_filters // 2, output_dim * 10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(output_dim * 10, 2),\n",
        "            nn.LogSoftmax(dim=1),\n",
        "        )\n",
        "        \n",
        "    def forward(self, text, alpha=1.0):\n",
        "        \n",
        "        #text = [batch size, sent len]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "        \n",
        "        #embedded = [batch size, emb dim, sent len]\n",
        "        \n",
        "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "            \n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "        \n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        x_feature = torch.cat(pooled, dim = 1)\n",
        "        \n",
        "        #x_feature = [batch size, n_filters * len(filter_sizes)]\n",
        "        \n",
        "        reverse_feature = GradientReversalFn.apply(x_feature, alpha)\n",
        "        # print(\"reverse_feature\",reverse_feature)\n",
        "    \n",
        "        regression_output = self.regression(x_feature)\n",
        "    \n",
        "        domain_classifier_output = self.domain_classifier(reverse_feature)\n",
        "\n",
        "\n",
        "        return regression_output, domain_classifier_output"
      ],
      "metadata": {
        "id": "sp8KeSpquZqB"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INPUT_DIM = len(field_tweet.vocab) # these change for each model\n",
        "EMBEDDING_DIM = 100\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [2, 3, 4, 5]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "# PAD_IDX = field_tweet.vocab.stoi[field_tweet.pad_token] # these change for each model"
      ],
      "metadata": {
        "id": "J2YB0TUfvZuM"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture Creation for each variant, Loading pre-trained embeddings"
      ],
      "metadata": {
        "id": "4fB6Whz3PTSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_model_arch ={}\n",
        "for name in list_name:\n",
        "  \n",
        "  # INPUT_DIM = len(field_tweet.vocab) # single model\n",
        "  INPUT_DIM = len(dict_fields[name]['Tweet'][1].vocab)\n",
        "  # print(INPUT_DIM)\n",
        "\n",
        "  # PAD_IDX = field_tweet.vocab.stoi[field_tweet.pad_token] # # single model\n",
        "  PAD_IDX = dict_fields[name]['Tweet'][1].vocab.stoi[dict_fields[name]['Tweet'][1].pad_token]\n",
        "  # print(PAD_IDX)\n",
        "\n",
        "  dict_model_arch[name] = CNN1d(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
        "  dict_model_arch[name].to(DEVICE)\n",
        "\n",
        "  # pretrained_embeddings = field_tweet.vocab.vectors # single model\n",
        "  pretrained_embeddings = dict_fields[name]['Tweet'][1].vocab.vectors\n",
        "\n",
        "  # model.embedding.weight.data.copy_(pretrained_embeddings) # single model\n",
        "  dict_model_arch[name].embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "  # UNK_IDX = field_tweet.vocab.stoi[field_tweet.unk_token] # single model\n",
        "  UNK_IDX = dict_fields[name]['Tweet'][1].vocab.stoi[dict_fields[name]['Tweet'][1].unk_token]\n",
        "\n",
        "  # model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM) # single model\n",
        "  dict_model_arch[name].embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM) \n",
        "  \n",
        "  # model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM) # single model\n",
        "  dict_model_arch[name].embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM) \n",
        "\n",
        "dict_model_arch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UjlCzBsJ7I5",
        "outputId": "a267c631-4bad-476d-9b32-cc93e65668af"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'EI_sadness': CNN1d(\n",
              "   (embedding): Embedding(4989, 100, padding_idx=1)\n",
              "   (convs): ModuleList(\n",
              "     (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
              "     (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "     (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
              "     (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "   )\n",
              "   (regression): Sequential(\n",
              "     (0): Dropout(p=0.5, inplace=False)\n",
              "     (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "     (2): ReLU()\n",
              "     (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "     (4): ReLU()\n",
              "     (5): Linear(in_features=10, out_features=1, bias=True)\n",
              "   )\n",
              "   (domain_classifier): Sequential(\n",
              "     (0): Dropout(p=0.5, inplace=False)\n",
              "     (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "     (2): ReLU()\n",
              "     (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "     (4): ReLU()\n",
              "     (5): Linear(in_features=10, out_features=2, bias=True)\n",
              "     (6): LogSoftmax(dim=1)\n",
              "   )\n",
              " ), 'EI_anger': CNN1d(\n",
              "   (embedding): Embedding(4824, 100, padding_idx=1)\n",
              "   (convs): ModuleList(\n",
              "     (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
              "     (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "     (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
              "     (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "   )\n",
              "   (regression): Sequential(\n",
              "     (0): Dropout(p=0.5, inplace=False)\n",
              "     (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "     (2): ReLU()\n",
              "     (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "     (4): ReLU()\n",
              "     (5): Linear(in_features=10, out_features=1, bias=True)\n",
              "   )\n",
              "   (domain_classifier): Sequential(\n",
              "     (0): Dropout(p=0.5, inplace=False)\n",
              "     (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "     (2): ReLU()\n",
              "     (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "     (4): ReLU()\n",
              "     (5): Linear(in_features=10, out_features=2, bias=True)\n",
              "     (6): LogSoftmax(dim=1)\n",
              "   )\n",
              " ), 'EI_fear': CNN1d(\n",
              "   (embedding): Embedding(5681, 100, padding_idx=1)\n",
              "   (convs): ModuleList(\n",
              "     (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
              "     (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "     (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
              "     (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "   )\n",
              "   (regression): Sequential(\n",
              "     (0): Dropout(p=0.5, inplace=False)\n",
              "     (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "     (2): ReLU()\n",
              "     (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "     (4): ReLU()\n",
              "     (5): Linear(in_features=10, out_features=1, bias=True)\n",
              "   )\n",
              "   (domain_classifier): Sequential(\n",
              "     (0): Dropout(p=0.5, inplace=False)\n",
              "     (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "     (2): ReLU()\n",
              "     (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "     (4): ReLU()\n",
              "     (5): Linear(in_features=10, out_features=2, bias=True)\n",
              "     (6): LogSoftmax(dim=1)\n",
              "   )\n",
              " ), 'V': CNN1d(\n",
              "   (embedding): Embedding(4449, 100, padding_idx=1)\n",
              "   (convs): ModuleList(\n",
              "     (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
              "     (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "     (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
              "     (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "   )\n",
              "   (regression): Sequential(\n",
              "     (0): Dropout(p=0.5, inplace=False)\n",
              "     (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "     (2): ReLU()\n",
              "     (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "     (4): ReLU()\n",
              "     (5): Linear(in_features=10, out_features=1, bias=True)\n",
              "   )\n",
              "   (domain_classifier): Sequential(\n",
              "     (0): Dropout(p=0.5, inplace=False)\n",
              "     (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "     (2): ReLU()\n",
              "     (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "     (4): ReLU()\n",
              "     (5): Linear(in_features=10, out_features=2, bias=True)\n",
              "     (6): LogSoftmax(dim=1)\n",
              "   )\n",
              " ), 'EI_joy': CNN1d(\n",
              "   (embedding): Embedding(4788, 100, padding_idx=1)\n",
              "   (convs): ModuleList(\n",
              "     (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
              "     (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "     (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
              "     (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "   )\n",
              "   (regression): Sequential(\n",
              "     (0): Dropout(p=0.5, inplace=False)\n",
              "     (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "     (2): ReLU()\n",
              "     (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "     (4): ReLU()\n",
              "     (5): Linear(in_features=10, out_features=1, bias=True)\n",
              "   )\n",
              "   (domain_classifier): Sequential(\n",
              "     (0): Dropout(p=0.5, inplace=False)\n",
              "     (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "     (2): ReLU()\n",
              "     (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "     (4): ReLU()\n",
              "     (5): Linear(in_features=10, out_features=2, bias=True)\n",
              "     (6): LogSoftmax(dim=1)\n",
              "   )\n",
              " )}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = CNN1d(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
        "# model.to(DEVICE)"
      ],
      "metadata": {
        "id": "lB-kw2t_voEm"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Pre trained embeddings\n",
        "we'll load the pre-trained *embeddings*"
      ],
      "metadata": {
        "id": "z4p5YOsBygPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrained_embeddings = field_tweet.vocab.vectors\n",
        "# model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "metadata": {
        "id": "42HOzqrZySOt"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# field_tweet.vocab.vectors.shape"
      ],
      "metadata": {
        "id": "03-leBf6zSk5"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNK_IDX = field_tweet.vocab.stoi[field_tweet.unk_token]\n",
        "\n",
        "# model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "# model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
      ],
      "metadata": {
        "id": "Lve6CyJk9zfA"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "kBjLi5QG9_YG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Without training one forward pass"
      ],
      "metadata": {
        "id": "dxM_9csG_O4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model_arch in dict_model_arch.items():\n",
        "  for batch in dict_iterator[name]['train_iterator']:\n",
        "    print(batch.tweet)\n",
        "    output = model_arch(batch.tweet)\n",
        "    print (output)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD0y4pNS_SUP",
        "outputId": "e4f242a0-4e19-44fb-ccec-502635df537c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   3, 2022,    2,    3, 1223,    2,    7,  159, 1796, 1614,  246,   62,\n",
            "          373,   56,   10,  192,  810,    8,   24,   16,    7,  417,    8,   49,\n",
            "           57,   99,  311,   57,   99,  192,    8,    3,   48,    2,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [ 350,  102, 1914,    9, 1848,   18,  575,   15,    7,  334,   34, 1924,\n",
            "          175,    4,   19, 4900,   24,  350,  102,   48,  250,   34,   19,  103,\n",
            "           50,  470,   58, 1000,   69,   41, 3367,   40,   22,   19,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [ 434,   20,  405,   28,   17, 3085,  160,   79,   34,    9,   10,  203,\n",
            "           24, 2190,   17,   12,    6,   34,   46,   24, 2323,   17,   16,   10,\n",
            "          614, 3740,   52,  685,  178,    7, 4914,  345,  980,  472,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   5,  179,    9,  202,   18,   51,  605,    4,   20, 1129,  141,    4,\n",
            "          260,  155,    8,  281,    6,  246, 1649,   59,   71,  713,    4,  102,\n",
            "          915,   14,   83,    4,    3,   48,    2,    3, 1084,    2,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   5, 4643,    9,  134,   62,   11,   21,  130,    9,   50, 3390,    4,\n",
            "           25,   44,   25, 1764,   11,   21,  317,   12,  221,  193,   72,   62,\n",
            "          112,   35,    4,   25, 3830,  413,  410,  649,    4,  475,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [ 109,   12,  116,  198,   26,    6,  158,   47,  610,   12,   20,  261,\n",
            "            4, 1631, 1560,   29,  360,  306,   23,   35,  794,    4,    6,   30,\n",
            "         1246,   29,  360,  306,   23,   35, 1587,   26,   26,  474,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   5,    6,   30,   29,   53,   24,   11,   21,    7,  104,   74,    6,\n",
            "           90,   13,   56,  108,  594,  400,   69, 3978,    4,   14,   11,   21,\n",
            "            7,  104,   80, 1008,   85,   82,    6,   30,  914,    4,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   6,   96,   56,   10, 1393,  292,   76,   24,    6, 4880,   38,    6,\n",
            "           28,   13,   75,   67,    9,   50,   69,   15,   35,  638,    6,   50,\n",
            "           36,  150,   46,    6,  208,   28,   16,  846,  228,  759,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1]], device='cuda:0')\n",
            "(tensor([[-0.1174],\n",
            "        [-0.0987],\n",
            "        [-0.1670],\n",
            "        [-0.1766],\n",
            "        [-0.0770],\n",
            "        [-0.0640],\n",
            "        [-0.1404],\n",
            "        [-0.0855]], device='cuda:0', grad_fn=<AddmmBackward>), tensor([[-0.4088, -1.0919],\n",
            "        [-0.4283, -1.0544],\n",
            "        [-0.4544, -1.0074],\n",
            "        [-0.3794, -1.1528],\n",
            "        [-0.4135, -1.0827],\n",
            "        [-0.3634, -1.1885],\n",
            "        [-0.3945, -1.1210],\n",
            "        [-0.3914, -1.1274]], device='cuda:0', grad_fn=<LogSoftmaxBackward>))\n",
            "tensor([[   5, 1008,   38,  297,   15,    7, 1284,   10, 1008,   38,  730,   15,\n",
            "            7,  621,    4,  418,    8,  243,   21,    7,   79,    3,  446, 3446,\n",
            "            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [  53,    6, 1350,  109,  164,  271,   21, 4335,   26,  271, 3139,    6,\n",
            "           50,   39,  119,   10,   37,   65,   43,   99,   17,   12,   22,  214,\n",
            "          540,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   5, 4800,  100, 3320,   25, 4633,   11,  404,    9,  295,  152,  137,\n",
            "            8,   25, 1293,   15, 2935,    4,   24,  157,  382,    4,   47,   18,\n",
            "           24,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   5,    5,    5,  629,   10, 3588,   18,  171,   46, 1419, 1419,  110,\n",
            "         1145,    6,   57,  510,   62,   13,   28, 1414,   32,   13,  127,  570,\n",
            "           28,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [1204,  439,   12,   15,    9,  560, 4585,    5,   92,   69, 3166, 4127,\n",
            "           16,  799,   10, 2687, 1889,   11, 3179,   21,  184,  186,   66,  211,\n",
            "          749,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [  21,   25,   10,  147,   54,   32,   31,  145,   30,  536,    8,   39,\n",
            "         2192,   63,  442, 1096,    4,    3,   48,  196,  414,    2,    3,   60,\n",
            "            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [  50,    6,  129,   13,   45,  768,    6,   35, 2374,   37,  333,   45,\n",
            "         1265,   50,   13,  133,   76,   28,   45,   64,    5,    3,  940,  711,\n",
            "            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   5,    3,   70,    2, 4485,   17,   51,   18,  890,  263,   18,   19,\n",
            "           12,   22,   43,  734,    3,  273,    2,   79, 1275,    3, 4491,    2,\n",
            "           18,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1]], device='cuda:0')\n",
            "(tensor([[-0.1861],\n",
            "        [ 0.0212],\n",
            "        [-0.1234],\n",
            "        [-0.0813],\n",
            "        [-0.1715],\n",
            "        [-0.2300],\n",
            "        [-0.1337],\n",
            "        [-0.0568]], device='cuda:0', grad_fn=<AddmmBackward>), tensor([[-0.7610, -0.6296],\n",
            "        [-0.8247, -0.5769],\n",
            "        [-0.7459, -0.6430],\n",
            "        [-0.7972, -0.5989],\n",
            "        [-0.7932, -0.6022],\n",
            "        [-0.7867, -0.6076],\n",
            "        [-0.7919, -0.6033],\n",
            "        [-0.6545, -0.7333]], device='cuda:0', grad_fn=<LogSoftmaxBackward>))\n",
            "tensor([[  46,  720,   42, 5626,   11,   22,  974,  140,   14,    6,  615,   98,\n",
            "          125,  146,  483,   12,   23,  120,   10,  334,    9,   97, 4259,  708,\n",
            "          448,   14,  823,    9,   96,  224,    8,  708,   17,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   3, 3822,  392,    2,    3,  536, 5502,  848,    2,   90,   31,   15,\n",
            "            3,   94,    2,   14,  434, 2431,    3,   79,    2,   33,  134,  101,\n",
            "          163,  175,  201, 1420,   20,  811,   26,   16,  359,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   5,  223,  234, 1477, 4495,   18,   19,   31,   16,  178,  122, 2839,\n",
            "           27, 1086,  573,    4,   60,   12,    3,   97,    2,    3, 2626,    2,\n",
            "           14, 1281,  257,   61,    7,  565,   42,  559,   18,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   5,   15,   52,  287,   79,   16,    8,   88,    4,   43,  178,  132,\n",
            "           56,   44,    7,  133,  414,    7,  232, 1262,   20,    7,   36, 1009,\n",
            "           35,  157,  125,  238,   20,    7, 5179,   26,   19,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [  21, 1412,   44,   20,   10,  352, 1452,    4,   19,    6,   30,  102,\n",
            "          173,   10,  603,    4,   19,   37,  218,   44,   63,  910,   69,   14,\n",
            "           37,   44,    7, 1544,   27,    7, 1418,    4,   19,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   5,   15,   28,   55,  360, 2082,   39,   15,  211,   10, 2516,    4,\n",
            "          250,  101,   39,   55,  326,    4,    3,  379,    2,    3,  138, 2785,\n",
            "            2,    3, 2258,    2,    3, 1203,  759, 1667,    2,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   5,  210,   28,    4,   19, 3045,   29,  139, 1724,    9,  139, 1647,\n",
            "           14,  111,   11,  238,   11, 1256,   12,  139,  134,   25,    7,  676,\n",
            "            8,  626,    4, 2607,   95, 2078,    3, 1321,    2,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [ 662,   91,    3,   70,    2, 1800,  195,    9,  145,   16,  159,  175,\n",
            "          101,    9,   33,  660,   10,    3,  275,   47,    7,  709,  754,  863,\n",
            "            2,    4,   67,    6,   28,   86,    8,  574,    4,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1]], device='cuda:0')\n",
            "(tensor([[0.2163],\n",
            "        [0.2749],\n",
            "        [0.1200],\n",
            "        [0.1624],\n",
            "        [0.1284],\n",
            "        [0.2497],\n",
            "        [0.2911],\n",
            "        [0.1071]], device='cuda:0', grad_fn=<AddmmBackward>), tensor([[-0.7126, -0.6740],\n",
            "        [-0.7246, -0.6626],\n",
            "        [-0.7517, -0.6379],\n",
            "        [-0.7234, -0.6637],\n",
            "        [-0.7192, -0.6678],\n",
            "        [-0.7747, -0.6178],\n",
            "        [-0.7125, -0.6742],\n",
            "        [-0.7344, -0.6535]], device='cuda:0', grad_fn=<LogSoftmaxBackward>))\n",
            "tensor([[  94,  937, 2280,   27,  613,  205,   15,    6, 4324,  893,   44, 1695,\n",
            "         3298, 2029,  646,   24,    3, 4170, 2399,    2,    3, 1381,    2,    3,\n",
            "          168,    2,    3,  200,    2,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   5,   54,   63,   13,   17,    3,   95,    2,   54,   28,  231,  878,\n",
            "            9,   30,  107,   80,   17,    3, 3149,    2,   27,    6,    3, 3568,\n",
            "            2,    5,    3, 1628,    2,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   5,    5,   13,   28,  609,    4,   16,   10,   22, 2807,  286,   25,\n",
            "          125,   23,  565,   11, 4395,   12, 3723,   60,   10,   22,  679,   20,\n",
            "           77,    4,    3,  173,    2,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   5,   29,    6, 2973,   20,   42,  110,    8, 1187,   12,  283,   20,\n",
            "         3954,    8, 2522, 2096,   33,    7,  643,  185,   24,  380,   15,  111,\n",
            "           13,  448,   28,    9,   29,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [ 569,   83,   16,   10,   22,  460,  332,  188,    5,    4,   34,   11,\n",
            "         4081,  188,    4,  334,   13,   80,  373,    8,  120,   12,  497,   15,\n",
            "           63,    4, 1351,    4,  758,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   5,    7,  112,   51,    9,   71,   55,    6, 1626,   15,    6, 1540,\n",
            "           10,   22,   97,   55,  980,    6,  547,    4,    7,  521,   53,   58,\n",
            "           47, 1091,   23, 2407,    4,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [1395,    9, 1694,  136,   47,  965,   11, 1692,   57,  510,  252,    8,\n",
            "          814,  584,    9,   11, 1030,  439,    8,   57, 2568,    6, 1345,   21,\n",
            "            3, 1076,    2,   14,   17,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   7,  149,   60,   26,    9,  126,  289,    9, 3827,   16,   66,   14,\n",
            "           11,    5,  744,    5,  375,  551,    4,  278,   27,  127,  255,    8,\n",
            "         1041,   71,   33,    4,   19,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1]], device='cuda:0')\n",
            "(tensor([[0.2757],\n",
            "        [0.2035],\n",
            "        [0.1979],\n",
            "        [0.1817],\n",
            "        [0.2059],\n",
            "        [0.1105],\n",
            "        [0.1955],\n",
            "        [0.2988]], device='cuda:0', grad_fn=<AddmmBackward>), tensor([[-0.6313, -0.7590],\n",
            "        [-0.5922, -0.8054],\n",
            "        [-0.5969, -0.7997],\n",
            "        [-0.6136, -0.7796],\n",
            "        [-0.6304, -0.7601],\n",
            "        [-0.6460, -0.7427],\n",
            "        [-0.6080, -0.7863],\n",
            "        [-0.6114, -0.7822]], device='cuda:0', grad_fn=<LogSoftmaxBackward>))\n",
            "tensor([[ 174,   46,    8,   36,    9,  164,   20,   25,  260,   45,   27,   23,\n",
            "          127,    9,  208,    4,   25,  803,  356,    7,    6,  135,    8, 4078,\n",
            "            7,  603, 1936,    7, 1225,    7,   35,    8,  127,  208,    4,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [2228,  140,  141, 1583, 3889,   51,  444,   11,   19, 2329,   28,   10,\n",
            "         1051,   66,   17,  115,   22,  249,   52,  475,  499,    9,    6, 1583,\n",
            "         3596,   11,   19,  980, 2314,    4,   18,   56,  192,  142,    4,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   8,   44,  114,    9,  195,  194,   76,  101,   94,   13,   37,  248,\n",
            "          113,   14,    3,   23,  304,    2,    3,   31,    2,    3,  126,    2,\n",
            "            3,  694,  179,    2,    3, 1479,    2,    3,   23,   31,    2,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   8,   57,   22, 2833,   77,    8,  116,  201,   96,    4,   18,    8,\n",
            "          116,  437,  266, 2375,    7,   82,    4,   18, 2908,    4,   17,  902,\n",
            "           61,   96,    8,   35,  292,  186, 4724,  335,  670,    4,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [ 155, 2077, 1063,   51, 1787,    7,  439,  896, 1840,  322,    7,  106,\n",
            "         1562,  255,   51, 1614,   20,   10, 1255,    9,    6,   20,   34, 1122,\n",
            "            4,    3, 1191,   38,  258, 1606,    2,    3,   31,    2,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [   6,  344,   21,   57,  513, 3329,   39, 1983,  276,    4,  456,   13,\n",
            "           33,   10, 1747,   16,    3,  207,    2,    7,    3,   73,    2,    7,\n",
            "            3, 1210,    2,   12,    3, 3623,    2,    4, 3291,  471,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [  22, 2170,  109,  105,  475,  193,  235,    9,   29,   70,  298,   77,\n",
            "            8,  143,   33,  112,   30,  470,  473,   64,   70,  443,   29,   40,\n",
            "         1784,   29,  282,    3,  483,  989,    2,    3,   31,    2,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1],\n",
            "        [ 214, 3310, 1031,   56, 4115, 4317,   20,   84,   34,   41,  347,   23,\n",
            "         2431, 1920,    3,  196,   19,  997,    2, 2263,    3,  151,    2,    3,\n",
            "         2385,  196,    2,    3,  402,  392,    2,    5, 3618, 2241,    1,    1,\n",
            "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "            1,    1]], device='cuda:0')\n",
            "(tensor([[ 0.0072],\n",
            "        [ 0.0310],\n",
            "        [ 0.0043],\n",
            "        [-0.0407],\n",
            "        [ 0.0419],\n",
            "        [-0.0363],\n",
            "        [-0.0265],\n",
            "        [-0.0309]], device='cuda:0', grad_fn=<AddmmBackward>), tensor([[-0.5518, -0.8578],\n",
            "        [-0.5453, -0.8667],\n",
            "        [-0.5469, -0.8646],\n",
            "        [-0.5425, -0.8706],\n",
            "        [-0.5539, -0.8550],\n",
            "        [-0.5490, -0.8616],\n",
            "        [-0.5508, -0.8592],\n",
            "        [-0.5320, -0.8853]], device='cuda:0', grad_fn=<LogSoftmaxBackward>))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.optim as optim\n",
        "\n",
        "# optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# model = model.to(DEVICE)\n",
        "# criterion = criterion.to(DEVICE)"
      ],
      "metadata": {
        "id": "MpixOSrh-Kv0"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Typical Train Model Function"
      ],
      "metadata": {
        "id": "JA0vjUUeXElw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Typical Training Function\n",
        "\n",
        "from tqdm import tqdm # for beautiful model training updates\n",
        "\n",
        "def train_model(model, device, train_loader, optimizer, epoch):\n",
        "    model.train() # setting the model in training mode\n",
        "    pbar = tqdm(train_loader) # putting the iterator in pbara\n",
        "    correct = 0 # for accuracy numerator\n",
        "    processed =0 # for accuracy denominator\n",
        "    epoch_loss = 0.0\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "\n",
        "        tweets, intensities = batch.tweet.to(device), batch.intensity.to(device)  # plural, we are not interested in domain\n",
        "        #sending data to CPU or GPU as per device\n",
        "\n",
        "        optimizer.zero_grad() # setting gradients to zero to avoid accumulation\n",
        "\n",
        "        y_preds,_ = model(tweets) # forward pass, result captured in y_preds (plural as there are many body in a batch)\n",
        "        # we are not interested in domain prediction\n",
        "        # the predictions are in one hot vector\n",
        "\n",
        "        regression_loss = regression_loss_function(y_preds,intensities.unsqueeze(1)) # Computing loss\n",
        "        # loss = F.mse_loss(y_preds,intensities.unsqueeze(1)) # Computing loss\n",
        "\n",
        "        train_regresion_losses.append(regression_loss.item()) # to capture loss over many epochs\n",
        "\n",
        "        regression_loss.backward() # backpropagation\n",
        "        optimizer.step() # updating the params\n",
        "\n",
        "        # preds = y_preds.argmax(dim=1, keepdim=True)  # get the index olf the max log-probability\n",
        "        # correct += preds.eq(labels.view_as(preds)).sum().item()\n",
        "        epoch_loss += regression_loss.item()\n",
        "\n",
        "        processed += len(tweets)\n",
        "\n",
        "        pbar.set_description(desc= f'Loss={regression_loss.item()} Batch_id={batch_idx} Epoch Average loss={100*epoch_loss/processed:0.4f}')\n",
        "    train_accuracy.append(100*epoch_loss/len(train_loader))"
      ],
      "metadata": {
        "id": "2QtsY9qU-Rg9"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Typical Test Function"
      ],
      "metadata": {
        "id": "DVOvpPcYCZwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model,device, data_loader, mode= 'test'):\n",
        "    model.eval() # setting the model in evaluation mode\n",
        "    loss = 0\n",
        "    correct = 0 # for accuracy numerator\n",
        "    test_regresion_losses =[] # for overall batches (summed over batches)\n",
        "    valid_regresion_losses =[] # for overall batches (summed over batches)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "\n",
        "            tweets, intensities  = batch.tweet.to(device), batch.intensity.to(device) #sending data to CPU or GPU as per device\n",
        "            # we are not interested in domains\n",
        "            \n",
        "            y_preds,_ = model(tweets) # forward pass, result captured in outputs (plural as there are many bodies in a batch)\n",
        "            # the outputs are in batch size x one hot vector \n",
        "            # not interested in domain output\n",
        "\n",
        "            regression_loss = regression_loss_function(y_preds,intensities.unsqueeze(1))\n",
        "\n",
        "            if mode == 'test':\n",
        "              test_regresion_losses.append(regression_loss.item())\n",
        "              # print(f'...in the batch...{regression_loss}')\n",
        "            else:\n",
        "              valid_regresion_losses.append(regression_loss.item())\n",
        "              # print(f'...in the batch...{regression_loss}')\n",
        "\n",
        "        # regression_loss.item() /= len(data_loader.dataset) # average test loss\n",
        "        if mode == 'test':\n",
        "          # total_test_regression_loss = sum(test_regresion_losses)\n",
        "          # test_regresion_losses.append(regression_loss) # to capture loss over many batches\n",
        "          # print('...Average test loss: {:.8f}'.format((total_test_regression_loss)/len(data_loader.dataset)))\n",
        "          print(f'TEST LOSS (Average) : {sum(test_regresion_losses) / len(data_loader)}')\n",
        "        else:\n",
        "          # valid_regresion_losses.append(regression_loss) # to capture loss over many batches\n",
        "          # total_valid_regression_loss = sum(valid_regresion_losses)\n",
        "          # print('...Average validation loss: {:.8f}'.format((total_valid_regression_loss)/len(data_loader.dataset)))\n",
        "          print(f'VALIDATION LOSS (Average) : {sum(valid_regresion_losses) / len(data_loader)}')"
      ],
      "metadata": {
        "id": "DcI1CYK5CYjh"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXECUTION (NON DANN) FOR MULTIPLE MODELS\n",
        "lr = 2e-5\n",
        "# EPOCHS = 2\n",
        "EPOCHS = 100\n",
        "dict_non_dann_model_saved= {}\n",
        "for name, model_arch in dict_model_arch.items():\n",
        "  model = model_arch\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "  domain_loss_function= nn.BCEWithLogitsLoss()\n",
        "  regression_loss_function = nn.L1Loss()\n",
        "  model = model.to(DEVICE)\n",
        "  domain_loss_function = domain_loss_function.to(DEVICE)\n",
        "  regression_loss_function = regression_loss_function.to(DEVICE)\n",
        "\n",
        "  # train_losses = [] # to capture train losses over training epochs\n",
        "  train_accuracy = [] # to capture train accuracy over training epochs\n",
        "  # val_losses = [] # to capture validation loss\n",
        "  # test_losses = [] # to capture test losses \n",
        "  # test_accuracy = [] # to capture test accuracy \n",
        "\n",
        "  # dict_val_loss = {}\n",
        "  # dict_test_loss = {}\n",
        "  train_regresion_losses = [] # to capture train losses over training epochs\n",
        "  train_domain_losses = []\n",
        "  train_accuracy = [] # to capture train accuracy over training epochs\n",
        "  # valid_regresion_losses = [] # to capture validation loss\n",
        "  # test_regresion_losses = [] # to capture test losses \n",
        "  total_test_regression_loss =[]\n",
        "  total_valid_regression_loss =[]\n",
        "  print(f'----------------------training started for {name}-----------------')\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch+1)\n",
        "    # train_model(model, DEVICE, train_iterator, optimizer, epoch) # single model\n",
        "    train_model(model, DEVICE, dict_iterator[name]['train_iterator'], optimizer, epoch)\n",
        "    \n",
        "    # test_model(model, DEVICE, valid_iterator, mode = 'val')# single model\n",
        "    test_model(model, DEVICE, dict_iterator[name]['val_iterator'], mode = 'val')\n",
        "\n",
        "    # test_model(model, DEVICE, test_iterator, mode = 'test')# single model\n",
        "    test_model(model, DEVICE, dict_iterator[name]['test_iterator'], mode = 'test')\n",
        "\n",
        "  # dict_val_loss[name] = val_losses\n",
        "  # dict_test_loss[name] = test_losses\n",
        "\n",
        "  model_name = name + \"_non_dann\"+\".pt\"\n",
        "  torch.save(model.state_dict(), os.path.join(MODEL_DIR, model_name))\n",
        "  dict_non_dann_model_saved[name]= model_name\n",
        "  print(f'----------------------training complete for {name}-----------------')\n",
        "# print(dict_val_loss.items())\n",
        "# print(dict_test_loss.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzwkT_9dQj1I",
        "outputId": "0bf05170-1a94-452d-9443-c2dd2a90d876"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------training started for EI_sadness-----------------\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.20032724738121033 Batch_id=191 Epoch Average loss=3.4211: 100%|██████████| 192/192 [00:01<00:00, 126.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1770876944065094\n",
            "TEST LOSS (Average) : 0.17588902016480765\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.19483885169029236 Batch_id=191 Epoch Average loss=2.3858: 100%|██████████| 192/192 [00:01<00:00, 125.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16846248507499695\n",
            "TEST LOSS (Average) : 0.16751219828923544\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.2689189910888672 Batch_id=191 Epoch Average loss=2.3289: 100%|██████████| 192/192 [00:01<00:00, 127.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16447247564792633\n",
            "TEST LOSS (Average) : 0.16321118672688803\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09471927583217621 Batch_id=191 Epoch Average loss=2.2923: 100%|██████████| 192/192 [00:01<00:00, 125.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16355103254318237\n",
            "TEST LOSS (Average) : 0.16171927253405252\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1420271098613739 Batch_id=191 Epoch Average loss=2.2470: 100%|██████████| 192/192 [00:01<00:00, 126.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15970204770565033\n",
            "TEST LOSS (Average) : 0.15746995309988657\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1827709674835205 Batch_id=191 Epoch Average loss=2.1434: 100%|██████████| 192/192 [00:01<00:00, 125.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15480032563209534\n",
            "TEST LOSS (Average) : 0.15248838067054749\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1809110790491104 Batch_id=191 Epoch Average loss=2.1507: 100%|██████████| 192/192 [00:01<00:00, 124.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.154260516166687\n",
            "TEST LOSS (Average) : 0.1513811449209849\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.38893356919288635 Batch_id=191 Epoch Average loss=2.1159: 100%|██████████| 192/192 [00:01<00:00, 126.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15706293284893036\n",
            "TEST LOSS (Average) : 0.1537850002447764\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.18764247000217438 Batch_id=191 Epoch Average loss=2.0773: 100%|██████████| 192/192 [00:01<00:00, 123.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15756143629550934\n",
            "TEST LOSS (Average) : 0.15362074971199036\n",
            "EPOCH: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.20371629297733307 Batch_id=191 Epoch Average loss=2.0032: 100%|██████████| 192/192 [00:01<00:00, 127.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15156283974647522\n",
            "TEST LOSS (Average) : 0.14738922317822775\n",
            "EPOCH: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12464869022369385 Batch_id=191 Epoch Average loss=1.9959: 100%|██████████| 192/192 [00:01<00:00, 125.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15463300049304962\n",
            "TEST LOSS (Average) : 0.1505985657374064\n",
            "EPOCH: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.18430949747562408 Batch_id=191 Epoch Average loss=1.9702: 100%|██████████| 192/192 [00:01<00:00, 127.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15318962931632996\n",
            "TEST LOSS (Average) : 0.14980323115984598\n",
            "EPOCH: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1999208629131317 Batch_id=191 Epoch Average loss=1.9226: 100%|██████████| 192/192 [00:01<00:00, 126.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15254712104797363\n",
            "TEST LOSS (Average) : 0.14941352605819702\n",
            "EPOCH: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.15536528825759888 Batch_id=191 Epoch Average loss=1.8786: 100%|██████████| 192/192 [00:01<00:00, 126.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16001009941101074\n",
            "TEST LOSS (Average) : 0.15728355944156647\n",
            "EPOCH: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1151968389749527 Batch_id=191 Epoch Average loss=1.8855: 100%|██████████| 192/192 [00:01<00:00, 116.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15249080955982208\n",
            "TEST LOSS (Average) : 0.1491974095503489\n",
            "EPOCH: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12588270008563995 Batch_id=191 Epoch Average loss=1.8891: 100%|██████████| 192/192 [00:01<00:00, 115.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13811570405960083\n",
            "TEST LOSS (Average) : 0.135306383172671\n",
            "EPOCH: 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.19827692210674286 Batch_id=191 Epoch Average loss=1.8173: 100%|██████████| 192/192 [00:01<00:00, 106.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14989864826202393\n",
            "TEST LOSS (Average) : 0.14637623727321625\n",
            "EPOCH: 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12907901406288147 Batch_id=191 Epoch Average loss=1.7826: 100%|██████████| 192/192 [00:01<00:00, 106.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14955025911331177\n",
            "TEST LOSS (Average) : 0.1463067332903544\n",
            "EPOCH: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14768314361572266 Batch_id=191 Epoch Average loss=1.7704: 100%|██████████| 192/192 [00:01<00:00, 126.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14736615121364594\n",
            "TEST LOSS (Average) : 0.14341293772061667\n",
            "EPOCH: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07166504859924316 Batch_id=191 Epoch Average loss=1.7312: 100%|██████████| 192/192 [00:01<00:00, 128.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14138604700565338\n",
            "TEST LOSS (Average) : 0.13741188247998556\n",
            "EPOCH: 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1599474847316742 Batch_id=191 Epoch Average loss=1.7451: 100%|██████████| 192/192 [00:01<00:00, 129.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1542002558708191\n",
            "TEST LOSS (Average) : 0.14973297715187073\n",
            "EPOCH: 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12772540748119354 Batch_id=191 Epoch Average loss=1.6955: 100%|██████████| 192/192 [00:01<00:00, 126.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15630939602851868\n",
            "TEST LOSS (Average) : 0.151913583278656\n",
            "EPOCH: 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14812666177749634 Batch_id=191 Epoch Average loss=1.6371: 100%|██████████| 192/192 [00:01<00:00, 116.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16070008277893066\n",
            "TEST LOSS (Average) : 0.15606453518072763\n",
            "EPOCH: 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10591054707765579 Batch_id=191 Epoch Average loss=1.6309: 100%|██████████| 192/192 [00:01<00:00, 105.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1566583663225174\n",
            "TEST LOSS (Average) : 0.15197389821211496\n",
            "EPOCH: 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12773680686950684 Batch_id=191 Epoch Average loss=1.6555: 100%|██████████| 192/192 [00:01<00:00, 103.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1511557400226593\n",
            "TEST LOSS (Average) : 0.14660919706026712\n",
            "EPOCH: 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1135113537311554 Batch_id=191 Epoch Average loss=1.6201: 100%|██████████| 192/192 [00:01<00:00, 99.60it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1574804186820984\n",
            "TEST LOSS (Average) : 0.15305256843566895\n",
            "EPOCH: 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1286589503288269 Batch_id=191 Epoch Average loss=1.5605: 100%|██████████| 192/192 [00:01<00:00, 124.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1454409807920456\n",
            "TEST LOSS (Average) : 0.14070579906304678\n",
            "EPOCH: 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07342694699764252 Batch_id=191 Epoch Average loss=1.5536: 100%|██████████| 192/192 [00:01<00:00, 127.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16554851830005646\n",
            "TEST LOSS (Average) : 0.16062803069750467\n",
            "EPOCH: 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10867875814437866 Batch_id=191 Epoch Average loss=1.5403: 100%|██████████| 192/192 [00:01<00:00, 124.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14819486439228058\n",
            "TEST LOSS (Average) : 0.14335433642069498\n",
            "EPOCH: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1899493932723999 Batch_id=191 Epoch Average loss=1.5187: 100%|██████████| 192/192 [00:01<00:00, 124.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14876653254032135\n",
            "TEST LOSS (Average) : 0.14335651199022928\n",
            "EPOCH: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.31306594610214233 Batch_id=191 Epoch Average loss=1.5269: 100%|██████████| 192/192 [00:01<00:00, 126.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.156467467546463\n",
            "TEST LOSS (Average) : 0.15151811639467874\n",
            "EPOCH: 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11387712508440018 Batch_id=191 Epoch Average loss=1.4740: 100%|██████████| 192/192 [00:01<00:00, 123.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15121802687644958\n",
            "TEST LOSS (Average) : 0.1459290087223053\n",
            "EPOCH: 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11925259232521057 Batch_id=191 Epoch Average loss=1.5075: 100%|██████████| 192/192 [00:01<00:00, 125.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16789427399635315\n",
            "TEST LOSS (Average) : 0.16286147634188333\n",
            "EPOCH: 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12871989607810974 Batch_id=191 Epoch Average loss=1.4511: 100%|██████████| 192/192 [00:01<00:00, 125.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16606971621513367\n",
            "TEST LOSS (Average) : 0.16143993536631265\n",
            "EPOCH: 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1118800938129425 Batch_id=191 Epoch Average loss=1.4613: 100%|██████████| 192/192 [00:01<00:00, 125.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16160672903060913\n",
            "TEST LOSS (Average) : 0.15682758887608847\n",
            "EPOCH: 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13832838833332062 Batch_id=191 Epoch Average loss=1.4496: 100%|██████████| 192/192 [00:01<00:00, 124.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16103968024253845\n",
            "TEST LOSS (Average) : 0.15592047572135925\n",
            "EPOCH: 37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10682384669780731 Batch_id=191 Epoch Average loss=1.4621: 100%|██████████| 192/192 [00:01<00:00, 125.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16672837734222412\n",
            "TEST LOSS (Average) : 0.16172623137633005\n",
            "EPOCH: 38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13648852705955505 Batch_id=191 Epoch Average loss=1.3960: 100%|██████████| 192/192 [00:01<00:00, 122.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15240760147571564\n",
            "TEST LOSS (Average) : 0.14739943047364554\n",
            "EPOCH: 39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11033150553703308 Batch_id=191 Epoch Average loss=1.3649: 100%|██████████| 192/192 [00:01<00:00, 122.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1544935256242752\n",
            "TEST LOSS (Average) : 0.1495845913887024\n",
            "EPOCH: 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12803329527378082 Batch_id=191 Epoch Average loss=1.3721: 100%|██████████| 192/192 [00:01<00:00, 100.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1568014770746231\n",
            "TEST LOSS (Average) : 0.1517230967680613\n",
            "EPOCH: 41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09985165297985077 Batch_id=191 Epoch Average loss=1.3565: 100%|██████████| 192/192 [00:02<00:00, 88.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1574012190103531\n",
            "TEST LOSS (Average) : 0.15258843203385672\n",
            "EPOCH: 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14707684516906738 Batch_id=191 Epoch Average loss=1.3557: 100%|██████████| 192/192 [00:01<00:00, 118.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1585526019334793\n",
            "TEST LOSS (Average) : 0.15368983149528503\n",
            "EPOCH: 43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07986168563365936 Batch_id=191 Epoch Average loss=1.3451: 100%|██████████| 192/192 [00:01<00:00, 110.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16255594789981842\n",
            "TEST LOSS (Average) : 0.15732542177041373\n",
            "EPOCH: 44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06905990093946457 Batch_id=191 Epoch Average loss=1.3895: 100%|██████████| 192/192 [00:01<00:00, 112.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16896557807922363\n",
            "TEST LOSS (Average) : 0.16408885022004446\n",
            "EPOCH: 45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11667992174625397 Batch_id=191 Epoch Average loss=1.3160: 100%|██████████| 192/192 [00:01<00:00, 106.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16128621995449066\n",
            "TEST LOSS (Average) : 0.1565520167350769\n",
            "EPOCH: 46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0873858779668808 Batch_id=191 Epoch Average loss=1.3089: 100%|██████████| 192/192 [00:01<00:00, 101.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15567313134670258\n",
            "TEST LOSS (Average) : 0.15052404503027597\n",
            "EPOCH: 47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10417583584785461 Batch_id=191 Epoch Average loss=1.2943: 100%|██████████| 192/192 [00:01<00:00, 119.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16202473640441895\n",
            "TEST LOSS (Average) : 0.15741309026877084\n",
            "EPOCH: 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.17943841218948364 Batch_id=191 Epoch Average loss=1.2806: 100%|██████████| 192/192 [00:01<00:00, 120.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17440193891525269\n",
            "TEST LOSS (Average) : 0.16995748380819956\n",
            "EPOCH: 49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13802121579647064 Batch_id=191 Epoch Average loss=1.2972: 100%|██████████| 192/192 [00:01<00:00, 123.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15479430556297302\n",
            "TEST LOSS (Average) : 0.15018190443515778\n",
            "EPOCH: 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1628715991973877 Batch_id=191 Epoch Average loss=1.2759: 100%|██████████| 192/192 [00:01<00:00, 122.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1749180257320404\n",
            "TEST LOSS (Average) : 0.17056125899155936\n",
            "EPOCH: 51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11330358684062958 Batch_id=191 Epoch Average loss=1.2352: 100%|██████████| 192/192 [00:01<00:00, 121.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17203289270401\n",
            "TEST LOSS (Average) : 0.16765985389550528\n",
            "EPOCH: 52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11870013177394867 Batch_id=191 Epoch Average loss=1.2333: 100%|██████████| 192/192 [00:01<00:00, 121.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15599516034126282\n",
            "TEST LOSS (Average) : 0.15094199776649475\n",
            "EPOCH: 53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14613166451454163 Batch_id=191 Epoch Average loss=1.2252: 100%|██████████| 192/192 [00:01<00:00, 120.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16731514036655426\n",
            "TEST LOSS (Average) : 0.1627730280160904\n",
            "EPOCH: 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11831865459680557 Batch_id=191 Epoch Average loss=1.2219: 100%|██████████| 192/192 [00:01<00:00, 121.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1868041753768921\n",
            "TEST LOSS (Average) : 0.18258850276470184\n",
            "EPOCH: 55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06787142157554626 Batch_id=191 Epoch Average loss=1.1954: 100%|██████████| 192/192 [00:01<00:00, 119.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17788203060626984\n",
            "TEST LOSS (Average) : 0.1741891403992971\n",
            "EPOCH: 56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09424257278442383 Batch_id=191 Epoch Average loss=1.2191: 100%|██████████| 192/192 [00:01<00:00, 121.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17138655483722687\n",
            "TEST LOSS (Average) : 0.1668028930823008\n",
            "EPOCH: 57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09566348791122437 Batch_id=191 Epoch Average loss=1.2083: 100%|██████████| 192/192 [00:01<00:00, 120.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17774416506290436\n",
            "TEST LOSS (Average) : 0.1723307321468989\n",
            "EPOCH: 58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04890842363238335 Batch_id=191 Epoch Average loss=1.1712: 100%|██████████| 192/192 [00:01<00:00, 120.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17981193959712982\n",
            "TEST LOSS (Average) : 0.17505055169264475\n",
            "EPOCH: 59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0685257613658905 Batch_id=191 Epoch Average loss=1.1852: 100%|██████████| 192/192 [00:01<00:00, 121.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1834859699010849\n",
            "TEST LOSS (Average) : 0.17866205672423044\n",
            "EPOCH: 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.15819916129112244 Batch_id=191 Epoch Average loss=1.1554: 100%|██████████| 192/192 [00:01<00:00, 118.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1684805154800415\n",
            "TEST LOSS (Average) : 0.16308998564879099\n",
            "EPOCH: 61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10528086125850677 Batch_id=191 Epoch Average loss=1.1699: 100%|██████████| 192/192 [00:01<00:00, 120.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1747235655784607\n",
            "TEST LOSS (Average) : 0.16956407825152078\n",
            "EPOCH: 62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12259022891521454 Batch_id=191 Epoch Average loss=1.1771: 100%|██████████| 192/192 [00:01<00:00, 114.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16014017164707184\n",
            "TEST LOSS (Average) : 0.15483810504277548\n",
            "EPOCH: 63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10547393560409546 Batch_id=191 Epoch Average loss=1.1526: 100%|██████████| 192/192 [00:01<00:00, 116.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16566701233386993\n",
            "TEST LOSS (Average) : 0.16055449843406677\n",
            "EPOCH: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0842626541852951 Batch_id=191 Epoch Average loss=1.1314: 100%|██████████| 192/192 [00:01<00:00, 118.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1736217439174652\n",
            "TEST LOSS (Average) : 0.1685013622045517\n",
            "EPOCH: 65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10711198300123215 Batch_id=191 Epoch Average loss=1.1377: 100%|██████████| 192/192 [00:01<00:00, 119.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1626070737838745\n",
            "TEST LOSS (Average) : 0.15742413202921549\n",
            "EPOCH: 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12024436891078949 Batch_id=191 Epoch Average loss=1.1693: 100%|██████████| 192/192 [00:01<00:00, 120.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16793280839920044\n",
            "TEST LOSS (Average) : 0.16324041287104288\n",
            "EPOCH: 67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10597093403339386 Batch_id=191 Epoch Average loss=1.1150: 100%|██████████| 192/192 [00:01<00:00, 120.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16174878180027008\n",
            "TEST LOSS (Average) : 0.15706166128317514\n",
            "EPOCH: 68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1278148740530014 Batch_id=191 Epoch Average loss=1.1391: 100%|██████████| 192/192 [00:01<00:00, 119.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1625589281320572\n",
            "TEST LOSS (Average) : 0.1577401359875997\n",
            "EPOCH: 69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.057155855000019073 Batch_id=191 Epoch Average loss=1.1133: 100%|██████████| 192/192 [00:01<00:00, 117.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1724189817905426\n",
            "TEST LOSS (Average) : 0.16753984491030374\n",
            "EPOCH: 70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11837361007928848 Batch_id=191 Epoch Average loss=1.1305: 100%|██████████| 192/192 [00:01<00:00, 120.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17738191783428192\n",
            "TEST LOSS (Average) : 0.17269299924373627\n",
            "EPOCH: 71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12205636501312256 Batch_id=191 Epoch Average loss=1.1151: 100%|██████████| 192/192 [00:01<00:00, 118.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1776479333639145\n",
            "TEST LOSS (Average) : 0.17310025791327158\n",
            "EPOCH: 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1181400865316391 Batch_id=191 Epoch Average loss=1.0945: 100%|██████████| 192/192 [00:01<00:00, 107.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17438070476055145\n",
            "TEST LOSS (Average) : 0.17077675461769104\n",
            "EPOCH: 73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10478329658508301 Batch_id=191 Epoch Average loss=1.0839: 100%|██████████| 192/192 [00:01<00:00, 108.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16804341971874237\n",
            "TEST LOSS (Average) : 0.16357144713401794\n",
            "EPOCH: 74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14526072144508362 Batch_id=191 Epoch Average loss=1.0663: 100%|██████████| 192/192 [00:01<00:00, 109.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1826973557472229\n",
            "TEST LOSS (Average) : 0.17881529529889426\n",
            "EPOCH: 75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10154401510953903 Batch_id=191 Epoch Average loss=1.0599: 100%|██████████| 192/192 [00:01<00:00, 102.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1571856141090393\n",
            "TEST LOSS (Average) : 0.1525178700685501\n",
            "EPOCH: 76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06501486897468567 Batch_id=191 Epoch Average loss=1.0592: 100%|██████████| 192/192 [00:01<00:00, 113.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.18110863864421844\n",
            "TEST LOSS (Average) : 0.17724759876728058\n",
            "EPOCH: 77\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09691998362541199 Batch_id=191 Epoch Average loss=1.0998: 100%|██████████| 192/192 [00:01<00:00, 116.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1798304170370102\n",
            "TEST LOSS (Average) : 0.17605410516262054\n",
            "EPOCH: 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06369341164827347 Batch_id=191 Epoch Average loss=1.0511: 100%|██████████| 192/192 [00:01<00:00, 116.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16499297320842743\n",
            "TEST LOSS (Average) : 0.16117723286151886\n",
            "EPOCH: 79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.054137133061885834 Batch_id=191 Epoch Average loss=1.0765: 100%|██████████| 192/192 [00:01<00:00, 119.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16269496083259583\n",
            "TEST LOSS (Average) : 0.15874085823694864\n",
            "EPOCH: 80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07198866456747055 Batch_id=191 Epoch Average loss=1.0374: 100%|██████████| 192/192 [00:01<00:00, 113.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16800083220005035\n",
            "TEST LOSS (Average) : 0.1637275516986847\n",
            "EPOCH: 81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06600761413574219 Batch_id=191 Epoch Average loss=1.0477: 100%|██████████| 192/192 [00:01<00:00, 116.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17542685568332672\n",
            "TEST LOSS (Average) : 0.17104961971441904\n",
            "EPOCH: 82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08926288783550262 Batch_id=191 Epoch Average loss=1.0077: 100%|██████████| 192/192 [00:01<00:00, 117.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17655132710933685\n",
            "TEST LOSS (Average) : 0.1719862421353658\n",
            "EPOCH: 83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13998137414455414 Batch_id=191 Epoch Average loss=1.0399: 100%|██████████| 192/192 [00:01<00:00, 118.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17692570388317108\n",
            "TEST LOSS (Average) : 0.17313983043034872\n",
            "EPOCH: 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04976985231041908 Batch_id=191 Epoch Average loss=1.0317: 100%|██████████| 192/192 [00:01<00:00, 117.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.163567915558815\n",
            "TEST LOSS (Average) : 0.1591785947481791\n",
            "EPOCH: 85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07342884689569473 Batch_id=191 Epoch Average loss=1.0140: 100%|██████████| 192/192 [00:01<00:00, 117.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1695927083492279\n",
            "TEST LOSS (Average) : 0.1657439668973287\n",
            "EPOCH: 86\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09104900807142258 Batch_id=191 Epoch Average loss=1.0119: 100%|██████████| 192/192 [00:01<00:00, 118.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17427825927734375\n",
            "TEST LOSS (Average) : 0.1697876751422882\n",
            "EPOCH: 87\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05836368352174759 Batch_id=191 Epoch Average loss=1.0075: 100%|██████████| 192/192 [00:01<00:00, 116.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17977739870548248\n",
            "TEST LOSS (Average) : 0.1756916046142578\n",
            "EPOCH: 88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09355323761701584 Batch_id=191 Epoch Average loss=1.0102: 100%|██████████| 192/192 [00:01<00:00, 121.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16606619954109192\n",
            "TEST LOSS (Average) : 0.1619881292184194\n",
            "EPOCH: 89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11060262471437454 Batch_id=191 Epoch Average loss=0.9732: 100%|██████████| 192/192 [00:01<00:00, 116.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17568804323673248\n",
            "TEST LOSS (Average) : 0.17163117229938507\n",
            "EPOCH: 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0560673251748085 Batch_id=191 Epoch Average loss=1.0064: 100%|██████████| 192/192 [00:01<00:00, 116.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17060881853103638\n",
            "TEST LOSS (Average) : 0.1664736568927765\n",
            "EPOCH: 91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08293984830379486 Batch_id=191 Epoch Average loss=1.0179: 100%|██████████| 192/192 [00:01<00:00, 117.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.18067419528961182\n",
            "TEST LOSS (Average) : 0.17704891165097555\n",
            "EPOCH: 92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.056649260222911835 Batch_id=191 Epoch Average loss=0.9758: 100%|██████████| 192/192 [00:01<00:00, 114.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17980672419071198\n",
            "TEST LOSS (Average) : 0.17614384988943735\n",
            "EPOCH: 93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0449063666164875 Batch_id=191 Epoch Average loss=0.9743: 100%|██████████| 192/192 [00:01<00:00, 115.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16736194491386414\n",
            "TEST LOSS (Average) : 0.1640555957953135\n",
            "EPOCH: 94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08159483969211578 Batch_id=191 Epoch Average loss=0.9685: 100%|██████████| 192/192 [00:01<00:00, 115.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.179632306098938\n",
            "TEST LOSS (Average) : 0.17558948695659637\n",
            "EPOCH: 95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08915423601865768 Batch_id=191 Epoch Average loss=0.9569: 100%|██████████| 192/192 [00:01<00:00, 114.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1811637580394745\n",
            "TEST LOSS (Average) : 0.17743894954522452\n",
            "EPOCH: 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05223268270492554 Batch_id=191 Epoch Average loss=0.9502: 100%|██████████| 192/192 [00:01<00:00, 114.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17326635122299194\n",
            "TEST LOSS (Average) : 0.17027624448140463\n",
            "EPOCH: 97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07736818492412567 Batch_id=191 Epoch Average loss=0.9287: 100%|██████████| 192/192 [00:01<00:00, 113.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17357850074768066\n",
            "TEST LOSS (Average) : 0.1700384964545568\n",
            "EPOCH: 98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06417345255613327 Batch_id=191 Epoch Average loss=0.9532: 100%|██████████| 192/192 [00:01<00:00, 116.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17622628808021545\n",
            "TEST LOSS (Average) : 0.17295404771963754\n",
            "EPOCH: 99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0643494576215744 Batch_id=191 Epoch Average loss=0.9771: 100%|██████████| 192/192 [00:01<00:00, 111.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17886923253536224\n",
            "TEST LOSS (Average) : 0.1755747844775518\n",
            "EPOCH: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04365438595414162 Batch_id=191 Epoch Average loss=0.9119: 100%|██████████| 192/192 [00:01<00:00, 116.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17144116759300232\n",
            "TEST LOSS (Average) : 0.16749908030033112\n",
            "----------------------training complete for EI_sadness-----------------\n",
            "----------------------training started for EI_anger-----------------\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13975220918655396 Batch_id=212 Epoch Average loss=3.2017: 100%|██████████| 213/213 [00:01<00:00, 109.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1743137389421463\n",
            "TEST LOSS (Average) : 0.16544394195079803\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14674806594848633 Batch_id=212 Epoch Average loss=1.8783: 100%|██████████| 213/213 [00:01<00:00, 106.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16944511234760284\n",
            "TEST LOSS (Average) : 0.15895315011342367\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06915632635354996 Batch_id=212 Epoch Average loss=1.8215: 100%|██████████| 213/213 [00:02<00:00, 98.72it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17609070241451263\n",
            "TEST LOSS (Average) : 0.1663437783718109\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.15435659885406494 Batch_id=212 Epoch Average loss=1.8009: 100%|██████████| 213/213 [00:01<00:00, 116.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17867380380630493\n",
            "TEST LOSS (Average) : 0.16942023237546286\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.18658900260925293 Batch_id=212 Epoch Average loss=1.7854: 100%|██████████| 213/213 [00:01<00:00, 116.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16967108845710754\n",
            "TEST LOSS (Average) : 0.15888100365797678\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07392192631959915 Batch_id=212 Epoch Average loss=1.7278: 100%|██████████| 213/213 [00:01<00:00, 116.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16744528710842133\n",
            "TEST LOSS (Average) : 0.1564194063345591\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11301708221435547 Batch_id=212 Epoch Average loss=1.7399: 100%|██████████| 213/213 [00:01<00:00, 115.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16790711879730225\n",
            "TEST LOSS (Average) : 0.15720992783705393\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1911080777645111 Batch_id=212 Epoch Average loss=1.7251: 100%|██████████| 213/213 [00:01<00:00, 117.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16974572837352753\n",
            "TEST LOSS (Average) : 0.15955874820550284\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10502663254737854 Batch_id=212 Epoch Average loss=1.7156: 100%|██████████| 213/213 [00:01<00:00, 115.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16743390262126923\n",
            "TEST LOSS (Average) : 0.1570525070031484\n",
            "EPOCH: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10142751038074493 Batch_id=212 Epoch Average loss=1.6708: 100%|██████████| 213/213 [00:01<00:00, 115.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1668083369731903\n",
            "TEST LOSS (Average) : 0.1563286085923513\n",
            "EPOCH: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0872095450758934 Batch_id=212 Epoch Average loss=1.6643: 100%|██████████| 213/213 [00:01<00:00, 116.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1717110276222229\n",
            "TEST LOSS (Average) : 0.16284423569838205\n",
            "EPOCH: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09808038920164108 Batch_id=212 Epoch Average loss=1.6241: 100%|██████████| 213/213 [00:01<00:00, 114.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1687821000814438\n",
            "TEST LOSS (Average) : 0.15919693807760874\n",
            "EPOCH: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1362878531217575 Batch_id=212 Epoch Average loss=1.6159: 100%|██████████| 213/213 [00:01<00:00, 113.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17011325061321259\n",
            "TEST LOSS (Average) : 0.1620989888906479\n",
            "EPOCH: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12324802577495575 Batch_id=212 Epoch Average loss=1.5895: 100%|██████████| 213/213 [00:01<00:00, 113.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16829530894756317\n",
            "TEST LOSS (Average) : 0.15951881309350333\n",
            "EPOCH: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.20251333713531494 Batch_id=212 Epoch Average loss=1.5935: 100%|██████████| 213/213 [00:01<00:00, 113.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1719086617231369\n",
            "TEST LOSS (Average) : 0.1647319495677948\n",
            "EPOCH: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12189720571041107 Batch_id=212 Epoch Average loss=1.5474: 100%|██████████| 213/213 [00:01<00:00, 115.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16683243215084076\n",
            "TEST LOSS (Average) : 0.15834558010101318\n",
            "EPOCH: 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.16116440296173096 Batch_id=212 Epoch Average loss=1.5216: 100%|██████████| 213/213 [00:01<00:00, 114.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16744455695152283\n",
            "TEST LOSS (Average) : 0.15883351365725198\n",
            "EPOCH: 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13705119490623474 Batch_id=212 Epoch Average loss=1.5245: 100%|██████████| 213/213 [00:01<00:00, 115.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17303553223609924\n",
            "TEST LOSS (Average) : 0.16653914749622345\n",
            "EPOCH: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11472553759813309 Batch_id=212 Epoch Average loss=1.5077: 100%|██████████| 213/213 [00:01<00:00, 114.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16956520080566406\n",
            "TEST LOSS (Average) : 0.1620603750149409\n",
            "EPOCH: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.2192152738571167 Batch_id=212 Epoch Average loss=1.4815: 100%|██████████| 213/213 [00:01<00:00, 114.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16728776693344116\n",
            "TEST LOSS (Average) : 0.16026630997657776\n",
            "EPOCH: 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09184585511684418 Batch_id=212 Epoch Average loss=1.4581: 100%|██████████| 213/213 [00:01<00:00, 115.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16466589272022247\n",
            "TEST LOSS (Average) : 0.15748129288355509\n",
            "EPOCH: 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.18041209876537323 Batch_id=212 Epoch Average loss=1.4687: 100%|██████████| 213/213 [00:01<00:00, 113.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16827338933944702\n",
            "TEST LOSS (Average) : 0.16079721848169962\n",
            "EPOCH: 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09608572721481323 Batch_id=212 Epoch Average loss=1.4480: 100%|██████████| 213/213 [00:01<00:00, 111.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16817009449005127\n",
            "TEST LOSS (Average) : 0.16042834023634592\n",
            "EPOCH: 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14444783329963684 Batch_id=212 Epoch Average loss=1.4685: 100%|██████████| 213/213 [00:01<00:00, 107.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16628533601760864\n",
            "TEST LOSS (Average) : 0.1594250649213791\n",
            "EPOCH: 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.18934446573257446 Batch_id=212 Epoch Average loss=1.4272: 100%|██████████| 213/213 [00:01<00:00, 109.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16707666218280792\n",
            "TEST LOSS (Average) : 0.16018185516198477\n",
            "EPOCH: 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1568031907081604 Batch_id=212 Epoch Average loss=1.4111: 100%|██████████| 213/213 [00:01<00:00, 110.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1686258167028427\n",
            "TEST LOSS (Average) : 0.16259720921516418\n",
            "EPOCH: 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09778228402137756 Batch_id=212 Epoch Average loss=1.3945: 100%|██████████| 213/213 [00:02<00:00, 102.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17039965093135834\n",
            "TEST LOSS (Average) : 0.16467489302158356\n",
            "EPOCH: 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13908283412456512 Batch_id=212 Epoch Average loss=1.3563: 100%|██████████| 213/213 [00:02<00:00, 99.33it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17701846361160278\n",
            "TEST LOSS (Average) : 0.17279099424680075\n",
            "EPOCH: 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08036145567893982 Batch_id=212 Epoch Average loss=1.3583: 100%|██████████| 213/213 [00:01<00:00, 109.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1554509550333023\n",
            "TEST LOSS (Average) : 0.14776488641897836\n",
            "EPOCH: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1104697585105896 Batch_id=212 Epoch Average loss=1.3445: 100%|██████████| 213/213 [00:01<00:00, 107.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17125581204891205\n",
            "TEST LOSS (Average) : 0.16592846314112344\n",
            "EPOCH: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10574690252542496 Batch_id=212 Epoch Average loss=1.3221: 100%|██████████| 213/213 [00:02<00:00, 103.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16369815170764923\n",
            "TEST LOSS (Average) : 0.15690617263317108\n",
            "EPOCH: 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09902213513851166 Batch_id=212 Epoch Average loss=1.3164: 100%|██████████| 213/213 [00:01<00:00, 110.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16148173809051514\n",
            "TEST LOSS (Average) : 0.1552139272292455\n",
            "EPOCH: 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06904670596122742 Batch_id=212 Epoch Average loss=1.3172: 100%|██████████| 213/213 [00:02<00:00, 95.76it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16332365572452545\n",
            "TEST LOSS (Average) : 0.15761244793732962\n",
            "EPOCH: 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12651562690734863 Batch_id=212 Epoch Average loss=1.2961: 100%|██████████| 213/213 [00:01<00:00, 112.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17250505089759827\n",
            "TEST LOSS (Average) : 0.1671459823846817\n",
            "EPOCH: 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12426034361124039 Batch_id=212 Epoch Average loss=1.2572: 100%|██████████| 213/213 [00:01<00:00, 112.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1645667850971222\n",
            "TEST LOSS (Average) : 0.15901302297910055\n",
            "EPOCH: 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09534292668104172 Batch_id=212 Epoch Average loss=1.2804: 100%|██████████| 213/213 [00:01<00:00, 112.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1542506217956543\n",
            "TEST LOSS (Average) : 0.14679142832756042\n",
            "EPOCH: 37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10157658159732819 Batch_id=212 Epoch Average loss=1.2473: 100%|██████████| 213/213 [00:01<00:00, 112.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1640024483203888\n",
            "TEST LOSS (Average) : 0.15799269576867422\n",
            "EPOCH: 38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1188017800450325 Batch_id=212 Epoch Average loss=1.2508: 100%|██████████| 213/213 [00:01<00:00, 111.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15852245688438416\n",
            "TEST LOSS (Average) : 0.15208695828914642\n",
            "EPOCH: 39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09781622141599655 Batch_id=212 Epoch Average loss=1.2327: 100%|██████████| 213/213 [00:01<00:00, 112.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17629992961883545\n",
            "TEST LOSS (Average) : 0.1724823663632075\n",
            "EPOCH: 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.17638860642910004 Batch_id=212 Epoch Average loss=1.2489: 100%|██████████| 213/213 [00:01<00:00, 112.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16465744376182556\n",
            "TEST LOSS (Average) : 0.1588024546702703\n",
            "EPOCH: 41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09230069071054459 Batch_id=212 Epoch Average loss=1.1999: 100%|██████████| 213/213 [00:01<00:00, 112.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.166305810213089\n",
            "TEST LOSS (Average) : 0.15994429091612497\n",
            "EPOCH: 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09382660686969757 Batch_id=212 Epoch Average loss=1.1946: 100%|██████████| 213/213 [00:01<00:00, 111.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16422536969184875\n",
            "TEST LOSS (Average) : 0.15779280165831247\n",
            "EPOCH: 43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11669514328241348 Batch_id=212 Epoch Average loss=1.1941: 100%|██████████| 213/213 [00:01<00:00, 112.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16552621126174927\n",
            "TEST LOSS (Average) : 0.15929264823595682\n",
            "EPOCH: 44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12702882289886475 Batch_id=212 Epoch Average loss=1.1939: 100%|██████████| 213/213 [00:01<00:00, 108.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16584226489067078\n",
            "TEST LOSS (Average) : 0.1594582200050354\n",
            "EPOCH: 45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07945626229047775 Batch_id=212 Epoch Average loss=1.1887: 100%|██████████| 213/213 [00:01<00:00, 112.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1606970876455307\n",
            "TEST LOSS (Average) : 0.15348395705223083\n",
            "EPOCH: 46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09539676457643509 Batch_id=212 Epoch Average loss=1.1789: 100%|██████████| 213/213 [00:01<00:00, 114.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16572695970535278\n",
            "TEST LOSS (Average) : 0.15898485978444418\n",
            "EPOCH: 47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1330745816230774 Batch_id=212 Epoch Average loss=1.1291: 100%|██████████| 213/213 [00:01<00:00, 113.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16328094899654388\n",
            "TEST LOSS (Average) : 0.15618659059206644\n",
            "EPOCH: 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0838531106710434 Batch_id=212 Epoch Average loss=1.1349: 100%|██████████| 213/213 [00:01<00:00, 112.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.163607656955719\n",
            "TEST LOSS (Average) : 0.15647131204605103\n",
            "EPOCH: 49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07751716673374176 Batch_id=212 Epoch Average loss=1.1396: 100%|██████████| 213/213 [00:01<00:00, 114.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16708825528621674\n",
            "TEST LOSS (Average) : 0.16075040896733603\n",
            "EPOCH: 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1121654361486435 Batch_id=212 Epoch Average loss=1.1260: 100%|██████████| 213/213 [00:01<00:00, 113.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17640390992164612\n",
            "TEST LOSS (Average) : 0.17100679874420166\n",
            "EPOCH: 51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06033112853765488 Batch_id=212 Epoch Average loss=1.1235: 100%|██████████| 213/213 [00:01<00:00, 111.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16002433001995087\n",
            "TEST LOSS (Average) : 0.15223089357217154\n",
            "EPOCH: 52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08314037322998047 Batch_id=212 Epoch Average loss=1.0813: 100%|██████████| 213/213 [00:02<00:00, 101.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.18018895387649536\n",
            "TEST LOSS (Average) : 0.17529037594795227\n",
            "EPOCH: 53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06642080843448639 Batch_id=212 Epoch Average loss=1.0980: 100%|██████████| 213/213 [00:02<00:00, 101.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16862046718597412\n",
            "TEST LOSS (Average) : 0.16233513255914053\n",
            "EPOCH: 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09093880653381348 Batch_id=212 Epoch Average loss=1.1036: 100%|██████████| 213/213 [00:01<00:00, 113.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16946804523468018\n",
            "TEST LOSS (Average) : 0.1629994809627533\n",
            "EPOCH: 55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0967397689819336 Batch_id=212 Epoch Average loss=1.0969: 100%|██████████| 213/213 [00:01<00:00, 111.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16707772016525269\n",
            "TEST LOSS (Average) : 0.16042961180210114\n",
            "EPOCH: 56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10107279568910599 Batch_id=212 Epoch Average loss=1.0918: 100%|██████████| 213/213 [00:01<00:00, 112.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1697118580341339\n",
            "TEST LOSS (Average) : 0.16390582422415415\n",
            "EPOCH: 57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07757812738418579 Batch_id=212 Epoch Average loss=1.0942: 100%|██████████| 213/213 [00:01<00:00, 113.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16355907917022705\n",
            "TEST LOSS (Average) : 0.15722304582595825\n",
            "EPOCH: 58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06452564895153046 Batch_id=212 Epoch Average loss=1.0152: 100%|██████████| 213/213 [00:01<00:00, 111.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17269258201122284\n",
            "TEST LOSS (Average) : 0.16784338653087616\n",
            "EPOCH: 59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09749309718608856 Batch_id=212 Epoch Average loss=1.0243: 100%|██████████| 213/213 [00:01<00:00, 112.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1651054471731186\n",
            "TEST LOSS (Average) : 0.15881560742855072\n",
            "EPOCH: 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06309860199689865 Batch_id=212 Epoch Average loss=1.0629: 100%|██████████| 213/213 [00:01<00:00, 107.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17249466478824615\n",
            "TEST LOSS (Average) : 0.16742868224779764\n",
            "EPOCH: 61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12548956274986267 Batch_id=212 Epoch Average loss=1.0297: 100%|██████████| 213/213 [00:01<00:00, 110.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16716159880161285\n",
            "TEST LOSS (Average) : 0.16136557360490164\n",
            "EPOCH: 62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09565620124340057 Batch_id=212 Epoch Average loss=1.0115: 100%|██████████| 213/213 [00:01<00:00, 111.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16757845878601074\n",
            "TEST LOSS (Average) : 0.16143870850404105\n",
            "EPOCH: 63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05177345126867294 Batch_id=212 Epoch Average loss=1.0062: 100%|██████████| 213/213 [00:01<00:00, 111.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1746751070022583\n",
            "TEST LOSS (Average) : 0.16942145923773447\n",
            "EPOCH: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05914723500609398 Batch_id=212 Epoch Average loss=1.0369: 100%|██████████| 213/213 [00:01<00:00, 110.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1769893616437912\n",
            "TEST LOSS (Average) : 0.1716047873099645\n",
            "EPOCH: 65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04860222712159157 Batch_id=212 Epoch Average loss=1.0171: 100%|██████████| 213/213 [00:01<00:00, 112.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16730238497257233\n",
            "TEST LOSS (Average) : 0.16091732680797577\n",
            "EPOCH: 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.039624668657779694 Batch_id=212 Epoch Average loss=1.0068: 100%|██████████| 213/213 [00:01<00:00, 111.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1650245487689972\n",
            "TEST LOSS (Average) : 0.15872461597124735\n",
            "EPOCH: 67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09390322864055634 Batch_id=212 Epoch Average loss=0.9987: 100%|██████████| 213/213 [00:01<00:00, 111.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17215971648693085\n",
            "TEST LOSS (Average) : 0.1664349933465322\n",
            "EPOCH: 68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07032164931297302 Batch_id=212 Epoch Average loss=0.9720: 100%|██████████| 213/213 [00:01<00:00, 109.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15797188878059387\n",
            "TEST LOSS (Average) : 0.15125146508216858\n",
            "EPOCH: 69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0729052871465683 Batch_id=212 Epoch Average loss=0.9774: 100%|██████████| 213/213 [00:01<00:00, 112.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1565583199262619\n",
            "TEST LOSS (Average) : 0.14959668616453806\n",
            "EPOCH: 70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12321017682552338 Batch_id=212 Epoch Average loss=0.9864: 100%|██████████| 213/213 [00:01<00:00, 110.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15986597537994385\n",
            "TEST LOSS (Average) : 0.1536647230386734\n",
            "EPOCH: 71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07763659954071045 Batch_id=212 Epoch Average loss=0.9727: 100%|██████████| 213/213 [00:01<00:00, 109.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16842229664325714\n",
            "TEST LOSS (Average) : 0.16220385332902273\n",
            "EPOCH: 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0743819996714592 Batch_id=212 Epoch Average loss=0.9512: 100%|██████████| 213/213 [00:01<00:00, 111.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17427696287631989\n",
            "TEST LOSS (Average) : 0.1686398188273112\n",
            "EPOCH: 73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07739505171775818 Batch_id=212 Epoch Average loss=0.9721: 100%|██████████| 213/213 [00:01<00:00, 114.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16264653205871582\n",
            "TEST LOSS (Average) : 0.1560695320367813\n",
            "EPOCH: 74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0810217410326004 Batch_id=212 Epoch Average loss=0.9209: 100%|██████████| 213/213 [00:01<00:00, 110.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16283518075942993\n",
            "TEST LOSS (Average) : 0.15636538962523142\n",
            "EPOCH: 75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05862162262201309 Batch_id=212 Epoch Average loss=0.9262: 100%|██████████| 213/213 [00:01<00:00, 116.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16855968534946442\n",
            "TEST LOSS (Average) : 0.16270288825035095\n",
            "EPOCH: 76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07006113976240158 Batch_id=212 Epoch Average loss=0.9293: 100%|██████████| 213/213 [00:02<00:00, 103.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16073645651340485\n",
            "TEST LOSS (Average) : 0.15401932100454965\n",
            "EPOCH: 77\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06850533932447433 Batch_id=212 Epoch Average loss=0.9461: 100%|██████████| 213/213 [00:02<00:00, 101.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1660594940185547\n",
            "TEST LOSS (Average) : 0.1597782572110494\n",
            "EPOCH: 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0782482773065567 Batch_id=212 Epoch Average loss=0.9251: 100%|██████████| 213/213 [00:02<00:00, 105.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16624502837657928\n",
            "TEST LOSS (Average) : 0.16021092732747397\n",
            "EPOCH: 79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10395987331867218 Batch_id=212 Epoch Average loss=0.9177: 100%|██████████| 213/213 [00:01<00:00, 109.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16332383453845978\n",
            "TEST LOSS (Average) : 0.15715429683526358\n",
            "EPOCH: 80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0939922109246254 Batch_id=212 Epoch Average loss=0.8827: 100%|██████████| 213/213 [00:01<00:00, 107.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1721295565366745\n",
            "TEST LOSS (Average) : 0.16645953555901846\n",
            "EPOCH: 81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05782724544405937 Batch_id=212 Epoch Average loss=0.9203: 100%|██████████| 213/213 [00:01<00:00, 111.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16157452762126923\n",
            "TEST LOSS (Average) : 0.154973566532135\n",
            "EPOCH: 82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0749536007642746 Batch_id=212 Epoch Average loss=0.9039: 100%|██████████| 213/213 [00:01<00:00, 111.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15784962475299835\n",
            "TEST LOSS (Average) : 0.15136829515298209\n",
            "EPOCH: 83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05128483846783638 Batch_id=212 Epoch Average loss=0.8909: 100%|██████████| 213/213 [00:01<00:00, 111.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1629423201084137\n",
            "TEST LOSS (Average) : 0.15659957627455393\n",
            "EPOCH: 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05947611853480339 Batch_id=212 Epoch Average loss=0.8964: 100%|██████████| 213/213 [00:01<00:00, 109.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15783075988292694\n",
            "TEST LOSS (Average) : 0.15102233986059824\n",
            "EPOCH: 85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06750377267599106 Batch_id=212 Epoch Average loss=0.8847: 100%|██████████| 213/213 [00:01<00:00, 109.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16692626476287842\n",
            "TEST LOSS (Average) : 0.1611454983552297\n",
            "EPOCH: 86\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.050468139350414276 Batch_id=212 Epoch Average loss=0.8688: 100%|██████████| 213/213 [00:01<00:00, 110.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16355089843273163\n",
            "TEST LOSS (Average) : 0.15729524691899618\n",
            "EPOCH: 87\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09604687988758087 Batch_id=212 Epoch Average loss=0.8425: 100%|██████████| 213/213 [00:01<00:00, 111.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16389982402324677\n",
            "TEST LOSS (Average) : 0.1577972024679184\n",
            "EPOCH: 88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09080980718135834 Batch_id=212 Epoch Average loss=0.8783: 100%|██████████| 213/213 [00:01<00:00, 111.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16864587366580963\n",
            "TEST LOSS (Average) : 0.16284175713857016\n",
            "EPOCH: 89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07323624193668365 Batch_id=212 Epoch Average loss=0.8639: 100%|██████████| 213/213 [00:01<00:00, 110.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.156910240650177\n",
            "TEST LOSS (Average) : 0.15016827980677286\n",
            "EPOCH: 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.049803122878074646 Batch_id=212 Epoch Average loss=0.8581: 100%|██████████| 213/213 [00:01<00:00, 110.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1644870638847351\n",
            "TEST LOSS (Average) : 0.1584491084019343\n",
            "EPOCH: 91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0554141066968441 Batch_id=212 Epoch Average loss=0.8564: 100%|██████████| 213/213 [00:01<00:00, 110.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16240310668945312\n",
            "TEST LOSS (Average) : 0.15630180140336355\n",
            "EPOCH: 92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06176455691456795 Batch_id=212 Epoch Average loss=0.8778: 100%|██████████| 213/213 [00:01<00:00, 111.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16469764709472656\n",
            "TEST LOSS (Average) : 0.15890295306841531\n",
            "EPOCH: 93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.040783751755952835 Batch_id=212 Epoch Average loss=0.8355: 100%|██████████| 213/213 [00:01<00:00, 110.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16295209527015686\n",
            "TEST LOSS (Average) : 0.15659708281358084\n",
            "EPOCH: 94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06004384905099869 Batch_id=212 Epoch Average loss=0.8411: 100%|██████████| 213/213 [00:01<00:00, 109.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1685490757226944\n",
            "TEST LOSS (Average) : 0.16263570884863535\n",
            "EPOCH: 95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06889286637306213 Batch_id=212 Epoch Average loss=0.8318: 100%|██████████| 213/213 [00:01<00:00, 109.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16546177864074707\n",
            "TEST LOSS (Average) : 0.1592424362897873\n",
            "EPOCH: 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.042455434799194336 Batch_id=212 Epoch Average loss=0.8347: 100%|██████████| 213/213 [00:01<00:00, 113.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17735090851783752\n",
            "TEST LOSS (Average) : 0.1724014182885488\n",
            "EPOCH: 97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07025381922721863 Batch_id=212 Epoch Average loss=0.8233: 100%|██████████| 213/213 [00:02<00:00, 102.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15780261158943176\n",
            "TEST LOSS (Average) : 0.1507808119058609\n",
            "EPOCH: 98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04644240066409111 Batch_id=212 Epoch Average loss=0.8229: 100%|██████████| 213/213 [00:01<00:00, 108.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16371144354343414\n",
            "TEST LOSS (Average) : 0.15758761763572693\n",
            "EPOCH: 99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12548595666885376 Batch_id=212 Epoch Average loss=0.8359: 100%|██████████| 213/213 [00:02<00:00, 102.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17126834392547607\n",
            "TEST LOSS (Average) : 0.1659498264392217\n",
            "EPOCH: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08962908387184143 Batch_id=212 Epoch Average loss=0.8267: 100%|██████████| 213/213 [00:02<00:00, 99.57it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16523241996765137\n",
            "TEST LOSS (Average) : 0.15902936458587646\n",
            "----------------------training complete for EI_anger-----------------\n",
            "----------------------training started for EI_fear-----------------\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.15096530318260193 Batch_id=281 Epoch Average loss=2.2525: 100%|██████████| 282/282 [00:02<00:00, 106.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15164706110954285\n",
            "TEST LOSS (Average) : 0.15009655555089316\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.2209426760673523 Batch_id=281 Epoch Average loss=2.1130: 100%|██████████| 282/282 [00:02<00:00, 108.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1468752920627594\n",
            "TEST LOSS (Average) : 0.14605269332726797\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12452797591686249 Batch_id=281 Epoch Average loss=2.0585: 100%|██████████| 282/282 [00:02<00:00, 110.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14614588022232056\n",
            "TEST LOSS (Average) : 0.14604982733726501\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.17900194227695465 Batch_id=281 Epoch Average loss=2.0340: 100%|██████████| 282/282 [00:02<00:00, 110.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1446685642004013\n",
            "TEST LOSS (Average) : 0.1450516680876414\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.19403047859668732 Batch_id=281 Epoch Average loss=2.0250: 100%|██████████| 282/282 [00:02<00:00, 109.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14907507598400116\n",
            "TEST LOSS (Average) : 0.14918234447638193\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.17945419251918793 Batch_id=281 Epoch Average loss=1.9899: 100%|██████████| 282/282 [00:02<00:00, 110.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1471279263496399\n",
            "TEST LOSS (Average) : 0.14746742943922678\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.16085606813430786 Batch_id=281 Epoch Average loss=1.9762: 100%|██████████| 282/282 [00:02<00:00, 109.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14255361258983612\n",
            "TEST LOSS (Average) : 0.14291357000668845\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14943072199821472 Batch_id=281 Epoch Average loss=1.9475: 100%|██████████| 282/282 [00:02<00:00, 111.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14282681047916412\n",
            "TEST LOSS (Average) : 0.14329258600870767\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.24929265677928925 Batch_id=281 Epoch Average loss=1.9218: 100%|██████████| 282/282 [00:02<00:00, 109.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1472102701663971\n",
            "TEST LOSS (Average) : 0.14775389432907104\n",
            "EPOCH: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.27154749631881714 Batch_id=281 Epoch Average loss=1.8879: 100%|██████████| 282/282 [00:02<00:00, 111.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14557281136512756\n",
            "TEST LOSS (Average) : 0.14625712235768637\n",
            "EPOCH: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09752236306667328 Batch_id=281 Epoch Average loss=1.8875: 100%|██████████| 282/282 [00:02<00:00, 109.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13776202499866486\n",
            "TEST LOSS (Average) : 0.13878259559472403\n",
            "EPOCH: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12118653953075409 Batch_id=281 Epoch Average loss=1.8484: 100%|██████████| 282/282 [00:02<00:00, 101.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1370505839586258\n",
            "TEST LOSS (Average) : 0.1380380392074585\n",
            "EPOCH: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.18739262223243713 Batch_id=281 Epoch Average loss=1.8379: 100%|██████████| 282/282 [00:02<00:00, 103.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13590271770954132\n",
            "TEST LOSS (Average) : 0.13707408805688223\n",
            "EPOCH: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12353144586086273 Batch_id=281 Epoch Average loss=1.7996: 100%|██████████| 282/282 [00:02<00:00, 105.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13952024281024933\n",
            "TEST LOSS (Average) : 0.13954660296440125\n",
            "EPOCH: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14312680065631866 Batch_id=281 Epoch Average loss=1.7967: 100%|██████████| 282/282 [00:02<00:00, 112.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1386842429637909\n",
            "TEST LOSS (Average) : 0.13873805105686188\n",
            "EPOCH: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.16306158900260925 Batch_id=281 Epoch Average loss=1.7561: 100%|██████████| 282/282 [00:02<00:00, 109.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13630881905555725\n",
            "TEST LOSS (Average) : 0.136342724164327\n",
            "EPOCH: 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08158743381500244 Batch_id=281 Epoch Average loss=1.7278: 100%|██████████| 282/282 [00:02<00:00, 101.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13701412081718445\n",
            "TEST LOSS (Average) : 0.1373173048098882\n",
            "EPOCH: 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12408943474292755 Batch_id=281 Epoch Average loss=1.6936: 100%|██████████| 282/282 [00:02<00:00, 101.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13650977611541748\n",
            "TEST LOSS (Average) : 0.13671151300271353\n",
            "EPOCH: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09750382602214813 Batch_id=281 Epoch Average loss=1.6844: 100%|██████████| 282/282 [00:02<00:00, 106.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13824623823165894\n",
            "TEST LOSS (Average) : 0.13798328240712485\n",
            "EPOCH: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.17147719860076904 Batch_id=281 Epoch Average loss=1.6355: 100%|██████████| 282/282 [00:02<00:00, 100.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1340717375278473\n",
            "TEST LOSS (Average) : 0.13421095410982767\n",
            "EPOCH: 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08945736289024353 Batch_id=281 Epoch Average loss=1.6111: 100%|██████████| 282/282 [00:02<00:00, 94.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1351010501384735\n",
            "TEST LOSS (Average) : 0.13441198567549387\n",
            "EPOCH: 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.103614941239357 Batch_id=281 Epoch Average loss=1.5981: 100%|██████████| 282/282 [00:02<00:00, 100.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13140222430229187\n",
            "TEST LOSS (Average) : 0.13096619149049124\n",
            "EPOCH: 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1181039810180664 Batch_id=281 Epoch Average loss=1.5705: 100%|██████████| 282/282 [00:02<00:00, 105.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.131147563457489\n",
            "TEST LOSS (Average) : 0.1306382641196251\n",
            "EPOCH: 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1747640073299408 Batch_id=281 Epoch Average loss=1.5545: 100%|██████████| 282/282 [00:02<00:00, 100.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14288844168186188\n",
            "TEST LOSS (Average) : 0.1406444509824117\n",
            "EPOCH: 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06408283114433289 Batch_id=281 Epoch Average loss=1.5306: 100%|██████████| 282/282 [00:02<00:00, 107.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14127516746520996\n",
            "TEST LOSS (Average) : 0.13874951004981995\n",
            "EPOCH: 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13335716724395752 Batch_id=281 Epoch Average loss=1.5256: 100%|██████████| 282/282 [00:02<00:00, 106.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12665659189224243\n",
            "TEST LOSS (Average) : 0.12558813641468683\n",
            "EPOCH: 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07840822637081146 Batch_id=281 Epoch Average loss=1.4813: 100%|██████████| 282/282 [00:02<00:00, 104.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12912292778491974\n",
            "TEST LOSS (Average) : 0.12679297477006912\n",
            "EPOCH: 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09402714669704437 Batch_id=281 Epoch Average loss=1.4855: 100%|██████████| 282/282 [00:02<00:00, 118.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13111697137355804\n",
            "TEST LOSS (Average) : 0.12843862920999527\n",
            "EPOCH: 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.16538846492767334 Batch_id=281 Epoch Average loss=1.4417: 100%|██████████| 282/282 [00:02<00:00, 110.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1330975741147995\n",
            "TEST LOSS (Average) : 0.12980567663908005\n",
            "EPOCH: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.17665138840675354 Batch_id=281 Epoch Average loss=1.4513: 100%|██████████| 282/282 [00:02<00:00, 109.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1374416947364807\n",
            "TEST LOSS (Average) : 0.1338749130566915\n",
            "EPOCH: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.16939963400363922 Batch_id=281 Epoch Average loss=1.4352: 100%|██████████| 282/282 [00:02<00:00, 107.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13289615511894226\n",
            "TEST LOSS (Average) : 0.12931512792905173\n",
            "EPOCH: 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09421277046203613 Batch_id=281 Epoch Average loss=1.4477: 100%|██████████| 282/282 [00:02<00:00, 106.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13851666450500488\n",
            "TEST LOSS (Average) : 0.13431577384471893\n",
            "EPOCH: 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13148146867752075 Batch_id=281 Epoch Average loss=1.4193: 100%|██████████| 282/282 [00:02<00:00, 108.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13568368554115295\n",
            "TEST LOSS (Average) : 0.1316161851088206\n",
            "EPOCH: 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12622056901454926 Batch_id=281 Epoch Average loss=1.3768: 100%|██████████| 282/282 [00:02<00:00, 102.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13375334441661835\n",
            "TEST LOSS (Average) : 0.1293653647104899\n",
            "EPOCH: 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11512766033411026 Batch_id=281 Epoch Average loss=1.3766: 100%|██████████| 282/282 [00:02<00:00, 105.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1333659589290619\n",
            "TEST LOSS (Average) : 0.12910478313763937\n",
            "EPOCH: 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08806063234806061 Batch_id=281 Epoch Average loss=1.3525: 100%|██████████| 282/282 [00:02<00:00, 107.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13247346878051758\n",
            "TEST LOSS (Average) : 0.1280333548784256\n",
            "EPOCH: 37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11667869240045547 Batch_id=281 Epoch Average loss=1.3448: 100%|██████████| 282/282 [00:02<00:00, 110.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13032284379005432\n",
            "TEST LOSS (Average) : 0.12539848933617273\n",
            "EPOCH: 38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07277732342481613 Batch_id=281 Epoch Average loss=1.3341: 100%|██████████| 282/282 [00:02<00:00, 107.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13636893033981323\n",
            "TEST LOSS (Average) : 0.13116522878408432\n",
            "EPOCH: 39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11275626718997955 Batch_id=281 Epoch Average loss=1.3107: 100%|██████████| 282/282 [00:02<00:00, 108.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1314656287431717\n",
            "TEST LOSS (Average) : 0.12619804094235101\n",
            "EPOCH: 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07355111837387085 Batch_id=281 Epoch Average loss=1.3063: 100%|██████████| 282/282 [00:02<00:00, 110.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13130515813827515\n",
            "TEST LOSS (Average) : 0.1258233239253362\n",
            "EPOCH: 41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10269056260585785 Batch_id=281 Epoch Average loss=1.2903: 100%|██████████| 282/282 [00:02<00:00, 109.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13958267867565155\n",
            "TEST LOSS (Average) : 0.1344410479068756\n",
            "EPOCH: 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.15353591740131378 Batch_id=281 Epoch Average loss=1.2617: 100%|██████████| 282/282 [00:02<00:00, 109.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12989161908626556\n",
            "TEST LOSS (Average) : 0.12491812060276668\n",
            "EPOCH: 43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09468931704759598 Batch_id=281 Epoch Average loss=1.2718: 100%|██████████| 282/282 [00:02<00:00, 107.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1271813064813614\n",
            "TEST LOSS (Average) : 0.12220825254917145\n",
            "EPOCH: 44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06682229042053223 Batch_id=281 Epoch Average loss=1.2526: 100%|██████████| 282/282 [00:02<00:00, 108.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13153576850891113\n",
            "TEST LOSS (Average) : 0.1260198230544726\n",
            "EPOCH: 45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10057409107685089 Batch_id=281 Epoch Average loss=1.2305: 100%|██████████| 282/282 [00:02<00:00, 108.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12788106501102448\n",
            "TEST LOSS (Average) : 0.12223811447620392\n",
            "EPOCH: 46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1253294050693512 Batch_id=281 Epoch Average loss=1.2129: 100%|██████████| 282/282 [00:02<00:00, 110.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1308574676513672\n",
            "TEST LOSS (Average) : 0.12521779785553613\n",
            "EPOCH: 47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06542804092168808 Batch_id=281 Epoch Average loss=1.2307: 100%|██████████| 282/282 [00:02<00:00, 108.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13309821486473083\n",
            "TEST LOSS (Average) : 0.12763393918673197\n",
            "EPOCH: 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0864635556936264 Batch_id=281 Epoch Average loss=1.2227: 100%|██████████| 282/282 [00:02<00:00, 108.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1300140768289566\n",
            "TEST LOSS (Average) : 0.12427883843580882\n",
            "EPOCH: 49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.061993591487407684 Batch_id=281 Epoch Average loss=1.1742: 100%|██████████| 282/282 [00:02<00:00, 107.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1336280256509781\n",
            "TEST LOSS (Average) : 0.12784181535243988\n",
            "EPOCH: 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11065386235713959 Batch_id=281 Epoch Average loss=1.1835: 100%|██████████| 282/282 [00:02<00:00, 110.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1304624229669571\n",
            "TEST LOSS (Average) : 0.1246927057703336\n",
            "EPOCH: 51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07576289772987366 Batch_id=281 Epoch Average loss=1.1736: 100%|██████████| 282/282 [00:02<00:00, 107.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12800569832324982\n",
            "TEST LOSS (Average) : 0.12251163770755132\n",
            "EPOCH: 52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10679056495428085 Batch_id=281 Epoch Average loss=1.1643: 100%|██████████| 282/282 [00:02<00:00, 98.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13584917783737183\n",
            "TEST LOSS (Average) : 0.1303876986106237\n",
            "EPOCH: 53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.139767587184906 Batch_id=281 Epoch Average loss=1.1518: 100%|██████████| 282/282 [00:02<00:00, 108.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13485033810138702\n",
            "TEST LOSS (Average) : 0.12930543720722198\n",
            "EPOCH: 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0870806947350502 Batch_id=281 Epoch Average loss=1.1360: 100%|██████████| 282/282 [00:02<00:00, 109.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13259682059288025\n",
            "TEST LOSS (Average) : 0.1268509030342102\n",
            "EPOCH: 55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.03644338995218277 Batch_id=281 Epoch Average loss=1.1516: 100%|██████████| 282/282 [00:02<00:00, 109.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13689245283603668\n",
            "TEST LOSS (Average) : 0.1307391549150149\n",
            "EPOCH: 56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09250938892364502 Batch_id=281 Epoch Average loss=1.1276: 100%|██████████| 282/282 [00:02<00:00, 107.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12944744527339935\n",
            "TEST LOSS (Average) : 0.12377780675888062\n",
            "EPOCH: 57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06387294828891754 Batch_id=281 Epoch Average loss=1.1217: 100%|██████████| 282/282 [00:02<00:00, 109.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1330420821905136\n",
            "TEST LOSS (Average) : 0.12742636104424795\n",
            "EPOCH: 58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07865758240222931 Batch_id=281 Epoch Average loss=1.1236: 100%|██████████| 282/282 [00:02<00:00, 106.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12842810153961182\n",
            "TEST LOSS (Average) : 0.12265874693791072\n",
            "EPOCH: 59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09185601770877838 Batch_id=281 Epoch Average loss=1.0999: 100%|██████████| 282/282 [00:02<00:00, 107.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13347452878952026\n",
            "TEST LOSS (Average) : 0.12746786326169968\n",
            "EPOCH: 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08225978165864944 Batch_id=281 Epoch Average loss=1.0806: 100%|██████████| 282/282 [00:02<00:00, 108.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13473093509674072\n",
            "TEST LOSS (Average) : 0.1284062092502912\n",
            "EPOCH: 61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09231777489185333 Batch_id=281 Epoch Average loss=1.0836: 100%|██████████| 282/282 [00:02<00:00, 107.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1316264122724533\n",
            "TEST LOSS (Average) : 0.12491047630707423\n",
            "EPOCH: 62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1467350870370865 Batch_id=281 Epoch Average loss=1.0843: 100%|██████████| 282/282 [00:02<00:00, 107.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12748835980892181\n",
            "TEST LOSS (Average) : 0.12093041837215424\n",
            "EPOCH: 63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0622607059776783 Batch_id=281 Epoch Average loss=1.0732: 100%|██████████| 282/282 [00:02<00:00, 107.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13296295702457428\n",
            "TEST LOSS (Average) : 0.12654169897238413\n",
            "EPOCH: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.050670720636844635 Batch_id=281 Epoch Average loss=1.0549: 100%|██████████| 282/282 [00:02<00:00, 108.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13133591413497925\n",
            "TEST LOSS (Average) : 0.12496682753165562\n",
            "EPOCH: 65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0805375725030899 Batch_id=281 Epoch Average loss=1.0683: 100%|██████████| 282/282 [00:02<00:00, 108.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12722118198871613\n",
            "TEST LOSS (Average) : 0.12041173626979192\n",
            "EPOCH: 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07728517055511475 Batch_id=281 Epoch Average loss=1.0403: 100%|██████████| 282/282 [00:02<00:00, 107.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13150689005851746\n",
            "TEST LOSS (Average) : 0.12436168392499287\n",
            "EPOCH: 67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08190742135047913 Batch_id=281 Epoch Average loss=1.0321: 100%|██████████| 282/282 [00:02<00:00, 110.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13699273765087128\n",
            "TEST LOSS (Average) : 0.13050522406895956\n",
            "EPOCH: 68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13388527929782867 Batch_id=281 Epoch Average loss=1.0521: 100%|██████████| 282/282 [00:02<00:00, 111.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12960600852966309\n",
            "TEST LOSS (Average) : 0.12322390824556351\n",
            "EPOCH: 69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04278142750263214 Batch_id=281 Epoch Average loss=1.0303: 100%|██████████| 282/282 [00:02<00:00, 107.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13575929403305054\n",
            "TEST LOSS (Average) : 0.12952149411042532\n",
            "EPOCH: 70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10703423619270325 Batch_id=281 Epoch Average loss=1.0210: 100%|██████████| 282/282 [00:02<00:00, 97.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13703171908855438\n",
            "TEST LOSS (Average) : 0.13112043092648187\n",
            "EPOCH: 71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07080551236867905 Batch_id=281 Epoch Average loss=1.0204: 100%|██████████| 282/282 [00:02<00:00, 106.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12826187908649445\n",
            "TEST LOSS (Average) : 0.1217220922311147\n",
            "EPOCH: 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11304336786270142 Batch_id=281 Epoch Average loss=0.9972: 100%|██████████| 282/282 [00:02<00:00, 116.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1340118795633316\n",
            "TEST LOSS (Average) : 0.12798578788836798\n",
            "EPOCH: 73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.054566845297813416 Batch_id=281 Epoch Average loss=0.9905: 100%|██████████| 282/282 [00:02<00:00, 108.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13553155958652496\n",
            "TEST LOSS (Average) : 0.13011240462462106\n",
            "EPOCH: 74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.057075925171375275 Batch_id=281 Epoch Average loss=0.9841: 100%|██████████| 282/282 [00:02<00:00, 108.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13146325945854187\n",
            "TEST LOSS (Average) : 0.1257691557208697\n",
            "EPOCH: 75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10170404613018036 Batch_id=281 Epoch Average loss=0.9844: 100%|██████████| 282/282 [00:02<00:00, 112.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1337670236825943\n",
            "TEST LOSS (Average) : 0.12743194897969565\n",
            "EPOCH: 76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.03769165277481079 Batch_id=281 Epoch Average loss=0.9823: 100%|██████████| 282/282 [00:02<00:00, 108.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1295878291130066\n",
            "TEST LOSS (Average) : 0.12324856718381245\n",
            "EPOCH: 77\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0790395513176918 Batch_id=281 Epoch Average loss=0.9768: 100%|██████████| 282/282 [00:02<00:00, 108.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1377050280570984\n",
            "TEST LOSS (Average) : 0.1321550856033961\n",
            "EPOCH: 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07130053639411926 Batch_id=281 Epoch Average loss=0.9632: 100%|██████████| 282/282 [00:02<00:00, 109.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12891234457492828\n",
            "TEST LOSS (Average) : 0.12301552047332127\n",
            "EPOCH: 79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10368663817644119 Batch_id=281 Epoch Average loss=0.9777: 100%|██████████| 282/282 [00:02<00:00, 109.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1338534653186798\n",
            "TEST LOSS (Average) : 0.127785953382651\n",
            "EPOCH: 80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07312741130590439 Batch_id=281 Epoch Average loss=0.9616: 100%|██████████| 282/282 [00:02<00:00, 108.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12689852714538574\n",
            "TEST LOSS (Average) : 0.12102057288090388\n",
            "EPOCH: 81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.03934410214424133 Batch_id=281 Epoch Average loss=0.9582: 100%|██████████| 282/282 [00:02<00:00, 108.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13175414502620697\n",
            "TEST LOSS (Average) : 0.12593602140744528\n",
            "EPOCH: 82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04577434062957764 Batch_id=281 Epoch Average loss=0.9199: 100%|██████████| 282/282 [00:02<00:00, 109.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12753285467624664\n",
            "TEST LOSS (Average) : 0.12168077379465103\n",
            "EPOCH: 83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07470998167991638 Batch_id=281 Epoch Average loss=0.9411: 100%|██████████| 282/282 [00:02<00:00, 107.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13052208721637726\n",
            "TEST LOSS (Average) : 0.12477525820334752\n",
            "EPOCH: 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11476367712020874 Batch_id=281 Epoch Average loss=0.9301: 100%|██████████| 282/282 [00:02<00:00, 101.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12863115966320038\n",
            "TEST LOSS (Average) : 0.12258862952391307\n",
            "EPOCH: 85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10035419464111328 Batch_id=281 Epoch Average loss=0.9329: 100%|██████████| 282/282 [00:02<00:00, 108.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12754149734973907\n",
            "TEST LOSS (Average) : 0.1211295872926712\n",
            "EPOCH: 86\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06949672102928162 Batch_id=281 Epoch Average loss=0.9231: 100%|██████████| 282/282 [00:02<00:00, 111.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13493523001670837\n",
            "TEST LOSS (Average) : 0.12946142256259918\n",
            "EPOCH: 87\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05204927176237106 Batch_id=281 Epoch Average loss=0.9108: 100%|██████████| 282/282 [00:02<00:00, 108.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13358664512634277\n",
            "TEST LOSS (Average) : 0.12823453297217688\n",
            "EPOCH: 88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06300395727157593 Batch_id=281 Epoch Average loss=0.9015: 100%|██████████| 282/282 [00:02<00:00, 114.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13224177062511444\n",
            "TEST LOSS (Average) : 0.12661472211281458\n",
            "EPOCH: 89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07769860327243805 Batch_id=281 Epoch Average loss=0.8982: 100%|██████████| 282/282 [00:02<00:00, 106.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12850433588027954\n",
            "TEST LOSS (Average) : 0.12252678722143173\n",
            "EPOCH: 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06698495149612427 Batch_id=281 Epoch Average loss=0.9117: 100%|██████████| 282/282 [00:02<00:00, 104.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13276652991771698\n",
            "TEST LOSS (Average) : 0.12700781722863516\n",
            "EPOCH: 91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06033296138048172 Batch_id=281 Epoch Average loss=0.9013: 100%|██████████| 282/282 [00:02<00:00, 97.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13375362753868103\n",
            "TEST LOSS (Average) : 0.12749847769737244\n",
            "EPOCH: 92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06517000496387482 Batch_id=281 Epoch Average loss=0.8813: 100%|██████████| 282/282 [00:02<00:00, 107.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1274912804365158\n",
            "TEST LOSS (Average) : 0.1207171157002449\n",
            "EPOCH: 93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07731080800294876 Batch_id=281 Epoch Average loss=0.8861: 100%|██████████| 282/282 [00:02<00:00, 107.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12934842705726624\n",
            "TEST LOSS (Average) : 0.12310276925563812\n",
            "EPOCH: 94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05854202061891556 Batch_id=281 Epoch Average loss=0.8793: 100%|██████████| 282/282 [00:02<00:00, 107.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12971076369285583\n",
            "TEST LOSS (Average) : 0.1236444686849912\n",
            "EPOCH: 95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07738684117794037 Batch_id=281 Epoch Average loss=0.8856: 100%|██████████| 282/282 [00:02<00:00, 111.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13570000231266022\n",
            "TEST LOSS (Average) : 0.12955233454704285\n",
            "EPOCH: 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07140703499317169 Batch_id=281 Epoch Average loss=0.8952: 100%|██████████| 282/282 [00:02<00:00, 108.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1313057690858841\n",
            "TEST LOSS (Average) : 0.1252330740292867\n",
            "EPOCH: 97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07051709294319153 Batch_id=281 Epoch Average loss=0.8661: 100%|██████████| 282/282 [00:02<00:00, 113.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.12862367928028107\n",
            "TEST LOSS (Average) : 0.12251063684622447\n",
            "EPOCH: 98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.036374710500240326 Batch_id=281 Epoch Average loss=0.8687: 100%|██████████| 282/282 [00:02<00:00, 107.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13449294865131378\n",
            "TEST LOSS (Average) : 0.12808352212111154\n",
            "EPOCH: 99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04510613530874252 Batch_id=281 Epoch Average loss=0.8766: 100%|██████████| 282/282 [00:02<00:00, 107.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13085871934890747\n",
            "TEST LOSS (Average) : 0.12351958205302556\n",
            "EPOCH: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05972272530198097 Batch_id=281 Epoch Average loss=0.8666: 100%|██████████| 282/282 [00:02<00:00, 109.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13286131620407104\n",
            "TEST LOSS (Average) : 0.1261419728398323\n",
            "----------------------training complete for EI_fear-----------------\n",
            "----------------------training started for V-----------------\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.19635765254497528 Batch_id=147 Epoch Average loss=2.5843: 100%|██████████| 148/148 [00:01<00:00, 104.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.19850040972232819\n",
            "TEST LOSS (Average) : 0.17701710760593414\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.15733389556407928 Batch_id=147 Epoch Average loss=2.2517: 100%|██████████| 148/148 [00:01<00:00, 108.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.19525812566280365\n",
            "TEST LOSS (Average) : 0.1740835209687551\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14680206775665283 Batch_id=147 Epoch Average loss=2.2170: 100%|██████████| 148/148 [00:01<00:00, 110.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.19603948295116425\n",
            "TEST LOSS (Average) : 0.17389731109142303\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.16926613450050354 Batch_id=147 Epoch Average loss=2.1869: 100%|██████████| 148/148 [00:01<00:00, 109.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.19447945058345795\n",
            "TEST LOSS (Average) : 0.17243412137031555\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.2509060800075531 Batch_id=147 Epoch Average loss=2.1482: 100%|██████████| 148/148 [00:01<00:00, 110.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.19167311489582062\n",
            "TEST LOSS (Average) : 0.17015481491883597\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1447833776473999 Batch_id=147 Epoch Average loss=2.1736: 100%|██████████| 148/148 [00:01<00:00, 99.94it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1904873251914978\n",
            "TEST LOSS (Average) : 0.16893295447031656\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1601853370666504 Batch_id=147 Epoch Average loss=2.1491: 100%|██████████| 148/148 [00:01<00:00, 101.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.18970078229904175\n",
            "TEST LOSS (Average) : 0.1680722584327062\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.2281607687473297 Batch_id=147 Epoch Average loss=2.1288: 100%|██████████| 148/148 [00:01<00:00, 103.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.18889456987380981\n",
            "TEST LOSS (Average) : 0.16744844615459442\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14653056859970093 Batch_id=147 Epoch Average loss=2.1109: 100%|██████████| 148/148 [00:01<00:00, 99.50it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.18774938583374023\n",
            "TEST LOSS (Average) : 0.1675670842329661\n",
            "EPOCH: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10215301811695099 Batch_id=147 Epoch Average loss=2.1172: 100%|██████████| 148/148 [00:01<00:00, 102.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.18677234649658203\n",
            "TEST LOSS (Average) : 0.16816909611225128\n",
            "EPOCH: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.16919495165348053 Batch_id=147 Epoch Average loss=2.0677: 100%|██████████| 148/148 [00:01<00:00, 110.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.18653064966201782\n",
            "TEST LOSS (Average) : 0.16454677283763885\n",
            "EPOCH: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12060017883777618 Batch_id=147 Epoch Average loss=2.0599: 100%|██████████| 148/148 [00:01<00:00, 104.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.18581664562225342\n",
            "TEST LOSS (Average) : 0.16328942775726318\n",
            "EPOCH: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.17019958794116974 Batch_id=147 Epoch Average loss=2.0610: 100%|██████████| 148/148 [00:01<00:00, 115.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.18372733891010284\n",
            "TEST LOSS (Average) : 0.16336594025293985\n",
            "EPOCH: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.20067867636680603 Batch_id=147 Epoch Average loss=2.0170: 100%|██████████| 148/148 [00:01<00:00, 116.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.18346023559570312\n",
            "TEST LOSS (Average) : 0.16690883537133536\n",
            "EPOCH: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09453082084655762 Batch_id=147 Epoch Average loss=1.9804: 100%|██████████| 148/148 [00:01<00:00, 116.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.18205316364765167\n",
            "TEST LOSS (Average) : 0.16291396816571554\n",
            "EPOCH: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.098931223154068 Batch_id=147 Epoch Average loss=1.9669: 100%|██████████| 148/148 [00:01<00:00, 115.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1814882457256317\n",
            "TEST LOSS (Average) : 0.15886175135771433\n",
            "EPOCH: 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.22178876399993896 Batch_id=147 Epoch Average loss=1.9733: 100%|██████████| 148/148 [00:01<00:00, 112.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1798076033592224\n",
            "TEST LOSS (Average) : 0.15849224726359049\n",
            "EPOCH: 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11790300905704498 Batch_id=147 Epoch Average loss=1.9581: 100%|██████████| 148/148 [00:01<00:00, 111.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17852254211902618\n",
            "TEST LOSS (Average) : 0.15686698257923126\n",
            "EPOCH: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13912196457386017 Batch_id=147 Epoch Average loss=1.9505: 100%|██████████| 148/148 [00:01<00:00, 113.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1780097931623459\n",
            "TEST LOSS (Average) : 0.1589446763197581\n",
            "EPOCH: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.18833927810192108 Batch_id=147 Epoch Average loss=1.8967: 100%|██████████| 148/148 [00:01<00:00, 111.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17803232371807098\n",
            "TEST LOSS (Average) : 0.16245567301909128\n",
            "EPOCH: 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12014982104301453 Batch_id=147 Epoch Average loss=1.8788: 100%|██████████| 148/148 [00:01<00:00, 110.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1804472953081131\n",
            "TEST LOSS (Average) : 0.17139282325903574\n",
            "EPOCH: 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0777832493185997 Batch_id=147 Epoch Average loss=1.8760: 100%|██████████| 148/148 [00:01<00:00, 106.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17442654073238373\n",
            "TEST LOSS (Average) : 0.15131393571694693\n",
            "EPOCH: 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13816095888614655 Batch_id=147 Epoch Average loss=1.7814: 100%|██████████| 148/148 [00:01<00:00, 115.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17355826497077942\n",
            "TEST LOSS (Average) : 0.14876115322113037\n",
            "EPOCH: 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.19611449539661407 Batch_id=147 Epoch Average loss=1.8148: 100%|██████████| 148/148 [00:01<00:00, 113.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17229196429252625\n",
            "TEST LOSS (Average) : 0.1473451852798462\n",
            "EPOCH: 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.21116507053375244 Batch_id=147 Epoch Average loss=1.7487: 100%|██████████| 148/148 [00:01<00:00, 113.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17110873758792877\n",
            "TEST LOSS (Average) : 0.14803201953570047\n",
            "EPOCH: 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11997672915458679 Batch_id=147 Epoch Average loss=1.7472: 100%|██████████| 148/148 [00:01<00:00, 115.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17312626540660858\n",
            "TEST LOSS (Average) : 0.15818405151367188\n",
            "EPOCH: 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1465863287448883 Batch_id=147 Epoch Average loss=1.7312: 100%|██████████| 148/148 [00:01<00:00, 115.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1686985194683075\n",
            "TEST LOSS (Average) : 0.14378741880257925\n",
            "EPOCH: 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09941297769546509 Batch_id=147 Epoch Average loss=1.7523: 100%|██████████| 148/148 [00:01<00:00, 103.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16717003285884857\n",
            "TEST LOSS (Average) : 0.14338115602731705\n",
            "EPOCH: 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.17051945626735687 Batch_id=147 Epoch Average loss=1.6844: 100%|██████████| 148/148 [00:01<00:00, 107.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16606909036636353\n",
            "TEST LOSS (Average) : 0.14378137389818826\n",
            "EPOCH: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.17361114919185638 Batch_id=147 Epoch Average loss=1.6132: 100%|██████████| 148/148 [00:01<00:00, 110.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16478624939918518\n",
            "TEST LOSS (Average) : 0.1379198357462883\n",
            "EPOCH: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.16776633262634277 Batch_id=147 Epoch Average loss=1.6316: 100%|██████████| 148/148 [00:01<00:00, 117.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1637364774942398\n",
            "TEST LOSS (Average) : 0.13785301397244135\n",
            "EPOCH: 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1558970808982849 Batch_id=147 Epoch Average loss=1.6142: 100%|██████████| 148/148 [00:01<00:00, 109.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16291090846061707\n",
            "TEST LOSS (Average) : 0.13714876274267832\n",
            "EPOCH: 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09569838643074036 Batch_id=147 Epoch Average loss=1.6342: 100%|██████████| 148/148 [00:01<00:00, 110.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16224779188632965\n",
            "TEST LOSS (Average) : 0.13511774192253748\n",
            "EPOCH: 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14350512623786926 Batch_id=147 Epoch Average loss=1.6091: 100%|██████████| 148/148 [00:01<00:00, 109.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1617426872253418\n",
            "TEST LOSS (Average) : 0.13795171429713568\n",
            "EPOCH: 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1140798032283783 Batch_id=147 Epoch Average loss=1.5653: 100%|██████████| 148/148 [00:01<00:00, 99.92it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16260941326618195\n",
            "TEST LOSS (Average) : 0.14490757882595062\n",
            "EPOCH: 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08642761409282684 Batch_id=147 Epoch Average loss=1.5772: 100%|██████████| 148/148 [00:01<00:00, 105.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16031229496002197\n",
            "TEST LOSS (Average) : 0.13270935664574304\n",
            "EPOCH: 37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12070321291685104 Batch_id=147 Epoch Average loss=1.5393: 100%|██████████| 148/148 [00:01<00:00, 106.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16017656028270721\n",
            "TEST LOSS (Average) : 0.1413533017039299\n",
            "EPOCH: 38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.16662172973155975 Batch_id=147 Epoch Average loss=1.5153: 100%|██████████| 148/148 [00:01<00:00, 108.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1584179401397705\n",
            "TEST LOSS (Average) : 0.13111765185991922\n",
            "EPOCH: 39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.104693204164505 Batch_id=147 Epoch Average loss=1.5093: 100%|██████████| 148/148 [00:01<00:00, 102.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15783590078353882\n",
            "TEST LOSS (Average) : 0.12994666894276938\n",
            "EPOCH: 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14704880118370056 Batch_id=147 Epoch Average loss=1.4564: 100%|██████████| 148/148 [00:01<00:00, 105.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15877315402030945\n",
            "TEST LOSS (Average) : 0.13803627341985703\n",
            "EPOCH: 41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10097768157720566 Batch_id=147 Epoch Average loss=1.4532: 100%|██████████| 148/148 [00:01<00:00, 104.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15748891234397888\n",
            "TEST LOSS (Average) : 0.1358171502749125\n",
            "EPOCH: 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11439738422632217 Batch_id=147 Epoch Average loss=1.4304: 100%|██████████| 148/148 [00:01<00:00, 96.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1570785641670227\n",
            "TEST LOSS (Average) : 0.12666376183430353\n",
            "EPOCH: 43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09886978566646576 Batch_id=147 Epoch Average loss=1.4199: 100%|██████████| 148/148 [00:01<00:00, 106.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15622478723526\n",
            "TEST LOSS (Average) : 0.12787328908840814\n",
            "EPOCH: 44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.054741021245718 Batch_id=147 Epoch Average loss=1.4725: 100%|██████████| 148/148 [00:01<00:00, 105.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15656837821006775\n",
            "TEST LOSS (Average) : 0.12637158234914145\n",
            "EPOCH: 45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.090419702231884 Batch_id=147 Epoch Average loss=1.4326: 100%|██████████| 148/148 [00:01<00:00, 110.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1556052416563034\n",
            "TEST LOSS (Average) : 0.1317158043384552\n",
            "EPOCH: 46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13026954233646393 Batch_id=147 Epoch Average loss=1.3613: 100%|██████████| 148/148 [00:01<00:00, 110.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15564994513988495\n",
            "TEST LOSS (Average) : 0.1292872354388237\n",
            "EPOCH: 47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09067768603563309 Batch_id=147 Epoch Average loss=1.3771: 100%|██████████| 148/148 [00:01<00:00, 99.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1550195813179016\n",
            "TEST LOSS (Average) : 0.12634013096491495\n",
            "EPOCH: 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10770640522241592 Batch_id=147 Epoch Average loss=1.4000: 100%|██████████| 148/148 [00:01<00:00, 111.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1551733762025833\n",
            "TEST LOSS (Average) : 0.12801694373289743\n",
            "EPOCH: 49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08874231576919556 Batch_id=147 Epoch Average loss=1.3574: 100%|██████████| 148/148 [00:01<00:00, 108.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1554279625415802\n",
            "TEST LOSS (Average) : 0.12493306895097096\n",
            "EPOCH: 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08697353303432465 Batch_id=147 Epoch Average loss=1.3748: 100%|██████████| 148/148 [00:01<00:00, 116.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15425099432468414\n",
            "TEST LOSS (Average) : 0.1254538098971049\n",
            "EPOCH: 51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07218652963638306 Batch_id=147 Epoch Average loss=1.3109: 100%|██████████| 148/148 [00:01<00:00, 114.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15562666952610016\n",
            "TEST LOSS (Average) : 0.13698070993026099\n",
            "EPOCH: 52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09388531744480133 Batch_id=147 Epoch Average loss=1.3170: 100%|██████████| 148/148 [00:01<00:00, 116.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15405862033367157\n",
            "TEST LOSS (Average) : 0.12920129795869192\n",
            "EPOCH: 53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06768697500228882 Batch_id=147 Epoch Average loss=1.3268: 100%|██████████| 148/148 [00:01<00:00, 116.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15443097054958344\n",
            "TEST LOSS (Average) : 0.12332675606012344\n",
            "EPOCH: 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1019696444272995 Batch_id=147 Epoch Average loss=1.3266: 100%|██████████| 148/148 [00:01<00:00, 117.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15468774735927582\n",
            "TEST LOSS (Average) : 0.13354955365260443\n",
            "EPOCH: 55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08699819445610046 Batch_id=147 Epoch Average loss=1.2825: 100%|██████████| 148/148 [00:01<00:00, 117.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15322700142860413\n",
            "TEST LOSS (Average) : 0.1246490627527237\n",
            "EPOCH: 56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1258399486541748 Batch_id=147 Epoch Average loss=1.3029: 100%|██████████| 148/148 [00:01<00:00, 112.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15317663550376892\n",
            "TEST LOSS (Average) : 0.127900168299675\n",
            "EPOCH: 57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05784575641155243 Batch_id=147 Epoch Average loss=1.2214: 100%|██████████| 148/148 [00:01<00:00, 104.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15268073976039886\n",
            "TEST LOSS (Average) : 0.12375560651222865\n",
            "EPOCH: 58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09662704169750214 Batch_id=147 Epoch Average loss=1.2813: 100%|██████████| 148/148 [00:01<00:00, 108.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15366722643375397\n",
            "TEST LOSS (Average) : 0.13115641474723816\n",
            "EPOCH: 59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10900741070508957 Batch_id=147 Epoch Average loss=1.2797: 100%|██████████| 148/148 [00:01<00:00, 102.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15223945677280426\n",
            "TEST LOSS (Average) : 0.12487310667832692\n",
            "EPOCH: 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.15019765496253967 Batch_id=147 Epoch Average loss=1.2152: 100%|██████████| 148/148 [00:01<00:00, 102.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15271027386188507\n",
            "TEST LOSS (Average) : 0.13157487163941065\n",
            "EPOCH: 61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09747698158025742 Batch_id=147 Epoch Average loss=1.2512: 100%|██████████| 148/148 [00:01<00:00, 105.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1522098034620285\n",
            "TEST LOSS (Average) : 0.12232872595389684\n",
            "EPOCH: 62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12816867232322693 Batch_id=147 Epoch Average loss=1.2784: 100%|██████████| 148/148 [00:01<00:00, 119.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1535719782114029\n",
            "TEST LOSS (Average) : 0.13632809619108835\n",
            "EPOCH: 63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.095004603266716 Batch_id=147 Epoch Average loss=1.2274: 100%|██████████| 148/148 [00:01<00:00, 116.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.152556374669075\n",
            "TEST LOSS (Average) : 0.1309137096007665\n",
            "EPOCH: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10421369969844818 Batch_id=147 Epoch Average loss=1.2591: 100%|██████████| 148/148 [00:01<00:00, 115.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1521657407283783\n",
            "TEST LOSS (Average) : 0.12612173706293106\n",
            "EPOCH: 65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08635616302490234 Batch_id=147 Epoch Average loss=1.1912: 100%|██████████| 148/148 [00:01<00:00, 113.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15169908106327057\n",
            "TEST LOSS (Average) : 0.12603704879681268\n",
            "EPOCH: 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11878447234630585 Batch_id=147 Epoch Average loss=1.2081: 100%|██████████| 148/148 [00:01<00:00, 119.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15143683552742004\n",
            "TEST LOSS (Average) : 0.12430604795614879\n",
            "EPOCH: 67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04130503535270691 Batch_id=147 Epoch Average loss=1.1498: 100%|██████████| 148/148 [00:01<00:00, 118.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.151410311460495\n",
            "TEST LOSS (Average) : 0.12033757319053014\n",
            "EPOCH: 68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1070677638053894 Batch_id=147 Epoch Average loss=1.2027: 100%|██████████| 148/148 [00:01<00:00, 105.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15154866874217987\n",
            "TEST LOSS (Average) : 0.12023979425430298\n",
            "EPOCH: 69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08423425257205963 Batch_id=147 Epoch Average loss=1.1683: 100%|██████████| 148/148 [00:01<00:00, 116.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15153063833713531\n",
            "TEST LOSS (Average) : 0.1250021532177925\n",
            "EPOCH: 70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06770540773868561 Batch_id=147 Epoch Average loss=1.1091: 100%|██████████| 148/148 [00:01<00:00, 118.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1507662832736969\n",
            "TEST LOSS (Average) : 0.12513462702433267\n",
            "EPOCH: 71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06615409255027771 Batch_id=147 Epoch Average loss=1.1559: 100%|██████████| 148/148 [00:01<00:00, 109.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15134914219379425\n",
            "TEST LOSS (Average) : 0.11901044100522995\n",
            "EPOCH: 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1044478788971901 Batch_id=147 Epoch Average loss=1.1390: 100%|██████████| 148/148 [00:01<00:00, 115.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15066087245941162\n",
            "TEST LOSS (Average) : 0.1195154959956805\n",
            "EPOCH: 73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12396829575300217 Batch_id=147 Epoch Average loss=1.1292: 100%|██████████| 148/148 [00:01<00:00, 117.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1512843370437622\n",
            "TEST LOSS (Average) : 0.12444064021110535\n",
            "EPOCH: 74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10449951887130737 Batch_id=147 Epoch Average loss=1.1358: 100%|██████████| 148/148 [00:01<00:00, 110.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15080392360687256\n",
            "TEST LOSS (Average) : 0.12016425530115764\n",
            "EPOCH: 75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11683429777622223 Batch_id=147 Epoch Average loss=1.0753: 100%|██████████| 148/148 [00:01<00:00, 100.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15006020665168762\n",
            "TEST LOSS (Average) : 0.12344737847646077\n",
            "EPOCH: 76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08562368154525757 Batch_id=147 Epoch Average loss=1.0802: 100%|██████████| 148/148 [00:01<00:00, 100.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1499781608581543\n",
            "TEST LOSS (Average) : 0.12251218656698863\n",
            "EPOCH: 77\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10865876078605652 Batch_id=147 Epoch Average loss=1.0780: 100%|██████████| 148/148 [00:01<00:00, 112.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14948201179504395\n",
            "TEST LOSS (Average) : 0.12014670670032501\n",
            "EPOCH: 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12719283998012543 Batch_id=147 Epoch Average loss=1.0984: 100%|██████████| 148/148 [00:01<00:00, 115.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14959031343460083\n",
            "TEST LOSS (Average) : 0.11867277820905049\n",
            "EPOCH: 79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07662905752658844 Batch_id=147 Epoch Average loss=1.0819: 100%|██████████| 148/148 [00:01<00:00, 117.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14887502789497375\n",
            "TEST LOSS (Average) : 0.12153999010721843\n",
            "EPOCH: 80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08083008974790573 Batch_id=147 Epoch Average loss=1.0918: 100%|██████████| 148/148 [00:01<00:00, 116.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14927572011947632\n",
            "TEST LOSS (Average) : 0.12465383609135945\n",
            "EPOCH: 81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07867651432752609 Batch_id=147 Epoch Average loss=1.0866: 100%|██████████| 148/148 [00:01<00:00, 116.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15010978281497955\n",
            "TEST LOSS (Average) : 0.12721645832061768\n",
            "EPOCH: 82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08924484252929688 Batch_id=147 Epoch Average loss=1.0778: 100%|██████████| 148/148 [00:01<00:00, 114.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15093223750591278\n",
            "TEST LOSS (Average) : 0.11824136972427368\n",
            "EPOCH: 83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09932538121938705 Batch_id=147 Epoch Average loss=1.0481: 100%|██████████| 148/148 [00:01<00:00, 116.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14982792735099792\n",
            "TEST LOSS (Average) : 0.1254893665512403\n",
            "EPOCH: 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07956209778785706 Batch_id=147 Epoch Average loss=1.0272: 100%|██████████| 148/148 [00:01<00:00, 115.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14905251562595367\n",
            "TEST LOSS (Average) : 0.121370330452919\n",
            "EPOCH: 85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0417819544672966 Batch_id=147 Epoch Average loss=1.0362: 100%|██████████| 148/148 [00:01<00:00, 115.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1493997871875763\n",
            "TEST LOSS (Average) : 0.12368741134802501\n",
            "EPOCH: 86\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.056834835559129715 Batch_id=147 Epoch Average loss=1.0173: 100%|██████████| 148/148 [00:01<00:00, 119.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14866353571414948\n",
            "TEST LOSS (Average) : 0.12100754678249359\n",
            "EPOCH: 87\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07461656630039215 Batch_id=147 Epoch Average loss=1.0604: 100%|██████████| 148/148 [00:01<00:00, 119.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1481814682483673\n",
            "TEST LOSS (Average) : 0.12079211572806041\n",
            "EPOCH: 88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10303660482168198 Batch_id=147 Epoch Average loss=1.0160: 100%|██████████| 148/148 [00:01<00:00, 115.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14815495908260345\n",
            "TEST LOSS (Average) : 0.11879805475473404\n",
            "EPOCH: 89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.048417627811431885 Batch_id=147 Epoch Average loss=1.0054: 100%|██████████| 148/148 [00:01<00:00, 114.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1484016478061676\n",
            "TEST LOSS (Average) : 0.12300581981738408\n",
            "EPOCH: 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.052309054881334305 Batch_id=147 Epoch Average loss=0.9890: 100%|██████████| 148/148 [00:01<00:00, 109.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14899660646915436\n",
            "TEST LOSS (Average) : 0.12722540895144144\n",
            "EPOCH: 91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09568527340888977 Batch_id=147 Epoch Average loss=0.9935: 100%|██████████| 148/148 [00:01<00:00, 107.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14798976480960846\n",
            "TEST LOSS (Average) : 0.1200966735680898\n",
            "EPOCH: 92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10748006403446198 Batch_id=147 Epoch Average loss=1.0089: 100%|██████████| 148/148 [00:01<00:00, 104.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14849045872688293\n",
            "TEST LOSS (Average) : 0.11831813553969066\n",
            "EPOCH: 93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07394780218601227 Batch_id=147 Epoch Average loss=0.9913: 100%|██████████| 148/148 [00:01<00:00, 105.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14830325543880463\n",
            "TEST LOSS (Average) : 0.12231036275625229\n",
            "EPOCH: 94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07968185842037201 Batch_id=147 Epoch Average loss=0.9910: 100%|██████████| 148/148 [00:01<00:00, 118.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14973627030849457\n",
            "TEST LOSS (Average) : 0.12891574203968048\n",
            "EPOCH: 95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0627770721912384 Batch_id=147 Epoch Average loss=1.0096: 100%|██████████| 148/148 [00:01<00:00, 114.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14942587912082672\n",
            "TEST LOSS (Average) : 0.11752515037854512\n",
            "EPOCH: 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0872771292924881 Batch_id=147 Epoch Average loss=1.0214: 100%|██████████| 148/148 [00:01<00:00, 107.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14901019632816315\n",
            "TEST LOSS (Average) : 0.12424022456010182\n",
            "EPOCH: 97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0866335779428482 Batch_id=147 Epoch Average loss=0.9757: 100%|██████████| 148/148 [00:01<00:00, 117.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14829817414283752\n",
            "TEST LOSS (Average) : 0.1250923921664556\n",
            "EPOCH: 98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.036632172763347626 Batch_id=147 Epoch Average loss=0.9330: 100%|██████████| 148/148 [00:01<00:00, 114.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14767181873321533\n",
            "TEST LOSS (Average) : 0.11917038510243098\n",
            "EPOCH: 99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04875949025154114 Batch_id=147 Epoch Average loss=0.9607: 100%|██████████| 148/148 [00:01<00:00, 101.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1479245126247406\n",
            "TEST LOSS (Average) : 0.11850942671298981\n",
            "EPOCH: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12812677025794983 Batch_id=147 Epoch Average loss=0.9653: 100%|██████████| 148/148 [00:01<00:00, 110.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14780981838703156\n",
            "TEST LOSS (Average) : 0.11946343133846919\n",
            "----------------------training complete for V-----------------\n",
            "----------------------training started for EI_joy-----------------\n",
            "EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13972339034080505 Batch_id=201 Epoch Average loss=2.9802: 100%|██████████| 202/202 [00:01<00:00, 106.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17243662476539612\n",
            "TEST LOSS (Average) : 0.16513079032301903\n",
            "EPOCH: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.19142062962055206 Batch_id=201 Epoch Average loss=2.2436: 100%|██████████| 202/202 [00:01<00:00, 109.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.17003996670246124\n",
            "TEST LOSS (Average) : 0.1619524471461773\n",
            "EPOCH: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.17934967577457428 Batch_id=201 Epoch Average loss=2.2396: 100%|██████████| 202/202 [00:01<00:00, 114.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.164939746260643\n",
            "TEST LOSS (Average) : 0.15982642397284508\n",
            "EPOCH: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.19620934128761292 Batch_id=201 Epoch Average loss=2.2113: 100%|██████████| 202/202 [00:01<00:00, 110.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16354699432849884\n",
            "TEST LOSS (Average) : 0.15786752477288246\n",
            "EPOCH: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.19588874280452728 Batch_id=201 Epoch Average loss=2.1635: 100%|██████████| 202/202 [00:01<00:00, 108.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1620473861694336\n",
            "TEST LOSS (Average) : 0.15707781165838242\n",
            "EPOCH: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14364632964134216 Batch_id=201 Epoch Average loss=2.1354: 100%|██████████| 202/202 [00:01<00:00, 112.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1613568514585495\n",
            "TEST LOSS (Average) : 0.15393919870257378\n",
            "EPOCH: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.16745740175247192 Batch_id=201 Epoch Average loss=2.1287: 100%|██████████| 202/202 [00:01<00:00, 113.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16130992770195007\n",
            "TEST LOSS (Average) : 0.15991520136594772\n",
            "EPOCH: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.21510732173919678 Batch_id=201 Epoch Average loss=2.1279: 100%|██████████| 202/202 [00:01<00:00, 114.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.16415318846702576\n",
            "TEST LOSS (Average) : 0.1654404141008854\n",
            "EPOCH: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1467793881893158 Batch_id=201 Epoch Average loss=2.0643: 100%|██████████| 202/202 [00:01<00:00, 112.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15680240094661713\n",
            "TEST LOSS (Average) : 0.15134277939796448\n",
            "EPOCH: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.19631175696849823 Batch_id=201 Epoch Average loss=2.0314: 100%|██████████| 202/202 [00:01<00:00, 105.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1566111445426941\n",
            "TEST LOSS (Average) : 0.15337782725691795\n",
            "EPOCH: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09098637104034424 Batch_id=201 Epoch Average loss=2.0134: 100%|██████████| 202/202 [00:01<00:00, 106.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1557936668395996\n",
            "TEST LOSS (Average) : 0.15321801975369453\n",
            "EPOCH: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1323861926794052 Batch_id=201 Epoch Average loss=2.0074: 100%|██████████| 202/202 [00:01<00:00, 104.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15438954532146454\n",
            "TEST LOSS (Average) : 0.15174871683120728\n",
            "EPOCH: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.18345576524734497 Batch_id=201 Epoch Average loss=1.9796: 100%|██████████| 202/202 [00:01<00:00, 108.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1520618349313736\n",
            "TEST LOSS (Average) : 0.14712124317884445\n",
            "EPOCH: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1533988118171692 Batch_id=201 Epoch Average loss=1.9899: 100%|██████████| 202/202 [00:01<00:00, 102.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15168392658233643\n",
            "TEST LOSS (Average) : 0.1481313705444336\n",
            "EPOCH: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.16982527077198029 Batch_id=201 Epoch Average loss=1.9830: 100%|██████████| 202/202 [00:01<00:00, 111.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.15182168781757355\n",
            "TEST LOSS (Average) : 0.15178238973021507\n",
            "EPOCH: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.2189919501543045 Batch_id=201 Epoch Average loss=1.9035: 100%|██████████| 202/202 [00:01<00:00, 113.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1489281952381134\n",
            "TEST LOSS (Average) : 0.14491447061300278\n",
            "EPOCH: 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.15505416691303253 Batch_id=201 Epoch Average loss=1.9151: 100%|██████████| 202/202 [00:01<00:00, 109.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1511850655078888\n",
            "TEST LOSS (Average) : 0.15189824998378754\n",
            "EPOCH: 18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10520271956920624 Batch_id=201 Epoch Average loss=1.8528: 100%|██████████| 202/202 [00:01<00:00, 114.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.146026611328125\n",
            "TEST LOSS (Average) : 0.14069511741399765\n",
            "EPOCH: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.155545175075531 Batch_id=201 Epoch Average loss=1.8555: 100%|██████████| 202/202 [00:01<00:00, 113.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14529652893543243\n",
            "TEST LOSS (Average) : 0.14101636037230492\n",
            "EPOCH: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1700681895017624 Batch_id=201 Epoch Average loss=1.8302: 100%|██████████| 202/202 [00:01<00:00, 110.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14461132884025574\n",
            "TEST LOSS (Average) : 0.14094344526529312\n",
            "EPOCH: 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.15638116002082825 Batch_id=201 Epoch Average loss=1.8204: 100%|██████████| 202/202 [00:01<00:00, 115.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14754633605480194\n",
            "TEST LOSS (Average) : 0.14701734110713005\n",
            "EPOCH: 22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.17218753695487976 Batch_id=201 Epoch Average loss=1.7867: 100%|██████████| 202/202 [00:01<00:00, 116.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1461624950170517\n",
            "TEST LOSS (Average) : 0.1452813260257244\n",
            "EPOCH: 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.17920204997062683 Batch_id=201 Epoch Average loss=1.7879: 100%|██████████| 202/202 [00:01<00:00, 111.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14525432884693146\n",
            "TEST LOSS (Average) : 0.1447143368422985\n",
            "EPOCH: 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07188813388347626 Batch_id=201 Epoch Average loss=1.7533: 100%|██████████| 202/202 [00:01<00:00, 113.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14156420528888702\n",
            "TEST LOSS (Average) : 0.13631610944867134\n",
            "EPOCH: 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14593710005283356 Batch_id=201 Epoch Average loss=1.7093: 100%|██████████| 202/202 [00:01<00:00, 105.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14307770133018494\n",
            "TEST LOSS (Average) : 0.14180009812116623\n",
            "EPOCH: 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.15420901775360107 Batch_id=201 Epoch Average loss=1.7308: 100%|██████████| 202/202 [00:01<00:00, 105.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14385223388671875\n",
            "TEST LOSS (Average) : 0.14359549805521965\n",
            "EPOCH: 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.18089130520820618 Batch_id=201 Epoch Average loss=1.7131: 100%|██████████| 202/202 [00:01<00:00, 105.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14476580917835236\n",
            "TEST LOSS (Average) : 0.14601575955748558\n",
            "EPOCH: 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1362500935792923 Batch_id=201 Epoch Average loss=1.6580: 100%|██████████| 202/202 [00:01<00:00, 111.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14065708220005035\n",
            "TEST LOSS (Average) : 0.1401553526520729\n",
            "EPOCH: 29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.18255379796028137 Batch_id=201 Epoch Average loss=1.6886: 100%|██████████| 202/202 [00:01<00:00, 104.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14012084901332855\n",
            "TEST LOSS (Average) : 0.13957490772008896\n",
            "EPOCH: 30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.20429638028144836 Batch_id=201 Epoch Average loss=1.6597: 100%|██████████| 202/202 [00:01<00:00, 103.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1433252990245819\n",
            "TEST LOSS (Average) : 0.14536268264055252\n",
            "EPOCH: 31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12278931587934494 Batch_id=201 Epoch Average loss=1.6031: 100%|██████████| 202/202 [00:02<00:00, 95.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13770875334739685\n",
            "TEST LOSS (Average) : 0.1373787485063076\n",
            "EPOCH: 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12028150260448456 Batch_id=201 Epoch Average loss=1.5816: 100%|██████████| 202/202 [00:01<00:00, 101.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1431461125612259\n",
            "TEST LOSS (Average) : 0.14670081436634064\n",
            "EPOCH: 33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1143960952758789 Batch_id=201 Epoch Average loss=1.5819: 100%|██████████| 202/202 [00:01<00:00, 109.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1407122015953064\n",
            "TEST LOSS (Average) : 0.14348607882857323\n",
            "EPOCH: 34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.14805927872657776 Batch_id=201 Epoch Average loss=1.5575: 100%|██████████| 202/202 [00:01<00:00, 117.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1360655128955841\n",
            "TEST LOSS (Average) : 0.1340470388531685\n",
            "EPOCH: 35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.16577215492725372 Batch_id=201 Epoch Average loss=1.5770: 100%|██████████| 202/202 [00:01<00:00, 117.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13627493381500244\n",
            "TEST LOSS (Average) : 0.13449242524802685\n",
            "EPOCH: 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1361004114151001 Batch_id=201 Epoch Average loss=1.5400: 100%|██████████| 202/202 [00:01<00:00, 105.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1361723095178604\n",
            "TEST LOSS (Average) : 0.1358010694384575\n",
            "EPOCH: 37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09928351640701294 Batch_id=201 Epoch Average loss=1.5357: 100%|██████████| 202/202 [00:01<00:00, 103.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14057043194770813\n",
            "TEST LOSS (Average) : 0.1438635103404522\n",
            "EPOCH: 38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.18900680541992188 Batch_id=201 Epoch Average loss=1.5208: 100%|██████████| 202/202 [00:01<00:00, 106.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13591106235980988\n",
            "TEST LOSS (Average) : 0.13201861083507538\n",
            "EPOCH: 39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08792855590581894 Batch_id=201 Epoch Average loss=1.5045: 100%|██████████| 202/202 [00:01<00:00, 106.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13597767055034637\n",
            "TEST LOSS (Average) : 0.13615806214511395\n",
            "EPOCH: 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.16247300803661346 Batch_id=201 Epoch Average loss=1.4566: 100%|██████████| 202/202 [00:01<00:00, 112.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13540542125701904\n",
            "TEST LOSS (Average) : 0.13479772955179214\n",
            "EPOCH: 41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09232045710086823 Batch_id=201 Epoch Average loss=1.4652: 100%|██████████| 202/202 [00:01<00:00, 116.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13604721426963806\n",
            "TEST LOSS (Average) : 0.13595813885331154\n",
            "EPOCH: 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13060647249221802 Batch_id=201 Epoch Average loss=1.4768: 100%|██████████| 202/202 [00:01<00:00, 116.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13639959692955017\n",
            "TEST LOSS (Average) : 0.13729555159807205\n",
            "EPOCH: 43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08123461902141571 Batch_id=201 Epoch Average loss=1.4255: 100%|██████████| 202/202 [00:01<00:00, 117.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1391349881887436\n",
            "TEST LOSS (Average) : 0.1418628916144371\n",
            "EPOCH: 44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10574457049369812 Batch_id=201 Epoch Average loss=1.4401: 100%|██████████| 202/202 [00:01<00:00, 117.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13541635870933533\n",
            "TEST LOSS (Average) : 0.13149544224143028\n",
            "EPOCH: 45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.15441089868545532 Batch_id=201 Epoch Average loss=1.4326: 100%|██████████| 202/202 [00:01<00:00, 117.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1364566683769226\n",
            "TEST LOSS (Average) : 0.137960072606802\n",
            "EPOCH: 46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09212775528430939 Batch_id=201 Epoch Average loss=1.4250: 100%|██████████| 202/202 [00:01<00:00, 116.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1349615901708603\n",
            "TEST LOSS (Average) : 0.13439648784697056\n",
            "EPOCH: 47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0947457104921341 Batch_id=201 Epoch Average loss=1.4157: 100%|██████████| 202/202 [00:01<00:00, 114.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13536471128463745\n",
            "TEST LOSS (Average) : 0.13583355955779552\n",
            "EPOCH: 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07972803711891174 Batch_id=201 Epoch Average loss=1.3733: 100%|██████████| 202/202 [00:01<00:00, 109.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13685418665409088\n",
            "TEST LOSS (Average) : 0.13952261209487915\n",
            "EPOCH: 49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10200046002864838 Batch_id=201 Epoch Average loss=1.3768: 100%|██████████| 202/202 [00:01<00:00, 109.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13562603294849396\n",
            "TEST LOSS (Average) : 0.13761751726269722\n",
            "EPOCH: 50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10189445316791534 Batch_id=201 Epoch Average loss=1.3903: 100%|██████████| 202/202 [00:01<00:00, 103.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13752642273902893\n",
            "TEST LOSS (Average) : 0.13975807279348373\n",
            "EPOCH: 51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0991230383515358 Batch_id=201 Epoch Average loss=1.3622: 100%|██████████| 202/202 [00:01<00:00, 101.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13586628437042236\n",
            "TEST LOSS (Average) : 0.13617979921400547\n",
            "EPOCH: 52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08429087698459625 Batch_id=201 Epoch Average loss=1.3676: 100%|██████████| 202/202 [00:01<00:00, 106.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1374504566192627\n",
            "TEST LOSS (Average) : 0.14018266275525093\n",
            "EPOCH: 53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09603390842676163 Batch_id=201 Epoch Average loss=1.3357: 100%|██████████| 202/202 [00:01<00:00, 112.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13693660497665405\n",
            "TEST LOSS (Average) : 0.13823287561535835\n",
            "EPOCH: 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10581880062818527 Batch_id=201 Epoch Average loss=1.2928: 100%|██████████| 202/202 [00:01<00:00, 111.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13456015288829803\n",
            "TEST LOSS (Average) : 0.13455489464104176\n",
            "EPOCH: 55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.15404309332370758 Batch_id=201 Epoch Average loss=1.3298: 100%|██████████| 202/202 [00:02<00:00, 98.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13441717624664307\n",
            "TEST LOSS (Average) : 0.13360676914453506\n",
            "EPOCH: 56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.049046725034713745 Batch_id=201 Epoch Average loss=1.2832: 100%|██████████| 202/202 [00:01<00:00, 104.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13567906618118286\n",
            "TEST LOSS (Average) : 0.13740496523678303\n",
            "EPOCH: 57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06969520449638367 Batch_id=201 Epoch Average loss=1.3110: 100%|██████████| 202/202 [00:01<00:00, 107.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13391803205013275\n",
            "TEST LOSS (Average) : 0.13317930698394775\n",
            "EPOCH: 58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11440691351890564 Batch_id=201 Epoch Average loss=1.2846: 100%|██████████| 202/202 [00:01<00:00, 103.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13850702345371246\n",
            "TEST LOSS (Average) : 0.143267173320055\n",
            "EPOCH: 59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.135311096906662 Batch_id=201 Epoch Average loss=1.3115: 100%|██████████| 202/202 [00:01<00:00, 109.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1360992193222046\n",
            "TEST LOSS (Average) : 0.1387577448040247\n",
            "EPOCH: 60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1378287523984909 Batch_id=201 Epoch Average loss=1.2854: 100%|██████████| 202/202 [00:01<00:00, 113.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13931147754192352\n",
            "TEST LOSS (Average) : 0.14506296068429947\n",
            "EPOCH: 61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07067036628723145 Batch_id=201 Epoch Average loss=1.2932: 100%|██████████| 202/202 [00:01<00:00, 118.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13796569406986237\n",
            "TEST LOSS (Average) : 0.14300580695271492\n",
            "EPOCH: 62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11953680217266083 Batch_id=201 Epoch Average loss=1.2530: 100%|██████████| 202/202 [00:01<00:00, 119.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.14166846871376038\n",
            "TEST LOSS (Average) : 0.14943839982151985\n",
            "EPOCH: 63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11698980629444122 Batch_id=201 Epoch Average loss=1.2733: 100%|██████████| 202/202 [00:01<00:00, 112.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13536185026168823\n",
            "TEST LOSS (Average) : 0.13922263495624065\n",
            "EPOCH: 64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08624537289142609 Batch_id=201 Epoch Average loss=1.2572: 100%|██████████| 202/202 [00:01<00:00, 119.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1332944929599762\n",
            "TEST LOSS (Average) : 0.13508917950093746\n",
            "EPOCH: 65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11447174847126007 Batch_id=201 Epoch Average loss=1.2243: 100%|██████████| 202/202 [00:01<00:00, 118.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1342199146747589\n",
            "TEST LOSS (Average) : 0.1357023399323225\n",
            "EPOCH: 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.087574303150177 Batch_id=201 Epoch Average loss=1.2038: 100%|██████████| 202/202 [00:01<00:00, 117.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1358892172574997\n",
            "TEST LOSS (Average) : 0.14050579443573952\n",
            "EPOCH: 67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08695519715547562 Batch_id=201 Epoch Average loss=1.2383: 100%|██████████| 202/202 [00:01<00:00, 118.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13544556498527527\n",
            "TEST LOSS (Average) : 0.13977421633899212\n",
            "EPOCH: 68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10538332164287567 Batch_id=201 Epoch Average loss=1.2136: 100%|██████████| 202/202 [00:02<00:00, 94.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13609579205513\n",
            "TEST LOSS (Average) : 0.14146430045366287\n",
            "EPOCH: 69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11420826613903046 Batch_id=201 Epoch Average loss=1.2101: 100%|██████████| 202/202 [00:03<00:00, 58.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1343328356742859\n",
            "TEST LOSS (Average) : 0.13680669106543064\n",
            "EPOCH: 70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12969456613063812 Batch_id=201 Epoch Average loss=1.2082: 100%|██████████| 202/202 [00:03<00:00, 55.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13310663402080536\n",
            "TEST LOSS (Average) : 0.13140515238046646\n",
            "EPOCH: 71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11833825707435608 Batch_id=201 Epoch Average loss=1.1883: 100%|██████████| 202/202 [00:03<00:00, 58.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13348226249217987\n",
            "TEST LOSS (Average) : 0.1347945760935545\n",
            "EPOCH: 72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10299916565418243 Batch_id=201 Epoch Average loss=1.1675: 100%|██████████| 202/202 [00:04<00:00, 50.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1347762495279312\n",
            "TEST LOSS (Average) : 0.13839640468358994\n",
            "EPOCH: 73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10292548686265945 Batch_id=201 Epoch Average loss=1.1852: 100%|██████████| 202/202 [00:02<00:00, 68.92it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13375434279441833\n",
            "TEST LOSS (Average) : 0.13646637089550495\n",
            "EPOCH: 74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09247377514839172 Batch_id=201 Epoch Average loss=1.1546: 100%|██████████| 202/202 [00:01<00:00, 101.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13486652076244354\n",
            "TEST LOSS (Average) : 0.1384521871805191\n",
            "EPOCH: 75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08034280687570572 Batch_id=201 Epoch Average loss=1.1839: 100%|██████████| 202/202 [00:02<00:00, 97.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13376714289188385\n",
            "TEST LOSS (Average) : 0.13550608232617378\n",
            "EPOCH: 76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10820771008729935 Batch_id=201 Epoch Average loss=1.1375: 100%|██████████| 202/202 [00:01<00:00, 102.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13490086793899536\n",
            "TEST LOSS (Average) : 0.13885095715522766\n",
            "EPOCH: 77\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07902395725250244 Batch_id=201 Epoch Average loss=1.1480: 100%|██████████| 202/202 [00:01<00:00, 116.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13587512075901031\n",
            "TEST LOSS (Average) : 0.1417105756700039\n",
            "EPOCH: 78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12075234949588776 Batch_id=201 Epoch Average loss=1.1332: 100%|██████████| 202/202 [00:01<00:00, 105.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13464310765266418\n",
            "TEST LOSS (Average) : 0.1393302083015442\n",
            "EPOCH: 79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08994348347187042 Batch_id=201 Epoch Average loss=1.1681: 100%|██████████| 202/202 [00:01<00:00, 109.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13922223448753357\n",
            "TEST LOSS (Average) : 0.14743338525295258\n",
            "EPOCH: 80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1350819617509842 Batch_id=201 Epoch Average loss=1.1110: 100%|██████████| 202/202 [00:01<00:00, 102.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1335238665342331\n",
            "TEST LOSS (Average) : 0.1367423553019762\n",
            "EPOCH: 81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.057924218475818634 Batch_id=201 Epoch Average loss=1.1219: 100%|██████████| 202/202 [00:01<00:00, 107.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13493026793003082\n",
            "TEST LOSS (Average) : 0.1413995623588562\n",
            "EPOCH: 82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1206340342760086 Batch_id=201 Epoch Average loss=1.1140: 100%|██████████| 202/202 [00:01<00:00, 113.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13839617371559143\n",
            "TEST LOSS (Average) : 0.1472393050789833\n",
            "EPOCH: 83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.11112993955612183 Batch_id=201 Epoch Average loss=1.1540: 100%|██████████| 202/202 [00:01<00:00, 117.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1359935700893402\n",
            "TEST LOSS (Average) : 0.14327314123511314\n",
            "EPOCH: 84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.13196463882923126 Batch_id=201 Epoch Average loss=1.0952: 100%|██████████| 202/202 [00:01<00:00, 117.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13336150348186493\n",
            "TEST LOSS (Average) : 0.13786509074270725\n",
            "EPOCH: 85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10096393525600433 Batch_id=201 Epoch Average loss=1.0593: 100%|██████████| 202/202 [00:01<00:00, 108.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13636650145053864\n",
            "TEST LOSS (Average) : 0.14457528665661812\n",
            "EPOCH: 86\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.02635658159852028 Batch_id=201 Epoch Average loss=1.1098: 100%|██████████| 202/202 [00:01<00:00, 103.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13899220526218414\n",
            "TEST LOSS (Average) : 0.14742526784539223\n",
            "EPOCH: 87\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.10095518827438354 Batch_id=201 Epoch Average loss=1.0950: 100%|██████████| 202/202 [00:01<00:00, 109.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1340363323688507\n",
            "TEST LOSS (Average) : 0.13805685378611088\n",
            "EPOCH: 88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09869363903999329 Batch_id=201 Epoch Average loss=1.0934: 100%|██████████| 202/202 [00:01<00:00, 108.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1333116590976715\n",
            "TEST LOSS (Average) : 0.13582784682512283\n",
            "EPOCH: 89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0819682776927948 Batch_id=201 Epoch Average loss=1.0591: 100%|██████████| 202/202 [00:01<00:00, 106.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13304898142814636\n",
            "TEST LOSS (Average) : 0.13503722473978996\n",
            "EPOCH: 90\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.05929652974009514 Batch_id=201 Epoch Average loss=1.0828: 100%|██████████| 202/202 [00:01<00:00, 104.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1368548572063446\n",
            "TEST LOSS (Average) : 0.14353755116462708\n",
            "EPOCH: 91\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.04397476091980934 Batch_id=201 Epoch Average loss=1.0598: 100%|██████████| 202/202 [00:01<00:00, 106.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13544166088104248\n",
            "TEST LOSS (Average) : 0.1408284790813923\n",
            "EPOCH: 92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06665900349617004 Batch_id=201 Epoch Average loss=1.0590: 100%|██████████| 202/202 [00:01<00:00, 103.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13411015272140503\n",
            "TEST LOSS (Average) : 0.13893266208469868\n",
            "EPOCH: 93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1381268948316574 Batch_id=201 Epoch Average loss=1.0518: 100%|██████████| 202/202 [00:01<00:00, 104.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1328735053539276\n",
            "TEST LOSS (Average) : 0.13683893717825413\n",
            "EPOCH: 94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.1385689675807953 Batch_id=201 Epoch Average loss=1.0363: 100%|██████████| 202/202 [00:01<00:00, 108.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13609422743320465\n",
            "TEST LOSS (Average) : 0.14205030351877213\n",
            "EPOCH: 95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.08244697749614716 Batch_id=201 Epoch Average loss=1.0497: 100%|██████████| 202/202 [00:01<00:00, 114.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13438045978546143\n",
            "TEST LOSS (Average) : 0.13845635950565338\n",
            "EPOCH: 96\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.12607896327972412 Batch_id=201 Epoch Average loss=1.0464: 100%|██████████| 202/202 [00:01<00:00, 115.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13706421852111816\n",
            "TEST LOSS (Average) : 0.14399509876966476\n",
            "EPOCH: 97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.07015901803970337 Batch_id=201 Epoch Average loss=1.0502: 100%|██████████| 202/202 [00:01<00:00, 113.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13461098074913025\n",
            "TEST LOSS (Average) : 0.1399858184158802\n",
            "EPOCH: 98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.09023132920265198 Batch_id=201 Epoch Average loss=1.0316: 100%|██████████| 202/202 [00:01<00:00, 116.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13308964669704437\n",
            "TEST LOSS (Average) : 0.13737276941537857\n",
            "EPOCH: 99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.06189858540892601 Batch_id=201 Epoch Average loss=1.0149: 100%|██████████| 202/202 [00:01<00:00, 110.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.13473623991012573\n",
            "TEST LOSS (Average) : 0.1402451228350401\n",
            "EPOCH: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss=0.0701405480504036 Batch_id=201 Epoch Average loss=1.0344: 100%|██████████| 202/202 [00:01<00:00, 106.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VALIDATION LOSS (Average) : 0.1361939013004303\n",
            "TEST LOSS (Average) : 0.14300091192126274\n",
            "----------------------training complete for EI_joy-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # EXECUTION\n",
        "\n",
        "# lr = 2e-5\n",
        "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "# domain_loss_function= nn.BCEWithLogitsLoss()\n",
        "# regression_loss_function = nn.L1Loss()\n",
        "\n",
        "\n",
        "# model = model.to(DEVICE)\n",
        "# domain_loss_function = domain_loss_function.to(DEVICE)\n",
        "# regression_loss_function = regression_loss_function.to(DEVICE)\n",
        "\n",
        "# # train_losses = [] # to capture train losses over training epochs\n",
        "# train_accuracy = [] # to capture train accuracy over training epochs\n",
        "# # val_losses = [] # to capture validation loss\n",
        "# # test_losses = [] # to capture test losses \n",
        "# # test_accuracy = [] # to capture test accuracy \n",
        "\n",
        "# # EPOCHS = 2\n",
        "# EPOCHS = 100\n",
        "# # dict_val_loss = {}\n",
        "# # dict_test_loss = {}\n",
        "\n",
        "\n",
        "# train_regresion_losses = [] # to capture train losses over training epochs\n",
        "# train_domain_losses = []\n",
        "# train_accuracy = [] # to capture train accuracy over training epochs\n",
        "# # valid_regresion_losses = [] # to capture validation loss\n",
        "# # test_regresion_losses = [] # to capture test losses \n",
        "# total_test_regression_loss =[]\n",
        "# total_valid_regression_loss =[]\n",
        "# # print(f'----------------------training started for {name}-----------------')\n",
        "# for epoch in range(EPOCHS):\n",
        "#   print(\"EPOCH:\", epoch+1)\n",
        "#   train_model(model, DEVICE, train_iterator, optimizer, epoch)\n",
        "#   # print(\"for validation.......\")\n",
        "#   # val_name = train_name.replace(\"train\", \"val\" )\n",
        "#   # test_model(typical_model, device, dict_val_loader[val_name], mode = 'val')\n",
        "#   test_model(model, DEVICE, valid_iterator, mode = 'val')\n",
        "\n",
        "\n",
        "#   # print(\"for test  .......\")\n",
        "#   # test_name = train_name.replace(\"train\", \"test\" )\n",
        "#   # test_model(typical_model, device, dict_test_loader[test_name], mode = 'test')\n",
        "#   test_model(model, DEVICE, test_iterator, mode = 'test')\n",
        "\n",
        "# # dict_val_loss[name] = val_losses\n",
        "# # dict_test_loss[name] = test_losses\n",
        "\n",
        "# model_name = \"Non_DANN\"+\".pt\"\n",
        "# torch.save(model.state_dict(), os.path.join(MODEL_DIR, model_name))\n",
        "# # print(f'----------------------training complete for {name}-----------------')\n",
        "# # print(dict_val_loss.items())\n",
        "# # print(dict_test_loss.items())"
      ],
      "metadata": {
        "id": "nLYNyFPTBmCT"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DANN Model - Training and Testing"
      ],
      "metadata": {
        "id": "cAjugcOJ5imB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "5qNV4sND5q7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(logits, labels):\n",
        "    \n",
        "    predicted_labels_dict = {\n",
        "      0: 0,\n",
        "      1: 0,\n",
        "    }\n",
        "    \n",
        "    predicted_label = logits.max(dim = 1)[1]\n",
        "    \n",
        "    for pred in predicted_label:\n",
        "        predicted_labels_dict[pred.item()] += 1\n",
        "    acc = (predicted_label == labels).float().mean()\n",
        "    \n",
        "    return acc, predicted_labels_dict"
      ],
      "metadata": {
        "id": "pFpxF3m4CUB0"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def binary_acc(y_pred, y_test):\n",
        "#     y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "#     correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "#     acc = correct_results_sum/y_test.shape[0]\n",
        "#     acc = torch.round(acc * 100)\n",
        "    \n",
        "#     return acc"
      ],
      "metadata": {
        "id": "UbiPnQaP5RZG"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def evaluate(model, dataloader, mode = 'test', percentage = 5):\n",
        "#     with torch.no_grad():\n",
        "#         predicted_labels_dict = {                                                   \n",
        "#           0: 0,                                                                     \n",
        "#           1: 0,                                                                     \n",
        "#         }\n",
        "        \n",
        "#         mean_accuracy = 0.0\n",
        "#         # total_batches = len(dataloader)\n",
        "#         # print(\"total_batches: \",total_batches )\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "#             sentiment_pred, _ = model(**inputs)\n",
        "#             accuracy, predicted_labels = compute_accuracy(sentiment_pred, inputs[\"labels\"])\n",
        "#             mean_accuracy += accuracy\n",
        "#             predicted_labels_dict[0] += predicted_labels[0]\n",
        "#             predicted_labels_dict[1] += predicted_labels[1]  \n",
        "#         print(predicted_labels_dict)\n",
        "#     return mean_accuracy/total_batches"
      ],
      "metadata": {
        "id": "FU-OwTIhSRpJ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DANN For multiple datasets\n",
        "n_epochs = 100 # number of epochs\n",
        "# n_epochs = 2 # number of epochs\n",
        "lr = 2e-5\n",
        "\n",
        "dict_dann_model_saved= {}\n",
        "for name, model_arch in dict_model_arch.items():\n",
        "  model = model_arch\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "  model = model.to(DEVICE)\n",
        "  domain_loss_function= nn.NLLLoss()\n",
        "  regression_loss_function = nn.L1Loss()\n",
        "  domain_loss_function = domain_loss_function.to(DEVICE)\n",
        "  regression_loss_function = regression_loss_function.to(DEVICE)\n",
        "  max_batches = min(len(dict_iterator[name]['train_iterator']), len(dict_target_iterator[name]))\n",
        "  # max_batches = min(len(train_iterator), len(target_data)//TARGET_BATCH_SIZE)\n",
        "  # max_batches = min(len(train_iterator), len(target_iterator))\n",
        "\n",
        "  # print(max_batches)\n",
        "  print(f'----------------------training started for DANN model - {name}-----------------')\n",
        "  for epoch_idx in range(n_epochs):\n",
        "      # source_iterator = iter(train_iterator) #single dataset\n",
        "      source_iterator = iter(dict_iterator[name]['train_iterator'])\n",
        "      # target_iterator = iter(target_iterator) #single dataset\n",
        "      target_iterator = iter(dict_target_iterator[name]) \n",
        "\n",
        "      for batch_idx in range(max_batches):\n",
        "          \n",
        "          p = float(batch_idx + epoch_idx * max_batches) / (n_epochs * max_batches)\n",
        "          alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "          alpha = torch.tensor(alpha)\n",
        "          \n",
        "          model.train()          \n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          ## SOURCE DATASET TRAINING UPDATE\n",
        "          \n",
        "          source_batch = next(source_iterator)\n",
        "          source_tweets, source_intensities = source_batch.tweet.to(DEVICE), source_batch.intensity.to(DEVICE)  # plural, we are not interested in domain\n",
        "          \n",
        "          source_intensity_outputs, source_domain_outputs = model(source_tweets, alpha = alpha)\n",
        "\n",
        "          loss_source_regression= regression_loss_function(source_intensity_outputs,source_intensities.unsqueeze(1)) # Computing regression loss\n",
        "\n",
        "          source_domain_inputs = torch.zeros(len(source_batch), dtype=torch.long).to(DEVICE) # source domain has 0 id\n",
        "          loss_source_domain = domain_loss_function(source_domain_outputs,source_domain_inputs)\n",
        "\n",
        "\n",
        "          ## TARGET DATASET TRAINING UPDATE\n",
        "          target_batch = next(iter(target_iterator))\n",
        "          target_tweets= target_batch.tweet.to(DEVICE) # plural\n",
        "\n",
        "          _, target_domain_outputs = model(target_tweets, alpha = alpha)\n",
        "\n",
        "          target_domain_inputs = torch.ones(len(target_batch), dtype=torch.long).to(DEVICE) # target domain has 1 id\n",
        "          loss_target_domain = domain_loss_function(target_domain_outputs,target_domain_inputs)\n",
        "\n",
        "          # COMBINING LOSS\n",
        "          loss = loss_source_regression + loss_source_domain + loss_target_domain\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if (batch_idx % 100 == 0):\n",
        "            print(\"Epoch [{}/{}] Step [{}/{}]: domain_loss_target={:.4f} / domain_loss_source={:.4f} / regression_loss_source={:.4f} / alpha={:.4f}\"\n",
        "                .format(epoch_idx + 1,\n",
        "                        n_epochs,\n",
        "                        batch_idx + 1,\n",
        "                        max_batches,\n",
        "                        loss_target_domain.item()\n",
        "                        ,loss_source_domain.item()\n",
        "                        ,loss_source_regression.item(),alpha))\n",
        "\n",
        "\n",
        "      # Evaluate the model after every epoch\n",
        "\n",
        "      # test_model(model, DEVICE, valid_iterator, mode = 'val') # single model\n",
        "      test_model(model, DEVICE, dict_iterator[name]['val_iterator'], mode = 'val') \n",
        "\n",
        "      # test_model(model, DEVICE, test_iterator, mode = 'test') # single model\n",
        "      test_model(model, DEVICE, dict_iterator[name]['test_iterator'], mode = 'test')\n",
        "\n",
        "  model_name = name + \"_dann.pt\"\n",
        "  torch.save(model.state_dict(), os.path.join(MODEL_DIR,model_name))\n",
        "  dict_dann_model_saved[name] = model_name\n",
        "  print(f'----------------------training complete for DANN model - {name}-----------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s2MIOjQfe_H",
        "outputId": "3297bde4-bde4-4ebc-e6fc-8f9da98e6671"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------training started for DANN model - EI_sadness-----------------\n",
            "Epoch [1/100] Step [1/192]: domain_loss_target=1.0917 / domain_loss_source=0.4158 / regression_loss_source=0.0686 / alpha=0.0000\n",
            "Epoch [1/100] Step [101/192]: domain_loss_target=1.0135 / domain_loss_source=0.4296 / regression_loss_source=0.1118 / alpha=0.0260\n",
            "VALIDATION LOSS (Average) : 0.17943166196346283\n",
            "TEST LOSS (Average) : 0.175674041112264\n",
            "Epoch [2/100] Step [1/192]: domain_loss_target=0.9915 / domain_loss_source=0.4277 / regression_loss_source=0.0901 / alpha=0.0500\n",
            "Epoch [2/100] Step [101/192]: domain_loss_target=0.9818 / domain_loss_source=0.4321 / regression_loss_source=0.0456 / alpha=0.0759\n",
            "VALIDATION LOSS (Average) : 0.17522600293159485\n",
            "TEST LOSS (Average) : 0.17108134428660074\n",
            "Epoch [3/100] Step [1/192]: domain_loss_target=0.9783 / domain_loss_source=0.3861 / regression_loss_source=0.0531 / alpha=0.0997\n",
            "Epoch [3/100] Step [101/192]: domain_loss_target=0.9176 / domain_loss_source=0.4125 / regression_loss_source=0.0747 / alpha=0.1254\n",
            "VALIDATION LOSS (Average) : 0.1735103875398636\n",
            "TEST LOSS (Average) : 0.16963107387224832\n",
            "Epoch [4/100] Step [1/192]: domain_loss_target=0.9176 / domain_loss_source=0.2944 / regression_loss_source=0.0396 / alpha=0.1489\n",
            "Epoch [4/100] Step [101/192]: domain_loss_target=0.8120 / domain_loss_source=0.2462 / regression_loss_source=0.0735 / alpha=0.1742\n",
            "VALIDATION LOSS (Average) : 0.1759716272354126\n",
            "TEST LOSS (Average) : 0.17219319939613342\n",
            "Epoch [5/100] Step [1/192]: domain_loss_target=0.8401 / domain_loss_source=0.2610 / regression_loss_source=0.0405 / alpha=0.1974\n",
            "Epoch [5/100] Step [101/192]: domain_loss_target=0.7650 / domain_loss_source=0.4405 / regression_loss_source=0.0455 / alpha=0.2223\n",
            "VALIDATION LOSS (Average) : 0.18645481765270233\n",
            "TEST LOSS (Average) : 0.18196930487950644\n",
            "Epoch [6/100] Step [1/192]: domain_loss_target=0.7732 / domain_loss_source=0.4457 / regression_loss_source=0.0597 / alpha=0.2449\n",
            "Epoch [6/100] Step [101/192]: domain_loss_target=1.0335 / domain_loss_source=0.5327 / regression_loss_source=0.0660 / alpha=0.2692\n",
            "VALIDATION LOSS (Average) : 0.1769905686378479\n",
            "TEST LOSS (Average) : 0.1741182655096054\n",
            "Epoch [7/100] Step [1/192]: domain_loss_target=1.1368 / domain_loss_source=0.6657 / regression_loss_source=0.1150 / alpha=0.2913\n",
            "Epoch [7/100] Step [101/192]: domain_loss_target=1.0463 / domain_loss_source=0.4497 / regression_loss_source=0.0598 / alpha=0.3150\n",
            "VALIDATION LOSS (Average) : 0.1757374107837677\n",
            "TEST LOSS (Average) : 0.1722559134165446\n",
            "Epoch [8/100] Step [1/192]: domain_loss_target=0.8908 / domain_loss_source=0.4656 / regression_loss_source=0.0753 / alpha=0.3364\n",
            "Epoch [8/100] Step [101/192]: domain_loss_target=0.9683 / domain_loss_source=0.4539 / regression_loss_source=0.0848 / alpha=0.3593\n",
            "VALIDATION LOSS (Average) : 0.176278755068779\n",
            "TEST LOSS (Average) : 0.17289570967356363\n",
            "Epoch [9/100] Step [1/192]: domain_loss_target=0.9974 / domain_loss_source=0.3739 / regression_loss_source=0.0565 / alpha=0.3799\n",
            "Epoch [9/100] Step [101/192]: domain_loss_target=0.9127 / domain_loss_source=0.4570 / regression_loss_source=0.1142 / alpha=0.4020\n",
            "VALIDATION LOSS (Average) : 0.18923595547676086\n",
            "TEST LOSS (Average) : 0.1841497321923574\n",
            "Epoch [10/100] Step [1/192]: domain_loss_target=0.9663 / domain_loss_source=0.5085 / regression_loss_source=0.1237 / alpha=0.4219\n",
            "Epoch [10/100] Step [101/192]: domain_loss_target=0.8744 / domain_loss_source=0.4246 / regression_loss_source=0.0463 / alpha=0.4431\n",
            "VALIDATION LOSS (Average) : 0.17803333699703217\n",
            "TEST LOSS (Average) : 0.17248923083146414\n",
            "Epoch [11/100] Step [1/192]: domain_loss_target=0.9906 / domain_loss_source=0.4478 / regression_loss_source=0.0957 / alpha=0.4621\n",
            "Epoch [11/100] Step [101/192]: domain_loss_target=0.9839 / domain_loss_source=0.4710 / regression_loss_source=0.0800 / alpha=0.4823\n",
            "VALIDATION LOSS (Average) : 0.17210441827774048\n",
            "TEST LOSS (Average) : 0.1662153700987498\n",
            "Epoch [12/100] Step [1/192]: domain_loss_target=0.9283 / domain_loss_source=0.4614 / regression_loss_source=0.0833 / alpha=0.5005\n",
            "Epoch [12/100] Step [101/192]: domain_loss_target=0.8999 / domain_loss_source=0.4730 / regression_loss_source=0.0570 / alpha=0.5198\n",
            "VALIDATION LOSS (Average) : 0.18604683876037598\n",
            "TEST LOSS (Average) : 0.1801956743001938\n",
            "Epoch [13/100] Step [1/192]: domain_loss_target=0.8713 / domain_loss_source=0.4148 / regression_loss_source=0.0891 / alpha=0.5370\n",
            "Epoch [13/100] Step [101/192]: domain_loss_target=0.8669 / domain_loss_source=0.4721 / regression_loss_source=0.0448 / alpha=0.5553\n",
            "VALIDATION LOSS (Average) : 0.1824737787246704\n",
            "TEST LOSS (Average) : 0.1774590661128362\n",
            "Epoch [14/100] Step [1/192]: domain_loss_target=0.8679 / domain_loss_source=0.4107 / regression_loss_source=0.0759 / alpha=0.5717\n",
            "Epoch [14/100] Step [101/192]: domain_loss_target=0.8609 / domain_loss_source=0.5019 / regression_loss_source=0.0485 / alpha=0.5889\n",
            "VALIDATION LOSS (Average) : 0.18887625634670258\n",
            "TEST LOSS (Average) : 0.18476957082748413\n",
            "Epoch [15/100] Step [1/192]: domain_loss_target=0.8826 / domain_loss_source=0.5487 / regression_loss_source=0.0577 / alpha=0.6044\n",
            "Epoch [15/100] Step [101/192]: domain_loss_target=0.8917 / domain_loss_source=0.5409 / regression_loss_source=0.0884 / alpha=0.6206\n",
            "VALIDATION LOSS (Average) : 0.1742352843284607\n",
            "TEST LOSS (Average) : 0.16988064845403036\n",
            "Epoch [16/100] Step [1/192]: domain_loss_target=0.8520 / domain_loss_source=0.5593 / regression_loss_source=0.0708 / alpha=0.6351\n",
            "Epoch [16/100] Step [101/192]: domain_loss_target=0.8274 / domain_loss_source=0.5199 / regression_loss_source=0.0771 / alpha=0.6504\n",
            "VALIDATION LOSS (Average) : 0.18493597209453583\n",
            "TEST LOSS (Average) : 0.18060564001401266\n",
            "Epoch [17/100] Step [1/192]: domain_loss_target=0.8465 / domain_loss_source=0.5428 / regression_loss_source=0.0654 / alpha=0.6640\n",
            "Epoch [17/100] Step [101/192]: domain_loss_target=0.8352 / domain_loss_source=0.5185 / regression_loss_source=0.0586 / alpha=0.6783\n",
            "VALIDATION LOSS (Average) : 0.1837940216064453\n",
            "TEST LOSS (Average) : 0.1792227029800415\n",
            "Epoch [18/100] Step [1/192]: domain_loss_target=0.8079 / domain_loss_source=0.4844 / regression_loss_source=0.0983 / alpha=0.6911\n",
            "Epoch [18/100] Step [101/192]: domain_loss_target=0.7969 / domain_loss_source=0.4488 / regression_loss_source=0.1079 / alpha=0.7044\n",
            "VALIDATION LOSS (Average) : 0.19286559522151947\n",
            "TEST LOSS (Average) : 0.18768190840880075\n",
            "Epoch [19/100] Step [1/192]: domain_loss_target=0.7990 / domain_loss_source=0.4899 / regression_loss_source=0.0633 / alpha=0.7163\n",
            "Epoch [19/100] Step [101/192]: domain_loss_target=0.7754 / domain_loss_source=0.5674 / regression_loss_source=0.0890 / alpha=0.7287\n",
            "VALIDATION LOSS (Average) : 0.18096958100795746\n",
            "TEST LOSS (Average) : 0.1758243590593338\n",
            "Epoch [20/100] Step [1/192]: domain_loss_target=0.8177 / domain_loss_source=0.5880 / regression_loss_source=0.1021 / alpha=0.7398\n",
            "Epoch [20/100] Step [101/192]: domain_loss_target=0.8170 / domain_loss_source=0.5209 / regression_loss_source=0.0282 / alpha=0.7513\n",
            "VALIDATION LOSS (Average) : 0.18899215757846832\n",
            "TEST LOSS (Average) : 0.18416611353556314\n",
            "Epoch [21/100] Step [1/192]: domain_loss_target=0.8174 / domain_loss_source=0.5814 / regression_loss_source=0.0755 / alpha=0.7616\n",
            "Epoch [21/100] Step [101/192]: domain_loss_target=0.7828 / domain_loss_source=0.5739 / regression_loss_source=0.0503 / alpha=0.7723\n",
            "VALIDATION LOSS (Average) : 0.17233631014823914\n",
            "TEST LOSS (Average) : 0.16782272855440775\n",
            "Epoch [22/100] Step [1/192]: domain_loss_target=0.7390 / domain_loss_source=0.5725 / regression_loss_source=0.0584 / alpha=0.7818\n",
            "Epoch [22/100] Step [101/192]: domain_loss_target=0.7759 / domain_loss_source=0.5624 / regression_loss_source=0.0577 / alpha=0.7917\n",
            "VALIDATION LOSS (Average) : 0.1832609325647354\n",
            "TEST LOSS (Average) : 0.17953354120254517\n",
            "Epoch [23/100] Step [1/192]: domain_loss_target=0.7892 / domain_loss_source=0.5464 / regression_loss_source=0.0825 / alpha=0.8005\n",
            "Epoch [23/100] Step [101/192]: domain_loss_target=0.8119 / domain_loss_source=0.5902 / regression_loss_source=0.0723 / alpha=0.8097\n",
            "VALIDATION LOSS (Average) : 0.18651993572711945\n",
            "TEST LOSS (Average) : 0.18350571393966675\n",
            "Epoch [24/100] Step [1/192]: domain_loss_target=0.7347 / domain_loss_source=0.5610 / regression_loss_source=0.0606 / alpha=0.8178\n",
            "Epoch [24/100] Step [101/192]: domain_loss_target=0.7056 / domain_loss_source=0.5777 / regression_loss_source=0.0806 / alpha=0.8262\n",
            "VALIDATION LOSS (Average) : 0.18456673622131348\n",
            "TEST LOSS (Average) : 0.18160098294417062\n",
            "Epoch [25/100] Step [1/192]: domain_loss_target=0.7838 / domain_loss_source=0.5203 / regression_loss_source=0.0978 / alpha=0.8337\n",
            "Epoch [25/100] Step [101/192]: domain_loss_target=0.8174 / domain_loss_source=0.6173 / regression_loss_source=0.0201 / alpha=0.8414\n",
            "VALIDATION LOSS (Average) : 0.18289226293563843\n",
            "TEST LOSS (Average) : 0.17958158254623413\n",
            "Epoch [26/100] Step [1/192]: domain_loss_target=0.7851 / domain_loss_source=0.5076 / regression_loss_source=0.0947 / alpha=0.8483\n",
            "Epoch [26/100] Step [101/192]: domain_loss_target=0.7086 / domain_loss_source=0.6605 / regression_loss_source=0.0725 / alpha=0.8554\n",
            "VALIDATION LOSS (Average) : 0.18806256353855133\n",
            "TEST LOSS (Average) : 0.18515144288539886\n",
            "Epoch [27/100] Step [1/192]: domain_loss_target=0.7354 / domain_loss_source=0.6197 / regression_loss_source=0.0599 / alpha=0.8617\n",
            "Epoch [27/100] Step [101/192]: domain_loss_target=0.7716 / domain_loss_source=0.6081 / regression_loss_source=0.0656 / alpha=0.8683\n",
            "VALIDATION LOSS (Average) : 0.17421525716781616\n",
            "TEST LOSS (Average) : 0.17079002658526102\n",
            "Epoch [28/100] Step [1/192]: domain_loss_target=0.7348 / domain_loss_source=0.5964 / regression_loss_source=0.0904 / alpha=0.8741\n",
            "Epoch [28/100] Step [101/192]: domain_loss_target=0.7042 / domain_loss_source=0.6286 / regression_loss_source=0.0719 / alpha=0.8801\n",
            "VALIDATION LOSS (Average) : 0.1781223565340042\n",
            "TEST LOSS (Average) : 0.17384840548038483\n",
            "Epoch [29/100] Step [1/192]: domain_loss_target=0.7456 / domain_loss_source=0.4871 / regression_loss_source=0.1005 / alpha=0.8854\n",
            "Epoch [29/100] Step [101/192]: domain_loss_target=0.7258 / domain_loss_source=0.6099 / regression_loss_source=0.0525 / alpha=0.8909\n",
            "VALIDATION LOSS (Average) : 0.18835890293121338\n",
            "TEST LOSS (Average) : 0.18418674170970917\n",
            "Epoch [30/100] Step [1/192]: domain_loss_target=0.7406 / domain_loss_source=0.6388 / regression_loss_source=0.0749 / alpha=0.8957\n",
            "Epoch [30/100] Step [101/192]: domain_loss_target=0.7519 / domain_loss_source=0.6417 / regression_loss_source=0.1163 / alpha=0.9007\n",
            "VALIDATION LOSS (Average) : 0.18220695853233337\n",
            "TEST LOSS (Average) : 0.1784350872039795\n",
            "Epoch [31/100] Step [1/192]: domain_loss_target=0.6934 / domain_loss_source=0.5934 / regression_loss_source=0.0681 / alpha=0.9051\n",
            "Epoch [31/100] Step [101/192]: domain_loss_target=0.7654 / domain_loss_source=0.7884 / regression_loss_source=0.1023 / alpha=0.9097\n",
            "VALIDATION LOSS (Average) : 0.1903182715177536\n",
            "TEST LOSS (Average) : 0.18659460544586182\n",
            "Epoch [32/100] Step [1/192]: domain_loss_target=0.7209 / domain_loss_source=0.7186 / regression_loss_source=0.0733 / alpha=0.9138\n",
            "Epoch [32/100] Step [101/192]: domain_loss_target=0.7731 / domain_loss_source=0.6118 / regression_loss_source=0.0461 / alpha=0.9180\n",
            "VALIDATION LOSS (Average) : 0.1833314597606659\n",
            "TEST LOSS (Average) : 0.1805104911327362\n",
            "Epoch [33/100] Step [1/192]: domain_loss_target=0.7081 / domain_loss_source=0.7359 / regression_loss_source=0.0837 / alpha=0.9217\n",
            "Epoch [33/100] Step [101/192]: domain_loss_target=0.7650 / domain_loss_source=0.7471 / regression_loss_source=0.0453 / alpha=0.9255\n",
            "VALIDATION LOSS (Average) : 0.1870497614145279\n",
            "TEST LOSS (Average) : 0.18464528520902\n",
            "Epoch [34/100] Step [1/192]: domain_loss_target=0.6934 / domain_loss_source=0.7361 / regression_loss_source=0.1098 / alpha=0.9289\n",
            "Epoch [34/100] Step [101/192]: domain_loss_target=0.7770 / domain_loss_source=0.6628 / regression_loss_source=0.0438 / alpha=0.9323\n",
            "VALIDATION LOSS (Average) : 0.18513771891593933\n",
            "TEST LOSS (Average) : 0.18278612196445465\n",
            "Epoch [35/100] Step [1/192]: domain_loss_target=0.6634 / domain_loss_source=0.7708 / regression_loss_source=0.0749 / alpha=0.9354\n",
            "Epoch [35/100] Step [101/192]: domain_loss_target=0.7301 / domain_loss_source=0.7268 / regression_loss_source=0.0942 / alpha=0.9386\n",
            "VALIDATION LOSS (Average) : 0.18990838527679443\n",
            "TEST LOSS (Average) : 0.18744691709677377\n",
            "Epoch [36/100] Step [1/192]: domain_loss_target=0.6911 / domain_loss_source=0.7016 / regression_loss_source=0.0704 / alpha=0.9414\n",
            "Epoch [36/100] Step [101/192]: domain_loss_target=0.7093 / domain_loss_source=0.7286 / regression_loss_source=0.0684 / alpha=0.9443\n",
            "VALIDATION LOSS (Average) : 0.18719013035297394\n",
            "TEST LOSS (Average) : 0.1846843957901001\n",
            "Epoch [37/100] Step [1/192]: domain_loss_target=0.7039 / domain_loss_source=0.7285 / regression_loss_source=0.0803 / alpha=0.9468\n",
            "Epoch [37/100] Step [101/192]: domain_loss_target=0.6934 / domain_loss_source=0.6292 / regression_loss_source=0.0880 / alpha=0.9494\n",
            "VALIDATION LOSS (Average) : 0.1842612624168396\n",
            "TEST LOSS (Average) : 0.181041752298673\n",
            "Epoch [38/100] Step [1/192]: domain_loss_target=0.5924 / domain_loss_source=0.7001 / regression_loss_source=0.0917 / alpha=0.9517\n",
            "Epoch [38/100] Step [101/192]: domain_loss_target=0.6844 / domain_loss_source=0.6932 / regression_loss_source=0.1108 / alpha=0.9541\n",
            "VALIDATION LOSS (Average) : 0.19033750891685486\n",
            "TEST LOSS (Average) : 0.18726828197638193\n",
            "Epoch [39/100] Step [1/192]: domain_loss_target=0.6760 / domain_loss_source=0.7294 / regression_loss_source=0.0804 / alpha=0.9562\n",
            "Epoch [39/100] Step [101/192]: domain_loss_target=0.6588 / domain_loss_source=0.7252 / regression_loss_source=0.0566 / alpha=0.9584\n",
            "VALIDATION LOSS (Average) : 0.188284769654274\n",
            "TEST LOSS (Average) : 0.18579940994580588\n",
            "Epoch [40/100] Step [1/192]: domain_loss_target=0.7289 / domain_loss_source=0.7252 / regression_loss_source=0.0801 / alpha=0.9603\n",
            "Epoch [40/100] Step [101/192]: domain_loss_target=0.6589 / domain_loss_source=0.6676 / regression_loss_source=0.0498 / alpha=0.9623\n",
            "VALIDATION LOSS (Average) : 0.20285813510417938\n",
            "TEST LOSS (Average) : 0.20043368140856424\n",
            "Epoch [41/100] Step [1/192]: domain_loss_target=0.7116 / domain_loss_source=0.6923 / regression_loss_source=0.0487 / alpha=0.9640\n",
            "Epoch [41/100] Step [101/192]: domain_loss_target=0.6958 / domain_loss_source=0.7167 / regression_loss_source=0.0479 / alpha=0.9658\n",
            "VALIDATION LOSS (Average) : 0.1876061111688614\n",
            "TEST LOSS (Average) : 0.18472901483376822\n",
            "Epoch [42/100] Step [1/192]: domain_loss_target=0.6876 / domain_loss_source=0.6644 / regression_loss_source=0.0894 / alpha=0.9674\n",
            "Epoch [42/100] Step [101/192]: domain_loss_target=0.6759 / domain_loss_source=0.7042 / regression_loss_source=0.0339 / alpha=0.9690\n",
            "VALIDATION LOSS (Average) : 0.17283733189105988\n",
            "TEST LOSS (Average) : 0.16951236625512442\n",
            "Epoch [43/100] Step [1/192]: domain_loss_target=0.6704 / domain_loss_source=0.7393 / regression_loss_source=0.0908 / alpha=0.9705\n",
            "Epoch [43/100] Step [101/192]: domain_loss_target=0.6623 / domain_loss_source=0.6879 / regression_loss_source=0.0819 / alpha=0.9719\n",
            "VALIDATION LOSS (Average) : 0.18349802494049072\n",
            "TEST LOSS (Average) : 0.17989955842494965\n",
            "Epoch [44/100] Step [1/192]: domain_loss_target=0.6512 / domain_loss_source=0.6970 / regression_loss_source=0.0911 / alpha=0.9732\n",
            "Epoch [44/100] Step [101/192]: domain_loss_target=0.6764 / domain_loss_source=0.6977 / regression_loss_source=0.0771 / alpha=0.9746\n",
            "VALIDATION LOSS (Average) : 0.1893739253282547\n",
            "TEST LOSS (Average) : 0.1855566749970118\n",
            "Epoch [45/100] Step [1/192]: domain_loss_target=0.6512 / domain_loss_source=0.7096 / regression_loss_source=0.0598 / alpha=0.9757\n",
            "Epoch [45/100] Step [101/192]: domain_loss_target=0.6919 / domain_loss_source=0.6399 / regression_loss_source=0.0557 / alpha=0.9770\n",
            "VALIDATION LOSS (Average) : 0.19519661366939545\n",
            "TEST LOSS (Average) : 0.1920169492562612\n",
            "Epoch [46/100] Step [1/192]: domain_loss_target=0.5849 / domain_loss_source=0.7149 / regression_loss_source=0.0709 / alpha=0.9780\n",
            "Epoch [46/100] Step [101/192]: domain_loss_target=0.6563 / domain_loss_source=0.7396 / regression_loss_source=0.0597 / alpha=0.9791\n",
            "VALIDATION LOSS (Average) : 0.1897750198841095\n",
            "TEST LOSS (Average) : 0.1866598924001058\n",
            "Epoch [47/100] Step [1/192]: domain_loss_target=0.7141 / domain_loss_source=0.6678 / regression_loss_source=0.0840 / alpha=0.9801\n",
            "Epoch [47/100] Step [101/192]: domain_loss_target=0.6819 / domain_loss_source=0.7757 / regression_loss_source=0.0450 / alpha=0.9811\n",
            "VALIDATION LOSS (Average) : 0.19460539519786835\n",
            "TEST LOSS (Average) : 0.1920496622721354\n",
            "Epoch [48/100] Step [1/192]: domain_loss_target=0.6857 / domain_loss_source=0.6952 / regression_loss_source=0.0599 / alpha=0.9820\n",
            "Epoch [48/100] Step [101/192]: domain_loss_target=0.7036 / domain_loss_source=0.7011 / regression_loss_source=0.0647 / alpha=0.9829\n",
            "VALIDATION LOSS (Average) : 0.18633916974067688\n",
            "TEST LOSS (Average) : 0.18376964827378592\n",
            "Epoch [49/100] Step [1/192]: domain_loss_target=0.6106 / domain_loss_source=0.7006 / regression_loss_source=0.0594 / alpha=0.9837\n",
            "Epoch [49/100] Step [101/192]: domain_loss_target=0.6350 / domain_loss_source=0.7794 / regression_loss_source=0.0673 / alpha=0.9845\n",
            "VALIDATION LOSS (Average) : 0.18963080644607544\n",
            "TEST LOSS (Average) : 0.1869798352320989\n",
            "Epoch [50/100] Step [1/192]: domain_loss_target=0.6740 / domain_loss_source=0.7675 / regression_loss_source=0.0715 / alpha=0.9852\n",
            "Epoch [50/100] Step [101/192]: domain_loss_target=0.6618 / domain_loss_source=0.7321 / regression_loss_source=0.0255 / alpha=0.9860\n",
            "VALIDATION LOSS (Average) : 0.18712879717350006\n",
            "TEST LOSS (Average) : 0.18478585282961527\n",
            "Epoch [51/100] Step [1/192]: domain_loss_target=0.6667 / domain_loss_source=0.6933 / regression_loss_source=0.0704 / alpha=0.9866\n",
            "Epoch [51/100] Step [101/192]: domain_loss_target=0.6929 / domain_loss_source=0.6665 / regression_loss_source=0.1087 / alpha=0.9873\n",
            "VALIDATION LOSS (Average) : 0.20661087334156036\n",
            "TEST LOSS (Average) : 0.20479286213715872\n",
            "Epoch [52/100] Step [1/192]: domain_loss_target=0.6384 / domain_loss_source=0.7255 / regression_loss_source=0.1101 / alpha=0.9879\n",
            "Epoch [52/100] Step [101/192]: domain_loss_target=0.7256 / domain_loss_source=0.7356 / regression_loss_source=0.0838 / alpha=0.9885\n",
            "VALIDATION LOSS (Average) : 0.1783701479434967\n",
            "TEST LOSS (Average) : 0.17541983723640442\n",
            "Epoch [53/100] Step [1/192]: domain_loss_target=0.6832 / domain_loss_source=0.6907 / regression_loss_source=0.0750 / alpha=0.9890\n",
            "Epoch [53/100] Step [101/192]: domain_loss_target=0.7122 / domain_loss_source=0.6554 / regression_loss_source=0.0954 / alpha=0.9896\n",
            "VALIDATION LOSS (Average) : 0.179584339261055\n",
            "TEST LOSS (Average) : 0.17644579708576202\n",
            "Epoch [54/100] Step [1/192]: domain_loss_target=0.6686 / domain_loss_source=0.6613 / regression_loss_source=0.0676 / alpha=0.9901\n",
            "Epoch [54/100] Step [101/192]: domain_loss_target=0.6802 / domain_loss_source=0.7240 / regression_loss_source=0.0693 / alpha=0.9906\n",
            "VALIDATION LOSS (Average) : 0.18722370266914368\n",
            "TEST LOSS (Average) : 0.18375911811987558\n",
            "Epoch [55/100] Step [1/192]: domain_loss_target=0.6802 / domain_loss_source=0.6749 / regression_loss_source=0.0408 / alpha=0.9910\n",
            "Epoch [55/100] Step [101/192]: domain_loss_target=0.6897 / domain_loss_source=0.6773 / regression_loss_source=0.0325 / alpha=0.9915\n",
            "VALIDATION LOSS (Average) : 0.17612607777118683\n",
            "TEST LOSS (Average) : 0.17326297362645468\n",
            "Epoch [56/100] Step [1/192]: domain_loss_target=0.7286 / domain_loss_source=0.7056 / regression_loss_source=0.1145 / alpha=0.9919\n",
            "Epoch [56/100] Step [101/192]: domain_loss_target=0.7408 / domain_loss_source=0.6609 / regression_loss_source=0.0879 / alpha=0.9923\n",
            "VALIDATION LOSS (Average) : 0.1824728548526764\n",
            "TEST LOSS (Average) : 0.17967326939105988\n",
            "Epoch [57/100] Step [1/192]: domain_loss_target=0.7146 / domain_loss_source=0.7326 / regression_loss_source=0.0641 / alpha=0.9926\n",
            "Epoch [57/100] Step [101/192]: domain_loss_target=0.7000 / domain_loss_source=0.7714 / regression_loss_source=0.0509 / alpha=0.9930\n",
            "VALIDATION LOSS (Average) : 0.2046891450881958\n",
            "TEST LOSS (Average) : 0.20184202988942465\n",
            "Epoch [58/100] Step [1/192]: domain_loss_target=0.7111 / domain_loss_source=0.6868 / regression_loss_source=0.0655 / alpha=0.9933\n",
            "Epoch [58/100] Step [101/192]: domain_loss_target=0.6897 / domain_loss_source=0.7254 / regression_loss_source=0.0284 / alpha=0.9937\n",
            "VALIDATION LOSS (Average) : 0.1938437819480896\n",
            "TEST LOSS (Average) : 0.19140920539697012\n",
            "Epoch [59/100] Step [1/192]: domain_loss_target=0.6645 / domain_loss_source=0.7013 / regression_loss_source=0.0832 / alpha=0.9940\n",
            "Epoch [59/100] Step [101/192]: domain_loss_target=0.6534 / domain_loss_source=0.6489 / regression_loss_source=0.0673 / alpha=0.9943\n",
            "VALIDATION LOSS (Average) : 0.1905098557472229\n",
            "TEST LOSS (Average) : 0.18806150058905283\n",
            "Epoch [60/100] Step [1/192]: domain_loss_target=0.7506 / domain_loss_source=0.7128 / regression_loss_source=0.0684 / alpha=0.9945\n",
            "Epoch [60/100] Step [101/192]: domain_loss_target=0.6932 / domain_loss_source=0.6800 / regression_loss_source=0.0445 / alpha=0.9948\n",
            "VALIDATION LOSS (Average) : 0.20032747089862823\n",
            "TEST LOSS (Average) : 0.1976261387268702\n",
            "Epoch [61/100] Step [1/192]: domain_loss_target=0.7084 / domain_loss_source=0.6829 / regression_loss_source=0.0610 / alpha=0.9951\n",
            "Epoch [61/100] Step [101/192]: domain_loss_target=0.6717 / domain_loss_source=0.6751 / regression_loss_source=0.0762 / alpha=0.9953\n",
            "VALIDATION LOSS (Average) : 0.18876458704471588\n",
            "TEST LOSS (Average) : 0.18561742206414542\n",
            "Epoch [62/100] Step [1/192]: domain_loss_target=0.6763 / domain_loss_source=0.7264 / regression_loss_source=0.0678 / alpha=0.9955\n",
            "Epoch [62/100] Step [101/192]: domain_loss_target=0.7107 / domain_loss_source=0.6229 / regression_loss_source=0.0562 / alpha=0.9958\n",
            "VALIDATION LOSS (Average) : 0.19304946064949036\n",
            "TEST LOSS (Average) : 0.18946468830108643\n",
            "Epoch [63/100] Step [1/192]: domain_loss_target=0.7205 / domain_loss_source=0.7067 / regression_loss_source=0.0551 / alpha=0.9959\n",
            "Epoch [63/100] Step [101/192]: domain_loss_target=0.7354 / domain_loss_source=0.6670 / regression_loss_source=0.0565 / alpha=0.9962\n",
            "VALIDATION LOSS (Average) : 0.19076010584831238\n",
            "TEST LOSS (Average) : 0.18782929082711539\n",
            "Epoch [64/100] Step [1/192]: domain_loss_target=0.6851 / domain_loss_source=0.6720 / regression_loss_source=0.0805 / alpha=0.9963\n",
            "Epoch [64/100] Step [101/192]: domain_loss_target=0.7159 / domain_loss_source=0.6783 / regression_loss_source=0.0515 / alpha=0.9965\n",
            "VALIDATION LOSS (Average) : 0.21109507977962494\n",
            "TEST LOSS (Average) : 0.20854650934537253\n",
            "Epoch [65/100] Step [1/192]: domain_loss_target=0.6575 / domain_loss_source=0.7188 / regression_loss_source=0.0732 / alpha=0.9967\n",
            "Epoch [65/100] Step [101/192]: domain_loss_target=0.6668 / domain_loss_source=0.6704 / regression_loss_source=0.0657 / alpha=0.9969\n",
            "VALIDATION LOSS (Average) : 0.19311019778251648\n",
            "TEST LOSS (Average) : 0.19065364201863608\n",
            "Epoch [66/100] Step [1/192]: domain_loss_target=0.6858 / domain_loss_source=0.6206 / regression_loss_source=0.0545 / alpha=0.9970\n",
            "Epoch [66/100] Step [101/192]: domain_loss_target=0.6387 / domain_loss_source=0.7376 / regression_loss_source=0.0496 / alpha=0.9971\n",
            "VALIDATION LOSS (Average) : 0.20540589094161987\n",
            "TEST LOSS (Average) : 0.20219269394874573\n",
            "Epoch [67/100] Step [1/192]: domain_loss_target=0.6683 / domain_loss_source=0.6495 / regression_loss_source=0.0558 / alpha=0.9973\n",
            "Epoch [67/100] Step [101/192]: domain_loss_target=0.6564 / domain_loss_source=0.6333 / regression_loss_source=0.0667 / alpha=0.9974\n",
            "VALIDATION LOSS (Average) : 0.19680815935134888\n",
            "TEST LOSS (Average) : 0.1946994960308075\n",
            "Epoch [68/100] Step [1/192]: domain_loss_target=0.7125 / domain_loss_source=0.7434 / regression_loss_source=0.0806 / alpha=0.9975\n",
            "Epoch [68/100] Step [101/192]: domain_loss_target=0.6508 / domain_loss_source=0.7408 / regression_loss_source=0.0689 / alpha=0.9977\n",
            "VALIDATION LOSS (Average) : 0.1939592957496643\n",
            "TEST LOSS (Average) : 0.19230986138184866\n",
            "Epoch [69/100] Step [1/192]: domain_loss_target=0.6544 / domain_loss_source=0.7333 / regression_loss_source=0.0527 / alpha=0.9978\n",
            "Epoch [69/100] Step [101/192]: domain_loss_target=0.6577 / domain_loss_source=0.6779 / regression_loss_source=0.0498 / alpha=0.9979\n",
            "VALIDATION LOSS (Average) : 0.2055981606245041\n",
            "TEST LOSS (Average) : 0.20369766155878702\n",
            "Epoch [70/100] Step [1/192]: domain_loss_target=0.6867 / domain_loss_source=0.6775 / regression_loss_source=0.0927 / alpha=0.9980\n",
            "Epoch [70/100] Step [101/192]: domain_loss_target=0.7209 / domain_loss_source=0.7425 / regression_loss_source=0.0793 / alpha=0.9981\n",
            "VALIDATION LOSS (Average) : 0.18643490970134735\n",
            "TEST LOSS (Average) : 0.18339658776919046\n",
            "Epoch [71/100] Step [1/192]: domain_loss_target=0.6479 / domain_loss_source=0.7115 / regression_loss_source=0.0537 / alpha=0.9982\n",
            "Epoch [71/100] Step [101/192]: domain_loss_target=0.6378 / domain_loss_source=0.6646 / regression_loss_source=0.0363 / alpha=0.9983\n",
            "VALIDATION LOSS (Average) : 0.19149577617645264\n",
            "TEST LOSS (Average) : 0.1881173551082611\n",
            "Epoch [72/100] Step [1/192]: domain_loss_target=0.7091 / domain_loss_source=0.7195 / regression_loss_source=0.0671 / alpha=0.9984\n",
            "Epoch [72/100] Step [101/192]: domain_loss_target=0.6514 / domain_loss_source=0.6830 / regression_loss_source=0.0605 / alpha=0.9984\n",
            "VALIDATION LOSS (Average) : 0.19737645983695984\n",
            "TEST LOSS (Average) : 0.1945041666428248\n",
            "Epoch [73/100] Step [1/192]: domain_loss_target=0.7080 / domain_loss_source=0.7266 / regression_loss_source=0.0486 / alpha=0.9985\n",
            "Epoch [73/100] Step [101/192]: domain_loss_target=0.6989 / domain_loss_source=0.7022 / regression_loss_source=0.0598 / alpha=0.9986\n",
            "VALIDATION LOSS (Average) : 0.18235789239406586\n",
            "TEST LOSS (Average) : 0.17958217859268188\n",
            "Epoch [74/100] Step [1/192]: domain_loss_target=0.7098 / domain_loss_source=0.6833 / regression_loss_source=0.0571 / alpha=0.9986\n",
            "Epoch [74/100] Step [101/192]: domain_loss_target=0.6752 / domain_loss_source=0.6594 / regression_loss_source=0.0346 / alpha=0.9987\n",
            "VALIDATION LOSS (Average) : 0.19322039186954498\n",
            "TEST LOSS (Average) : 0.19043973088264465\n",
            "Epoch [75/100] Step [1/192]: domain_loss_target=0.6920 / domain_loss_source=0.6888 / regression_loss_source=0.0521 / alpha=0.9988\n",
            "Epoch [75/100] Step [101/192]: domain_loss_target=0.6871 / domain_loss_source=0.6981 / regression_loss_source=0.0564 / alpha=0.9988\n",
            "VALIDATION LOSS (Average) : 0.19341245293617249\n",
            "TEST LOSS (Average) : 0.1902080923318863\n",
            "Epoch [76/100] Step [1/192]: domain_loss_target=0.6390 / domain_loss_source=0.6219 / regression_loss_source=0.0953 / alpha=0.9989\n",
            "Epoch [76/100] Step [101/192]: domain_loss_target=0.7158 / domain_loss_source=0.7411 / regression_loss_source=0.0357 / alpha=0.9990\n",
            "VALIDATION LOSS (Average) : 0.1914851814508438\n",
            "TEST LOSS (Average) : 0.1883788655201594\n",
            "Epoch [77/100] Step [1/192]: domain_loss_target=0.7404 / domain_loss_source=0.6894 / regression_loss_source=0.0969 / alpha=0.9990\n",
            "Epoch [77/100] Step [101/192]: domain_loss_target=0.6923 / domain_loss_source=0.6889 / regression_loss_source=0.0528 / alpha=0.9991\n",
            "VALIDATION LOSS (Average) : 0.1938350349664688\n",
            "TEST LOSS (Average) : 0.19110807279745737\n",
            "Epoch [78/100] Step [1/192]: domain_loss_target=0.7048 / domain_loss_source=0.6770 / regression_loss_source=0.0541 / alpha=0.9991\n",
            "Epoch [78/100] Step [101/192]: domain_loss_target=0.6958 / domain_loss_source=0.6814 / regression_loss_source=0.0568 / alpha=0.9991\n",
            "VALIDATION LOSS (Average) : 0.1992473006248474\n",
            "TEST LOSS (Average) : 0.19698854287465414\n",
            "Epoch [79/100] Step [1/192]: domain_loss_target=0.6273 / domain_loss_source=0.7753 / regression_loss_source=0.0601 / alpha=0.9992\n",
            "Epoch [79/100] Step [101/192]: domain_loss_target=0.6093 / domain_loss_source=0.6804 / regression_loss_source=0.0692 / alpha=0.9992\n",
            "VALIDATION LOSS (Average) : 0.19037841260433197\n",
            "TEST LOSS (Average) : 0.18805896242459616\n",
            "Epoch [80/100] Step [1/192]: domain_loss_target=0.6145 / domain_loss_source=0.6863 / regression_loss_source=0.0513 / alpha=0.9993\n",
            "Epoch [80/100] Step [101/192]: domain_loss_target=0.6700 / domain_loss_source=0.6899 / regression_loss_source=0.0388 / alpha=0.9993\n",
            "VALIDATION LOSS (Average) : 0.1929771602153778\n",
            "TEST LOSS (Average) : 0.19077247381210327\n",
            "Epoch [81/100] Step [1/192]: domain_loss_target=0.6787 / domain_loss_source=0.6809 / regression_loss_source=0.0718 / alpha=0.9993\n",
            "Epoch [81/100] Step [101/192]: domain_loss_target=0.7126 / domain_loss_source=0.6626 / regression_loss_source=0.1098 / alpha=0.9994\n",
            "VALIDATION LOSS (Average) : 0.2021102011203766\n",
            "TEST LOSS (Average) : 0.19992933173974356\n",
            "Epoch [82/100] Step [1/192]: domain_loss_target=0.7106 / domain_loss_source=0.7076 / regression_loss_source=0.0702 / alpha=0.9994\n",
            "Epoch [82/100] Step [101/192]: domain_loss_target=0.6935 / domain_loss_source=0.6976 / regression_loss_source=0.0840 / alpha=0.9994\n",
            "VALIDATION LOSS (Average) : 0.20071250200271606\n",
            "TEST LOSS (Average) : 0.1980318675438563\n",
            "Epoch [83/100] Step [1/192]: domain_loss_target=0.6732 / domain_loss_source=0.7181 / regression_loss_source=0.0564 / alpha=0.9995\n",
            "Epoch [83/100] Step [101/192]: domain_loss_target=0.6860 / domain_loss_source=0.7015 / regression_loss_source=0.0651 / alpha=0.9995\n",
            "VALIDATION LOSS (Average) : 0.19477646052837372\n",
            "TEST LOSS (Average) : 0.19214798510074615\n",
            "Epoch [84/100] Step [1/192]: domain_loss_target=0.6911 / domain_loss_source=0.7171 / regression_loss_source=0.1009 / alpha=0.9995\n",
            "Epoch [84/100] Step [101/192]: domain_loss_target=0.6906 / domain_loss_source=0.6973 / regression_loss_source=0.0482 / alpha=0.9995\n",
            "VALIDATION LOSS (Average) : 0.17881280183792114\n",
            "TEST LOSS (Average) : 0.17674767474333444\n",
            "Epoch [85/100] Step [1/192]: domain_loss_target=0.6874 / domain_loss_source=0.7070 / regression_loss_source=0.0735 / alpha=0.9996\n",
            "Epoch [85/100] Step [101/192]: domain_loss_target=0.6929 / domain_loss_source=0.6635 / regression_loss_source=0.0852 / alpha=0.9996\n",
            "VALIDATION LOSS (Average) : 0.20913346111774445\n",
            "TEST LOSS (Average) : 0.20684818426767984\n",
            "Epoch [86/100] Step [1/192]: domain_loss_target=0.6936 / domain_loss_source=0.7388 / regression_loss_source=0.0766 / alpha=0.9996\n",
            "Epoch [86/100] Step [101/192]: domain_loss_target=0.6531 / domain_loss_source=0.6406 / regression_loss_source=0.0614 / alpha=0.9996\n",
            "VALIDATION LOSS (Average) : 0.18901169300079346\n",
            "TEST LOSS (Average) : 0.18750187754631042\n",
            "Epoch [87/100] Step [1/192]: domain_loss_target=0.6685 / domain_loss_source=0.6842 / regression_loss_source=0.0753 / alpha=0.9996\n",
            "Epoch [87/100] Step [101/192]: domain_loss_target=0.5824 / domain_loss_source=0.7526 / regression_loss_source=0.0655 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.20575986802577972\n",
            "TEST LOSS (Average) : 0.2043036768833796\n",
            "Epoch [88/100] Step [1/192]: domain_loss_target=0.6776 / domain_loss_source=0.6843 / regression_loss_source=0.0412 / alpha=0.9997\n",
            "Epoch [88/100] Step [101/192]: domain_loss_target=0.6795 / domain_loss_source=0.7614 / regression_loss_source=0.0747 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.20042572915554047\n",
            "TEST LOSS (Average) : 0.1986516167720159\n",
            "Epoch [89/100] Step [1/192]: domain_loss_target=0.7011 / domain_loss_source=0.6284 / regression_loss_source=0.0426 / alpha=0.9997\n",
            "Epoch [89/100] Step [101/192]: domain_loss_target=0.6806 / domain_loss_source=0.7011 / regression_loss_source=0.0449 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.19088436663150787\n",
            "TEST LOSS (Average) : 0.18914933502674103\n",
            "Epoch [90/100] Step [1/192]: domain_loss_target=0.7247 / domain_loss_source=0.6861 / regression_loss_source=0.0722 / alpha=0.9997\n",
            "Epoch [90/100] Step [101/192]: domain_loss_target=0.6863 / domain_loss_source=0.7002 / regression_loss_source=0.0503 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.19319665431976318\n",
            "TEST LOSS (Average) : 0.19199201464653015\n",
            "Epoch [91/100] Step [1/192]: domain_loss_target=0.7093 / domain_loss_source=0.6853 / regression_loss_source=0.0745 / alpha=0.9998\n",
            "Epoch [91/100] Step [101/192]: domain_loss_target=0.6843 / domain_loss_source=0.7017 / regression_loss_source=0.0544 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.18862605094909668\n",
            "TEST LOSS (Average) : 0.18736731012662253\n",
            "Epoch [92/100] Step [1/192]: domain_loss_target=0.6664 / domain_loss_source=0.6659 / regression_loss_source=0.1002 / alpha=0.9998\n",
            "Epoch [92/100] Step [101/192]: domain_loss_target=0.7012 / domain_loss_source=0.6572 / regression_loss_source=0.0510 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.188691645860672\n",
            "TEST LOSS (Average) : 0.18704823156197867\n",
            "Epoch [93/100] Step [1/192]: domain_loss_target=0.6978 / domain_loss_source=0.6994 / regression_loss_source=0.0594 / alpha=0.9998\n",
            "Epoch [93/100] Step [101/192]: domain_loss_target=0.6494 / domain_loss_source=0.7367 / regression_loss_source=0.0722 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.19773593544960022\n",
            "TEST LOSS (Average) : 0.19562425712744394\n",
            "Epoch [94/100] Step [1/192]: domain_loss_target=0.6718 / domain_loss_source=0.7256 / regression_loss_source=0.0527 / alpha=0.9998\n",
            "Epoch [94/100] Step [101/192]: domain_loss_target=0.6696 / domain_loss_source=0.7236 / regression_loss_source=0.0730 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.18823398649692535\n",
            "TEST LOSS (Average) : 0.18591890732447305\n",
            "Epoch [95/100] Step [1/192]: domain_loss_target=0.6867 / domain_loss_source=0.7497 / regression_loss_source=0.0296 / alpha=0.9998\n",
            "Epoch [95/100] Step [101/192]: domain_loss_target=0.7068 / domain_loss_source=0.6797 / regression_loss_source=0.0560 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.19149184226989746\n",
            "TEST LOSS (Average) : 0.18944243590037027\n",
            "Epoch [96/100] Step [1/192]: domain_loss_target=0.6893 / domain_loss_source=0.7425 / regression_loss_source=0.0694 / alpha=0.9999\n",
            "Epoch [96/100] Step [101/192]: domain_loss_target=0.7038 / domain_loss_source=0.6797 / regression_loss_source=0.0716 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.21004003286361694\n",
            "TEST LOSS (Average) : 0.2080646107594172\n",
            "Epoch [97/100] Step [1/192]: domain_loss_target=0.6951 / domain_loss_source=0.7241 / regression_loss_source=0.0504 / alpha=0.9999\n",
            "Epoch [97/100] Step [101/192]: domain_loss_target=0.6698 / domain_loss_source=0.7071 / regression_loss_source=0.0772 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.1955738514661789\n",
            "TEST LOSS (Average) : 0.19414873917897543\n",
            "Epoch [98/100] Step [1/192]: domain_loss_target=0.7046 / domain_loss_source=0.6253 / regression_loss_source=0.1052 / alpha=0.9999\n",
            "Epoch [98/100] Step [101/192]: domain_loss_target=0.6798 / domain_loss_source=0.7064 / regression_loss_source=0.1103 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.1976054608821869\n",
            "TEST LOSS (Average) : 0.19571473201115927\n",
            "Epoch [99/100] Step [1/192]: domain_loss_target=0.7008 / domain_loss_source=0.7210 / regression_loss_source=0.0755 / alpha=0.9999\n",
            "Epoch [99/100] Step [101/192]: domain_loss_target=0.7310 / domain_loss_source=0.6445 / regression_loss_source=0.0389 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.19944053888320923\n",
            "TEST LOSS (Average) : 0.19720777869224548\n",
            "Epoch [100/100] Step [1/192]: domain_loss_target=0.6768 / domain_loss_source=0.7043 / regression_loss_source=0.0935 / alpha=0.9999\n",
            "Epoch [100/100] Step [101/192]: domain_loss_target=0.7132 / domain_loss_source=0.6784 / regression_loss_source=0.0824 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.20096662640571594\n",
            "TEST LOSS (Average) : 0.1989594300587972\n",
            "----------------------training complete for DANN model - EI_sadness-----------------\n",
            "----------------------training started for DANN model - EI_anger-----------------\n",
            "Epoch [1/100] Step [1/213]: domain_loss_target=0.6708 / domain_loss_source=0.7074 / regression_loss_source=0.1039 / alpha=0.0000\n",
            "Epoch [1/100] Step [101/213]: domain_loss_target=0.7313 / domain_loss_source=0.5573 / regression_loss_source=0.0533 / alpha=0.0235\n",
            "Epoch [1/100] Step [201/213]: domain_loss_target=0.7465 / domain_loss_source=0.4387 / regression_loss_source=0.0570 / alpha=0.0469\n",
            "VALIDATION LOSS (Average) : 0.16224472224712372\n",
            "TEST LOSS (Average) : 0.15532247722148895\n",
            "Epoch [2/100] Step [1/213]: domain_loss_target=0.7469 / domain_loss_source=0.3692 / regression_loss_source=0.0380 / alpha=0.0500\n",
            "Epoch [2/100] Step [101/213]: domain_loss_target=0.7435 / domain_loss_source=0.5321 / regression_loss_source=0.0535 / alpha=0.0733\n",
            "Epoch [2/100] Step [201/213]: domain_loss_target=0.6883 / domain_loss_source=0.3439 / regression_loss_source=0.0738 / alpha=0.0966\n",
            "VALIDATION LOSS (Average) : 0.16956639289855957\n",
            "TEST LOSS (Average) : 0.16314053038756052\n",
            "Epoch [3/100] Step [1/213]: domain_loss_target=0.6952 / domain_loss_source=0.4601 / regression_loss_source=0.1042 / alpha=0.0997\n",
            "Epoch [3/100] Step [101/213]: domain_loss_target=0.6431 / domain_loss_source=0.4050 / regression_loss_source=0.0705 / alpha=0.1229\n",
            "Epoch [3/100] Step [201/213]: domain_loss_target=0.7280 / domain_loss_source=0.2675 / regression_loss_source=0.0945 / alpha=0.1459\n",
            "VALIDATION LOSS (Average) : 0.1705394983291626\n",
            "TEST LOSS (Average) : 0.1641714721918106\n",
            "Epoch [4/100] Step [1/213]: domain_loss_target=0.7226 / domain_loss_source=0.3013 / regression_loss_source=0.1133 / alpha=0.1489\n",
            "Epoch [4/100] Step [101/213]: domain_loss_target=0.7513 / domain_loss_source=0.3288 / regression_loss_source=0.1156 / alpha=0.1718\n",
            "Epoch [4/100] Step [201/213]: domain_loss_target=0.7052 / domain_loss_source=0.4887 / regression_loss_source=0.0641 / alpha=0.1944\n",
            "VALIDATION LOSS (Average) : 0.16655246913433075\n",
            "TEST LOSS (Average) : 0.15964033206303915\n",
            "Epoch [5/100] Step [1/213]: domain_loss_target=0.6915 / domain_loss_source=0.2831 / regression_loss_source=0.0814 / alpha=0.1974\n",
            "Epoch [5/100] Step [101/213]: domain_loss_target=0.8365 / domain_loss_source=0.3805 / regression_loss_source=0.0582 / alpha=0.2198\n",
            "Epoch [5/100] Step [201/213]: domain_loss_target=0.8453 / domain_loss_source=0.4327 / regression_loss_source=0.0576 / alpha=0.2420\n",
            "VALIDATION LOSS (Average) : 0.16362442076206207\n",
            "TEST LOSS (Average) : 0.15769324700037637\n",
            "Epoch [6/100] Step [1/213]: domain_loss_target=0.8218 / domain_loss_source=0.3600 / regression_loss_source=0.0568 / alpha=0.2449\n",
            "Epoch [6/100] Step [101/213]: domain_loss_target=0.8533 / domain_loss_source=0.3642 / regression_loss_source=0.0734 / alpha=0.2669\n",
            "Epoch [6/100] Step [201/213]: domain_loss_target=0.8523 / domain_loss_source=0.3568 / regression_loss_source=0.0686 / alpha=0.2885\n",
            "VALIDATION LOSS (Average) : 0.15704433619976044\n",
            "TEST LOSS (Average) : 0.1513034850358963\n",
            "Epoch [7/100] Step [1/213]: domain_loss_target=0.8013 / domain_loss_source=0.5686 / regression_loss_source=0.0656 / alpha=0.2913\n",
            "Epoch [7/100] Step [101/213]: domain_loss_target=0.7071 / domain_loss_source=0.3724 / regression_loss_source=0.0534 / alpha=0.3126\n",
            "Epoch [7/100] Step [201/213]: domain_loss_target=0.8646 / domain_loss_source=0.4302 / regression_loss_source=0.1223 / alpha=0.3337\n",
            "VALIDATION LOSS (Average) : 0.17143359780311584\n",
            "TEST LOSS (Average) : 0.16646820306777954\n",
            "Epoch [8/100] Step [1/213]: domain_loss_target=0.7380 / domain_loss_source=0.4027 / regression_loss_source=0.0358 / alpha=0.3364\n",
            "Epoch [8/100] Step [101/213]: domain_loss_target=0.8657 / domain_loss_source=0.3829 / regression_loss_source=0.0687 / alpha=0.3570\n",
            "Epoch [8/100] Step [201/213]: domain_loss_target=0.8136 / domain_loss_source=0.4722 / regression_loss_source=0.0534 / alpha=0.3773\n",
            "VALIDATION LOSS (Average) : 0.17564159631729126\n",
            "TEST LOSS (Average) : 0.17106035351753235\n",
            "Epoch [9/100] Step [1/213]: domain_loss_target=0.7723 / domain_loss_source=0.5774 / regression_loss_source=0.0522 / alpha=0.3799\n",
            "Epoch [9/100] Step [101/213]: domain_loss_target=0.7811 / domain_loss_source=0.3619 / regression_loss_source=0.1129 / alpha=0.3999\n",
            "Epoch [9/100] Step [201/213]: domain_loss_target=0.8573 / domain_loss_source=0.3900 / regression_loss_source=0.0703 / alpha=0.4194\n",
            "VALIDATION LOSS (Average) : 0.16837969422340393\n",
            "TEST LOSS (Average) : 0.16261694331963858\n",
            "Epoch [10/100] Step [1/213]: domain_loss_target=0.7196 / domain_loss_source=0.3847 / regression_loss_source=0.0616 / alpha=0.4219\n",
            "Epoch [10/100] Step [101/213]: domain_loss_target=0.6475 / domain_loss_source=0.4522 / regression_loss_source=0.0736 / alpha=0.4410\n",
            "Epoch [10/100] Step [201/213]: domain_loss_target=0.8992 / domain_loss_source=0.7468 / regression_loss_source=0.0819 / alpha=0.4597\n",
            "VALIDATION LOSS (Average) : 0.1659664660692215\n",
            "TEST LOSS (Average) : 0.15986183285713196\n",
            "Epoch [11/100] Step [1/213]: domain_loss_target=0.7068 / domain_loss_source=0.4546 / regression_loss_source=0.0736 / alpha=0.4621\n",
            "Epoch [11/100] Step [101/213]: domain_loss_target=0.7607 / domain_loss_source=0.4449 / regression_loss_source=0.0730 / alpha=0.4804\n",
            "Epoch [11/100] Step [201/213]: domain_loss_target=0.8532 / domain_loss_source=0.4165 / regression_loss_source=0.0539 / alpha=0.4982\n",
            "VALIDATION LOSS (Average) : 0.1737070381641388\n",
            "TEST LOSS (Average) : 0.16831287244955698\n",
            "Epoch [12/100] Step [1/213]: domain_loss_target=0.6954 / domain_loss_source=0.4505 / regression_loss_source=0.0847 / alpha=0.5005\n",
            "Epoch [12/100] Step [101/213]: domain_loss_target=0.6754 / domain_loss_source=0.5539 / regression_loss_source=0.0972 / alpha=0.5179\n",
            "Epoch [12/100] Step [201/213]: domain_loss_target=0.8285 / domain_loss_source=0.5280 / regression_loss_source=0.0446 / alpha=0.5349\n",
            "VALIDATION LOSS (Average) : 0.17064933478832245\n",
            "TEST LOSS (Average) : 0.16636932889620462\n",
            "Epoch [13/100] Step [1/213]: domain_loss_target=0.7982 / domain_loss_source=0.5564 / regression_loss_source=0.0646 / alpha=0.5370\n",
            "Epoch [13/100] Step [101/213]: domain_loss_target=0.5966 / domain_loss_source=0.4069 / regression_loss_source=0.0856 / alpha=0.5535\n",
            "Epoch [13/100] Step [201/213]: domain_loss_target=0.6153 / domain_loss_source=0.5756 / regression_loss_source=0.0533 / alpha=0.5696\n",
            "VALIDATION LOSS (Average) : 0.16310159862041473\n",
            "TEST LOSS (Average) : 0.1584316243728002\n",
            "Epoch [14/100] Step [1/213]: domain_loss_target=0.6429 / domain_loss_source=0.6611 / regression_loss_source=0.0559 / alpha=0.5717\n",
            "Epoch [14/100] Step [101/213]: domain_loss_target=0.7142 / domain_loss_source=0.7718 / regression_loss_source=0.0605 / alpha=0.5873\n",
            "Epoch [14/100] Step [201/213]: domain_loss_target=0.7837 / domain_loss_source=0.5741 / regression_loss_source=0.0547 / alpha=0.6024\n",
            "VALIDATION LOSS (Average) : 0.16213491559028625\n",
            "TEST LOSS (Average) : 0.15719098846117655\n",
            "Epoch [15/100] Step [1/213]: domain_loss_target=0.7146 / domain_loss_source=0.7571 / regression_loss_source=0.0477 / alpha=0.6044\n",
            "Epoch [15/100] Step [101/213]: domain_loss_target=0.7755 / domain_loss_source=0.5952 / regression_loss_source=0.0439 / alpha=0.6191\n",
            "Epoch [15/100] Step [201/213]: domain_loss_target=0.5919 / domain_loss_source=0.5352 / regression_loss_source=0.0681 / alpha=0.6333\n",
            "VALIDATION LOSS (Average) : 0.16835862398147583\n",
            "TEST LOSS (Average) : 0.16354929904143015\n",
            "Epoch [16/100] Step [1/213]: domain_loss_target=0.7224 / domain_loss_source=0.6650 / regression_loss_source=0.0418 / alpha=0.6351\n",
            "Epoch [16/100] Step [101/213]: domain_loss_target=0.6114 / domain_loss_source=0.7516 / regression_loss_source=0.0884 / alpha=0.6489\n",
            "Epoch [16/100] Step [201/213]: domain_loss_target=0.6484 / domain_loss_source=0.5226 / regression_loss_source=0.1046 / alpha=0.6623\n",
            "VALIDATION LOSS (Average) : 0.17435316741466522\n",
            "TEST LOSS (Average) : 0.17040019234021506\n",
            "Epoch [17/100] Step [1/213]: domain_loss_target=0.6605 / domain_loss_source=0.7202 / regression_loss_source=0.0564 / alpha=0.6640\n",
            "Epoch [17/100] Step [101/213]: domain_loss_target=0.6721 / domain_loss_source=0.6760 / regression_loss_source=0.0730 / alpha=0.6770\n",
            "Epoch [17/100] Step [201/213]: domain_loss_target=0.6009 / domain_loss_source=0.6626 / regression_loss_source=0.0883 / alpha=0.6895\n",
            "VALIDATION LOSS (Average) : 0.16384273767471313\n",
            "TEST LOSS (Average) : 0.15969370305538177\n",
            "Epoch [18/100] Step [1/213]: domain_loss_target=0.6939 / domain_loss_source=0.5924 / regression_loss_source=0.0538 / alpha=0.6911\n",
            "Epoch [18/100] Step [101/213]: domain_loss_target=0.6739 / domain_loss_source=0.7172 / regression_loss_source=0.0492 / alpha=0.7031\n",
            "Epoch [18/100] Step [201/213]: domain_loss_target=0.6586 / domain_loss_source=0.7017 / regression_loss_source=0.1045 / alpha=0.7148\n",
            "VALIDATION LOSS (Average) : 0.17057561874389648\n",
            "TEST LOSS (Average) : 0.16702492535114288\n",
            "Epoch [19/100] Step [1/213]: domain_loss_target=0.6306 / domain_loss_source=0.6208 / regression_loss_source=0.0798 / alpha=0.7163\n",
            "Epoch [19/100] Step [101/213]: domain_loss_target=0.6869 / domain_loss_source=0.6898 / regression_loss_source=0.0961 / alpha=0.7275\n",
            "Epoch [19/100] Step [201/213]: domain_loss_target=0.6338 / domain_loss_source=0.9446 / regression_loss_source=0.1019 / alpha=0.7384\n",
            "VALIDATION LOSS (Average) : 0.16255713999271393\n",
            "TEST LOSS (Average) : 0.1576707363128662\n",
            "Epoch [20/100] Step [1/213]: domain_loss_target=0.6725 / domain_loss_source=0.6133 / regression_loss_source=0.0553 / alpha=0.7398\n",
            "Epoch [20/100] Step [101/213]: domain_loss_target=0.6897 / domain_loss_source=0.5700 / regression_loss_source=0.0914 / alpha=0.7502\n",
            "Epoch [20/100] Step [201/213]: domain_loss_target=0.6375 / domain_loss_source=0.7902 / regression_loss_source=0.1082 / alpha=0.7603\n",
            "VALIDATION LOSS (Average) : 0.1685985028743744\n",
            "TEST LOSS (Average) : 0.164287139972051\n",
            "Epoch [21/100] Step [1/213]: domain_loss_target=0.6738 / domain_loss_source=0.5302 / regression_loss_source=0.0725 / alpha=0.7616\n",
            "Epoch [21/100] Step [101/213]: domain_loss_target=0.7376 / domain_loss_source=0.7021 / regression_loss_source=0.0928 / alpha=0.7713\n",
            "Epoch [21/100] Step [201/213]: domain_loss_target=0.6415 / domain_loss_source=0.7882 / regression_loss_source=0.0670 / alpha=0.7806\n",
            "VALIDATION LOSS (Average) : 0.16627274453639984\n",
            "TEST LOSS (Average) : 0.16181780894597372\n",
            "Epoch [22/100] Step [1/213]: domain_loss_target=0.6083 / domain_loss_source=0.7531 / regression_loss_source=0.0648 / alpha=0.7818\n",
            "Epoch [22/100] Step [101/213]: domain_loss_target=0.5948 / domain_loss_source=0.7744 / regression_loss_source=0.0607 / alpha=0.7908\n",
            "Epoch [22/100] Step [201/213]: domain_loss_target=0.7022 / domain_loss_source=0.8456 / regression_loss_source=0.0459 / alpha=0.7994\n",
            "VALIDATION LOSS (Average) : 0.16672566533088684\n",
            "TEST LOSS (Average) : 0.16282534102598825\n",
            "Epoch [23/100] Step [1/213]: domain_loss_target=0.6722 / domain_loss_source=0.7509 / regression_loss_source=0.0641 / alpha=0.8005\n",
            "Epoch [23/100] Step [101/213]: domain_loss_target=0.6135 / domain_loss_source=0.7474 / regression_loss_source=0.0965 / alpha=0.8088\n",
            "Epoch [23/100] Step [201/213]: domain_loss_target=0.6991 / domain_loss_source=0.7591 / regression_loss_source=0.0921 / alpha=0.8167\n",
            "VALIDATION LOSS (Average) : 0.16781853139400482\n",
            "TEST LOSS (Average) : 0.16323894759019217\n",
            "Epoch [24/100] Step [1/213]: domain_loss_target=0.7444 / domain_loss_source=0.8890 / regression_loss_source=0.0896 / alpha=0.8178\n",
            "Epoch [24/100] Step [101/213]: domain_loss_target=0.5960 / domain_loss_source=0.8357 / regression_loss_source=0.0845 / alpha=0.8254\n",
            "Epoch [24/100] Step [201/213]: domain_loss_target=0.7604 / domain_loss_source=0.6225 / regression_loss_source=0.0909 / alpha=0.8327\n",
            "VALIDATION LOSS (Average) : 0.1751515418291092\n",
            "TEST LOSS (Average) : 0.17031810184319815\n",
            "Epoch [25/100] Step [1/213]: domain_loss_target=0.6312 / domain_loss_source=0.8014 / regression_loss_source=0.1170 / alpha=0.8337\n",
            "Epoch [25/100] Step [101/213]: domain_loss_target=0.6381 / domain_loss_source=0.8335 / regression_loss_source=0.0251 / alpha=0.8407\n",
            "Epoch [25/100] Step [201/213]: domain_loss_target=0.6603 / domain_loss_source=0.7727 / regression_loss_source=0.1223 / alpha=0.8474\n",
            "VALIDATION LOSS (Average) : 0.16855710744857788\n",
            "TEST LOSS (Average) : 0.16312154630819956\n",
            "Epoch [26/100] Step [1/213]: domain_loss_target=0.6712 / domain_loss_source=0.7881 / regression_loss_source=0.0584 / alpha=0.8483\n",
            "Epoch [26/100] Step [101/213]: domain_loss_target=0.6515 / domain_loss_source=0.8381 / regression_loss_source=0.0594 / alpha=0.8547\n",
            "Epoch [26/100] Step [201/213]: domain_loss_target=0.6541 / domain_loss_source=0.8057 / regression_loss_source=0.0503 / alpha=0.8609\n",
            "VALIDATION LOSS (Average) : 0.17229720950126648\n",
            "TEST LOSS (Average) : 0.16772347191969553\n",
            "Epoch [27/100] Step [1/213]: domain_loss_target=0.6627 / domain_loss_source=0.8181 / regression_loss_source=0.0513 / alpha=0.8617\n",
            "Epoch [27/100] Step [101/213]: domain_loss_target=0.7421 / domain_loss_source=0.7351 / regression_loss_source=0.0384 / alpha=0.8676\n",
            "Epoch [27/100] Step [201/213]: domain_loss_target=0.6093 / domain_loss_source=0.7560 / regression_loss_source=0.0411 / alpha=0.8733\n",
            "VALIDATION LOSS (Average) : 0.16136397421360016\n",
            "TEST LOSS (Average) : 0.15554146965344748\n",
            "Epoch [28/100] Step [1/213]: domain_loss_target=0.6612 / domain_loss_source=0.7759 / regression_loss_source=0.0621 / alpha=0.8741\n",
            "Epoch [28/100] Step [101/213]: domain_loss_target=0.6189 / domain_loss_source=0.8464 / regression_loss_source=0.0765 / alpha=0.8795\n",
            "Epoch [28/100] Step [201/213]: domain_loss_target=0.7045 / domain_loss_source=0.8198 / regression_loss_source=0.0875 / alpha=0.8847\n",
            "VALIDATION LOSS (Average) : 0.17108385264873505\n",
            "TEST LOSS (Average) : 0.16606861849625906\n",
            "Epoch [29/100] Step [1/213]: domain_loss_target=0.6097 / domain_loss_source=0.7400 / regression_loss_source=0.1018 / alpha=0.8854\n",
            "Epoch [29/100] Step [101/213]: domain_loss_target=0.6246 / domain_loss_source=0.7864 / regression_loss_source=0.0736 / alpha=0.8903\n",
            "Epoch [29/100] Step [201/213]: domain_loss_target=0.6276 / domain_loss_source=0.7350 / regression_loss_source=0.0658 / alpha=0.8951\n",
            "VALIDATION LOSS (Average) : 0.16384337842464447\n",
            "TEST LOSS (Average) : 0.15794319411118826\n",
            "Epoch [30/100] Step [1/213]: domain_loss_target=0.6234 / domain_loss_source=0.7816 / regression_loss_source=0.0358 / alpha=0.8957\n",
            "Epoch [30/100] Step [101/213]: domain_loss_target=0.6414 / domain_loss_source=0.7761 / regression_loss_source=0.1041 / alpha=0.9002\n",
            "Epoch [30/100] Step [201/213]: domain_loss_target=0.6015 / domain_loss_source=0.8168 / regression_loss_source=0.0506 / alpha=0.9046\n",
            "VALIDATION LOSS (Average) : 0.16252894699573517\n",
            "TEST LOSS (Average) : 0.15728449821472168\n",
            "Epoch [31/100] Step [1/213]: domain_loss_target=0.6452 / domain_loss_source=0.7887 / regression_loss_source=0.0724 / alpha=0.9051\n",
            "Epoch [31/100] Step [101/213]: domain_loss_target=0.6393 / domain_loss_source=0.7441 / regression_loss_source=0.0742 / alpha=0.9093\n",
            "Epoch [31/100] Step [201/213]: domain_loss_target=0.6165 / domain_loss_source=0.7408 / regression_loss_source=0.0567 / alpha=0.9133\n",
            "VALIDATION LOSS (Average) : 0.169260635972023\n",
            "TEST LOSS (Average) : 0.16375803450743356\n",
            "Epoch [32/100] Step [1/213]: domain_loss_target=0.6133 / domain_loss_source=0.7895 / regression_loss_source=0.0511 / alpha=0.9138\n",
            "Epoch [32/100] Step [101/213]: domain_loss_target=0.6491 / domain_loss_source=0.7810 / regression_loss_source=0.0308 / alpha=0.9176\n",
            "Epoch [32/100] Step [201/213]: domain_loss_target=0.6482 / domain_loss_source=0.7418 / regression_loss_source=0.0586 / alpha=0.9212\n",
            "VALIDATION LOSS (Average) : 0.17578430473804474\n",
            "TEST LOSS (Average) : 0.17108804484208426\n",
            "Epoch [33/100] Step [1/213]: domain_loss_target=0.6403 / domain_loss_source=0.7217 / regression_loss_source=0.0385 / alpha=0.9217\n",
            "Epoch [33/100] Step [101/213]: domain_loss_target=0.6138 / domain_loss_source=0.7191 / regression_loss_source=0.0967 / alpha=0.9251\n",
            "Epoch [33/100] Step [201/213]: domain_loss_target=0.6385 / domain_loss_source=0.7133 / regression_loss_source=0.0805 / alpha=0.9284\n",
            "VALIDATION LOSS (Average) : 0.1688288152217865\n",
            "TEST LOSS (Average) : 0.1639008770386378\n",
            "Epoch [34/100] Step [1/213]: domain_loss_target=0.6323 / domain_loss_source=0.7628 / regression_loss_source=0.0880 / alpha=0.9289\n",
            "Epoch [34/100] Step [101/213]: domain_loss_target=0.6515 / domain_loss_source=0.7282 / regression_loss_source=0.0778 / alpha=0.9320\n",
            "Epoch [34/100] Step [201/213]: domain_loss_target=0.6725 / domain_loss_source=0.7459 / regression_loss_source=0.0651 / alpha=0.9350\n",
            "VALIDATION LOSS (Average) : 0.18079151213169098\n",
            "TEST LOSS (Average) : 0.17655802766482034\n",
            "Epoch [35/100] Step [1/213]: domain_loss_target=0.6386 / domain_loss_source=0.7159 / regression_loss_source=0.0360 / alpha=0.9354\n",
            "Epoch [35/100] Step [101/213]: domain_loss_target=0.6408 / domain_loss_source=0.7569 / regression_loss_source=0.0779 / alpha=0.9383\n",
            "Epoch [35/100] Step [201/213]: domain_loss_target=0.6589 / domain_loss_source=0.7695 / regression_loss_source=0.0408 / alpha=0.9410\n",
            "VALIDATION LOSS (Average) : 0.16777226328849792\n",
            "TEST LOSS (Average) : 0.1623930831750234\n",
            "Epoch [36/100] Step [1/213]: domain_loss_target=0.6248 / domain_loss_source=0.7709 / regression_loss_source=0.0625 / alpha=0.9414\n",
            "Epoch [36/100] Step [101/213]: domain_loss_target=0.6505 / domain_loss_source=0.7225 / regression_loss_source=0.0573 / alpha=0.9440\n",
            "Epoch [36/100] Step [201/213]: domain_loss_target=0.6059 / domain_loss_source=0.7479 / regression_loss_source=0.0775 / alpha=0.9465\n",
            "VALIDATION LOSS (Average) : 0.1595182865858078\n",
            "TEST LOSS (Average) : 0.15362111230691275\n",
            "Epoch [37/100] Step [1/213]: domain_loss_target=0.6413 / domain_loss_source=0.7418 / regression_loss_source=0.0491 / alpha=0.9468\n",
            "Epoch [37/100] Step [101/213]: domain_loss_target=0.6306 / domain_loss_source=0.7448 / regression_loss_source=0.0461 / alpha=0.9492\n",
            "Epoch [37/100] Step [201/213]: domain_loss_target=0.6579 / domain_loss_source=0.7554 / regression_loss_source=0.0695 / alpha=0.9515\n",
            "VALIDATION LOSS (Average) : 0.168419748544693\n",
            "TEST LOSS (Average) : 0.16341665387153625\n",
            "Epoch [38/100] Step [1/213]: domain_loss_target=0.6699 / domain_loss_source=0.7033 / regression_loss_source=0.0273 / alpha=0.9517\n",
            "Epoch [38/100] Step [101/213]: domain_loss_target=0.6406 / domain_loss_source=0.7582 / regression_loss_source=0.0327 / alpha=0.9539\n",
            "Epoch [38/100] Step [201/213]: domain_loss_target=0.6123 / domain_loss_source=0.7223 / regression_loss_source=0.0807 / alpha=0.9560\n",
            "VALIDATION LOSS (Average) : 0.1626259684562683\n",
            "TEST LOSS (Average) : 0.15628006060918173\n",
            "Epoch [39/100] Step [1/213]: domain_loss_target=0.6365 / domain_loss_source=0.7281 / regression_loss_source=0.0514 / alpha=0.9562\n",
            "Epoch [39/100] Step [101/213]: domain_loss_target=0.6720 / domain_loss_source=0.7060 / regression_loss_source=0.1002 / alpha=0.9582\n",
            "Epoch [39/100] Step [201/213]: domain_loss_target=0.6305 / domain_loss_source=0.7116 / regression_loss_source=0.0822 / alpha=0.9601\n",
            "VALIDATION LOSS (Average) : 0.1672065705060959\n",
            "TEST LOSS (Average) : 0.1612337330977122\n",
            "Epoch [40/100] Step [1/213]: domain_loss_target=0.6070 / domain_loss_source=0.7179 / regression_loss_source=0.0783 / alpha=0.9603\n",
            "Epoch [40/100] Step [101/213]: domain_loss_target=0.6171 / domain_loss_source=0.7516 / regression_loss_source=0.0368 / alpha=0.9621\n",
            "Epoch [40/100] Step [201/213]: domain_loss_target=0.6565 / domain_loss_source=0.7457 / regression_loss_source=0.1062 / alpha=0.9638\n",
            "VALIDATION LOSS (Average) : 0.16392512619495392\n",
            "TEST LOSS (Average) : 0.15862313409646353\n",
            "Epoch [41/100] Step [1/213]: domain_loss_target=0.6746 / domain_loss_source=0.7069 / regression_loss_source=0.0508 / alpha=0.9640\n",
            "Epoch [41/100] Step [101/213]: domain_loss_target=0.6504 / domain_loss_source=0.6669 / regression_loss_source=0.0879 / alpha=0.9656\n",
            "Epoch [41/100] Step [201/213]: domain_loss_target=0.6700 / domain_loss_source=0.6988 / regression_loss_source=0.0612 / alpha=0.9672\n",
            "VALIDATION LOSS (Average) : 0.1693377047777176\n",
            "TEST LOSS (Average) : 0.16461467246214548\n",
            "Epoch [42/100] Step [1/213]: domain_loss_target=0.6322 / domain_loss_source=0.7192 / regression_loss_source=0.0724 / alpha=0.9674\n",
            "Epoch [42/100] Step [101/213]: domain_loss_target=0.6210 / domain_loss_source=0.6776 / regression_loss_source=0.0861 / alpha=0.9689\n",
            "Epoch [42/100] Step [201/213]: domain_loss_target=0.6441 / domain_loss_source=0.6967 / regression_loss_source=0.0492 / alpha=0.9703\n",
            "VALIDATION LOSS (Average) : 0.16552752256393433\n",
            "TEST LOSS (Average) : 0.16031270225842795\n",
            "Epoch [43/100] Step [1/213]: domain_loss_target=0.5759 / domain_loss_source=0.6692 / regression_loss_source=0.0247 / alpha=0.9705\n",
            "Epoch [43/100] Step [101/213]: domain_loss_target=0.6685 / domain_loss_source=0.6900 / regression_loss_source=0.0870 / alpha=0.9718\n",
            "Epoch [43/100] Step [201/213]: domain_loss_target=0.7031 / domain_loss_source=0.7271 / regression_loss_source=0.0570 / alpha=0.9731\n",
            "VALIDATION LOSS (Average) : 0.16353259980678558\n",
            "TEST LOSS (Average) : 0.15780633687973022\n",
            "Epoch [44/100] Step [1/213]: domain_loss_target=0.6338 / domain_loss_source=0.7410 / regression_loss_source=0.0756 / alpha=0.9732\n",
            "Epoch [44/100] Step [101/213]: domain_loss_target=0.6817 / domain_loss_source=0.6890 / regression_loss_source=0.0776 / alpha=0.9744\n",
            "Epoch [44/100] Step [201/213]: domain_loss_target=0.7062 / domain_loss_source=0.7033 / regression_loss_source=0.0671 / alpha=0.9756\n",
            "VALIDATION LOSS (Average) : 0.17144928872585297\n",
            "TEST LOSS (Average) : 0.16607649127642313\n",
            "Epoch [45/100] Step [1/213]: domain_loss_target=0.6855 / domain_loss_source=0.6678 / regression_loss_source=0.0434 / alpha=0.9757\n",
            "Epoch [45/100] Step [101/213]: domain_loss_target=0.6748 / domain_loss_source=0.7368 / regression_loss_source=0.0692 / alpha=0.9768\n",
            "Epoch [45/100] Step [201/213]: domain_loss_target=0.7180 / domain_loss_source=0.6763 / regression_loss_source=0.0999 / alpha=0.9779\n",
            "VALIDATION LOSS (Average) : 0.16565550863742828\n",
            "TEST LOSS (Average) : 0.16057361165682474\n",
            "Epoch [46/100] Step [1/213]: domain_loss_target=0.7461 / domain_loss_source=0.6697 / regression_loss_source=0.0465 / alpha=0.9780\n",
            "Epoch [46/100] Step [101/213]: domain_loss_target=0.6618 / domain_loss_source=0.6812 / regression_loss_source=0.0623 / alpha=0.9790\n",
            "Epoch [46/100] Step [201/213]: domain_loss_target=0.7067 / domain_loss_source=0.6667 / regression_loss_source=0.0200 / alpha=0.9800\n",
            "VALIDATION LOSS (Average) : 0.16667261719703674\n",
            "TEST LOSS (Average) : 0.16085971891880035\n",
            "Epoch [47/100] Step [1/213]: domain_loss_target=0.6837 / domain_loss_source=0.7267 / regression_loss_source=0.0590 / alpha=0.9801\n",
            "Epoch [47/100] Step [101/213]: domain_loss_target=0.6850 / domain_loss_source=0.7000 / regression_loss_source=0.0505 / alpha=0.9810\n",
            "Epoch [47/100] Step [201/213]: domain_loss_target=0.7025 / domain_loss_source=0.6839 / regression_loss_source=0.0523 / alpha=0.9819\n",
            "VALIDATION LOSS (Average) : 0.17211364209651947\n",
            "TEST LOSS (Average) : 0.1669334371884664\n",
            "Epoch [48/100] Step [1/213]: domain_loss_target=0.6760 / domain_loss_source=0.7025 / regression_loss_source=0.0450 / alpha=0.9820\n",
            "Epoch [48/100] Step [101/213]: domain_loss_target=0.7069 / domain_loss_source=0.6741 / regression_loss_source=0.0823 / alpha=0.9828\n",
            "Epoch [48/100] Step [201/213]: domain_loss_target=0.6991 / domain_loss_source=0.7000 / regression_loss_source=0.0458 / alpha=0.9836\n",
            "VALIDATION LOSS (Average) : 0.16421271860599518\n",
            "TEST LOSS (Average) : 0.15888054172197977\n",
            "Epoch [49/100] Step [1/213]: domain_loss_target=0.6888 / domain_loss_source=0.7387 / regression_loss_source=0.0622 / alpha=0.9837\n",
            "Epoch [49/100] Step [101/213]: domain_loss_target=0.7176 / domain_loss_source=0.7126 / regression_loss_source=0.0616 / alpha=0.9844\n",
            "Epoch [49/100] Step [201/213]: domain_loss_target=0.6891 / domain_loss_source=0.6547 / regression_loss_source=0.0860 / alpha=0.9851\n",
            "VALIDATION LOSS (Average) : 0.17097097635269165\n",
            "TEST LOSS (Average) : 0.16647085547447205\n",
            "Epoch [50/100] Step [1/213]: domain_loss_target=0.6930 / domain_loss_source=0.6518 / regression_loss_source=0.0633 / alpha=0.9852\n",
            "Epoch [50/100] Step [101/213]: domain_loss_target=0.7036 / domain_loss_source=0.6211 / regression_loss_source=0.0689 / alpha=0.9859\n",
            "Epoch [50/100] Step [201/213]: domain_loss_target=0.6536 / domain_loss_source=0.7118 / regression_loss_source=0.0594 / alpha=0.9865\n",
            "VALIDATION LOSS (Average) : 0.1618620902299881\n",
            "TEST LOSS (Average) : 0.15693884094556174\n",
            "Epoch [51/100] Step [1/213]: domain_loss_target=0.6562 / domain_loss_source=0.6865 / regression_loss_source=0.0604 / alpha=0.9866\n",
            "Epoch [51/100] Step [101/213]: domain_loss_target=0.7490 / domain_loss_source=0.6400 / regression_loss_source=0.0646 / alpha=0.9872\n",
            "Epoch [51/100] Step [201/213]: domain_loss_target=0.7741 / domain_loss_source=0.7106 / regression_loss_source=0.0788 / alpha=0.9878\n",
            "VALIDATION LOSS (Average) : 0.16122622787952423\n",
            "TEST LOSS (Average) : 0.15539628267288208\n",
            "Epoch [52/100] Step [1/213]: domain_loss_target=0.6435 / domain_loss_source=0.7108 / regression_loss_source=0.0718 / alpha=0.9879\n",
            "Epoch [52/100] Step [101/213]: domain_loss_target=0.7310 / domain_loss_source=0.6722 / regression_loss_source=0.0843 / alpha=0.9884\n",
            "Epoch [52/100] Step [201/213]: domain_loss_target=0.6800 / domain_loss_source=0.6941 / regression_loss_source=0.0788 / alpha=0.9890\n",
            "VALIDATION LOSS (Average) : 0.17325599491596222\n",
            "TEST LOSS (Average) : 0.16740142305692038\n",
            "Epoch [53/100] Step [1/213]: domain_loss_target=0.7336 / domain_loss_source=0.7282 / regression_loss_source=0.0596 / alpha=0.9890\n",
            "Epoch [53/100] Step [101/213]: domain_loss_target=0.6756 / domain_loss_source=0.6627 / regression_loss_source=0.0884 / alpha=0.9895\n",
            "Epoch [53/100] Step [201/213]: domain_loss_target=0.7194 / domain_loss_source=0.6946 / regression_loss_source=0.0298 / alpha=0.9900\n",
            "VALIDATION LOSS (Average) : 0.16406241059303284\n",
            "TEST LOSS (Average) : 0.15736485520998636\n",
            "Epoch [54/100] Step [1/213]: domain_loss_target=0.7030 / domain_loss_source=0.6493 / regression_loss_source=0.0389 / alpha=0.9901\n",
            "Epoch [54/100] Step [101/213]: domain_loss_target=0.7094 / domain_loss_source=0.6831 / regression_loss_source=0.0787 / alpha=0.9905\n",
            "Epoch [54/100] Step [201/213]: domain_loss_target=0.6545 / domain_loss_source=0.6470 / regression_loss_source=0.0906 / alpha=0.9910\n",
            "VALIDATION LOSS (Average) : 0.16905996203422546\n",
            "TEST LOSS (Average) : 0.16342826187610626\n",
            "Epoch [55/100] Step [1/213]: domain_loss_target=0.6749 / domain_loss_source=0.6672 / regression_loss_source=0.1250 / alpha=0.9910\n",
            "Epoch [55/100] Step [101/213]: domain_loss_target=0.6183 / domain_loss_source=0.6525 / regression_loss_source=0.0405 / alpha=0.9914\n",
            "Epoch [55/100] Step [201/213]: domain_loss_target=0.6899 / domain_loss_source=0.6789 / regression_loss_source=0.0708 / alpha=0.9918\n",
            "VALIDATION LOSS (Average) : 0.16826660931110382\n",
            "TEST LOSS (Average) : 0.16221141318480173\n",
            "Epoch [56/100] Step [1/213]: domain_loss_target=0.6974 / domain_loss_source=0.6842 / regression_loss_source=0.0554 / alpha=0.9919\n",
            "Epoch [56/100] Step [101/213]: domain_loss_target=0.6658 / domain_loss_source=0.6786 / regression_loss_source=0.0593 / alpha=0.9922\n",
            "Epoch [56/100] Step [201/213]: domain_loss_target=0.7085 / domain_loss_source=0.6292 / regression_loss_source=0.0349 / alpha=0.9926\n",
            "VALIDATION LOSS (Average) : 0.1679023653268814\n",
            "TEST LOSS (Average) : 0.16301630437374115\n",
            "Epoch [57/100] Step [1/213]: domain_loss_target=0.6914 / domain_loss_source=0.7048 / regression_loss_source=0.0344 / alpha=0.9926\n",
            "Epoch [57/100] Step [101/213]: domain_loss_target=0.7037 / domain_loss_source=0.6679 / regression_loss_source=0.0653 / alpha=0.9930\n",
            "Epoch [57/100] Step [201/213]: domain_loss_target=0.6960 / domain_loss_source=0.6566 / regression_loss_source=0.0849 / alpha=0.9933\n",
            "VALIDATION LOSS (Average) : 0.16903230547904968\n",
            "TEST LOSS (Average) : 0.16454272468884787\n",
            "Epoch [58/100] Step [1/213]: domain_loss_target=0.6702 / domain_loss_source=0.7172 / regression_loss_source=0.0431 / alpha=0.9933\n",
            "Epoch [58/100] Step [101/213]: domain_loss_target=0.6729 / domain_loss_source=0.6729 / regression_loss_source=0.0592 / alpha=0.9936\n",
            "Epoch [58/100] Step [201/213]: domain_loss_target=0.6874 / domain_loss_source=0.6227 / regression_loss_source=0.0385 / alpha=0.9939\n",
            "VALIDATION LOSS (Average) : 0.16365770995616913\n",
            "TEST LOSS (Average) : 0.1585158258676529\n",
            "Epoch [59/100] Step [1/213]: domain_loss_target=0.7382 / domain_loss_source=0.6853 / regression_loss_source=0.0741 / alpha=0.9940\n",
            "Epoch [59/100] Step [101/213]: domain_loss_target=0.6666 / domain_loss_source=0.6067 / regression_loss_source=0.0658 / alpha=0.9942\n",
            "Epoch [59/100] Step [201/213]: domain_loss_target=0.6358 / domain_loss_source=0.7263 / regression_loss_source=0.1233 / alpha=0.9945\n",
            "VALIDATION LOSS (Average) : 0.17267316579818726\n",
            "TEST LOSS (Average) : 0.1677092065413793\n",
            "Epoch [60/100] Step [1/213]: domain_loss_target=0.6680 / domain_loss_source=0.6911 / regression_loss_source=0.0432 / alpha=0.9945\n",
            "Epoch [60/100] Step [101/213]: domain_loss_target=0.6445 / domain_loss_source=0.7020 / regression_loss_source=0.0411 / alpha=0.9948\n",
            "Epoch [60/100] Step [201/213]: domain_loss_target=0.7325 / domain_loss_source=0.6855 / regression_loss_source=0.0369 / alpha=0.9950\n",
            "VALIDATION LOSS (Average) : 0.1756230890750885\n",
            "TEST LOSS (Average) : 0.16967645287513733\n",
            "Epoch [61/100] Step [1/213]: domain_loss_target=0.7509 / domain_loss_source=0.6476 / regression_loss_source=0.0563 / alpha=0.9951\n",
            "Epoch [61/100] Step [101/213]: domain_loss_target=0.6854 / domain_loss_source=0.6786 / regression_loss_source=0.0534 / alpha=0.9953\n",
            "Epoch [61/100] Step [201/213]: domain_loss_target=0.6926 / domain_loss_source=0.6998 / regression_loss_source=0.0488 / alpha=0.9955\n",
            "VALIDATION LOSS (Average) : 0.174712672829628\n",
            "TEST LOSS (Average) : 0.16878209014733633\n",
            "Epoch [62/100] Step [1/213]: domain_loss_target=0.7317 / domain_loss_source=0.7434 / regression_loss_source=0.0760 / alpha=0.9955\n",
            "Epoch [62/100] Step [101/213]: domain_loss_target=0.7320 / domain_loss_source=0.7214 / regression_loss_source=0.0762 / alpha=0.9957\n",
            "Epoch [62/100] Step [201/213]: domain_loss_target=0.7059 / domain_loss_source=0.6119 / regression_loss_source=0.0411 / alpha=0.9959\n",
            "VALIDATION LOSS (Average) : 0.16728033125400543\n",
            "TEST LOSS (Average) : 0.16207585235436758\n",
            "Epoch [63/100] Step [1/213]: domain_loss_target=0.7392 / domain_loss_source=0.6747 / regression_loss_source=0.0570 / alpha=0.9959\n",
            "Epoch [63/100] Step [101/213]: domain_loss_target=0.7041 / domain_loss_source=0.7469 / regression_loss_source=0.0738 / alpha=0.9961\n",
            "Epoch [63/100] Step [201/213]: domain_loss_target=0.6344 / domain_loss_source=0.7007 / regression_loss_source=0.0590 / alpha=0.9963\n",
            "VALIDATION LOSS (Average) : 0.16776134073734283\n",
            "TEST LOSS (Average) : 0.16333792606989542\n",
            "Epoch [64/100] Step [1/213]: domain_loss_target=0.6560 / domain_loss_source=0.7192 / regression_loss_source=0.0622 / alpha=0.9963\n",
            "Epoch [64/100] Step [101/213]: domain_loss_target=0.6445 / domain_loss_source=0.6937 / regression_loss_source=0.0431 / alpha=0.9965\n",
            "Epoch [64/100] Step [201/213]: domain_loss_target=0.6546 / domain_loss_source=0.6815 / regression_loss_source=0.0508 / alpha=0.9967\n",
            "VALIDATION LOSS (Average) : 0.1696142703294754\n",
            "TEST LOSS (Average) : 0.16466580828030905\n",
            "Epoch [65/100] Step [1/213]: domain_loss_target=0.6736 / domain_loss_source=0.6471 / regression_loss_source=0.0603 / alpha=0.9967\n",
            "Epoch [65/100] Step [101/213]: domain_loss_target=0.6511 / domain_loss_source=0.6877 / regression_loss_source=0.0604 / alpha=0.9968\n",
            "Epoch [65/100] Step [201/213]: domain_loss_target=0.7184 / domain_loss_source=0.7301 / regression_loss_source=0.0893 / alpha=0.9970\n",
            "VALIDATION LOSS (Average) : 0.1669851690530777\n",
            "TEST LOSS (Average) : 0.1615523397922516\n",
            "Epoch [66/100] Step [1/213]: domain_loss_target=0.6812 / domain_loss_source=0.7306 / regression_loss_source=0.0504 / alpha=0.9970\n",
            "Epoch [66/100] Step [101/213]: domain_loss_target=0.6519 / domain_loss_source=0.6997 / regression_loss_source=0.0523 / alpha=0.9971\n",
            "Epoch [66/100] Step [201/213]: domain_loss_target=0.7397 / domain_loss_source=0.6712 / regression_loss_source=0.0532 / alpha=0.9973\n",
            "VALIDATION LOSS (Average) : 0.16692057251930237\n",
            "TEST LOSS (Average) : 0.1611521045366923\n",
            "Epoch [67/100] Step [1/213]: domain_loss_target=0.7005 / domain_loss_source=0.7452 / regression_loss_source=0.0360 / alpha=0.9973\n",
            "Epoch [67/100] Step [101/213]: domain_loss_target=0.6543 / domain_loss_source=0.6767 / regression_loss_source=0.0324 / alpha=0.9974\n",
            "Epoch [67/100] Step [201/213]: domain_loss_target=0.6497 / domain_loss_source=0.7219 / regression_loss_source=0.0482 / alpha=0.9975\n",
            "VALIDATION LOSS (Average) : 0.16827018558979034\n",
            "TEST LOSS (Average) : 0.1624916891256968\n",
            "Epoch [68/100] Step [1/213]: domain_loss_target=0.6651 / domain_loss_source=0.7496 / regression_loss_source=0.0401 / alpha=0.9975\n",
            "Epoch [68/100] Step [101/213]: domain_loss_target=0.6614 / domain_loss_source=0.6982 / regression_loss_source=0.0288 / alpha=0.9977\n",
            "Epoch [68/100] Step [201/213]: domain_loss_target=0.6955 / domain_loss_source=0.6752 / regression_loss_source=0.0417 / alpha=0.9978\n",
            "VALIDATION LOSS (Average) : 0.16909953951835632\n",
            "TEST LOSS (Average) : 0.16307659447193146\n",
            "Epoch [69/100] Step [1/213]: domain_loss_target=0.7273 / domain_loss_source=0.6547 / regression_loss_source=0.0704 / alpha=0.9978\n",
            "Epoch [69/100] Step [101/213]: domain_loss_target=0.6832 / domain_loss_source=0.6671 / regression_loss_source=0.0395 / alpha=0.9979\n",
            "Epoch [69/100] Step [201/213]: domain_loss_target=0.6530 / domain_loss_source=0.7355 / regression_loss_source=0.0358 / alpha=0.9980\n",
            "VALIDATION LOSS (Average) : 0.1683174967765808\n",
            "TEST LOSS (Average) : 0.1626592973868052\n",
            "Epoch [70/100] Step [1/213]: domain_loss_target=0.6820 / domain_loss_source=0.7473 / regression_loss_source=0.0431 / alpha=0.9980\n",
            "Epoch [70/100] Step [101/213]: domain_loss_target=0.6632 / domain_loss_source=0.6446 / regression_loss_source=0.0445 / alpha=0.9981\n",
            "Epoch [70/100] Step [201/213]: domain_loss_target=0.6568 / domain_loss_source=0.7367 / regression_loss_source=0.0848 / alpha=0.9982\n",
            "VALIDATION LOSS (Average) : 0.1692463606595993\n",
            "TEST LOSS (Average) : 0.16380521655082703\n",
            "Epoch [71/100] Step [1/213]: domain_loss_target=0.6870 / domain_loss_source=0.6724 / regression_loss_source=0.0432 / alpha=0.9982\n",
            "Epoch [71/100] Step [101/213]: domain_loss_target=0.6625 / domain_loss_source=0.7013 / regression_loss_source=0.0828 / alpha=0.9983\n",
            "Epoch [71/100] Step [201/213]: domain_loss_target=0.6714 / domain_loss_source=0.6773 / regression_loss_source=0.0593 / alpha=0.9983\n",
            "VALIDATION LOSS (Average) : 0.17880305647850037\n",
            "TEST LOSS (Average) : 0.17352300882339478\n",
            "Epoch [72/100] Step [1/213]: domain_loss_target=0.6964 / domain_loss_source=0.6923 / regression_loss_source=0.0555 / alpha=0.9984\n",
            "Epoch [72/100] Step [101/213]: domain_loss_target=0.6953 / domain_loss_source=0.7193 / regression_loss_source=0.0226 / alpha=0.9984\n",
            "Epoch [72/100] Step [201/213]: domain_loss_target=0.6912 / domain_loss_source=0.6335 / regression_loss_source=0.0865 / alpha=0.9985\n",
            "VALIDATION LOSS (Average) : 0.16446812450885773\n",
            "TEST LOSS (Average) : 0.15794728696346283\n",
            "Epoch [73/100] Step [1/213]: domain_loss_target=0.6787 / domain_loss_source=0.6842 / regression_loss_source=0.0665 / alpha=0.9985\n",
            "Epoch [73/100] Step [101/213]: domain_loss_target=0.6783 / domain_loss_source=0.6680 / regression_loss_source=0.0727 / alpha=0.9986\n",
            "Epoch [73/100] Step [201/213]: domain_loss_target=0.7321 / domain_loss_source=0.6856 / regression_loss_source=0.0607 / alpha=0.9986\n",
            "VALIDATION LOSS (Average) : 0.17034265398979187\n",
            "TEST LOSS (Average) : 0.1640772968530655\n",
            "Epoch [74/100] Step [1/213]: domain_loss_target=0.6787 / domain_loss_source=0.6770 / regression_loss_source=0.0360 / alpha=0.9986\n",
            "Epoch [74/100] Step [101/213]: domain_loss_target=0.6642 / domain_loss_source=0.6388 / regression_loss_source=0.0586 / alpha=0.9987\n",
            "Epoch [74/100] Step [201/213]: domain_loss_target=0.7265 / domain_loss_source=0.6714 / regression_loss_source=0.0494 / alpha=0.9988\n",
            "VALIDATION LOSS (Average) : 0.16924060881137848\n",
            "TEST LOSS (Average) : 0.16279617448647818\n",
            "Epoch [75/100] Step [1/213]: domain_loss_target=0.7018 / domain_loss_source=0.6938 / regression_loss_source=0.0476 / alpha=0.9988\n",
            "Epoch [75/100] Step [101/213]: domain_loss_target=0.6686 / domain_loss_source=0.6956 / regression_loss_source=0.0335 / alpha=0.9988\n",
            "Epoch [75/100] Step [201/213]: domain_loss_target=0.7041 / domain_loss_source=0.7039 / regression_loss_source=0.0642 / alpha=0.9989\n",
            "VALIDATION LOSS (Average) : 0.17437048256397247\n",
            "TEST LOSS (Average) : 0.16847274700800577\n",
            "Epoch [76/100] Step [1/213]: domain_loss_target=0.6732 / domain_loss_source=0.6512 / regression_loss_source=0.0582 / alpha=0.9989\n",
            "Epoch [76/100] Step [101/213]: domain_loss_target=0.6411 / domain_loss_source=0.6899 / regression_loss_source=0.0409 / alpha=0.9989\n",
            "Epoch [76/100] Step [201/213]: domain_loss_target=0.7085 / domain_loss_source=0.7052 / regression_loss_source=0.0694 / alpha=0.9990\n",
            "VALIDATION LOSS (Average) : 0.16703113913536072\n",
            "TEST LOSS (Average) : 0.16032873590787253\n",
            "Epoch [77/100] Step [1/213]: domain_loss_target=0.6937 / domain_loss_source=0.6861 / regression_loss_source=0.0348 / alpha=0.9990\n",
            "Epoch [77/100] Step [101/213]: domain_loss_target=0.6917 / domain_loss_source=0.6245 / regression_loss_source=0.0600 / alpha=0.9990\n",
            "Epoch [77/100] Step [201/213]: domain_loss_target=0.6824 / domain_loss_source=0.7221 / regression_loss_source=0.0692 / alpha=0.9991\n",
            "VALIDATION LOSS (Average) : 0.17083103954792023\n",
            "TEST LOSS (Average) : 0.1652063230673472\n",
            "Epoch [78/100] Step [1/213]: domain_loss_target=0.7281 / domain_loss_source=0.7638 / regression_loss_source=0.0546 / alpha=0.9991\n",
            "Epoch [78/100] Step [101/213]: domain_loss_target=0.6395 / domain_loss_source=0.6819 / regression_loss_source=0.0706 / alpha=0.9991\n",
            "Epoch [78/100] Step [201/213]: domain_loss_target=0.6738 / domain_loss_source=0.6973 / regression_loss_source=0.0536 / alpha=0.9992\n",
            "VALIDATION LOSS (Average) : 0.15290413796901703\n",
            "TEST LOSS (Average) : 0.1454255779584249\n",
            "Epoch [79/100] Step [1/213]: domain_loss_target=0.7189 / domain_loss_source=0.7674 / regression_loss_source=0.0588 / alpha=0.9992\n",
            "Epoch [79/100] Step [101/213]: domain_loss_target=0.6944 / domain_loss_source=0.6782 / regression_loss_source=0.0410 / alpha=0.9992\n",
            "Epoch [79/100] Step [201/213]: domain_loss_target=0.6724 / domain_loss_source=0.6679 / regression_loss_source=0.0745 / alpha=0.9993\n",
            "VALIDATION LOSS (Average) : 0.16582244634628296\n",
            "TEST LOSS (Average) : 0.16005480786164603\n",
            "Epoch [80/100] Step [1/213]: domain_loss_target=0.7145 / domain_loss_source=0.7174 / regression_loss_source=0.0849 / alpha=0.9993\n",
            "Epoch [80/100] Step [101/213]: domain_loss_target=0.6947 / domain_loss_source=0.7343 / regression_loss_source=0.0582 / alpha=0.9993\n",
            "Epoch [80/100] Step [201/213]: domain_loss_target=0.6299 / domain_loss_source=0.6600 / regression_loss_source=0.0396 / alpha=0.9993\n",
            "VALIDATION LOSS (Average) : 0.16465532779693604\n",
            "TEST LOSS (Average) : 0.15876282254854837\n",
            "Epoch [81/100] Step [1/213]: domain_loss_target=0.6833 / domain_loss_source=0.7108 / regression_loss_source=0.0399 / alpha=0.9993\n",
            "Epoch [81/100] Step [101/213]: domain_loss_target=0.6581 / domain_loss_source=0.7325 / regression_loss_source=0.0571 / alpha=0.9994\n",
            "Epoch [81/100] Step [201/213]: domain_loss_target=0.7012 / domain_loss_source=0.7409 / regression_loss_source=0.0382 / alpha=0.9994\n",
            "VALIDATION LOSS (Average) : 0.17405715584754944\n",
            "TEST LOSS (Average) : 0.16890769203503928\n",
            "Epoch [82/100] Step [1/213]: domain_loss_target=0.6901 / domain_loss_source=0.6427 / regression_loss_source=0.0838 / alpha=0.9994\n",
            "Epoch [82/100] Step [101/213]: domain_loss_target=0.5954 / domain_loss_source=0.7567 / regression_loss_source=0.0532 / alpha=0.9994\n",
            "Epoch [82/100] Step [201/213]: domain_loss_target=0.7023 / domain_loss_source=0.6776 / regression_loss_source=0.0693 / alpha=0.9994\n",
            "VALIDATION LOSS (Average) : 0.16349337995052338\n",
            "TEST LOSS (Average) : 0.1571075121561686\n",
            "Epoch [83/100] Step [1/213]: domain_loss_target=0.7058 / domain_loss_source=0.7042 / regression_loss_source=0.0386 / alpha=0.9995\n",
            "Epoch [83/100] Step [101/213]: domain_loss_target=0.6743 / domain_loss_source=0.6636 / regression_loss_source=0.0383 / alpha=0.9995\n",
            "Epoch [83/100] Step [201/213]: domain_loss_target=0.6698 / domain_loss_source=0.6842 / regression_loss_source=0.0647 / alpha=0.9995\n",
            "VALIDATION LOSS (Average) : 0.1696448177099228\n",
            "TEST LOSS (Average) : 0.16308549543221793\n",
            "Epoch [84/100] Step [1/213]: domain_loss_target=0.6797 / domain_loss_source=0.7294 / regression_loss_source=0.0529 / alpha=0.9995\n",
            "Epoch [84/100] Step [101/213]: domain_loss_target=0.6599 / domain_loss_source=0.6746 / regression_loss_source=0.0793 / alpha=0.9995\n",
            "Epoch [84/100] Step [201/213]: domain_loss_target=0.6764 / domain_loss_source=0.6445 / regression_loss_source=0.0483 / alpha=0.9995\n",
            "VALIDATION LOSS (Average) : 0.1741822361946106\n",
            "TEST LOSS (Average) : 0.16789167622725168\n",
            "Epoch [85/100] Step [1/213]: domain_loss_target=0.7317 / domain_loss_source=0.6635 / regression_loss_source=0.0550 / alpha=0.9996\n",
            "Epoch [85/100] Step [101/213]: domain_loss_target=0.6491 / domain_loss_source=0.6941 / regression_loss_source=0.0434 / alpha=0.9996\n",
            "Epoch [85/100] Step [201/213]: domain_loss_target=0.6514 / domain_loss_source=0.7079 / regression_loss_source=0.0568 / alpha=0.9996\n",
            "VALIDATION LOSS (Average) : 0.17350760102272034\n",
            "TEST LOSS (Average) : 0.16741901636123657\n",
            "Epoch [86/100] Step [1/213]: domain_loss_target=0.6931 / domain_loss_source=0.7587 / regression_loss_source=0.0538 / alpha=0.9996\n",
            "Epoch [86/100] Step [101/213]: domain_loss_target=0.6173 / domain_loss_source=0.6633 / regression_loss_source=0.0575 / alpha=0.9996\n",
            "Epoch [86/100] Step [201/213]: domain_loss_target=0.6505 / domain_loss_source=0.6877 / regression_loss_source=0.0712 / alpha=0.9996\n",
            "VALIDATION LOSS (Average) : 0.16925854980945587\n",
            "TEST LOSS (Average) : 0.16356532275676727\n",
            "Epoch [87/100] Step [1/213]: domain_loss_target=0.7187 / domain_loss_source=0.6670 / regression_loss_source=0.0588 / alpha=0.9996\n",
            "Epoch [87/100] Step [101/213]: domain_loss_target=0.6362 / domain_loss_source=0.6879 / regression_loss_source=0.0413 / alpha=0.9996\n",
            "Epoch [87/100] Step [201/213]: domain_loss_target=0.6849 / domain_loss_source=0.7198 / regression_loss_source=0.0535 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.16433218121528625\n",
            "TEST LOSS (Average) : 0.15856259564558664\n",
            "Epoch [88/100] Step [1/213]: domain_loss_target=0.6586 / domain_loss_source=0.6958 / regression_loss_source=0.0827 / alpha=0.9997\n",
            "Epoch [88/100] Step [101/213]: domain_loss_target=0.6851 / domain_loss_source=0.6512 / regression_loss_source=0.0569 / alpha=0.9997\n",
            "Epoch [88/100] Step [201/213]: domain_loss_target=0.6774 / domain_loss_source=0.7447 / regression_loss_source=0.0493 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.1721944808959961\n",
            "TEST LOSS (Average) : 0.16683811942736307\n",
            "Epoch [89/100] Step [1/213]: domain_loss_target=0.6903 / domain_loss_source=0.6445 / regression_loss_source=0.0969 / alpha=0.9997\n",
            "Epoch [89/100] Step [101/213]: domain_loss_target=0.6694 / domain_loss_source=0.6634 / regression_loss_source=0.0669 / alpha=0.9997\n",
            "Epoch [89/100] Step [201/213]: domain_loss_target=0.6710 / domain_loss_source=0.6397 / regression_loss_source=0.0622 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.17077645659446716\n",
            "TEST LOSS (Average) : 0.16466166575749716\n",
            "Epoch [90/100] Step [1/213]: domain_loss_target=0.7055 / domain_loss_source=0.6778 / regression_loss_source=0.1041 / alpha=0.9997\n",
            "Epoch [90/100] Step [101/213]: domain_loss_target=0.7443 / domain_loss_source=0.6371 / regression_loss_source=0.0371 / alpha=0.9997\n",
            "Epoch [90/100] Step [201/213]: domain_loss_target=0.7079 / domain_loss_source=0.6329 / regression_loss_source=0.0832 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.16850095987319946\n",
            "TEST LOSS (Average) : 0.16231236855189005\n",
            "Epoch [91/100] Step [1/213]: domain_loss_target=0.7351 / domain_loss_source=0.5486 / regression_loss_source=0.0598 / alpha=0.9998\n",
            "Epoch [91/100] Step [101/213]: domain_loss_target=0.7077 / domain_loss_source=0.6967 / regression_loss_source=0.0468 / alpha=0.9998\n",
            "Epoch [91/100] Step [201/213]: domain_loss_target=0.6695 / domain_loss_source=0.6949 / regression_loss_source=0.0539 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.17363278567790985\n",
            "TEST LOSS (Average) : 0.16785365343093872\n",
            "Epoch [92/100] Step [1/213]: domain_loss_target=0.6967 / domain_loss_source=0.6882 / regression_loss_source=0.0499 / alpha=0.9998\n",
            "Epoch [92/100] Step [101/213]: domain_loss_target=0.7603 / domain_loss_source=0.6839 / regression_loss_source=0.0501 / alpha=0.9998\n",
            "Epoch [92/100] Step [201/213]: domain_loss_target=0.6541 / domain_loss_source=0.6888 / regression_loss_source=0.0555 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.1687622368335724\n",
            "TEST LOSS (Average) : 0.16273861626784006\n",
            "Epoch [93/100] Step [1/213]: domain_loss_target=0.6825 / domain_loss_source=0.6735 / regression_loss_source=0.0634 / alpha=0.9998\n",
            "Epoch [93/100] Step [101/213]: domain_loss_target=0.7005 / domain_loss_source=0.6504 / regression_loss_source=0.0391 / alpha=0.9998\n",
            "Epoch [93/100] Step [201/213]: domain_loss_target=0.6662 / domain_loss_source=0.6939 / regression_loss_source=0.0590 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.16842162609100342\n",
            "TEST LOSS (Average) : 0.1625368446111679\n",
            "Epoch [94/100] Step [1/213]: domain_loss_target=0.6723 / domain_loss_source=0.6368 / regression_loss_source=0.0846 / alpha=0.9998\n",
            "Epoch [94/100] Step [101/213]: domain_loss_target=0.6621 / domain_loss_source=0.6346 / regression_loss_source=0.0422 / alpha=0.9998\n",
            "Epoch [94/100] Step [201/213]: domain_loss_target=0.6665 / domain_loss_source=0.7022 / regression_loss_source=0.0627 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.17162159085273743\n",
            "TEST LOSS (Average) : 0.16601285338401794\n",
            "Epoch [95/100] Step [1/213]: domain_loss_target=0.6876 / domain_loss_source=0.7376 / regression_loss_source=0.0680 / alpha=0.9998\n",
            "Epoch [95/100] Step [101/213]: domain_loss_target=0.6706 / domain_loss_source=0.6395 / regression_loss_source=0.0545 / alpha=0.9998\n",
            "Epoch [95/100] Step [201/213]: domain_loss_target=0.6789 / domain_loss_source=0.7020 / regression_loss_source=0.0565 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.17803791165351868\n",
            "TEST LOSS (Average) : 0.1728760451078415\n",
            "Epoch [96/100] Step [1/213]: domain_loss_target=0.6700 / domain_loss_source=0.7023 / regression_loss_source=0.0525 / alpha=0.9999\n",
            "Epoch [96/100] Step [101/213]: domain_loss_target=0.6210 / domain_loss_source=0.6914 / regression_loss_source=0.0756 / alpha=0.9999\n",
            "Epoch [96/100] Step [201/213]: domain_loss_target=0.6912 / domain_loss_source=0.6461 / regression_loss_source=0.0704 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.1626649796962738\n",
            "TEST LOSS (Average) : 0.1564785142739614\n",
            "Epoch [97/100] Step [1/213]: domain_loss_target=0.6601 / domain_loss_source=0.6597 / regression_loss_source=0.0358 / alpha=0.9999\n",
            "Epoch [97/100] Step [101/213]: domain_loss_target=0.7271 / domain_loss_source=0.5966 / regression_loss_source=0.0660 / alpha=0.9999\n",
            "Epoch [97/100] Step [201/213]: domain_loss_target=0.7280 / domain_loss_source=0.6642 / regression_loss_source=0.0772 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.17127107083797455\n",
            "TEST LOSS (Average) : 0.16615745921929678\n",
            "Epoch [98/100] Step [1/213]: domain_loss_target=0.7564 / domain_loss_source=0.7590 / regression_loss_source=0.0618 / alpha=0.9999\n",
            "Epoch [98/100] Step [101/213]: domain_loss_target=0.6659 / domain_loss_source=0.7150 / regression_loss_source=0.0636 / alpha=0.9999\n",
            "Epoch [98/100] Step [201/213]: domain_loss_target=0.7413 / domain_loss_source=0.6540 / regression_loss_source=0.0400 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.1659458875656128\n",
            "TEST LOSS (Average) : 0.1607894996802012\n",
            "Epoch [99/100] Step [1/213]: domain_loss_target=0.6984 / domain_loss_source=0.7447 / regression_loss_source=0.0654 / alpha=0.9999\n",
            "Epoch [99/100] Step [101/213]: domain_loss_target=0.7090 / domain_loss_source=0.6761 / regression_loss_source=0.0605 / alpha=0.9999\n",
            "Epoch [99/100] Step [201/213]: domain_loss_target=0.7007 / domain_loss_source=0.7106 / regression_loss_source=0.0754 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.1725698560476303\n",
            "TEST LOSS (Average) : 0.16782113909721375\n",
            "Epoch [100/100] Step [1/213]: domain_loss_target=0.7337 / domain_loss_source=0.6638 / regression_loss_source=0.0560 / alpha=0.9999\n",
            "Epoch [100/100] Step [101/213]: domain_loss_target=0.6840 / domain_loss_source=0.6581 / regression_loss_source=0.0543 / alpha=0.9999\n",
            "Epoch [100/100] Step [201/213]: domain_loss_target=0.7198 / domain_loss_source=0.6422 / regression_loss_source=0.0852 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.1727413684129715\n",
            "TEST LOSS (Average) : 0.16761071979999542\n",
            "----------------------training complete for DANN model - EI_anger-----------------\n",
            "----------------------training started for DANN model - EI_fear-----------------\n",
            "Epoch [1/100] Step [1/282]: domain_loss_target=0.6572 / domain_loss_source=0.7266 / regression_loss_source=0.0223 / alpha=0.0000\n",
            "Epoch [1/100] Step [101/282]: domain_loss_target=0.6864 / domain_loss_source=0.6032 / regression_loss_source=0.0490 / alpha=0.0177\n",
            "Epoch [1/100] Step [201/282]: domain_loss_target=0.7285 / domain_loss_source=0.5530 / regression_loss_source=0.0873 / alpha=0.0354\n",
            "VALIDATION LOSS (Average) : 0.12516479194164276\n",
            "TEST LOSS (Average) : 0.11816640694936116\n",
            "Epoch [2/100] Step [1/282]: domain_loss_target=0.7636 / domain_loss_source=0.4938 / regression_loss_source=0.0638 / alpha=0.0500\n",
            "Epoch [2/100] Step [101/282]: domain_loss_target=0.7912 / domain_loss_source=0.3925 / regression_loss_source=0.0673 / alpha=0.0676\n",
            "Epoch [2/100] Step [201/282]: domain_loss_target=0.7673 / domain_loss_source=0.4309 / regression_loss_source=0.0741 / alpha=0.0853\n",
            "VALIDATION LOSS (Average) : 0.12668374180793762\n",
            "TEST LOSS (Average) : 0.11960588892300923\n",
            "Epoch [3/100] Step [1/282]: domain_loss_target=0.7836 / domain_loss_source=0.2972 / regression_loss_source=0.0481 / alpha=0.0997\n",
            "Epoch [3/100] Step [101/282]: domain_loss_target=0.7402 / domain_loss_source=0.2618 / regression_loss_source=0.0392 / alpha=0.1172\n",
            "Epoch [3/100] Step [201/282]: domain_loss_target=0.8633 / domain_loss_source=0.3989 / regression_loss_source=0.0352 / alpha=0.1346\n",
            "VALIDATION LOSS (Average) : 0.12843924760818481\n",
            "TEST LOSS (Average) : 0.12171871215105057\n",
            "Epoch [4/100] Step [1/282]: domain_loss_target=0.7583 / domain_loss_source=0.4037 / regression_loss_source=0.0464 / alpha=0.1489\n",
            "Epoch [4/100] Step [101/282]: domain_loss_target=0.7827 / domain_loss_source=0.3598 / regression_loss_source=0.0785 / alpha=0.1662\n",
            "Epoch [4/100] Step [201/282]: domain_loss_target=0.8379 / domain_loss_source=0.3789 / regression_loss_source=0.0580 / alpha=0.1834\n",
            "VALIDATION LOSS (Average) : 0.12626487016677856\n",
            "TEST LOSS (Average) : 0.118780051668485\n",
            "Epoch [5/100] Step [1/282]: domain_loss_target=0.8744 / domain_loss_source=0.4246 / regression_loss_source=0.0597 / alpha=0.1974\n",
            "Epoch [5/100] Step [101/282]: domain_loss_target=0.7996 / domain_loss_source=0.3335 / regression_loss_source=0.0829 / alpha=0.2144\n",
            "Epoch [5/100] Step [201/282]: domain_loss_target=0.7246 / domain_loss_source=0.5239 / regression_loss_source=0.0553 / alpha=0.2312\n",
            "VALIDATION LOSS (Average) : 0.12579068541526794\n",
            "TEST LOSS (Average) : 0.1195571596423785\n",
            "Epoch [6/100] Step [1/282]: domain_loss_target=0.8702 / domain_loss_source=0.4678 / regression_loss_source=0.0702 / alpha=0.2449\n",
            "Epoch [6/100] Step [101/282]: domain_loss_target=0.8247 / domain_loss_source=0.3329 / regression_loss_source=0.1337 / alpha=0.2615\n",
            "Epoch [6/100] Step [201/282]: domain_loss_target=0.8436 / domain_loss_source=0.5920 / regression_loss_source=0.0497 / alpha=0.2780\n",
            "VALIDATION LOSS (Average) : 0.12758785486221313\n",
            "TEST LOSS (Average) : 0.12086833516756694\n",
            "Epoch [7/100] Step [1/282]: domain_loss_target=0.7665 / domain_loss_source=0.3787 / regression_loss_source=0.0746 / alpha=0.2913\n",
            "Epoch [7/100] Step [101/282]: domain_loss_target=0.6343 / domain_loss_source=0.3344 / regression_loss_source=0.0983 / alpha=0.3075\n",
            "Epoch [7/100] Step [201/282]: domain_loss_target=0.8872 / domain_loss_source=0.5185 / regression_loss_source=0.0770 / alpha=0.3234\n",
            "VALIDATION LOSS (Average) : 0.12721697986125946\n",
            "TEST LOSS (Average) : 0.12101736664772034\n",
            "Epoch [8/100] Step [1/282]: domain_loss_target=0.7803 / domain_loss_source=0.5268 / regression_loss_source=0.0435 / alpha=0.3364\n",
            "Epoch [8/100] Step [101/282]: domain_loss_target=0.8244 / domain_loss_source=0.5627 / regression_loss_source=0.0756 / alpha=0.3520\n",
            "Epoch [8/100] Step [201/282]: domain_loss_target=0.8909 / domain_loss_source=0.4813 / regression_loss_source=0.0385 / alpha=0.3674\n",
            "VALIDATION LOSS (Average) : 0.12926051020622253\n",
            "TEST LOSS (Average) : 0.1228419120113055\n",
            "Epoch [9/100] Step [1/282]: domain_loss_target=0.7727 / domain_loss_source=0.4829 / regression_loss_source=0.0735 / alpha=0.3799\n",
            "Epoch [9/100] Step [101/282]: domain_loss_target=0.7492 / domain_loss_source=0.6309 / regression_loss_source=0.0481 / alpha=0.3950\n",
            "Epoch [9/100] Step [201/282]: domain_loss_target=0.8812 / domain_loss_source=0.4011 / regression_loss_source=0.0429 / alpha=0.4099\n",
            "VALIDATION LOSS (Average) : 0.12951400876045227\n",
            "TEST LOSS (Average) : 0.12379184861977895\n",
            "Epoch [10/100] Step [1/282]: domain_loss_target=0.7070 / domain_loss_source=0.5191 / regression_loss_source=0.0577 / alpha=0.4219\n",
            "Epoch [10/100] Step [101/282]: domain_loss_target=0.7305 / domain_loss_source=0.5606 / regression_loss_source=0.0805 / alpha=0.4364\n",
            "Epoch [10/100] Step [201/282]: domain_loss_target=0.8029 / domain_loss_source=0.5951 / regression_loss_source=0.0421 / alpha=0.4506\n",
            "VALIDATION LOSS (Average) : 0.12558065354824066\n",
            "TEST LOSS (Average) : 0.11857086420059204\n",
            "Epoch [11/100] Step [1/282]: domain_loss_target=0.7071 / domain_loss_source=0.6522 / regression_loss_source=0.0591 / alpha=0.4621\n",
            "Epoch [11/100] Step [101/282]: domain_loss_target=0.7020 / domain_loss_source=0.6666 / regression_loss_source=0.0661 / alpha=0.4759\n",
            "Epoch [11/100] Step [201/282]: domain_loss_target=0.7330 / domain_loss_source=0.5914 / regression_loss_source=0.1084 / alpha=0.4895\n",
            "VALIDATION LOSS (Average) : 0.12561722099781036\n",
            "TEST LOSS (Average) : 0.1184032882253329\n",
            "Epoch [12/100] Step [1/282]: domain_loss_target=0.6326 / domain_loss_source=0.6354 / regression_loss_source=0.0431 / alpha=0.5005\n",
            "Epoch [12/100] Step [101/282]: domain_loss_target=0.7321 / domain_loss_source=0.7722 / regression_loss_source=0.0848 / alpha=0.5137\n",
            "Epoch [12/100] Step [201/282]: domain_loss_target=0.7726 / domain_loss_source=0.6657 / regression_loss_source=0.1227 / alpha=0.5266\n",
            "VALIDATION LOSS (Average) : 0.12656567990779877\n",
            "TEST LOSS (Average) : 0.12031041582425435\n",
            "Epoch [13/100] Step [1/282]: domain_loss_target=0.7853 / domain_loss_source=0.6002 / regression_loss_source=0.0882 / alpha=0.5370\n",
            "Epoch [13/100] Step [101/282]: domain_loss_target=0.6720 / domain_loss_source=0.6456 / regression_loss_source=0.0612 / alpha=0.5495\n",
            "Epoch [13/100] Step [201/282]: domain_loss_target=0.7046 / domain_loss_source=0.6738 / regression_loss_source=0.0941 / alpha=0.5618\n",
            "VALIDATION LOSS (Average) : 0.12615562975406647\n",
            "TEST LOSS (Average) : 0.11933918793996175\n",
            "Epoch [14/100] Step [1/282]: domain_loss_target=0.6671 / domain_loss_source=0.8831 / regression_loss_source=0.0872 / alpha=0.5717\n",
            "Epoch [14/100] Step [101/282]: domain_loss_target=0.6741 / domain_loss_source=0.6376 / regression_loss_source=0.0715 / alpha=0.5835\n",
            "Epoch [14/100] Step [201/282]: domain_loss_target=0.6654 / domain_loss_source=0.6869 / regression_loss_source=0.0577 / alpha=0.5951\n",
            "VALIDATION LOSS (Average) : 0.12952139973640442\n",
            "TEST LOSS (Average) : 0.12387799471616745\n",
            "Epoch [15/100] Step [1/282]: domain_loss_target=0.7164 / domain_loss_source=0.7575 / regression_loss_source=0.0535 / alpha=0.6044\n",
            "Epoch [15/100] Step [101/282]: domain_loss_target=0.6557 / domain_loss_source=0.6256 / regression_loss_source=0.0718 / alpha=0.6155\n",
            "Epoch [15/100] Step [201/282]: domain_loss_target=0.6703 / domain_loss_source=0.7051 / regression_loss_source=0.0441 / alpha=0.6264\n",
            "VALIDATION LOSS (Average) : 0.12551186978816986\n",
            "TEST LOSS (Average) : 0.11803646634022395\n",
            "Epoch [16/100] Step [1/282]: domain_loss_target=0.7117 / domain_loss_source=0.7293 / regression_loss_source=0.0597 / alpha=0.6351\n",
            "Epoch [16/100] Step [101/282]: domain_loss_target=0.6329 / domain_loss_source=0.8016 / regression_loss_source=0.0849 / alpha=0.6456\n",
            "Epoch [16/100] Step [201/282]: domain_loss_target=0.6519 / domain_loss_source=0.7906 / regression_loss_source=0.0630 / alpha=0.6558\n",
            "VALIDATION LOSS (Average) : 0.12791000306606293\n",
            "TEST LOSS (Average) : 0.12146808952093124\n",
            "Epoch [17/100] Step [1/282]: domain_loss_target=0.6467 / domain_loss_source=0.8246 / regression_loss_source=0.0740 / alpha=0.6640\n",
            "Epoch [17/100] Step [101/282]: domain_loss_target=0.6777 / domain_loss_source=0.7902 / regression_loss_source=0.1211 / alpha=0.6738\n",
            "Epoch [17/100] Step [201/282]: domain_loss_target=0.6525 / domain_loss_source=0.7432 / regression_loss_source=0.1108 / alpha=0.6834\n",
            "VALIDATION LOSS (Average) : 0.12513920664787292\n",
            "TEST LOSS (Average) : 0.1187494695186615\n",
            "Epoch [18/100] Step [1/282]: domain_loss_target=0.6506 / domain_loss_source=0.8020 / regression_loss_source=0.0952 / alpha=0.6911\n",
            "Epoch [18/100] Step [101/282]: domain_loss_target=0.6382 / domain_loss_source=0.7831 / regression_loss_source=0.0650 / alpha=0.7002\n",
            "Epoch [18/100] Step [201/282]: domain_loss_target=0.6538 / domain_loss_source=0.7374 / regression_loss_source=0.0739 / alpha=0.7091\n",
            "VALIDATION LOSS (Average) : 0.12539690732955933\n",
            "TEST LOSS (Average) : 0.11968835691610973\n",
            "Epoch [19/100] Step [1/282]: domain_loss_target=0.6621 / domain_loss_source=0.7353 / regression_loss_source=0.0452 / alpha=0.7163\n",
            "Epoch [19/100] Step [101/282]: domain_loss_target=0.6369 / domain_loss_source=0.7467 / regression_loss_source=0.0198 / alpha=0.7248\n",
            "Epoch [19/100] Step [201/282]: domain_loss_target=0.6544 / domain_loss_source=0.7620 / regression_loss_source=0.0784 / alpha=0.7331\n",
            "VALIDATION LOSS (Average) : 0.12918442487716675\n",
            "TEST LOSS (Average) : 0.12378105024496715\n",
            "Epoch [20/100] Step [1/282]: domain_loss_target=0.6209 / domain_loss_source=0.8215 / regression_loss_source=0.0315 / alpha=0.7398\n",
            "Epoch [20/100] Step [101/282]: domain_loss_target=0.6200 / domain_loss_source=0.7281 / regression_loss_source=0.1208 / alpha=0.7477\n",
            "Epoch [20/100] Step [201/282]: domain_loss_target=0.6352 / domain_loss_source=0.7282 / regression_loss_source=0.0714 / alpha=0.7554\n",
            "VALIDATION LOSS (Average) : 0.12751449644565582\n",
            "TEST LOSS (Average) : 0.12046888967355092\n",
            "Epoch [21/100] Step [1/282]: domain_loss_target=0.6403 / domain_loss_source=0.8056 / regression_loss_source=0.0927 / alpha=0.7616\n",
            "Epoch [21/100] Step [101/282]: domain_loss_target=0.6276 / domain_loss_source=0.7877 / regression_loss_source=0.0703 / alpha=0.7689\n",
            "Epoch [21/100] Step [201/282]: domain_loss_target=0.6305 / domain_loss_source=0.7707 / regression_loss_source=0.1099 / alpha=0.7761\n",
            "VALIDATION LOSS (Average) : 0.1273796409368515\n",
            "TEST LOSS (Average) : 0.12020646284023921\n",
            "Epoch [22/100] Step [1/282]: domain_loss_target=0.6138 / domain_loss_source=0.7642 / regression_loss_source=0.0521 / alpha=0.7818\n",
            "Epoch [22/100] Step [101/282]: domain_loss_target=0.6141 / domain_loss_source=0.7947 / regression_loss_source=0.0665 / alpha=0.7886\n",
            "Epoch [22/100] Step [201/282]: domain_loss_target=0.6312 / domain_loss_source=0.7782 / regression_loss_source=0.0567 / alpha=0.7952\n",
            "VALIDATION LOSS (Average) : 0.12529881298542023\n",
            "TEST LOSS (Average) : 0.11647557218869527\n",
            "Epoch [23/100] Step [1/282]: domain_loss_target=0.6104 / domain_loss_source=0.7668 / regression_loss_source=0.0648 / alpha=0.8005\n",
            "Epoch [23/100] Step [101/282]: domain_loss_target=0.6164 / domain_loss_source=0.8208 / regression_loss_source=0.0938 / alpha=0.8068\n",
            "Epoch [23/100] Step [201/282]: domain_loss_target=0.6292 / domain_loss_source=0.8052 / regression_loss_source=0.0556 / alpha=0.8129\n",
            "VALIDATION LOSS (Average) : 0.127457857131958\n",
            "TEST LOSS (Average) : 0.12142474452654521\n",
            "Epoch [24/100] Step [1/282]: domain_loss_target=0.6275 / domain_loss_source=0.7385 / regression_loss_source=0.0859 / alpha=0.8178\n",
            "Epoch [24/100] Step [101/282]: domain_loss_target=0.6390 / domain_loss_source=0.7779 / regression_loss_source=0.0612 / alpha=0.8235\n",
            "Epoch [24/100] Step [201/282]: domain_loss_target=0.6185 / domain_loss_source=0.7727 / regression_loss_source=0.0528 / alpha=0.8292\n",
            "VALIDATION LOSS (Average) : 0.12633657455444336\n",
            "TEST LOSS (Average) : 0.12106370677550633\n",
            "Epoch [25/100] Step [1/282]: domain_loss_target=0.6336 / domain_loss_source=0.7557 / regression_loss_source=0.0451 / alpha=0.8337\n",
            "Epoch [25/100] Step [101/282]: domain_loss_target=0.5852 / domain_loss_source=0.7610 / regression_loss_source=0.0661 / alpha=0.8390\n",
            "Epoch [25/100] Step [201/282]: domain_loss_target=0.6341 / domain_loss_source=0.7687 / regression_loss_source=0.0561 / alpha=0.8442\n",
            "VALIDATION LOSS (Average) : 0.12566746771335602\n",
            "TEST LOSS (Average) : 0.12024279683828354\n",
            "Epoch [26/100] Step [1/282]: domain_loss_target=0.6089 / domain_loss_source=0.7672 / regression_loss_source=0.0652 / alpha=0.8483\n",
            "Epoch [26/100] Step [101/282]: domain_loss_target=0.6310 / domain_loss_source=0.7524 / regression_loss_source=0.0567 / alpha=0.8532\n",
            "Epoch [26/100] Step [201/282]: domain_loss_target=0.6069 / domain_loss_source=0.7607 / regression_loss_source=0.0408 / alpha=0.8579\n",
            "VALIDATION LOSS (Average) : 0.12661588191986084\n",
            "TEST LOSS (Average) : 0.12158036231994629\n",
            "Epoch [27/100] Step [1/282]: domain_loss_target=0.6690 / domain_loss_source=0.7259 / regression_loss_source=0.0599 / alpha=0.8617\n",
            "Epoch [27/100] Step [101/282]: domain_loss_target=0.6029 / domain_loss_source=0.7246 / regression_loss_source=0.0394 / alpha=0.8662\n",
            "Epoch [27/100] Step [201/282]: domain_loss_target=0.6207 / domain_loss_source=0.7748 / regression_loss_source=0.1004 / alpha=0.8706\n",
            "VALIDATION LOSS (Average) : 0.1252899467945099\n",
            "TEST LOSS (Average) : 0.11984558155139287\n",
            "Epoch [28/100] Step [1/282]: domain_loss_target=0.6237 / domain_loss_source=0.7892 / regression_loss_source=0.0752 / alpha=0.8741\n",
            "Epoch [28/100] Step [101/282]: domain_loss_target=0.6619 / domain_loss_source=0.7585 / regression_loss_source=0.0840 / alpha=0.8782\n",
            "Epoch [28/100] Step [201/282]: domain_loss_target=0.5770 / domain_loss_source=0.7645 / regression_loss_source=0.0447 / alpha=0.8822\n",
            "VALIDATION LOSS (Average) : 0.12585633993148804\n",
            "TEST LOSS (Average) : 0.11974397053321202\n",
            "Epoch [29/100] Step [1/282]: domain_loss_target=0.6234 / domain_loss_source=0.7224 / regression_loss_source=0.0773 / alpha=0.8854\n",
            "Epoch [29/100] Step [101/282]: domain_loss_target=0.6173 / domain_loss_source=0.7366 / regression_loss_source=0.0576 / alpha=0.8891\n",
            "Epoch [29/100] Step [201/282]: domain_loss_target=0.6213 / domain_loss_source=0.7388 / regression_loss_source=0.0752 / alpha=0.8928\n",
            "VALIDATION LOSS (Average) : 0.12678727507591248\n",
            "TEST LOSS (Average) : 0.12098659326632817\n",
            "Epoch [30/100] Step [1/282]: domain_loss_target=0.6621 / domain_loss_source=0.7577 / regression_loss_source=0.0610 / alpha=0.8957\n",
            "Epoch [30/100] Step [101/282]: domain_loss_target=0.7088 / domain_loss_source=0.7575 / regression_loss_source=0.0904 / alpha=0.8991\n",
            "Epoch [30/100] Step [201/282]: domain_loss_target=0.6166 / domain_loss_source=0.7883 / regression_loss_source=0.0152 / alpha=0.9025\n",
            "VALIDATION LOSS (Average) : 0.12592542171478271\n",
            "TEST LOSS (Average) : 0.12065202742815018\n",
            "Epoch [31/100] Step [1/282]: domain_loss_target=0.7203 / domain_loss_source=0.7783 / regression_loss_source=0.0640 / alpha=0.9051\n",
            "Epoch [31/100] Step [101/282]: domain_loss_target=0.6878 / domain_loss_source=0.7200 / regression_loss_source=0.0372 / alpha=0.9083\n",
            "Epoch [31/100] Step [201/282]: domain_loss_target=0.7257 / domain_loss_source=0.7133 / regression_loss_source=0.0741 / alpha=0.9114\n",
            "VALIDATION LOSS (Average) : 0.12570177018642426\n",
            "TEST LOSS (Average) : 0.12038405984640121\n",
            "Epoch [32/100] Step [1/282]: domain_loss_target=0.7164 / domain_loss_source=0.6831 / regression_loss_source=0.0749 / alpha=0.9138\n",
            "Epoch [32/100] Step [101/282]: domain_loss_target=0.7124 / domain_loss_source=0.6860 / regression_loss_source=0.0580 / alpha=0.9167\n",
            "Epoch [32/100] Step [201/282]: domain_loss_target=0.6986 / domain_loss_source=0.6842 / regression_loss_source=0.0774 / alpha=0.9195\n",
            "VALIDATION LOSS (Average) : 0.12495654821395874\n",
            "TEST LOSS (Average) : 0.11896008749802907\n",
            "Epoch [33/100] Step [1/282]: domain_loss_target=0.6880 / domain_loss_source=0.6740 / regression_loss_source=0.0501 / alpha=0.9217\n",
            "Epoch [33/100] Step [101/282]: domain_loss_target=0.6913 / domain_loss_source=0.6816 / regression_loss_source=0.0436 / alpha=0.9243\n",
            "Epoch [33/100] Step [201/282]: domain_loss_target=0.6911 / domain_loss_source=0.6875 / regression_loss_source=0.0686 / alpha=0.9268\n",
            "VALIDATION LOSS (Average) : 0.12715880572795868\n",
            "TEST LOSS (Average) : 0.12168056021134059\n",
            "Epoch [34/100] Step [1/282]: domain_loss_target=0.6840 / domain_loss_source=0.6919 / regression_loss_source=0.1551 / alpha=0.9289\n",
            "Epoch [34/100] Step [101/282]: domain_loss_target=0.6893 / domain_loss_source=0.6891 / regression_loss_source=0.0757 / alpha=0.9313\n",
            "Epoch [34/100] Step [201/282]: domain_loss_target=0.6844 / domain_loss_source=0.6742 / regression_loss_source=0.1013 / alpha=0.9336\n",
            "VALIDATION LOSS (Average) : 0.1242828369140625\n",
            "TEST LOSS (Average) : 0.11860592663288116\n",
            "Epoch [35/100] Step [1/282]: domain_loss_target=0.7028 / domain_loss_source=0.6767 / regression_loss_source=0.0424 / alpha=0.9354\n",
            "Epoch [35/100] Step [101/282]: domain_loss_target=0.6949 / domain_loss_source=0.6887 / regression_loss_source=0.0330 / alpha=0.9376\n",
            "Epoch [35/100] Step [201/282]: domain_loss_target=0.6917 / domain_loss_source=0.6860 / regression_loss_source=0.0564 / alpha=0.9397\n",
            "VALIDATION LOSS (Average) : 0.12459173798561096\n",
            "TEST LOSS (Average) : 0.11880144725243251\n",
            "Epoch [36/100] Step [1/282]: domain_loss_target=0.6777 / domain_loss_source=0.6996 / regression_loss_source=0.0540 / alpha=0.9414\n",
            "Epoch [36/100] Step [101/282]: domain_loss_target=0.6864 / domain_loss_source=0.6866 / regression_loss_source=0.0648 / alpha=0.9434\n",
            "Epoch [36/100] Step [201/282]: domain_loss_target=0.6959 / domain_loss_source=0.6853 / regression_loss_source=0.0758 / alpha=0.9453\n",
            "VALIDATION LOSS (Average) : 0.12538467347621918\n",
            "TEST LOSS (Average) : 0.11910109470287959\n",
            "Epoch [37/100] Step [1/282]: domain_loss_target=0.6742 / domain_loss_source=0.6938 / regression_loss_source=0.0763 / alpha=0.9468\n",
            "Epoch [37/100] Step [101/282]: domain_loss_target=0.6834 / domain_loss_source=0.6991 / regression_loss_source=0.0481 / alpha=0.9486\n",
            "Epoch [37/100] Step [201/282]: domain_loss_target=0.6787 / domain_loss_source=0.6848 / regression_loss_source=0.0300 / alpha=0.9504\n",
            "VALIDATION LOSS (Average) : 0.12912917137145996\n",
            "TEST LOSS (Average) : 0.12434036533037822\n",
            "Epoch [38/100] Step [1/282]: domain_loss_target=0.6862 / domain_loss_source=0.7158 / regression_loss_source=0.0682 / alpha=0.9517\n",
            "Epoch [38/100] Step [101/282]: domain_loss_target=0.6672 / domain_loss_source=0.7163 / regression_loss_source=0.0520 / alpha=0.9534\n",
            "Epoch [38/100] Step [201/282]: domain_loss_target=0.6791 / domain_loss_source=0.6964 / regression_loss_source=0.0610 / alpha=0.9550\n",
            "VALIDATION LOSS (Average) : 0.12424013018608093\n",
            "TEST LOSS (Average) : 0.11814996600151062\n",
            "Epoch [39/100] Step [1/282]: domain_loss_target=0.7018 / domain_loss_source=0.6783 / regression_loss_source=0.0646 / alpha=0.9562\n",
            "Epoch [39/100] Step [101/282]: domain_loss_target=0.6927 / domain_loss_source=0.6798 / regression_loss_source=0.0508 / alpha=0.9577\n",
            "Epoch [39/100] Step [201/282]: domain_loss_target=0.6875 / domain_loss_source=0.6779 / regression_loss_source=0.0442 / alpha=0.9592\n",
            "VALIDATION LOSS (Average) : 0.12500938773155212\n",
            "TEST LOSS (Average) : 0.1202263484398524\n",
            "Epoch [40/100] Step [1/282]: domain_loss_target=0.6791 / domain_loss_source=0.6969 / regression_loss_source=0.0664 / alpha=0.9603\n",
            "Epoch [40/100] Step [101/282]: domain_loss_target=0.6809 / domain_loss_source=0.6971 / regression_loss_source=0.0505 / alpha=0.9617\n",
            "Epoch [40/100] Step [201/282]: domain_loss_target=0.7075 / domain_loss_source=0.6889 / regression_loss_source=0.0535 / alpha=0.9630\n",
            "VALIDATION LOSS (Average) : 0.12259149551391602\n",
            "TEST LOSS (Average) : 0.11757433414459229\n",
            "Epoch [41/100] Step [1/282]: domain_loss_target=0.7140 / domain_loss_source=0.7037 / regression_loss_source=0.0577 / alpha=0.9640\n",
            "Epoch [41/100] Step [101/282]: domain_loss_target=0.6929 / domain_loss_source=0.6954 / regression_loss_source=0.0522 / alpha=0.9653\n",
            "Epoch [41/100] Step [201/282]: domain_loss_target=0.6750 / domain_loss_source=0.7155 / regression_loss_source=0.0656 / alpha=0.9664\n",
            "VALIDATION LOSS (Average) : 0.12332974374294281\n",
            "TEST LOSS (Average) : 0.11899528900782268\n",
            "Epoch [42/100] Step [1/282]: domain_loss_target=0.6759 / domain_loss_source=0.7183 / regression_loss_source=0.0281 / alpha=0.9674\n",
            "Epoch [42/100] Step [101/282]: domain_loss_target=0.6783 / domain_loss_source=0.7043 / regression_loss_source=0.0993 / alpha=0.9685\n",
            "Epoch [42/100] Step [201/282]: domain_loss_target=0.6852 / domain_loss_source=0.7020 / regression_loss_source=0.0719 / alpha=0.9696\n",
            "VALIDATION LOSS (Average) : 0.12399372458457947\n",
            "TEST LOSS (Average) : 0.11903625478347142\n",
            "Epoch [43/100] Step [1/282]: domain_loss_target=0.6874 / domain_loss_source=0.6991 / regression_loss_source=0.0216 / alpha=0.9705\n",
            "Epoch [43/100] Step [101/282]: domain_loss_target=0.6920 / domain_loss_source=0.6922 / regression_loss_source=0.0446 / alpha=0.9715\n",
            "Epoch [43/100] Step [201/282]: domain_loss_target=0.6915 / domain_loss_source=0.6909 / regression_loss_source=0.0448 / alpha=0.9724\n",
            "VALIDATION LOSS (Average) : 0.12319806963205338\n",
            "TEST LOSS (Average) : 0.1174759070078532\n",
            "Epoch [44/100] Step [1/282]: domain_loss_target=0.6954 / domain_loss_source=0.6852 / regression_loss_source=0.0527 / alpha=0.9732\n",
            "Epoch [44/100] Step [101/282]: domain_loss_target=0.6925 / domain_loss_source=0.6771 / regression_loss_source=0.0464 / alpha=0.9741\n",
            "Epoch [44/100] Step [201/282]: domain_loss_target=0.6968 / domain_loss_source=0.6820 / regression_loss_source=0.0515 / alpha=0.9750\n",
            "VALIDATION LOSS (Average) : 0.1227777972817421\n",
            "TEST LOSS (Average) : 0.11692261199156444\n",
            "Epoch [45/100] Step [1/282]: domain_loss_target=0.6905 / domain_loss_source=0.6707 / regression_loss_source=0.0707 / alpha=0.9757\n",
            "Epoch [45/100] Step [101/282]: domain_loss_target=0.6936 / domain_loss_source=0.6851 / regression_loss_source=0.0383 / alpha=0.9766\n",
            "Epoch [45/100] Step [201/282]: domain_loss_target=0.6904 / domain_loss_source=0.6878 / regression_loss_source=0.0605 / alpha=0.9774\n",
            "VALIDATION LOSS (Average) : 0.1244148537516594\n",
            "TEST LOSS (Average) : 0.11868788053592046\n",
            "Epoch [46/100] Step [1/282]: domain_loss_target=0.6913 / domain_loss_source=0.6922 / regression_loss_source=0.0396 / alpha=0.9780\n",
            "Epoch [46/100] Step [101/282]: domain_loss_target=0.6922 / domain_loss_source=0.6826 / regression_loss_source=0.0611 / alpha=0.9788\n",
            "Epoch [46/100] Step [201/282]: domain_loss_target=0.6869 / domain_loss_source=0.6959 / regression_loss_source=0.0497 / alpha=0.9795\n",
            "VALIDATION LOSS (Average) : 0.12437190115451813\n",
            "TEST LOSS (Average) : 0.11840963363647461\n",
            "Epoch [47/100] Step [1/282]: domain_loss_target=0.6965 / domain_loss_source=0.6981 / regression_loss_source=0.0350 / alpha=0.9801\n",
            "Epoch [47/100] Step [101/282]: domain_loss_target=0.6903 / domain_loss_source=0.7020 / regression_loss_source=0.0378 / alpha=0.9808\n",
            "Epoch [47/100] Step [201/282]: domain_loss_target=0.6941 / domain_loss_source=0.6951 / regression_loss_source=0.0355 / alpha=0.9814\n",
            "VALIDATION LOSS (Average) : 0.12393222004175186\n",
            "TEST LOSS (Average) : 0.11711726834376653\n",
            "Epoch [48/100] Step [1/282]: domain_loss_target=0.7044 / domain_loss_source=0.6915 / regression_loss_source=0.0655 / alpha=0.9820\n",
            "Epoch [48/100] Step [101/282]: domain_loss_target=0.6930 / domain_loss_source=0.6962 / regression_loss_source=0.0532 / alpha=0.9826\n",
            "Epoch [48/100] Step [201/282]: domain_loss_target=0.6950 / domain_loss_source=0.6738 / regression_loss_source=0.0607 / alpha=0.9832\n",
            "VALIDATION LOSS (Average) : 0.12562669813632965\n",
            "TEST LOSS (Average) : 0.11913064618905385\n",
            "Epoch [49/100] Step [1/282]: domain_loss_target=0.7020 / domain_loss_source=0.6772 / regression_loss_source=0.0677 / alpha=0.9837\n",
            "Epoch [49/100] Step [101/282]: domain_loss_target=0.7006 / domain_loss_source=0.6785 / regression_loss_source=0.0831 / alpha=0.9842\n",
            "Epoch [49/100] Step [201/282]: domain_loss_target=0.6859 / domain_loss_source=0.6914 / regression_loss_source=0.0546 / alpha=0.9848\n",
            "VALIDATION LOSS (Average) : 0.12370431423187256\n",
            "TEST LOSS (Average) : 0.11704349766174953\n",
            "Epoch [50/100] Step [1/282]: domain_loss_target=0.6753 / domain_loss_source=0.6917 / regression_loss_source=0.0514 / alpha=0.9852\n",
            "Epoch [50/100] Step [101/282]: domain_loss_target=0.6796 / domain_loss_source=0.6976 / regression_loss_source=0.0384 / alpha=0.9857\n",
            "Epoch [50/100] Step [201/282]: domain_loss_target=0.6923 / domain_loss_source=0.7039 / regression_loss_source=0.0450 / alpha=0.9862\n",
            "VALIDATION LOSS (Average) : 0.12520313262939453\n",
            "TEST LOSS (Average) : 0.11984895169734955\n",
            "Epoch [51/100] Step [1/282]: domain_loss_target=0.6816 / domain_loss_source=0.6967 / regression_loss_source=0.0631 / alpha=0.9866\n",
            "Epoch [51/100] Step [101/282]: domain_loss_target=0.6934 / domain_loss_source=0.6899 / regression_loss_source=0.0922 / alpha=0.9871\n",
            "Epoch [51/100] Step [201/282]: domain_loss_target=0.6896 / domain_loss_source=0.6899 / regression_loss_source=0.1158 / alpha=0.9875\n",
            "VALIDATION LOSS (Average) : 0.12417832016944885\n",
            "TEST LOSS (Average) : 0.11886130273342133\n",
            "Epoch [52/100] Step [1/282]: domain_loss_target=0.6895 / domain_loss_source=0.6923 / regression_loss_source=0.0663 / alpha=0.9879\n",
            "Epoch [52/100] Step [101/282]: domain_loss_target=0.6888 / domain_loss_source=0.6850 / regression_loss_source=0.0713 / alpha=0.9883\n",
            "Epoch [52/100] Step [201/282]: domain_loss_target=0.6990 / domain_loss_source=0.6699 / regression_loss_source=0.0644 / alpha=0.9887\n",
            "VALIDATION LOSS (Average) : 0.12665292620658875\n",
            "TEST LOSS (Average) : 0.12266263862450917\n",
            "Epoch [53/100] Step [1/282]: domain_loss_target=0.7159 / domain_loss_source=0.6847 / regression_loss_source=0.0538 / alpha=0.9890\n",
            "Epoch [53/100] Step [101/282]: domain_loss_target=0.6962 / domain_loss_source=0.6801 / regression_loss_source=0.0748 / alpha=0.9894\n",
            "Epoch [53/100] Step [201/282]: domain_loss_target=0.6927 / domain_loss_source=0.6884 / regression_loss_source=0.0532 / alpha=0.9898\n",
            "VALIDATION LOSS (Average) : 0.12296756356954575\n",
            "TEST LOSS (Average) : 0.11772757520278294\n",
            "Epoch [54/100] Step [1/282]: domain_loss_target=0.6882 / domain_loss_source=0.6829 / regression_loss_source=0.0742 / alpha=0.9901\n",
            "Epoch [54/100] Step [101/282]: domain_loss_target=0.6963 / domain_loss_source=0.6717 / regression_loss_source=0.0678 / alpha=0.9904\n",
            "Epoch [54/100] Step [201/282]: domain_loss_target=0.6849 / domain_loss_source=0.6814 / regression_loss_source=0.1169 / alpha=0.9907\n",
            "VALIDATION LOSS (Average) : 0.12270282953977585\n",
            "TEST LOSS (Average) : 0.11753230790297191\n",
            "Epoch [55/100] Step [1/282]: domain_loss_target=0.7002 / domain_loss_source=0.6722 / regression_loss_source=0.0749 / alpha=0.9910\n",
            "Epoch [55/100] Step [101/282]: domain_loss_target=0.6827 / domain_loss_source=0.6834 / regression_loss_source=0.0752 / alpha=0.9913\n",
            "Epoch [55/100] Step [201/282]: domain_loss_target=0.6866 / domain_loss_source=0.6897 / regression_loss_source=0.0562 / alpha=0.9916\n",
            "VALIDATION LOSS (Average) : 0.12286288291215897\n",
            "TEST LOSS (Average) : 0.11717536797126134\n",
            "Epoch [56/100] Step [1/282]: domain_loss_target=0.6955 / domain_loss_source=0.6805 / regression_loss_source=0.0509 / alpha=0.9919\n",
            "Epoch [56/100] Step [101/282]: domain_loss_target=0.6973 / domain_loss_source=0.6772 / regression_loss_source=0.0473 / alpha=0.9921\n",
            "Epoch [56/100] Step [201/282]: domain_loss_target=0.6829 / domain_loss_source=0.6856 / regression_loss_source=0.0551 / alpha=0.9924\n",
            "VALIDATION LOSS (Average) : 0.1229790598154068\n",
            "TEST LOSS (Average) : 0.11723439147075017\n",
            "Epoch [57/100] Step [1/282]: domain_loss_target=0.7043 / domain_loss_source=0.6844 / regression_loss_source=0.0514 / alpha=0.9926\n",
            "Epoch [57/100] Step [101/282]: domain_loss_target=0.6801 / domain_loss_source=0.6982 / regression_loss_source=0.0528 / alpha=0.9929\n",
            "Epoch [57/100] Step [201/282]: domain_loss_target=0.6792 / domain_loss_source=0.6797 / regression_loss_source=0.0662 / alpha=0.9931\n",
            "VALIDATION LOSS (Average) : 0.12372046709060669\n",
            "TEST LOSS (Average) : 0.11866036305824916\n",
            "Epoch [58/100] Step [1/282]: domain_loss_target=0.6879 / domain_loss_source=0.6949 / regression_loss_source=0.0269 / alpha=0.9933\n",
            "Epoch [58/100] Step [101/282]: domain_loss_target=0.6783 / domain_loss_source=0.7052 / regression_loss_source=0.0489 / alpha=0.9936\n",
            "Epoch [58/100] Step [201/282]: domain_loss_target=0.6789 / domain_loss_source=0.6805 / regression_loss_source=0.0753 / alpha=0.9938\n",
            "VALIDATION LOSS (Average) : 0.12445225566625595\n",
            "TEST LOSS (Average) : 0.11963611096143723\n",
            "Epoch [59/100] Step [1/282]: domain_loss_target=0.6755 / domain_loss_source=0.6966 / regression_loss_source=0.0560 / alpha=0.9940\n",
            "Epoch [59/100] Step [101/282]: domain_loss_target=0.6644 / domain_loss_source=0.6900 / regression_loss_source=0.0351 / alpha=0.9942\n",
            "Epoch [59/100] Step [201/282]: domain_loss_target=0.6799 / domain_loss_source=0.6743 / regression_loss_source=0.0460 / alpha=0.9944\n",
            "VALIDATION LOSS (Average) : 0.12382017821073532\n",
            "TEST LOSS (Average) : 0.11862894644339879\n",
            "Epoch [60/100] Step [1/282]: domain_loss_target=0.6796 / domain_loss_source=0.6594 / regression_loss_source=0.1078 / alpha=0.9945\n",
            "Epoch [60/100] Step [101/282]: domain_loss_target=0.6860 / domain_loss_source=0.6889 / regression_loss_source=0.0429 / alpha=0.9947\n",
            "Epoch [60/100] Step [201/282]: domain_loss_target=0.6635 / domain_loss_source=0.6806 / regression_loss_source=0.0600 / alpha=0.9949\n",
            "VALIDATION LOSS (Average) : 0.12527967989444733\n",
            "TEST LOSS (Average) : 0.12183327972888947\n",
            "Epoch [61/100] Step [1/282]: domain_loss_target=0.6784 / domain_loss_source=0.6847 / regression_loss_source=0.0328 / alpha=0.9951\n",
            "Epoch [61/100] Step [101/282]: domain_loss_target=0.6959 / domain_loss_source=0.6433 / regression_loss_source=0.0764 / alpha=0.9952\n",
            "Epoch [61/100] Step [201/282]: domain_loss_target=0.6724 / domain_loss_source=0.6867 / regression_loss_source=0.0497 / alpha=0.9954\n",
            "VALIDATION LOSS (Average) : 0.12474075704813004\n",
            "TEST LOSS (Average) : 0.120762233932813\n",
            "Epoch [62/100] Step [1/282]: domain_loss_target=0.6760 / domain_loss_source=0.6649 / regression_loss_source=0.0831 / alpha=0.9955\n",
            "Epoch [62/100] Step [101/282]: domain_loss_target=0.6775 / domain_loss_source=0.6646 / regression_loss_source=0.0689 / alpha=0.9957\n",
            "Epoch [62/100] Step [201/282]: domain_loss_target=0.6796 / domain_loss_source=0.6707 / regression_loss_source=0.0756 / alpha=0.9958\n",
            "VALIDATION LOSS (Average) : 0.12411799281835556\n",
            "TEST LOSS (Average) : 0.11854034662246704\n",
            "Epoch [63/100] Step [1/282]: domain_loss_target=0.6978 / domain_loss_source=0.6968 / regression_loss_source=0.0648 / alpha=0.9959\n",
            "Epoch [63/100] Step [101/282]: domain_loss_target=0.6764 / domain_loss_source=0.6770 / regression_loss_source=0.0264 / alpha=0.9961\n",
            "Epoch [63/100] Step [201/282]: domain_loss_target=0.6806 / domain_loss_source=0.7060 / regression_loss_source=0.0455 / alpha=0.9962\n",
            "VALIDATION LOSS (Average) : 0.12336519360542297\n",
            "TEST LOSS (Average) : 0.11776494483153026\n",
            "Epoch [64/100] Step [1/282]: domain_loss_target=0.7207 / domain_loss_source=0.6447 / regression_loss_source=0.0719 / alpha=0.9963\n",
            "Epoch [64/100] Step [101/282]: domain_loss_target=0.7135 / domain_loss_source=0.6840 / regression_loss_source=0.0851 / alpha=0.9965\n",
            "Epoch [64/100] Step [201/282]: domain_loss_target=0.6885 / domain_loss_source=0.6777 / regression_loss_source=0.0581 / alpha=0.9966\n",
            "VALIDATION LOSS (Average) : 0.12357879430055618\n",
            "TEST LOSS (Average) : 0.11914073179165523\n",
            "Epoch [65/100] Step [1/282]: domain_loss_target=0.6870 / domain_loss_source=0.7016 / regression_loss_source=0.0808 / alpha=0.9967\n",
            "Epoch [65/100] Step [101/282]: domain_loss_target=0.7040 / domain_loss_source=0.6579 / regression_loss_source=0.0824 / alpha=0.9968\n",
            "Epoch [65/100] Step [201/282]: domain_loss_target=0.7080 / domain_loss_source=0.6686 / regression_loss_source=0.0862 / alpha=0.9969\n",
            "VALIDATION LOSS (Average) : 0.12399131804704666\n",
            "TEST LOSS (Average) : 0.12004736810922623\n",
            "Epoch [66/100] Step [1/282]: domain_loss_target=0.6900 / domain_loss_source=0.6811 / regression_loss_source=0.0267 / alpha=0.9970\n",
            "Epoch [66/100] Step [101/282]: domain_loss_target=0.6781 / domain_loss_source=0.6799 / regression_loss_source=0.0429 / alpha=0.9971\n",
            "Epoch [66/100] Step [201/282]: domain_loss_target=0.7444 / domain_loss_source=0.6642 / regression_loss_source=0.0730 / alpha=0.9972\n",
            "VALIDATION LOSS (Average) : 0.12372959405183792\n",
            "TEST LOSS (Average) : 0.12010189145803452\n",
            "Epoch [67/100] Step [1/282]: domain_loss_target=0.6908 / domain_loss_source=0.6767 / regression_loss_source=0.0726 / alpha=0.9973\n",
            "Epoch [67/100] Step [101/282]: domain_loss_target=0.6954 / domain_loss_source=0.6550 / regression_loss_source=0.0602 / alpha=0.9974\n",
            "Epoch [67/100] Step [201/282]: domain_loss_target=0.6946 / domain_loss_source=0.6487 / regression_loss_source=0.0944 / alpha=0.9975\n",
            "VALIDATION LOSS (Average) : 0.12334997206926346\n",
            "TEST LOSS (Average) : 0.1185737947622935\n",
            "Epoch [68/100] Step [1/282]: domain_loss_target=0.7049 / domain_loss_source=0.6569 / regression_loss_source=0.0604 / alpha=0.9975\n",
            "Epoch [68/100] Step [101/282]: domain_loss_target=0.7243 / domain_loss_source=0.6960 / regression_loss_source=0.0538 / alpha=0.9976\n",
            "Epoch [68/100] Step [201/282]: domain_loss_target=0.7125 / domain_loss_source=0.6802 / regression_loss_source=0.0771 / alpha=0.9977\n",
            "VALIDATION LOSS (Average) : 0.12511847913265228\n",
            "TEST LOSS (Average) : 0.12120152513186137\n",
            "Epoch [69/100] Step [1/282]: domain_loss_target=0.7082 / domain_loss_source=0.6930 / regression_loss_source=0.0586 / alpha=0.9978\n",
            "Epoch [69/100] Step [101/282]: domain_loss_target=0.6838 / domain_loss_source=0.6696 / regression_loss_source=0.0745 / alpha=0.9979\n",
            "Epoch [69/100] Step [201/282]: domain_loss_target=0.7081 / domain_loss_source=0.7089 / regression_loss_source=0.0590 / alpha=0.9979\n",
            "VALIDATION LOSS (Average) : 0.12335681915283203\n",
            "TEST LOSS (Average) : 0.11892940600713094\n",
            "Epoch [70/100] Step [1/282]: domain_loss_target=0.6811 / domain_loss_source=0.6528 / regression_loss_source=0.0439 / alpha=0.9980\n",
            "Epoch [70/100] Step [101/282]: domain_loss_target=0.6955 / domain_loss_source=0.7019 / regression_loss_source=0.0608 / alpha=0.9981\n",
            "Epoch [70/100] Step [201/282]: domain_loss_target=0.7030 / domain_loss_source=0.7005 / regression_loss_source=0.0582 / alpha=0.9981\n",
            "VALIDATION LOSS (Average) : 0.12547244131565094\n",
            "TEST LOSS (Average) : 0.12120468666156133\n",
            "Epoch [71/100] Step [1/282]: domain_loss_target=0.6863 / domain_loss_source=0.6858 / regression_loss_source=0.0644 / alpha=0.9982\n",
            "Epoch [71/100] Step [101/282]: domain_loss_target=0.7123 / domain_loss_source=0.6750 / regression_loss_source=0.0487 / alpha=0.9982\n",
            "Epoch [71/100] Step [201/282]: domain_loss_target=0.6739 / domain_loss_source=0.6769 / regression_loss_source=0.0530 / alpha=0.9983\n",
            "VALIDATION LOSS (Average) : 0.12413771450519562\n",
            "TEST LOSS (Average) : 0.11899504065513611\n",
            "Epoch [72/100] Step [1/282]: domain_loss_target=0.6782 / domain_loss_source=0.7008 / regression_loss_source=0.0627 / alpha=0.9984\n",
            "Epoch [72/100] Step [101/282]: domain_loss_target=0.6943 / domain_loss_source=0.6747 / regression_loss_source=0.0380 / alpha=0.9984\n",
            "Epoch [72/100] Step [201/282]: domain_loss_target=0.6850 / domain_loss_source=0.6955 / regression_loss_source=0.1313 / alpha=0.9985\n",
            "VALIDATION LOSS (Average) : 0.1238279789686203\n",
            "TEST LOSS (Average) : 0.11834248155355453\n",
            "Epoch [73/100] Step [1/282]: domain_loss_target=0.6862 / domain_loss_source=0.7077 / regression_loss_source=0.0584 / alpha=0.9985\n",
            "Epoch [73/100] Step [101/282]: domain_loss_target=0.6919 / domain_loss_source=0.7007 / regression_loss_source=0.0492 / alpha=0.9986\n",
            "Epoch [73/100] Step [201/282]: domain_loss_target=0.7059 / domain_loss_source=0.6669 / regression_loss_source=0.0402 / alpha=0.9986\n",
            "VALIDATION LOSS (Average) : 0.1246291995048523\n",
            "TEST LOSS (Average) : 0.12037433435519536\n",
            "Epoch [74/100] Step [1/282]: domain_loss_target=0.6837 / domain_loss_source=0.6741 / regression_loss_source=0.0485 / alpha=0.9986\n",
            "Epoch [74/100] Step [101/282]: domain_loss_target=0.6611 / domain_loss_source=0.6865 / regression_loss_source=0.0668 / alpha=0.9987\n",
            "Epoch [74/100] Step [201/282]: domain_loss_target=0.6563 / domain_loss_source=0.7204 / regression_loss_source=0.0454 / alpha=0.9987\n",
            "VALIDATION LOSS (Average) : 0.1248704046010971\n",
            "TEST LOSS (Average) : 0.12022007753451665\n",
            "Epoch [75/100] Step [1/282]: domain_loss_target=0.6666 / domain_loss_source=0.7239 / regression_loss_source=0.0492 / alpha=0.9988\n",
            "Epoch [75/100] Step [101/282]: domain_loss_target=0.6899 / domain_loss_source=0.7147 / regression_loss_source=0.0438 / alpha=0.9988\n",
            "Epoch [75/100] Step [201/282]: domain_loss_target=0.6869 / domain_loss_source=0.7034 / regression_loss_source=0.0320 / alpha=0.9989\n",
            "VALIDATION LOSS (Average) : 0.12329044938087463\n",
            "TEST LOSS (Average) : 0.11955669273932774\n",
            "Epoch [76/100] Step [1/282]: domain_loss_target=0.6830 / domain_loss_source=0.6817 / regression_loss_source=0.0424 / alpha=0.9989\n",
            "Epoch [76/100] Step [101/282]: domain_loss_target=0.6853 / domain_loss_source=0.7096 / regression_loss_source=0.0734 / alpha=0.9989\n",
            "Epoch [76/100] Step [201/282]: domain_loss_target=0.7000 / domain_loss_source=0.6539 / regression_loss_source=0.0692 / alpha=0.9990\n",
            "VALIDATION LOSS (Average) : 0.12547975778579712\n",
            "TEST LOSS (Average) : 0.12288901954889297\n",
            "Epoch [77/100] Step [1/282]: domain_loss_target=0.7427 / domain_loss_source=0.6537 / regression_loss_source=0.0373 / alpha=0.9990\n",
            "Epoch [77/100] Step [101/282]: domain_loss_target=0.6917 / domain_loss_source=0.6725 / regression_loss_source=0.0704 / alpha=0.9990\n",
            "Epoch [77/100] Step [201/282]: domain_loss_target=0.6706 / domain_loss_source=0.6915 / regression_loss_source=0.1152 / alpha=0.9991\n",
            "VALIDATION LOSS (Average) : 0.12545111775398254\n",
            "TEST LOSS (Average) : 0.12266805271307628\n",
            "Epoch [78/100] Step [1/282]: domain_loss_target=0.6446 / domain_loss_source=0.7055 / regression_loss_source=0.0405 / alpha=0.9991\n",
            "Epoch [78/100] Step [101/282]: domain_loss_target=0.6635 / domain_loss_source=0.7101 / regression_loss_source=0.0411 / alpha=0.9991\n",
            "Epoch [78/100] Step [201/282]: domain_loss_target=0.6972 / domain_loss_source=0.6994 / regression_loss_source=0.0886 / alpha=0.9992\n",
            "VALIDATION LOSS (Average) : 0.12366826087236404\n",
            "TEST LOSS (Average) : 0.1212068647146225\n",
            "Epoch [79/100] Step [1/282]: domain_loss_target=0.6994 / domain_loss_source=0.7091 / regression_loss_source=0.1235 / alpha=0.9992\n",
            "Epoch [79/100] Step [101/282]: domain_loss_target=0.6991 / domain_loss_source=0.6789 / regression_loss_source=0.0660 / alpha=0.9992\n",
            "Epoch [79/100] Step [201/282]: domain_loss_target=0.7310 / domain_loss_source=0.6773 / regression_loss_source=0.0606 / alpha=0.9992\n",
            "VALIDATION LOSS (Average) : 0.1232796385884285\n",
            "TEST LOSS (Average) : 0.12018335610628128\n",
            "Epoch [80/100] Step [1/282]: domain_loss_target=0.6607 / domain_loss_source=0.6735 / regression_loss_source=0.0804 / alpha=0.9993\n",
            "Epoch [80/100] Step [101/282]: domain_loss_target=0.7008 / domain_loss_source=0.6969 / regression_loss_source=0.0598 / alpha=0.9993\n",
            "Epoch [80/100] Step [201/282]: domain_loss_target=0.6878 / domain_loss_source=0.7049 / regression_loss_source=0.0485 / alpha=0.9993\n",
            "VALIDATION LOSS (Average) : 0.12467921525239944\n",
            "TEST LOSS (Average) : 0.12155865629514058\n",
            "Epoch [81/100] Step [1/282]: domain_loss_target=0.6724 / domain_loss_source=0.6866 / regression_loss_source=0.0742 / alpha=0.9993\n",
            "Epoch [81/100] Step [101/282]: domain_loss_target=0.6744 / domain_loss_source=0.6817 / regression_loss_source=0.0560 / alpha=0.9994\n",
            "Epoch [81/100] Step [201/282]: domain_loss_target=0.6605 / domain_loss_source=0.7075 / regression_loss_source=0.0637 / alpha=0.9994\n",
            "VALIDATION LOSS (Average) : 0.12303143739700317\n",
            "TEST LOSS (Average) : 0.11961806813875835\n",
            "Epoch [82/100] Step [1/282]: domain_loss_target=0.6654 / domain_loss_source=0.7125 / regression_loss_source=0.0409 / alpha=0.9994\n",
            "Epoch [82/100] Step [101/282]: domain_loss_target=0.7084 / domain_loss_source=0.7095 / regression_loss_source=0.0583 / alpha=0.9994\n",
            "Epoch [82/100] Step [201/282]: domain_loss_target=0.6975 / domain_loss_source=0.6812 / regression_loss_source=0.0817 / alpha=0.9994\n",
            "VALIDATION LOSS (Average) : 0.12326124310493469\n",
            "TEST LOSS (Average) : 0.1191336711247762\n",
            "Epoch [83/100] Step [1/282]: domain_loss_target=0.6947 / domain_loss_source=0.6549 / regression_loss_source=0.0659 / alpha=0.9995\n",
            "Epoch [83/100] Step [101/282]: domain_loss_target=0.7227 / domain_loss_source=0.6787 / regression_loss_source=0.0496 / alpha=0.9995\n",
            "Epoch [83/100] Step [201/282]: domain_loss_target=0.6950 / domain_loss_source=0.6561 / regression_loss_source=0.0551 / alpha=0.9995\n",
            "VALIDATION LOSS (Average) : 0.123038649559021\n",
            "TEST LOSS (Average) : 0.1200457215309143\n",
            "Epoch [84/100] Step [1/282]: domain_loss_target=0.6881 / domain_loss_source=0.6927 / regression_loss_source=0.0700 / alpha=0.9995\n",
            "Epoch [84/100] Step [101/282]: domain_loss_target=0.6715 / domain_loss_source=0.7095 / regression_loss_source=0.0671 / alpha=0.9995\n",
            "Epoch [84/100] Step [201/282]: domain_loss_target=0.6946 / domain_loss_source=0.6882 / regression_loss_source=0.0632 / alpha=0.9995\n",
            "VALIDATION LOSS (Average) : 0.12324918061494827\n",
            "TEST LOSS (Average) : 0.11973170191049576\n",
            "Epoch [85/100] Step [1/282]: domain_loss_target=0.6767 / domain_loss_source=0.7126 / regression_loss_source=0.0463 / alpha=0.9996\n",
            "Epoch [85/100] Step [101/282]: domain_loss_target=0.7086 / domain_loss_source=0.6841 / regression_loss_source=0.0622 / alpha=0.9996\n",
            "Epoch [85/100] Step [201/282]: domain_loss_target=0.7130 / domain_loss_source=0.6672 / regression_loss_source=0.0494 / alpha=0.9996\n",
            "VALIDATION LOSS (Average) : 0.1228087767958641\n",
            "TEST LOSS (Average) : 0.11946682135264079\n",
            "Epoch [86/100] Step [1/282]: domain_loss_target=0.7147 / domain_loss_source=0.6750 / regression_loss_source=0.0602 / alpha=0.9996\n",
            "Epoch [86/100] Step [101/282]: domain_loss_target=0.6993 / domain_loss_source=0.6822 / regression_loss_source=0.0551 / alpha=0.9996\n",
            "Epoch [86/100] Step [201/282]: domain_loss_target=0.6964 / domain_loss_source=0.7119 / regression_loss_source=0.0587 / alpha=0.9996\n",
            "VALIDATION LOSS (Average) : 0.12360940873622894\n",
            "TEST LOSS (Average) : 0.11963719129562378\n",
            "Epoch [87/100] Step [1/282]: domain_loss_target=0.7013 / domain_loss_source=0.6984 / regression_loss_source=0.0482 / alpha=0.9996\n",
            "Epoch [87/100] Step [101/282]: domain_loss_target=0.6710 / domain_loss_source=0.7085 / regression_loss_source=0.0804 / alpha=0.9996\n",
            "Epoch [87/100] Step [201/282]: domain_loss_target=0.6984 / domain_loss_source=0.6794 / regression_loss_source=0.0775 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.12255845963954926\n",
            "TEST LOSS (Average) : 0.11927679429451625\n",
            "Epoch [88/100] Step [1/282]: domain_loss_target=0.6613 / domain_loss_source=0.6977 / regression_loss_source=0.0547 / alpha=0.9997\n",
            "Epoch [88/100] Step [101/282]: domain_loss_target=0.6794 / domain_loss_source=0.6822 / regression_loss_source=0.0767 / alpha=0.9997\n",
            "Epoch [88/100] Step [201/282]: domain_loss_target=0.6791 / domain_loss_source=0.7027 / regression_loss_source=0.0598 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.1223110780119896\n",
            "TEST LOSS (Average) : 0.11864058425029118\n",
            "Epoch [89/100] Step [1/282]: domain_loss_target=0.7007 / domain_loss_source=0.6990 / regression_loss_source=0.0384 / alpha=0.9997\n",
            "Epoch [89/100] Step [101/282]: domain_loss_target=0.7119 / domain_loss_source=0.6867 / regression_loss_source=0.0497 / alpha=0.9997\n",
            "Epoch [89/100] Step [201/282]: domain_loss_target=0.7166 / domain_loss_source=0.6503 / regression_loss_source=0.0449 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.12353309988975525\n",
            "TEST LOSS (Average) : 0.1204602171977361\n",
            "Epoch [90/100] Step [1/282]: domain_loss_target=0.7118 / domain_loss_source=0.6840 / regression_loss_source=0.0665 / alpha=0.9997\n",
            "Epoch [90/100] Step [101/282]: domain_loss_target=0.7031 / domain_loss_source=0.6933 / regression_loss_source=0.0592 / alpha=0.9997\n",
            "Epoch [90/100] Step [201/282]: domain_loss_target=0.6909 / domain_loss_source=0.6729 / regression_loss_source=0.0835 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.12149937450885773\n",
            "TEST LOSS (Average) : 0.11841098964214325\n",
            "Epoch [91/100] Step [1/282]: domain_loss_target=0.7025 / domain_loss_source=0.6949 / regression_loss_source=0.0470 / alpha=0.9998\n",
            "Epoch [91/100] Step [101/282]: domain_loss_target=0.7067 / domain_loss_source=0.6941 / regression_loss_source=0.0751 / alpha=0.9998\n",
            "Epoch [91/100] Step [201/282]: domain_loss_target=0.7214 / domain_loss_source=0.6586 / regression_loss_source=0.0581 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.12231267988681793\n",
            "TEST LOSS (Average) : 0.11785049239794414\n",
            "Epoch [92/100] Step [1/282]: domain_loss_target=0.7081 / domain_loss_source=0.6742 / regression_loss_source=0.0701 / alpha=0.9998\n",
            "Epoch [92/100] Step [101/282]: domain_loss_target=0.7082 / domain_loss_source=0.7073 / regression_loss_source=0.0413 / alpha=0.9998\n",
            "Epoch [92/100] Step [201/282]: domain_loss_target=0.6863 / domain_loss_source=0.6915 / regression_loss_source=0.0605 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.12319496273994446\n",
            "TEST LOSS (Average) : 0.12009679029385249\n",
            "Epoch [93/100] Step [1/282]: domain_loss_target=0.7191 / domain_loss_source=0.7018 / regression_loss_source=0.0684 / alpha=0.9998\n",
            "Epoch [93/100] Step [101/282]: domain_loss_target=0.7222 / domain_loss_source=0.7062 / regression_loss_source=0.0597 / alpha=0.9998\n",
            "Epoch [93/100] Step [201/282]: domain_loss_target=0.6882 / domain_loss_source=0.6859 / regression_loss_source=0.0851 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.12211007624864578\n",
            "TEST LOSS (Average) : 0.11843125522136688\n",
            "Epoch [94/100] Step [1/282]: domain_loss_target=0.7072 / domain_loss_source=0.7096 / regression_loss_source=0.0350 / alpha=0.9998\n",
            "Epoch [94/100] Step [101/282]: domain_loss_target=0.6545 / domain_loss_source=0.7143 / regression_loss_source=0.0460 / alpha=0.9998\n",
            "Epoch [94/100] Step [201/282]: domain_loss_target=0.6846 / domain_loss_source=0.7122 / regression_loss_source=0.0663 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.12164123356342316\n",
            "TEST LOSS (Average) : 0.11790035665035248\n",
            "Epoch [95/100] Step [1/282]: domain_loss_target=0.6673 / domain_loss_source=0.7126 / regression_loss_source=0.0671 / alpha=0.9998\n",
            "Epoch [95/100] Step [101/282]: domain_loss_target=0.6657 / domain_loss_source=0.6979 / regression_loss_source=0.0569 / alpha=0.9998\n",
            "Epoch [95/100] Step [201/282]: domain_loss_target=0.6798 / domain_loss_source=0.6816 / regression_loss_source=0.0758 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.12216459214687347\n",
            "TEST LOSS (Average) : 0.11821146309375763\n",
            "Epoch [96/100] Step [1/282]: domain_loss_target=0.7115 / domain_loss_source=0.6800 / regression_loss_source=0.0683 / alpha=0.9999\n",
            "Epoch [96/100] Step [101/282]: domain_loss_target=0.6961 / domain_loss_source=0.7046 / regression_loss_source=0.0363 / alpha=0.9999\n",
            "Epoch [96/100] Step [201/282]: domain_loss_target=0.6759 / domain_loss_source=0.7245 / regression_loss_source=0.0603 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.12289153784513474\n",
            "TEST LOSS (Average) : 0.11883712808291118\n",
            "Epoch [97/100] Step [1/282]: domain_loss_target=0.6996 / domain_loss_source=0.6961 / regression_loss_source=0.0597 / alpha=0.9999\n",
            "Epoch [97/100] Step [101/282]: domain_loss_target=0.7003 / domain_loss_source=0.6816 / regression_loss_source=0.0756 / alpha=0.9999\n",
            "Epoch [97/100] Step [201/282]: domain_loss_target=0.7280 / domain_loss_source=0.6541 / regression_loss_source=0.0568 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.12145750969648361\n",
            "TEST LOSS (Average) : 0.11827693382898967\n",
            "Epoch [98/100] Step [1/282]: domain_loss_target=0.7157 / domain_loss_source=0.6711 / regression_loss_source=0.0579 / alpha=0.9999\n",
            "Epoch [98/100] Step [101/282]: domain_loss_target=0.6725 / domain_loss_source=0.7096 / regression_loss_source=0.0639 / alpha=0.9999\n",
            "Epoch [98/100] Step [201/282]: domain_loss_target=0.6849 / domain_loss_source=0.6962 / regression_loss_source=0.0579 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.12060944736003876\n",
            "TEST LOSS (Average) : 0.11708265542984009\n",
            "Epoch [99/100] Step [1/282]: domain_loss_target=0.6966 / domain_loss_source=0.6923 / regression_loss_source=0.0334 / alpha=0.9999\n",
            "Epoch [99/100] Step [101/282]: domain_loss_target=0.7226 / domain_loss_source=0.6860 / regression_loss_source=0.0470 / alpha=0.9999\n",
            "Epoch [99/100] Step [201/282]: domain_loss_target=0.6828 / domain_loss_source=0.6443 / regression_loss_source=0.0695 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.12241280823945999\n",
            "TEST LOSS (Average) : 0.11808732151985168\n",
            "Epoch [100/100] Step [1/282]: domain_loss_target=0.7063 / domain_loss_source=0.7083 / regression_loss_source=0.0926 / alpha=0.9999\n",
            "Epoch [100/100] Step [101/282]: domain_loss_target=0.6659 / domain_loss_source=0.6985 / regression_loss_source=0.0652 / alpha=0.9999\n",
            "Epoch [100/100] Step [201/282]: domain_loss_target=0.6606 / domain_loss_source=0.6981 / regression_loss_source=0.0337 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.12070055305957794\n",
            "TEST LOSS (Average) : 0.11676945288976033\n",
            "----------------------training complete for DANN model - EI_fear-----------------\n",
            "----------------------training started for DANN model - V-----------------\n",
            "Epoch [1/100] Step [1/148]: domain_loss_target=0.7819 / domain_loss_source=0.6230 / regression_loss_source=0.1237 / alpha=0.0000\n",
            "Epoch [1/100] Step [101/148]: domain_loss_target=0.8136 / domain_loss_source=0.5686 / regression_loss_source=0.0506 / alpha=0.0338\n",
            "VALIDATION LOSS (Average) : 0.14801301062107086\n",
            "TEST LOSS (Average) : 0.1212267056107521\n",
            "Epoch [2/100] Step [1/148]: domain_loss_target=0.8203 / domain_loss_source=0.5424 / regression_loss_source=0.0773 / alpha=0.0500\n",
            "Epoch [2/100] Step [101/148]: domain_loss_target=0.7851 / domain_loss_source=0.4359 / regression_loss_source=0.0468 / alpha=0.0836\n",
            "VALIDATION LOSS (Average) : 0.1505449265241623\n",
            "TEST LOSS (Average) : 0.1270854870478312\n",
            "Epoch [3/100] Step [1/148]: domain_loss_target=0.7785 / domain_loss_source=0.4572 / regression_loss_source=0.0844 / alpha=0.0997\n",
            "Epoch [3/100] Step [101/148]: domain_loss_target=0.7448 / domain_loss_source=0.3786 / regression_loss_source=0.0728 / alpha=0.1330\n",
            "VALIDATION LOSS (Average) : 0.14938266575336456\n",
            "TEST LOSS (Average) : 0.12207887818415959\n",
            "Epoch [4/100] Step [1/148]: domain_loss_target=0.8150 / domain_loss_source=0.3569 / regression_loss_source=0.1039 / alpha=0.1489\n",
            "Epoch [4/100] Step [101/148]: domain_loss_target=0.7098 / domain_loss_source=0.3424 / regression_loss_source=0.0621 / alpha=0.1817\n",
            "VALIDATION LOSS (Average) : 0.14941170811653137\n",
            "TEST LOSS (Average) : 0.12698020786046982\n",
            "Epoch [5/100] Step [1/148]: domain_loss_target=0.6798 / domain_loss_source=0.3651 / regression_loss_source=0.0612 / alpha=0.1974\n",
            "Epoch [5/100] Step [101/148]: domain_loss_target=0.7540 / domain_loss_source=0.3855 / regression_loss_source=0.0579 / alpha=0.2296\n",
            "VALIDATION LOSS (Average) : 0.14912624657154083\n",
            "TEST LOSS (Average) : 0.12656301508347192\n",
            "Epoch [6/100] Step [1/148]: domain_loss_target=0.7953 / domain_loss_source=0.3490 / regression_loss_source=0.0991 / alpha=0.2449\n",
            "Epoch [6/100] Step [101/148]: domain_loss_target=0.9223 / domain_loss_source=0.3378 / regression_loss_source=0.0737 / alpha=0.2764\n",
            "VALIDATION LOSS (Average) : 0.14894722402095795\n",
            "TEST LOSS (Average) : 0.1207393209139506\n",
            "Epoch [7/100] Step [1/148]: domain_loss_target=0.8999 / domain_loss_source=0.3334 / regression_loss_source=0.1139 / alpha=0.2913\n",
            "Epoch [7/100] Step [101/148]: domain_loss_target=0.9518 / domain_loss_source=0.5807 / regression_loss_source=0.0550 / alpha=0.3219\n",
            "VALIDATION LOSS (Average) : 0.14990156888961792\n",
            "TEST LOSS (Average) : 0.12700003385543823\n",
            "Epoch [8/100] Step [1/148]: domain_loss_target=0.9471 / domain_loss_source=0.4176 / regression_loss_source=0.0599 / alpha=0.3364\n",
            "Epoch [8/100] Step [101/148]: domain_loss_target=0.8163 / domain_loss_source=0.3503 / regression_loss_source=0.0943 / alpha=0.3660\n",
            "VALIDATION LOSS (Average) : 0.15020008385181427\n",
            "TEST LOSS (Average) : 0.12686837464571\n",
            "Epoch [9/100] Step [1/148]: domain_loss_target=0.9943 / domain_loss_source=0.5254 / regression_loss_source=0.1035 / alpha=0.3799\n",
            "Epoch [9/100] Step [101/148]: domain_loss_target=0.9208 / domain_loss_source=0.3987 / regression_loss_source=0.0755 / alpha=0.4085\n",
            "VALIDATION LOSS (Average) : 0.1487116813659668\n",
            "TEST LOSS (Average) : 0.12191258370876312\n",
            "Epoch [10/100] Step [1/148]: domain_loss_target=0.9597 / domain_loss_source=0.3942 / regression_loss_source=0.0897 / alpha=0.4219\n",
            "Epoch [10/100] Step [101/148]: domain_loss_target=0.7157 / domain_loss_source=0.3560 / regression_loss_source=0.0508 / alpha=0.4493\n",
            "VALIDATION LOSS (Average) : 0.14833320677280426\n",
            "TEST LOSS (Average) : 0.12150502949953079\n",
            "Epoch [11/100] Step [1/148]: domain_loss_target=0.8603 / domain_loss_source=0.4062 / regression_loss_source=0.1165 / alpha=0.4621\n",
            "Epoch [11/100] Step [101/148]: domain_loss_target=0.8438 / domain_loss_source=0.3830 / regression_loss_source=0.0667 / alpha=0.4883\n",
            "VALIDATION LOSS (Average) : 0.1490965187549591\n",
            "TEST LOSS (Average) : 0.12849197536706924\n",
            "Epoch [12/100] Step [1/148]: domain_loss_target=0.8254 / domain_loss_source=0.4586 / regression_loss_source=0.0829 / alpha=0.5005\n",
            "Epoch [12/100] Step [101/148]: domain_loss_target=0.8476 / domain_loss_source=0.4193 / regression_loss_source=0.1337 / alpha=0.5254\n",
            "VALIDATION LOSS (Average) : 0.14955724775791168\n",
            "TEST LOSS (Average) : 0.13042697310447693\n",
            "Epoch [13/100] Step [1/148]: domain_loss_target=0.7897 / domain_loss_source=0.4491 / regression_loss_source=0.1163 / alpha=0.5370\n",
            "Epoch [13/100] Step [101/148]: domain_loss_target=0.7507 / domain_loss_source=0.4159 / regression_loss_source=0.0594 / alpha=0.5607\n",
            "VALIDATION LOSS (Average) : 0.14847669005393982\n",
            "TEST LOSS (Average) : 0.12775593996047974\n",
            "Epoch [14/100] Step [1/148]: domain_loss_target=0.6758 / domain_loss_source=0.5500 / regression_loss_source=0.1294 / alpha=0.5717\n",
            "Epoch [14/100] Step [101/148]: domain_loss_target=0.7598 / domain_loss_source=0.4506 / regression_loss_source=0.0776 / alpha=0.5940\n",
            "VALIDATION LOSS (Average) : 0.14864350855350494\n",
            "TEST LOSS (Average) : 0.1248033915956815\n",
            "Epoch [15/100] Step [1/148]: domain_loss_target=0.7570 / domain_loss_source=0.5442 / regression_loss_source=0.0990 / alpha=0.6044\n",
            "Epoch [15/100] Step [101/148]: domain_loss_target=0.7783 / domain_loss_source=0.4349 / regression_loss_source=0.0940 / alpha=0.6254\n",
            "VALIDATION LOSS (Average) : 0.15121813118457794\n",
            "TEST LOSS (Average) : 0.13318379720052084\n",
            "Epoch [16/100] Step [1/148]: domain_loss_target=0.7790 / domain_loss_source=0.5905 / regression_loss_source=0.0513 / alpha=0.6351\n",
            "Epoch [16/100] Step [101/148]: domain_loss_target=0.6845 / domain_loss_source=0.5881 / regression_loss_source=0.0802 / alpha=0.6549\n",
            "VALIDATION LOSS (Average) : 0.1512247771024704\n",
            "TEST LOSS (Average) : 0.13014053056637445\n",
            "Epoch [17/100] Step [1/148]: domain_loss_target=0.7675 / domain_loss_source=0.5629 / regression_loss_source=0.0680 / alpha=0.6640\n",
            "Epoch [17/100] Step [101/148]: domain_loss_target=0.7893 / domain_loss_source=0.5158 / regression_loss_source=0.0503 / alpha=0.6825\n",
            "VALIDATION LOSS (Average) : 0.15084969997406006\n",
            "TEST LOSS (Average) : 0.1284301926692327\n",
            "Epoch [18/100] Step [1/148]: domain_loss_target=0.7510 / domain_loss_source=0.6655 / regression_loss_source=0.0400 / alpha=0.6911\n",
            "Epoch [18/100] Step [101/148]: domain_loss_target=0.7933 / domain_loss_source=0.5995 / regression_loss_source=0.0774 / alpha=0.7083\n",
            "VALIDATION LOSS (Average) : 0.15024790167808533\n",
            "TEST LOSS (Average) : 0.12458799034357071\n",
            "Epoch [19/100] Step [1/148]: domain_loss_target=0.7599 / domain_loss_source=0.6390 / regression_loss_source=0.1182 / alpha=0.7163\n",
            "Epoch [19/100] Step [101/148]: domain_loss_target=0.6412 / domain_loss_source=0.6656 / regression_loss_source=0.0814 / alpha=0.7324\n",
            "VALIDATION LOSS (Average) : 0.15207931399345398\n",
            "TEST LOSS (Average) : 0.13277012606461844\n",
            "Epoch [20/100] Step [1/148]: domain_loss_target=0.7691 / domain_loss_source=0.6354 / regression_loss_source=0.0514 / alpha=0.7398\n",
            "Epoch [20/100] Step [101/148]: domain_loss_target=0.7731 / domain_loss_source=0.5157 / regression_loss_source=0.0934 / alpha=0.7547\n",
            "VALIDATION LOSS (Average) : 0.15063773095607758\n",
            "TEST LOSS (Average) : 0.12774495283762613\n",
            "Epoch [21/100] Step [1/148]: domain_loss_target=0.7091 / domain_loss_source=0.5507 / regression_loss_source=0.1071 / alpha=0.7616\n",
            "Epoch [21/100] Step [101/148]: domain_loss_target=0.7928 / domain_loss_source=0.4986 / regression_loss_source=0.1321 / alpha=0.7754\n",
            "VALIDATION LOSS (Average) : 0.15392273664474487\n",
            "TEST LOSS (Average) : 0.1385661413272222\n",
            "Epoch [22/100] Step [1/148]: domain_loss_target=0.6189 / domain_loss_source=0.6044 / regression_loss_source=0.0778 / alpha=0.7818\n",
            "Epoch [22/100] Step [101/148]: domain_loss_target=0.7400 / domain_loss_source=0.6218 / regression_loss_source=0.0908 / alpha=0.7946\n",
            "VALIDATION LOSS (Average) : 0.15358130633831024\n",
            "TEST LOSS (Average) : 0.13755094508330026\n",
            "Epoch [23/100] Step [1/148]: domain_loss_target=0.7449 / domain_loss_source=0.5404 / regression_loss_source=0.1066 / alpha=0.8005\n",
            "Epoch [23/100] Step [101/148]: domain_loss_target=0.8079 / domain_loss_source=0.6966 / regression_loss_source=0.0699 / alpha=0.8123\n",
            "VALIDATION LOSS (Average) : 0.1506803035736084\n",
            "TEST LOSS (Average) : 0.12674723068873087\n",
            "Epoch [24/100] Step [1/148]: domain_loss_target=0.6664 / domain_loss_source=0.6050 / regression_loss_source=0.0554 / alpha=0.8178\n",
            "Epoch [24/100] Step [101/148]: domain_loss_target=0.6844 / domain_loss_source=0.6385 / regression_loss_source=0.1124 / alpha=0.8286\n",
            "VALIDATION LOSS (Average) : 0.1515274941921234\n",
            "TEST LOSS (Average) : 0.12900265802939734\n",
            "Epoch [25/100] Step [1/148]: domain_loss_target=0.7180 / domain_loss_source=0.7089 / regression_loss_source=0.0911 / alpha=0.8337\n",
            "Epoch [25/100] Step [101/148]: domain_loss_target=0.7899 / domain_loss_source=0.6871 / regression_loss_source=0.0753 / alpha=0.8437\n",
            "VALIDATION LOSS (Average) : 0.1526518017053604\n",
            "TEST LOSS (Average) : 0.13350370526313782\n",
            "Epoch [26/100] Step [1/148]: domain_loss_target=0.6874 / domain_loss_source=0.6536 / regression_loss_source=0.0837 / alpha=0.8483\n",
            "Epoch [26/100] Step [101/148]: domain_loss_target=0.6846 / domain_loss_source=0.5984 / regression_loss_source=0.0851 / alpha=0.8575\n",
            "VALIDATION LOSS (Average) : 0.15263663232326508\n",
            "TEST LOSS (Average) : 0.1329128717382749\n",
            "Epoch [27/100] Step [1/148]: domain_loss_target=0.6653 / domain_loss_source=0.6923 / regression_loss_source=0.1097 / alpha=0.8617\n",
            "Epoch [27/100] Step [101/148]: domain_loss_target=0.6805 / domain_loss_source=0.6889 / regression_loss_source=0.0723 / alpha=0.8702\n",
            "VALIDATION LOSS (Average) : 0.15407365560531616\n",
            "TEST LOSS (Average) : 0.13415006548166275\n",
            "Epoch [28/100] Step [1/148]: domain_loss_target=0.6897 / domain_loss_source=0.7032 / regression_loss_source=0.1407 / alpha=0.8741\n",
            "Epoch [28/100] Step [101/148]: domain_loss_target=0.6553 / domain_loss_source=0.6389 / regression_loss_source=0.0572 / alpha=0.8818\n",
            "VALIDATION LOSS (Average) : 0.15426863729953766\n",
            "TEST LOSS (Average) : 0.13287603358427683\n",
            "Epoch [29/100] Step [1/148]: domain_loss_target=0.7427 / domain_loss_source=0.7427 / regression_loss_source=0.0719 / alpha=0.8854\n",
            "Epoch [29/100] Step [101/148]: domain_loss_target=0.7253 / domain_loss_source=0.6732 / regression_loss_source=0.0826 / alpha=0.8924\n",
            "VALIDATION LOSS (Average) : 0.1537104994058609\n",
            "TEST LOSS (Average) : 0.12666626771291098\n",
            "Epoch [30/100] Step [1/148]: domain_loss_target=0.7253 / domain_loss_source=0.7115 / regression_loss_source=0.0933 / alpha=0.8957\n",
            "Epoch [30/100] Step [101/148]: domain_loss_target=0.7226 / domain_loss_source=0.7176 / regression_loss_source=0.0932 / alpha=0.9022\n",
            "VALIDATION LOSS (Average) : 0.15312907099723816\n",
            "TEST LOSS (Average) : 0.12617189437150955\n",
            "Epoch [31/100] Step [1/148]: domain_loss_target=0.6530 / domain_loss_source=0.7613 / regression_loss_source=0.0709 / alpha=0.9051\n",
            "Epoch [31/100] Step [101/148]: domain_loss_target=0.6712 / domain_loss_source=0.7787 / regression_loss_source=0.0828 / alpha=0.9111\n",
            "VALIDATION LOSS (Average) : 0.153431236743927\n",
            "TEST LOSS (Average) : 0.13064166406790415\n",
            "Epoch [32/100] Step [1/148]: domain_loss_target=0.7002 / domain_loss_source=0.7788 / regression_loss_source=0.0651 / alpha=0.9138\n",
            "Epoch [32/100] Step [101/148]: domain_loss_target=0.7204 / domain_loss_source=0.7292 / regression_loss_source=0.0689 / alpha=0.9192\n",
            "VALIDATION LOSS (Average) : 0.15553636848926544\n",
            "TEST LOSS (Average) : 0.13836574306090674\n",
            "Epoch [33/100] Step [1/148]: domain_loss_target=0.7282 / domain_loss_source=0.7908 / regression_loss_source=0.1005 / alpha=0.9217\n",
            "Epoch [33/100] Step [101/148]: domain_loss_target=0.6658 / domain_loss_source=0.7016 / regression_loss_source=0.1192 / alpha=0.9266\n",
            "VALIDATION LOSS (Average) : 0.15282033383846283\n",
            "TEST LOSS (Average) : 0.1287642444173495\n",
            "Epoch [34/100] Step [1/148]: domain_loss_target=0.6062 / domain_loss_source=0.7823 / regression_loss_source=0.0633 / alpha=0.9289\n",
            "Epoch [34/100] Step [101/148]: domain_loss_target=0.6665 / domain_loss_source=0.7390 / regression_loss_source=0.0873 / alpha=0.9334\n",
            "VALIDATION LOSS (Average) : 0.15249307453632355\n",
            "TEST LOSS (Average) : 0.13263455281654993\n",
            "Epoch [35/100] Step [1/148]: domain_loss_target=0.7220 / domain_loss_source=0.7513 / regression_loss_source=0.1002 / alpha=0.9354\n",
            "Epoch [35/100] Step [101/148]: domain_loss_target=0.6508 / domain_loss_source=0.7343 / regression_loss_source=0.0939 / alpha=0.9395\n",
            "VALIDATION LOSS (Average) : 0.15200184285640717\n",
            "TEST LOSS (Average) : 0.13238313297430673\n",
            "Epoch [36/100] Step [1/148]: domain_loss_target=0.6534 / domain_loss_source=0.7119 / regression_loss_source=0.1165 / alpha=0.9414\n",
            "Epoch [36/100] Step [101/148]: domain_loss_target=0.6829 / domain_loss_source=0.7473 / regression_loss_source=0.0865 / alpha=0.9451\n",
            "VALIDATION LOSS (Average) : 0.1514824628829956\n",
            "TEST LOSS (Average) : 0.13152674088875452\n",
            "Epoch [37/100] Step [1/148]: domain_loss_target=0.6733 / domain_loss_source=0.6974 / regression_loss_source=0.0803 / alpha=0.9468\n",
            "Epoch [37/100] Step [101/148]: domain_loss_target=0.6841 / domain_loss_source=0.7415 / regression_loss_source=0.0400 / alpha=0.9502\n",
            "VALIDATION LOSS (Average) : 0.15336324274539948\n",
            "TEST LOSS (Average) : 0.1264617716272672\n",
            "Epoch [38/100] Step [1/148]: domain_loss_target=0.6789 / domain_loss_source=0.6713 / regression_loss_source=0.1152 / alpha=0.9517\n",
            "Epoch [38/100] Step [101/148]: domain_loss_target=0.6914 / domain_loss_source=0.7306 / regression_loss_source=0.0736 / alpha=0.9548\n",
            "VALIDATION LOSS (Average) : 0.15303899347782135\n",
            "TEST LOSS (Average) : 0.13047784815231958\n",
            "Epoch [39/100] Step [1/148]: domain_loss_target=0.6891 / domain_loss_source=0.7227 / regression_loss_source=0.0821 / alpha=0.9562\n",
            "Epoch [39/100] Step [101/148]: domain_loss_target=0.6428 / domain_loss_source=0.7456 / regression_loss_source=0.0829 / alpha=0.9590\n",
            "VALIDATION LOSS (Average) : 0.15281634032726288\n",
            "TEST LOSS (Average) : 0.12599505732456842\n",
            "Epoch [40/100] Step [1/148]: domain_loss_target=0.6943 / domain_loss_source=0.7180 / regression_loss_source=0.0724 / alpha=0.9603\n",
            "Epoch [40/100] Step [101/148]: domain_loss_target=0.6741 / domain_loss_source=0.7379 / regression_loss_source=0.0584 / alpha=0.9629\n",
            "VALIDATION LOSS (Average) : 0.15376333892345428\n",
            "TEST LOSS (Average) : 0.13340773185094199\n",
            "Epoch [41/100] Step [1/148]: domain_loss_target=0.6431 / domain_loss_source=0.7684 / regression_loss_source=0.0716 / alpha=0.9640\n",
            "Epoch [41/100] Step [101/148]: domain_loss_target=0.6643 / domain_loss_source=0.7783 / regression_loss_source=0.0509 / alpha=0.9663\n",
            "VALIDATION LOSS (Average) : 0.15232211351394653\n",
            "TEST LOSS (Average) : 0.12976698329051337\n",
            "Epoch [42/100] Step [1/148]: domain_loss_target=0.6884 / domain_loss_source=0.7242 / regression_loss_source=0.0742 / alpha=0.9674\n",
            "Epoch [42/100] Step [101/148]: domain_loss_target=0.6369 / domain_loss_source=0.7492 / regression_loss_source=0.0650 / alpha=0.9695\n",
            "VALIDATION LOSS (Average) : 0.15253369510173798\n",
            "TEST LOSS (Average) : 0.12846404810746512\n",
            "Epoch [43/100] Step [1/148]: domain_loss_target=0.6586 / domain_loss_source=0.7058 / regression_loss_source=0.0693 / alpha=0.9705\n",
            "Epoch [43/100] Step [101/148]: domain_loss_target=0.6665 / domain_loss_source=0.6758 / regression_loss_source=0.0491 / alpha=0.9724\n",
            "VALIDATION LOSS (Average) : 0.15253104269504547\n",
            "TEST LOSS (Average) : 0.12929972261190414\n",
            "Epoch [44/100] Step [1/148]: domain_loss_target=0.6269 / domain_loss_source=0.7257 / regression_loss_source=0.0766 / alpha=0.9732\n",
            "Epoch [44/100] Step [101/148]: domain_loss_target=0.7154 / domain_loss_source=0.7687 / regression_loss_source=0.0751 / alpha=0.9750\n",
            "VALIDATION LOSS (Average) : 0.15221387147903442\n",
            "TEST LOSS (Average) : 0.1285900721947352\n",
            "Epoch [45/100] Step [1/148]: domain_loss_target=0.6586 / domain_loss_source=0.7149 / regression_loss_source=0.0635 / alpha=0.9757\n",
            "Epoch [45/100] Step [101/148]: domain_loss_target=0.6952 / domain_loss_source=0.7490 / regression_loss_source=0.0463 / alpha=0.9773\n",
            "VALIDATION LOSS (Average) : 0.1521925926208496\n",
            "TEST LOSS (Average) : 0.12762144953012466\n",
            "Epoch [46/100] Step [1/148]: domain_loss_target=0.6267 / domain_loss_source=0.7665 / regression_loss_source=0.0543 / alpha=0.9780\n",
            "Epoch [46/100] Step [101/148]: domain_loss_target=0.6287 / domain_loss_source=0.7488 / regression_loss_source=0.1113 / alpha=0.9794\n",
            "VALIDATION LOSS (Average) : 0.15241236984729767\n",
            "TEST LOSS (Average) : 0.13099025934934616\n",
            "Epoch [47/100] Step [1/148]: domain_loss_target=0.6402 / domain_loss_source=0.7452 / regression_loss_source=0.0737 / alpha=0.9801\n",
            "Epoch [47/100] Step [101/148]: domain_loss_target=0.6162 / domain_loss_source=0.7686 / regression_loss_source=0.0537 / alpha=0.9814\n",
            "VALIDATION LOSS (Average) : 0.15344569087028503\n",
            "TEST LOSS (Average) : 0.13402561843395233\n",
            "Epoch [48/100] Step [1/148]: domain_loss_target=0.6593 / domain_loss_source=0.7570 / regression_loss_source=0.0557 / alpha=0.9820\n",
            "Epoch [48/100] Step [101/148]: domain_loss_target=0.6608 / domain_loss_source=0.7491 / regression_loss_source=0.0767 / alpha=0.9831\n",
            "VALIDATION LOSS (Average) : 0.15163558721542358\n",
            "TEST LOSS (Average) : 0.12675126641988754\n",
            "Epoch [49/100] Step [1/148]: domain_loss_target=0.6612 / domain_loss_source=0.7397 / regression_loss_source=0.0789 / alpha=0.9837\n",
            "Epoch [49/100] Step [101/148]: domain_loss_target=0.6744 / domain_loss_source=0.7234 / regression_loss_source=0.0768 / alpha=0.9847\n",
            "VALIDATION LOSS (Average) : 0.15243898332118988\n",
            "TEST LOSS (Average) : 0.1285523052016894\n",
            "Epoch [50/100] Step [1/148]: domain_loss_target=0.6641 / domain_loss_source=0.7052 / regression_loss_source=0.0548 / alpha=0.9852\n",
            "Epoch [50/100] Step [101/148]: domain_loss_target=0.6755 / domain_loss_source=0.7343 / regression_loss_source=0.0451 / alpha=0.9862\n",
            "VALIDATION LOSS (Average) : 0.15210837125778198\n",
            "TEST LOSS (Average) : 0.12533501038948694\n",
            "Epoch [51/100] Step [1/148]: domain_loss_target=0.6619 / domain_loss_source=0.7405 / regression_loss_source=0.0711 / alpha=0.9866\n",
            "Epoch [51/100] Step [101/148]: domain_loss_target=0.6374 / domain_loss_source=0.7240 / regression_loss_source=0.0849 / alpha=0.9875\n",
            "VALIDATION LOSS (Average) : 0.15216825902462006\n",
            "TEST LOSS (Average) : 0.12807153662045798\n",
            "Epoch [52/100] Step [1/148]: domain_loss_target=0.6538 / domain_loss_source=0.7202 / regression_loss_source=0.0782 / alpha=0.9879\n",
            "Epoch [52/100] Step [101/148]: domain_loss_target=0.6712 / domain_loss_source=0.7300 / regression_loss_source=0.0810 / alpha=0.9887\n",
            "VALIDATION LOSS (Average) : 0.15305353701114655\n",
            "TEST LOSS (Average) : 0.1298187772432963\n",
            "Epoch [53/100] Step [1/148]: domain_loss_target=0.6340 / domain_loss_source=0.7543 / regression_loss_source=0.1258 / alpha=0.9890\n",
            "Epoch [53/100] Step [101/148]: domain_loss_target=0.6661 / domain_loss_source=0.7626 / regression_loss_source=0.0577 / alpha=0.9897\n",
            "VALIDATION LOSS (Average) : 0.15166014432907104\n",
            "TEST LOSS (Average) : 0.127614364027977\n",
            "Epoch [54/100] Step [1/148]: domain_loss_target=0.6703 / domain_loss_source=0.7298 / regression_loss_source=0.0661 / alpha=0.9901\n",
            "Epoch [54/100] Step [101/148]: domain_loss_target=0.6441 / domain_loss_source=0.6931 / regression_loss_source=0.0507 / alpha=0.9907\n",
            "VALIDATION LOSS (Average) : 0.15127761662006378\n",
            "TEST LOSS (Average) : 0.12662189453840256\n",
            "Epoch [55/100] Step [1/148]: domain_loss_target=0.7124 / domain_loss_source=0.7108 / regression_loss_source=0.0823 / alpha=0.9910\n",
            "Epoch [55/100] Step [101/148]: domain_loss_target=0.6847 / domain_loss_source=0.7196 / regression_loss_source=0.0575 / alpha=0.9916\n",
            "VALIDATION LOSS (Average) : 0.1505790650844574\n",
            "TEST LOSS (Average) : 0.1265781174103419\n",
            "Epoch [56/100] Step [1/148]: domain_loss_target=0.6592 / domain_loss_source=0.6764 / regression_loss_source=0.0492 / alpha=0.9919\n",
            "Epoch [56/100] Step [101/148]: domain_loss_target=0.7001 / domain_loss_source=0.6622 / regression_loss_source=0.0433 / alpha=0.9924\n",
            "VALIDATION LOSS (Average) : 0.1508779376745224\n",
            "TEST LOSS (Average) : 0.12520833065112433\n",
            "Epoch [57/100] Step [1/148]: domain_loss_target=0.7219 / domain_loss_source=0.7284 / regression_loss_source=0.0909 / alpha=0.9926\n",
            "Epoch [57/100] Step [101/148]: domain_loss_target=0.6933 / domain_loss_source=0.6830 / regression_loss_source=0.0671 / alpha=0.9931\n",
            "VALIDATION LOSS (Average) : 0.1505751609802246\n",
            "TEST LOSS (Average) : 0.12677688399950662\n",
            "Epoch [58/100] Step [1/148]: domain_loss_target=0.6719 / domain_loss_source=0.7558 / regression_loss_source=0.0978 / alpha=0.9933\n",
            "Epoch [58/100] Step [101/148]: domain_loss_target=0.6804 / domain_loss_source=0.7312 / regression_loss_source=0.0513 / alpha=0.9938\n",
            "VALIDATION LOSS (Average) : 0.149711474776268\n",
            "TEST LOSS (Average) : 0.127911110719045\n",
            "Epoch [59/100] Step [1/148]: domain_loss_target=0.6739 / domain_loss_source=0.7177 / regression_loss_source=0.0564 / alpha=0.9940\n",
            "Epoch [59/100] Step [101/148]: domain_loss_target=0.6664 / domain_loss_source=0.7141 / regression_loss_source=0.0679 / alpha=0.9944\n",
            "VALIDATION LOSS (Average) : 0.15137585997581482\n",
            "TEST LOSS (Average) : 0.1326800137758255\n",
            "Epoch [60/100] Step [1/148]: domain_loss_target=0.6740 / domain_loss_source=0.6939 / regression_loss_source=0.0444 / alpha=0.9945\n",
            "Epoch [60/100] Step [101/148]: domain_loss_target=0.6586 / domain_loss_source=0.6773 / regression_loss_source=0.0794 / alpha=0.9949\n",
            "VALIDATION LOSS (Average) : 0.1493370532989502\n",
            "TEST LOSS (Average) : 0.12440456201632817\n",
            "Epoch [61/100] Step [1/148]: domain_loss_target=0.6923 / domain_loss_source=0.7139 / regression_loss_source=0.0451 / alpha=0.9951\n",
            "Epoch [61/100] Step [101/148]: domain_loss_target=0.7159 / domain_loss_source=0.6681 / regression_loss_source=0.1029 / alpha=0.9954\n",
            "VALIDATION LOSS (Average) : 0.14989973604679108\n",
            "TEST LOSS (Average) : 0.1262922684351603\n",
            "Epoch [62/100] Step [1/148]: domain_loss_target=0.6888 / domain_loss_source=0.6523 / regression_loss_source=0.0565 / alpha=0.9955\n",
            "Epoch [62/100] Step [101/148]: domain_loss_target=0.6889 / domain_loss_source=0.6747 / regression_loss_source=0.0594 / alpha=0.9958\n",
            "VALIDATION LOSS (Average) : 0.1508777141571045\n",
            "TEST LOSS (Average) : 0.12776759763558707\n",
            "Epoch [63/100] Step [1/148]: domain_loss_target=0.6834 / domain_loss_source=0.6732 / regression_loss_source=0.0643 / alpha=0.9959\n",
            "Epoch [63/100] Step [101/148]: domain_loss_target=0.7047 / domain_loss_source=0.6734 / regression_loss_source=0.0487 / alpha=0.9962\n",
            "VALIDATION LOSS (Average) : 0.15229560434818268\n",
            "TEST LOSS (Average) : 0.12565415104230246\n",
            "Epoch [64/100] Step [1/148]: domain_loss_target=0.7159 / domain_loss_source=0.6607 / regression_loss_source=0.0405 / alpha=0.9963\n",
            "Epoch [64/100] Step [101/148]: domain_loss_target=0.6952 / domain_loss_source=0.6826 / regression_loss_source=0.0727 / alpha=0.9966\n",
            "VALIDATION LOSS (Average) : 0.15237674117088318\n",
            "TEST LOSS (Average) : 0.13199346015850702\n",
            "Epoch [65/100] Step [1/148]: domain_loss_target=0.6848 / domain_loss_source=0.7099 / regression_loss_source=0.0600 / alpha=0.9967\n",
            "Epoch [65/100] Step [101/148]: domain_loss_target=0.7151 / domain_loss_source=0.6610 / regression_loss_source=0.0906 / alpha=0.9969\n",
            "VALIDATION LOSS (Average) : 0.1520189344882965\n",
            "TEST LOSS (Average) : 0.13370417058467865\n",
            "Epoch [66/100] Step [1/148]: domain_loss_target=0.7134 / domain_loss_source=0.6781 / regression_loss_source=0.0770 / alpha=0.9970\n",
            "Epoch [66/100] Step [101/148]: domain_loss_target=0.7268 / domain_loss_source=0.6799 / regression_loss_source=0.0403 / alpha=0.9972\n",
            "VALIDATION LOSS (Average) : 0.15148630738258362\n",
            "TEST LOSS (Average) : 0.1262004921833674\n",
            "Epoch [67/100] Step [1/148]: domain_loss_target=0.7060 / domain_loss_source=0.6886 / regression_loss_source=0.0856 / alpha=0.9973\n",
            "Epoch [67/100] Step [101/148]: domain_loss_target=0.7107 / domain_loss_source=0.6739 / regression_loss_source=0.0918 / alpha=0.9975\n",
            "VALIDATION LOSS (Average) : 0.15073782205581665\n",
            "TEST LOSS (Average) : 0.12746746838092804\n",
            "Epoch [68/100] Step [1/148]: domain_loss_target=0.7373 / domain_loss_source=0.6642 / regression_loss_source=0.0456 / alpha=0.9975\n",
            "Epoch [68/100] Step [101/148]: domain_loss_target=0.7126 / domain_loss_source=0.6762 / regression_loss_source=0.0493 / alpha=0.9977\n",
            "VALIDATION LOSS (Average) : 0.15072228014469147\n",
            "TEST LOSS (Average) : 0.12383213390906651\n",
            "Epoch [69/100] Step [1/148]: domain_loss_target=0.7135 / domain_loss_source=0.6681 / regression_loss_source=0.0635 / alpha=0.9978\n",
            "Epoch [69/100] Step [101/148]: domain_loss_target=0.7089 / domain_loss_source=0.6621 / regression_loss_source=0.0693 / alpha=0.9979\n",
            "VALIDATION LOSS (Average) : 0.15010367333889008\n",
            "TEST LOSS (Average) : 0.1262723704179128\n",
            "Epoch [70/100] Step [1/148]: domain_loss_target=0.7141 / domain_loss_source=0.6959 / regression_loss_source=0.1250 / alpha=0.9980\n",
            "Epoch [70/100] Step [101/148]: domain_loss_target=0.7195 / domain_loss_source=0.6751 / regression_loss_source=0.0797 / alpha=0.9981\n",
            "VALIDATION LOSS (Average) : 0.1502750962972641\n",
            "TEST LOSS (Average) : 0.129344309369723\n",
            "Epoch [71/100] Step [1/148]: domain_loss_target=0.6950 / domain_loss_source=0.6767 / regression_loss_source=0.0364 / alpha=0.9982\n",
            "Epoch [71/100] Step [101/148]: domain_loss_target=0.7076 / domain_loss_source=0.6851 / regression_loss_source=0.0628 / alpha=0.9983\n",
            "VALIDATION LOSS (Average) : 0.14947302639484406\n",
            "TEST LOSS (Average) : 0.12579203148682913\n",
            "Epoch [72/100] Step [1/148]: domain_loss_target=0.7333 / domain_loss_source=0.6713 / regression_loss_source=0.0456 / alpha=0.9984\n",
            "Epoch [72/100] Step [101/148]: domain_loss_target=0.7072 / domain_loss_source=0.6610 / regression_loss_source=0.0445 / alpha=0.9985\n",
            "VALIDATION LOSS (Average) : 0.15021075308322906\n",
            "TEST LOSS (Average) : 0.12910948197046915\n",
            "Epoch [73/100] Step [1/148]: domain_loss_target=0.7063 / domain_loss_source=0.6658 / regression_loss_source=0.0647 / alpha=0.9985\n",
            "Epoch [73/100] Step [101/148]: domain_loss_target=0.7155 / domain_loss_source=0.6613 / regression_loss_source=0.0473 / alpha=0.9986\n",
            "VALIDATION LOSS (Average) : 0.1503669023513794\n",
            "TEST LOSS (Average) : 0.12811808288097382\n",
            "Epoch [74/100] Step [1/148]: domain_loss_target=0.7204 / domain_loss_source=0.6683 / regression_loss_source=0.1108 / alpha=0.9986\n",
            "Epoch [74/100] Step [101/148]: domain_loss_target=0.7096 / domain_loss_source=0.6635 / regression_loss_source=0.0597 / alpha=0.9987\n",
            "VALIDATION LOSS (Average) : 0.1514013111591339\n",
            "TEST LOSS (Average) : 0.12970521052678427\n",
            "Epoch [75/100] Step [1/148]: domain_loss_target=0.7135 / domain_loss_source=0.6766 / regression_loss_source=0.0562 / alpha=0.9988\n",
            "Epoch [75/100] Step [101/148]: domain_loss_target=0.7004 / domain_loss_source=0.6784 / regression_loss_source=0.0861 / alpha=0.9989\n",
            "VALIDATION LOSS (Average) : 0.15054607391357422\n",
            "TEST LOSS (Average) : 0.12828191369771957\n",
            "Epoch [76/100] Step [1/148]: domain_loss_target=0.7201 / domain_loss_source=0.6589 / regression_loss_source=0.0601 / alpha=0.9989\n",
            "Epoch [76/100] Step [101/148]: domain_loss_target=0.7224 / domain_loss_source=0.6600 / regression_loss_source=0.0980 / alpha=0.9990\n",
            "VALIDATION LOSS (Average) : 0.15107697248458862\n",
            "TEST LOSS (Average) : 0.12623614569505057\n",
            "Epoch [77/100] Step [1/148]: domain_loss_target=0.7268 / domain_loss_source=0.6534 / regression_loss_source=0.0720 / alpha=0.9990\n",
            "Epoch [77/100] Step [101/148]: domain_loss_target=0.7363 / domain_loss_source=0.6634 / regression_loss_source=0.0463 / alpha=0.9991\n",
            "VALIDATION LOSS (Average) : 0.1507362276315689\n",
            "TEST LOSS (Average) : 0.1271918093164762\n",
            "Epoch [78/100] Step [1/148]: domain_loss_target=0.7259 / domain_loss_source=0.6556 / regression_loss_source=0.0919 / alpha=0.9991\n",
            "Epoch [78/100] Step [101/148]: domain_loss_target=0.6877 / domain_loss_source=0.6502 / regression_loss_source=0.0789 / alpha=0.9992\n",
            "VALIDATION LOSS (Average) : 0.1517234742641449\n",
            "TEST LOSS (Average) : 0.13031450659036636\n",
            "Epoch [79/100] Step [1/148]: domain_loss_target=0.7308 / domain_loss_source=0.6725 / regression_loss_source=0.0199 / alpha=0.9992\n",
            "Epoch [79/100] Step [101/148]: domain_loss_target=0.6855 / domain_loss_source=0.6936 / regression_loss_source=0.0292 / alpha=0.9992\n",
            "VALIDATION LOSS (Average) : 0.1520760953426361\n",
            "TEST LOSS (Average) : 0.12987583130598068\n",
            "Epoch [80/100] Step [1/148]: domain_loss_target=0.7211 / domain_loss_source=0.6112 / regression_loss_source=0.0803 / alpha=0.9993\n",
            "Epoch [80/100] Step [101/148]: domain_loss_target=0.7040 / domain_loss_source=0.7017 / regression_loss_source=0.0849 / alpha=0.9993\n",
            "VALIDATION LOSS (Average) : 0.15225420892238617\n",
            "TEST LOSS (Average) : 0.1292986993988355\n",
            "Epoch [81/100] Step [1/148]: domain_loss_target=0.6981 / domain_loss_source=0.6571 / regression_loss_source=0.0505 / alpha=0.9993\n",
            "Epoch [81/100] Step [101/148]: domain_loss_target=0.6877 / domain_loss_source=0.6999 / regression_loss_source=0.0437 / alpha=0.9994\n",
            "VALIDATION LOSS (Average) : 0.15144416689872742\n",
            "TEST LOSS (Average) : 0.12537733962138495\n",
            "Epoch [82/100] Step [1/148]: domain_loss_target=0.6896 / domain_loss_source=0.6923 / regression_loss_source=0.0466 / alpha=0.9994\n",
            "Epoch [82/100] Step [101/148]: domain_loss_target=0.7291 / domain_loss_source=0.6804 / regression_loss_source=0.0750 / alpha=0.9994\n",
            "VALIDATION LOSS (Average) : 0.15211278200149536\n",
            "TEST LOSS (Average) : 0.12873398512601852\n",
            "Epoch [83/100] Step [1/148]: domain_loss_target=0.6959 / domain_loss_source=0.6822 / regression_loss_source=0.0715 / alpha=0.9995\n",
            "Epoch [83/100] Step [101/148]: domain_loss_target=0.7216 / domain_loss_source=0.6772 / regression_loss_source=0.0506 / alpha=0.9995\n",
            "VALIDATION LOSS (Average) : 0.15434211492538452\n",
            "TEST LOSS (Average) : 0.13721715907255808\n",
            "Epoch [84/100] Step [1/148]: domain_loss_target=0.7019 / domain_loss_source=0.6723 / regression_loss_source=0.0617 / alpha=0.9995\n",
            "Epoch [84/100] Step [101/148]: domain_loss_target=0.7031 / domain_loss_source=0.6619 / regression_loss_source=0.0476 / alpha=0.9995\n",
            "VALIDATION LOSS (Average) : 0.15198172628879547\n",
            "TEST LOSS (Average) : 0.12867650389671326\n",
            "Epoch [85/100] Step [1/148]: domain_loss_target=0.7102 / domain_loss_source=0.6646 / regression_loss_source=0.0554 / alpha=0.9996\n",
            "Epoch [85/100] Step [101/148]: domain_loss_target=0.7115 / domain_loss_source=0.6761 / regression_loss_source=0.0479 / alpha=0.9996\n",
            "VALIDATION LOSS (Average) : 0.15162041783332825\n",
            "TEST LOSS (Average) : 0.1263955682516098\n",
            "Epoch [86/100] Step [1/148]: domain_loss_target=0.7049 / domain_loss_source=0.6791 / regression_loss_source=0.0528 / alpha=0.9996\n",
            "Epoch [86/100] Step [101/148]: domain_loss_target=0.7041 / domain_loss_source=0.6712 / regression_loss_source=0.0408 / alpha=0.9996\n",
            "VALIDATION LOSS (Average) : 0.15155400335788727\n",
            "TEST LOSS (Average) : 0.12475205957889557\n",
            "Epoch [87/100] Step [1/148]: domain_loss_target=0.7044 / domain_loss_source=0.6775 / regression_loss_source=0.0506 / alpha=0.9996\n",
            "Epoch [87/100] Step [101/148]: domain_loss_target=0.7138 / domain_loss_source=0.6697 / regression_loss_source=0.0409 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.15103836357593536\n",
            "TEST LOSS (Average) : 0.12318464120229085\n",
            "Epoch [88/100] Step [1/148]: domain_loss_target=0.7081 / domain_loss_source=0.6714 / regression_loss_source=0.0353 / alpha=0.9997\n",
            "Epoch [88/100] Step [101/148]: domain_loss_target=0.7158 / domain_loss_source=0.6914 / regression_loss_source=0.0610 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.15274113416671753\n",
            "TEST LOSS (Average) : 0.12967075407505035\n",
            "Epoch [89/100] Step [1/148]: domain_loss_target=0.7140 / domain_loss_source=0.6680 / regression_loss_source=0.0914 / alpha=0.9997\n",
            "Epoch [89/100] Step [101/148]: domain_loss_target=0.6830 / domain_loss_source=0.6763 / regression_loss_source=0.0417 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.15226291120052338\n",
            "TEST LOSS (Average) : 0.1270891527334849\n",
            "Epoch [90/100] Step [1/148]: domain_loss_target=0.7121 / domain_loss_source=0.6898 / regression_loss_source=0.0586 / alpha=0.9997\n",
            "Epoch [90/100] Step [101/148]: domain_loss_target=0.7089 / domain_loss_source=0.6702 / regression_loss_source=0.0370 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.15207356214523315\n",
            "TEST LOSS (Average) : 0.12684672077496847\n",
            "Epoch [91/100] Step [1/148]: domain_loss_target=0.7088 / domain_loss_source=0.6498 / regression_loss_source=0.0906 / alpha=0.9998\n",
            "Epoch [91/100] Step [101/148]: domain_loss_target=0.7124 / domain_loss_source=0.6850 / regression_loss_source=0.1204 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.152798593044281\n",
            "TEST LOSS (Average) : 0.1252098853389422\n",
            "Epoch [92/100] Step [1/148]: domain_loss_target=0.6965 / domain_loss_source=0.6795 / regression_loss_source=0.0347 / alpha=0.9998\n",
            "Epoch [92/100] Step [101/148]: domain_loss_target=0.6884 / domain_loss_source=0.6850 / regression_loss_source=0.0429 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.15219856798648834\n",
            "TEST LOSS (Average) : 0.12662132581075033\n",
            "Epoch [93/100] Step [1/148]: domain_loss_target=0.6879 / domain_loss_source=0.6900 / regression_loss_source=0.0723 / alpha=0.9998\n",
            "Epoch [93/100] Step [101/148]: domain_loss_target=0.6746 / domain_loss_source=0.6629 / regression_loss_source=0.0899 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.15174928307533264\n",
            "TEST LOSS (Average) : 0.12529762834310532\n",
            "Epoch [94/100] Step [1/148]: domain_loss_target=0.6915 / domain_loss_source=0.7011 / regression_loss_source=0.0707 / alpha=0.9998\n",
            "Epoch [94/100] Step [101/148]: domain_loss_target=0.6948 / domain_loss_source=0.6989 / regression_loss_source=0.0509 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.1519029587507248\n",
            "TEST LOSS (Average) : 0.12995273619890213\n",
            "Epoch [95/100] Step [1/148]: domain_loss_target=0.6921 / domain_loss_source=0.6726 / regression_loss_source=0.0297 / alpha=0.9998\n",
            "Epoch [95/100] Step [101/148]: domain_loss_target=0.6884 / domain_loss_source=0.6732 / regression_loss_source=0.0352 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.14974360167980194\n",
            "TEST LOSS (Average) : 0.12729397664467493\n",
            "Epoch [96/100] Step [1/148]: domain_loss_target=0.7005 / domain_loss_source=0.6826 / regression_loss_source=0.0598 / alpha=0.9999\n",
            "Epoch [96/100] Step [101/148]: domain_loss_target=0.6940 / domain_loss_source=0.7082 / regression_loss_source=0.0664 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.1507108211517334\n",
            "TEST LOSS (Average) : 0.12855443358421326\n",
            "Epoch [97/100] Step [1/148]: domain_loss_target=0.7026 / domain_loss_source=0.6911 / regression_loss_source=0.0628 / alpha=0.9999\n",
            "Epoch [97/100] Step [101/148]: domain_loss_target=0.6979 / domain_loss_source=0.7115 / regression_loss_source=0.0621 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.15077443420886993\n",
            "TEST LOSS (Average) : 0.12830952058235803\n",
            "Epoch [98/100] Step [1/148]: domain_loss_target=0.6874 / domain_loss_source=0.6687 / regression_loss_source=0.0544 / alpha=0.9999\n",
            "Epoch [98/100] Step [101/148]: domain_loss_target=0.6996 / domain_loss_source=0.6870 / regression_loss_source=0.0981 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.15095606446266174\n",
            "TEST LOSS (Average) : 0.12865597009658813\n",
            "Epoch [99/100] Step [1/148]: domain_loss_target=0.6986 / domain_loss_source=0.6806 / regression_loss_source=0.0572 / alpha=0.9999\n",
            "Epoch [99/100] Step [101/148]: domain_loss_target=0.6949 / domain_loss_source=0.7033 / regression_loss_source=0.0661 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.15216681361198425\n",
            "TEST LOSS (Average) : 0.12844353914260864\n",
            "Epoch [100/100] Step [1/148]: domain_loss_target=0.6617 / domain_loss_source=0.6653 / regression_loss_source=0.0541 / alpha=0.9999\n",
            "Epoch [100/100] Step [101/148]: domain_loss_target=0.6747 / domain_loss_source=0.6885 / regression_loss_source=0.0714 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.15231174230575562\n",
            "TEST LOSS (Average) : 0.12875580290953317\n",
            "----------------------training complete for DANN model - V-----------------\n",
            "----------------------training started for DANN model - EI_joy-----------------\n",
            "Epoch [1/100] Step [1/202]: domain_loss_target=0.8690 / domain_loss_source=0.5644 / regression_loss_source=0.0679 / alpha=0.0000\n",
            "Epoch [1/100] Step [101/202]: domain_loss_target=0.8554 / domain_loss_source=0.5316 / regression_loss_source=0.0616 / alpha=0.0247\n",
            "Epoch [1/100] Step [201/202]: domain_loss_target=0.8551 / domain_loss_source=0.5260 / regression_loss_source=0.0964 / alpha=0.0495\n",
            "VALIDATION LOSS (Average) : 0.13760624825954437\n",
            "TEST LOSS (Average) : 0.14566223323345184\n",
            "Epoch [2/100] Step [1/202]: domain_loss_target=0.8535 / domain_loss_source=0.5150 / regression_loss_source=0.0655 / alpha=0.0500\n",
            "Epoch [2/100] Step [101/202]: domain_loss_target=0.7971 / domain_loss_source=0.4934 / regression_loss_source=0.1111 / alpha=0.0746\n",
            "Epoch [2/100] Step [201/202]: domain_loss_target=0.7278 / domain_loss_source=0.5449 / regression_loss_source=0.0923 / alpha=0.0992\n",
            "VALIDATION LOSS (Average) : 0.13290269672870636\n",
            "TEST LOSS (Average) : 0.13654104247689247\n",
            "Epoch [3/100] Step [1/202]: domain_loss_target=0.7594 / domain_loss_source=0.5304 / regression_loss_source=0.0584 / alpha=0.0997\n",
            "Epoch [3/100] Step [101/202]: domain_loss_target=0.6882 / domain_loss_source=0.3725 / regression_loss_source=0.1103 / alpha=0.1241\n",
            "Epoch [3/100] Step [201/202]: domain_loss_target=0.7410 / domain_loss_source=0.3651 / regression_loss_source=0.0495 / alpha=0.1484\n",
            "VALIDATION LOSS (Average) : 0.13388116657733917\n",
            "TEST LOSS (Average) : 0.13911670073866844\n",
            "Epoch [4/100] Step [1/202]: domain_loss_target=0.6996 / domain_loss_source=0.3799 / regression_loss_source=0.1211 / alpha=0.1489\n",
            "Epoch [4/100] Step [101/202]: domain_loss_target=0.7431 / domain_loss_source=0.4401 / regression_loss_source=0.0459 / alpha=0.1730\n",
            "Epoch [4/100] Step [201/202]: domain_loss_target=0.7710 / domain_loss_source=0.4170 / regression_loss_source=0.1287 / alpha=0.1969\n",
            "VALIDATION LOSS (Average) : 0.1338397115468979\n",
            "TEST LOSS (Average) : 0.1380314752459526\n",
            "Epoch [5/100] Step [1/202]: domain_loss_target=0.7938 / domain_loss_source=0.5309 / regression_loss_source=0.0791 / alpha=0.1974\n",
            "Epoch [5/100] Step [101/202]: domain_loss_target=0.8937 / domain_loss_source=0.4161 / regression_loss_source=0.0954 / alpha=0.2210\n",
            "Epoch [5/100] Step [201/202]: domain_loss_target=0.9462 / domain_loss_source=0.6522 / regression_loss_source=0.0589 / alpha=0.2445\n",
            "VALIDATION LOSS (Average) : 0.13377666473388672\n",
            "TEST LOSS (Average) : 0.13575243577361107\n",
            "Epoch [6/100] Step [1/202]: domain_loss_target=1.0134 / domain_loss_source=0.3250 / regression_loss_source=0.0772 / alpha=0.2449\n",
            "Epoch [6/100] Step [101/202]: domain_loss_target=1.0306 / domain_loss_source=0.4700 / regression_loss_source=0.0339 / alpha=0.2680\n",
            "Epoch [6/100] Step [201/202]: domain_loss_target=0.9469 / domain_loss_source=0.3462 / regression_loss_source=0.1301 / alpha=0.2909\n",
            "VALIDATION LOSS (Average) : 0.13455407321453094\n",
            "TEST LOSS (Average) : 0.1371747236698866\n",
            "Epoch [7/100] Step [1/202]: domain_loss_target=0.8768 / domain_loss_source=0.4617 / regression_loss_source=0.0986 / alpha=0.2913\n",
            "Epoch [7/100] Step [101/202]: domain_loss_target=0.8429 / domain_loss_source=0.3844 / regression_loss_source=0.0607 / alpha=0.3138\n",
            "Epoch [7/100] Step [201/202]: domain_loss_target=0.8719 / domain_loss_source=0.3777 / regression_loss_source=0.0943 / alpha=0.3359\n",
            "VALIDATION LOSS (Average) : 0.13477341830730438\n",
            "TEST LOSS (Average) : 0.13897797092795372\n",
            "Epoch [8/100] Step [1/202]: domain_loss_target=0.8600 / domain_loss_source=0.4187 / regression_loss_source=0.0639 / alpha=0.3364\n",
            "Epoch [8/100] Step [101/202]: domain_loss_target=0.9205 / domain_loss_source=0.4237 / regression_loss_source=0.0663 / alpha=0.3581\n",
            "Epoch [8/100] Step [201/202]: domain_loss_target=0.9752 / domain_loss_source=0.4357 / regression_loss_source=0.1021 / alpha=0.3795\n",
            "VALIDATION LOSS (Average) : 0.13334737718105316\n",
            "TEST LOSS (Average) : 0.1369058471173048\n",
            "Epoch [9/100] Step [1/202]: domain_loss_target=0.8445 / domain_loss_source=0.5026 / regression_loss_source=0.0501 / alpha=0.3799\n",
            "Epoch [9/100] Step [101/202]: domain_loss_target=0.8939 / domain_loss_source=0.4180 / regression_loss_source=0.0884 / alpha=0.4009\n",
            "Epoch [9/100] Step [201/202]: domain_loss_target=0.9848 / domain_loss_source=0.4419 / regression_loss_source=0.0991 / alpha=0.4215\n",
            "VALIDATION LOSS (Average) : 0.13683506846427917\n",
            "TEST LOSS (Average) : 0.1440972164273262\n",
            "Epoch [10/100] Step [1/202]: domain_loss_target=0.9099 / domain_loss_source=0.3665 / regression_loss_source=0.1238 / alpha=0.4219\n",
            "Epoch [10/100] Step [101/202]: domain_loss_target=0.7404 / domain_loss_source=0.5400 / regression_loss_source=0.1007 / alpha=0.4420\n",
            "Epoch [10/100] Step [201/202]: domain_loss_target=0.8659 / domain_loss_source=0.5357 / regression_loss_source=0.0862 / alpha=0.4617\n",
            "VALIDATION LOSS (Average) : 0.13796494901180267\n",
            "TEST LOSS (Average) : 0.1453864723443985\n",
            "Epoch [11/100] Step [1/202]: domain_loss_target=0.7960 / domain_loss_source=0.4313 / regression_loss_source=0.1121 / alpha=0.4621\n",
            "Epoch [11/100] Step [101/202]: domain_loss_target=0.8493 / domain_loss_source=0.3908 / regression_loss_source=0.0862 / alpha=0.4814\n",
            "Epoch [11/100] Step [201/202]: domain_loss_target=0.9553 / domain_loss_source=0.5359 / regression_loss_source=0.0822 / alpha=0.5001\n",
            "VALIDATION LOSS (Average) : 0.13651730120182037\n",
            "TEST LOSS (Average) : 0.14217832311987877\n",
            "Epoch [12/100] Step [1/202]: domain_loss_target=0.8144 / domain_loss_source=0.4454 / regression_loss_source=0.0877 / alpha=0.5005\n",
            "Epoch [12/100] Step [101/202]: domain_loss_target=0.8832 / domain_loss_source=0.5646 / regression_loss_source=0.0955 / alpha=0.5188\n",
            "Epoch [12/100] Step [201/202]: domain_loss_target=0.8740 / domain_loss_source=0.4234 / regression_loss_source=0.0970 / alpha=0.5367\n",
            "VALIDATION LOSS (Average) : 0.13543450832366943\n",
            "TEST LOSS (Average) : 0.14037295058369637\n",
            "Epoch [13/100] Step [1/202]: domain_loss_target=0.9540 / domain_loss_source=0.4911 / regression_loss_source=0.0801 / alpha=0.5370\n",
            "Epoch [13/100] Step [101/202]: domain_loss_target=0.8374 / domain_loss_source=0.5401 / regression_loss_source=0.0575 / alpha=0.5544\n",
            "Epoch [13/100] Step [201/202]: domain_loss_target=0.8417 / domain_loss_source=0.4860 / regression_loss_source=0.1096 / alpha=0.5713\n",
            "VALIDATION LOSS (Average) : 0.13228824734687805\n",
            "TEST LOSS (Average) : 0.13590886630117893\n",
            "Epoch [14/100] Step [1/202]: domain_loss_target=0.7998 / domain_loss_source=0.5448 / regression_loss_source=0.1301 / alpha=0.5717\n",
            "Epoch [14/100] Step [101/202]: domain_loss_target=0.7383 / domain_loss_source=0.5546 / regression_loss_source=0.0853 / alpha=0.5881\n",
            "Epoch [14/100] Step [201/202]: domain_loss_target=0.7970 / domain_loss_source=0.5321 / regression_loss_source=0.0812 / alpha=0.6041\n",
            "VALIDATION LOSS (Average) : 0.14351235330104828\n",
            "TEST LOSS (Average) : 0.15385499969124794\n",
            "Epoch [15/100] Step [1/202]: domain_loss_target=0.7585 / domain_loss_source=0.4976 / regression_loss_source=0.0959 / alpha=0.6044\n",
            "Epoch [15/100] Step [101/202]: domain_loss_target=0.7974 / domain_loss_source=0.5051 / regression_loss_source=0.0900 / alpha=0.6198\n",
            "Epoch [15/100] Step [201/202]: domain_loss_target=0.8028 / domain_loss_source=0.5618 / regression_loss_source=0.0805 / alpha=0.6349\n",
            "VALIDATION LOSS (Average) : 0.13392768800258636\n",
            "TEST LOSS (Average) : 0.14165212213993073\n",
            "Epoch [16/100] Step [1/202]: domain_loss_target=0.7093 / domain_loss_source=0.5905 / regression_loss_source=0.0859 / alpha=0.6351\n",
            "Epoch [16/100] Step [101/202]: domain_loss_target=0.7946 / domain_loss_source=0.5688 / regression_loss_source=0.0727 / alpha=0.6497\n",
            "Epoch [16/100] Step [201/202]: domain_loss_target=0.8130 / domain_loss_source=0.4826 / regression_loss_source=0.1141 / alpha=0.6638\n",
            "VALIDATION LOSS (Average) : 0.13167090713977814\n",
            "TEST LOSS (Average) : 0.1395743452012539\n",
            "Epoch [17/100] Step [1/202]: domain_loss_target=0.7541 / domain_loss_source=0.6209 / regression_loss_source=0.0563 / alpha=0.6640\n",
            "Epoch [17/100] Step [101/202]: domain_loss_target=0.7763 / domain_loss_source=0.5532 / regression_loss_source=0.0708 / alpha=0.6776\n",
            "Epoch [17/100] Step [201/202]: domain_loss_target=0.7698 / domain_loss_source=0.5827 / regression_loss_source=0.1108 / alpha=0.6908\n",
            "VALIDATION LOSS (Average) : 0.13718172907829285\n",
            "TEST LOSS (Average) : 0.14817556366324425\n",
            "Epoch [18/100] Step [1/202]: domain_loss_target=0.8048 / domain_loss_source=0.6372 / regression_loss_source=0.0500 / alpha=0.6911\n",
            "Epoch [18/100] Step [101/202]: domain_loss_target=0.7478 / domain_loss_source=0.5925 / regression_loss_source=0.0552 / alpha=0.7038\n",
            "Epoch [18/100] Step [201/202]: domain_loss_target=0.7133 / domain_loss_source=0.6230 / regression_loss_source=0.0843 / alpha=0.7161\n",
            "VALIDATION LOSS (Average) : 0.13284818828105927\n",
            "TEST LOSS (Average) : 0.14074617251753807\n",
            "Epoch [19/100] Step [1/202]: domain_loss_target=0.7261 / domain_loss_source=0.6497 / regression_loss_source=0.0700 / alpha=0.7163\n",
            "Epoch [19/100] Step [101/202]: domain_loss_target=0.7215 / domain_loss_source=0.6676 / regression_loss_source=0.0798 / alpha=0.7281\n",
            "Epoch [19/100] Step [201/202]: domain_loss_target=0.7320 / domain_loss_source=0.5497 / regression_loss_source=0.0817 / alpha=0.7396\n",
            "VALIDATION LOSS (Average) : 0.13746166229248047\n",
            "TEST LOSS (Average) : 0.14767231792211533\n",
            "Epoch [20/100] Step [1/202]: domain_loss_target=0.7371 / domain_loss_source=0.5886 / regression_loss_source=0.0796 / alpha=0.7398\n",
            "Epoch [20/100] Step [101/202]: domain_loss_target=0.7297 / domain_loss_source=0.7050 / regression_loss_source=0.0768 / alpha=0.7508\n",
            "Epoch [20/100] Step [201/202]: domain_loss_target=0.7431 / domain_loss_source=0.5027 / regression_loss_source=0.0913 / alpha=0.7614\n",
            "VALIDATION LOSS (Average) : 0.14051669836044312\n",
            "TEST LOSS (Average) : 0.15314138308167458\n",
            "Epoch [21/100] Step [1/202]: domain_loss_target=0.7389 / domain_loss_source=0.5880 / regression_loss_source=0.1379 / alpha=0.7616\n",
            "Epoch [21/100] Step [101/202]: domain_loss_target=0.7369 / domain_loss_source=0.6170 / regression_loss_source=0.0960 / alpha=0.7718\n",
            "Epoch [21/100] Step [201/202]: domain_loss_target=0.8331 / domain_loss_source=0.6806 / regression_loss_source=0.0606 / alpha=0.7816\n",
            "VALIDATION LOSS (Average) : 0.13510064780712128\n",
            "TEST LOSS (Average) : 0.14728956297039986\n",
            "Epoch [22/100] Step [1/202]: domain_loss_target=0.6801 / domain_loss_source=0.6641 / regression_loss_source=0.1284 / alpha=0.7818\n",
            "Epoch [22/100] Step [101/202]: domain_loss_target=0.7355 / domain_loss_source=0.6929 / regression_loss_source=0.2036 / alpha=0.7912\n",
            "Epoch [22/100] Step [201/202]: domain_loss_target=0.7787 / domain_loss_source=0.6462 / regression_loss_source=0.1123 / alpha=0.8003\n",
            "VALIDATION LOSS (Average) : 0.13170292973518372\n",
            "TEST LOSS (Average) : 0.14136072248220444\n",
            "Epoch [23/100] Step [1/202]: domain_loss_target=0.6857 / domain_loss_source=0.5857 / regression_loss_source=0.0774 / alpha=0.8005\n",
            "Epoch [23/100] Step [101/202]: domain_loss_target=0.6814 / domain_loss_source=0.6748 / regression_loss_source=0.0951 / alpha=0.8092\n",
            "Epoch [23/100] Step [201/202]: domain_loss_target=0.7454 / domain_loss_source=0.6449 / regression_loss_source=0.1210 / alpha=0.8176\n",
            "VALIDATION LOSS (Average) : 0.13938887417316437\n",
            "TEST LOSS (Average) : 0.1518879197537899\n",
            "Epoch [24/100] Step [1/202]: domain_loss_target=0.7211 / domain_loss_source=0.6687 / regression_loss_source=0.0697 / alpha=0.8178\n",
            "Epoch [24/100] Step [101/202]: domain_loss_target=0.7112 / domain_loss_source=0.7149 / regression_loss_source=0.0633 / alpha=0.8258\n",
            "Epoch [24/100] Step [201/202]: domain_loss_target=0.8604 / domain_loss_source=0.6723 / regression_loss_source=0.0448 / alpha=0.8335\n",
            "VALIDATION LOSS (Average) : 0.13236232101917267\n",
            "TEST LOSS (Average) : 0.1406264305114746\n",
            "Epoch [25/100] Step [1/202]: domain_loss_target=0.7724 / domain_loss_source=0.6319 / regression_loss_source=0.0924 / alpha=0.8337\n",
            "Epoch [25/100] Step [101/202]: domain_loss_target=0.7590 / domain_loss_source=0.6776 / regression_loss_source=0.1147 / alpha=0.8411\n",
            "Epoch [25/100] Step [201/202]: domain_loss_target=0.7075 / domain_loss_source=0.6138 / regression_loss_source=0.0698 / alpha=0.8481\n",
            "VALIDATION LOSS (Average) : 0.13737428188323975\n",
            "TEST LOSS (Average) : 0.14698879048228264\n",
            "Epoch [26/100] Step [1/202]: domain_loss_target=0.7386 / domain_loss_source=0.6854 / regression_loss_source=0.1068 / alpha=0.8483\n",
            "Epoch [26/100] Step [101/202]: domain_loss_target=0.7358 / domain_loss_source=0.6781 / regression_loss_source=0.1068 / alpha=0.8551\n",
            "Epoch [26/100] Step [201/202]: domain_loss_target=0.6702 / domain_loss_source=0.7138 / regression_loss_source=0.0612 / alpha=0.8616\n",
            "VALIDATION LOSS (Average) : 0.13665230572223663\n",
            "TEST LOSS (Average) : 0.14660487323999405\n",
            "Epoch [27/100] Step [1/202]: domain_loss_target=0.7028 / domain_loss_source=0.7315 / regression_loss_source=0.0471 / alpha=0.8617\n",
            "Epoch [27/100] Step [101/202]: domain_loss_target=0.7235 / domain_loss_source=0.6638 / regression_loss_source=0.0791 / alpha=0.8680\n",
            "Epoch [27/100] Step [201/202]: domain_loss_target=0.6928 / domain_loss_source=0.6815 / regression_loss_source=0.0623 / alpha=0.8739\n",
            "VALIDATION LOSS (Average) : 0.13945433497428894\n",
            "TEST LOSS (Average) : 0.15071159228682518\n",
            "Epoch [28/100] Step [1/202]: domain_loss_target=0.6885 / domain_loss_source=0.6527 / regression_loss_source=0.0965 / alpha=0.8741\n",
            "Epoch [28/100] Step [101/202]: domain_loss_target=0.6445 / domain_loss_source=0.7187 / regression_loss_source=0.0711 / alpha=0.8798\n",
            "Epoch [28/100] Step [201/202]: domain_loss_target=0.6953 / domain_loss_source=0.6323 / regression_loss_source=0.0927 / alpha=0.8852\n",
            "VALIDATION LOSS (Average) : 0.14065085351467133\n",
            "TEST LOSS (Average) : 0.15314818546175957\n",
            "Epoch [29/100] Step [1/202]: domain_loss_target=0.7132 / domain_loss_source=0.6763 / regression_loss_source=0.0895 / alpha=0.8854\n",
            "Epoch [29/100] Step [101/202]: domain_loss_target=0.7179 / domain_loss_source=0.6922 / regression_loss_source=0.0707 / alpha=0.8906\n",
            "Epoch [29/100] Step [201/202]: domain_loss_target=0.6701 / domain_loss_source=0.6933 / regression_loss_source=0.0581 / alpha=0.8956\n",
            "VALIDATION LOSS (Average) : 0.1385616958141327\n",
            "TEST LOSS (Average) : 0.151276595890522\n",
            "Epoch [30/100] Step [1/202]: domain_loss_target=0.6722 / domain_loss_source=0.7331 / regression_loss_source=0.0986 / alpha=0.8957\n",
            "Epoch [30/100] Step [101/202]: domain_loss_target=0.7007 / domain_loss_source=0.6104 / regression_loss_source=0.0378 / alpha=0.9005\n",
            "Epoch [30/100] Step [201/202]: domain_loss_target=0.7313 / domain_loss_source=0.6646 / regression_loss_source=0.1157 / alpha=0.9051\n",
            "VALIDATION LOSS (Average) : 0.1375797986984253\n",
            "TEST LOSS (Average) : 0.14930792525410652\n",
            "Epoch [31/100] Step [1/202]: domain_loss_target=0.6828 / domain_loss_source=0.6917 / regression_loss_source=0.1050 / alpha=0.9051\n",
            "Epoch [31/100] Step [101/202]: domain_loss_target=0.7090 / domain_loss_source=0.6434 / regression_loss_source=0.1042 / alpha=0.9095\n",
            "Epoch [31/100] Step [201/202]: domain_loss_target=0.6417 / domain_loss_source=0.6868 / regression_loss_source=0.0911 / alpha=0.9137\n",
            "VALIDATION LOSS (Average) : 0.1360742747783661\n",
            "TEST LOSS (Average) : 0.147163026034832\n",
            "Epoch [32/100] Step [1/202]: domain_loss_target=0.6851 / domain_loss_source=0.7721 / regression_loss_source=0.0491 / alpha=0.9138\n",
            "Epoch [32/100] Step [101/202]: domain_loss_target=0.6347 / domain_loss_source=0.7381 / regression_loss_source=0.0996 / alpha=0.9178\n",
            "Epoch [32/100] Step [201/202]: domain_loss_target=0.7014 / domain_loss_source=0.7561 / regression_loss_source=0.0682 / alpha=0.9216\n",
            "VALIDATION LOSS (Average) : 0.14003121852874756\n",
            "TEST LOSS (Average) : 0.151776522397995\n",
            "Epoch [33/100] Step [1/202]: domain_loss_target=0.6639 / domain_loss_source=0.7108 / regression_loss_source=0.0808 / alpha=0.9217\n",
            "Epoch [33/100] Step [101/202]: domain_loss_target=0.6573 / domain_loss_source=0.7224 / regression_loss_source=0.0688 / alpha=0.9253\n",
            "Epoch [33/100] Step [201/202]: domain_loss_target=0.7065 / domain_loss_source=0.7461 / regression_loss_source=0.0802 / alpha=0.9288\n",
            "VALIDATION LOSS (Average) : 0.13842879235744476\n",
            "TEST LOSS (Average) : 0.14962733909487724\n",
            "Epoch [34/100] Step [1/202]: domain_loss_target=0.6863 / domain_loss_source=0.7549 / regression_loss_source=0.0737 / alpha=0.9289\n",
            "Epoch [34/100] Step [101/202]: domain_loss_target=0.6809 / domain_loss_source=0.7227 / regression_loss_source=0.0423 / alpha=0.9322\n",
            "Epoch [34/100] Step [201/202]: domain_loss_target=0.7092 / domain_loss_source=0.7788 / regression_loss_source=0.0590 / alpha=0.9353\n",
            "VALIDATION LOSS (Average) : 0.13519147038459778\n",
            "TEST LOSS (Average) : 0.14504120126366615\n",
            "Epoch [35/100] Step [1/202]: domain_loss_target=0.6755 / domain_loss_source=0.7261 / regression_loss_source=0.0432 / alpha=0.9354\n",
            "Epoch [35/100] Step [101/202]: domain_loss_target=0.6626 / domain_loss_source=0.7240 / regression_loss_source=0.0811 / alpha=0.9384\n",
            "Epoch [35/100] Step [201/202]: domain_loss_target=0.6878 / domain_loss_source=0.6997 / regression_loss_source=0.0801 / alpha=0.9413\n",
            "VALIDATION LOSS (Average) : 0.140808567404747\n",
            "TEST LOSS (Average) : 0.1524002030491829\n",
            "Epoch [36/100] Step [1/202]: domain_loss_target=0.6825 / domain_loss_source=0.6782 / regression_loss_source=0.0838 / alpha=0.9414\n",
            "Epoch [36/100] Step [101/202]: domain_loss_target=0.6733 / domain_loss_source=0.7431 / regression_loss_source=0.0799 / alpha=0.9441\n",
            "Epoch [36/100] Step [201/202]: domain_loss_target=0.6903 / domain_loss_source=0.7071 / regression_loss_source=0.0835 / alpha=0.9468\n",
            "VALIDATION LOSS (Average) : 0.13996821641921997\n",
            "TEST LOSS (Average) : 0.1509498544037342\n",
            "Epoch [37/100] Step [1/202]: domain_loss_target=0.6836 / domain_loss_source=0.7291 / regression_loss_source=0.0454 / alpha=0.9468\n",
            "Epoch [37/100] Step [101/202]: domain_loss_target=0.6664 / domain_loss_source=0.7076 / regression_loss_source=0.0564 / alpha=0.9493\n",
            "Epoch [37/100] Step [201/202]: domain_loss_target=0.6596 / domain_loss_source=0.7385 / regression_loss_source=0.0366 / alpha=0.9517\n",
            "VALIDATION LOSS (Average) : 0.1383918821811676\n",
            "TEST LOSS (Average) : 0.1492844820022583\n",
            "Epoch [38/100] Step [1/202]: domain_loss_target=0.7200 / domain_loss_source=0.7112 / regression_loss_source=0.1118 / alpha=0.9517\n",
            "Epoch [38/100] Step [101/202]: domain_loss_target=0.6523 / domain_loss_source=0.7617 / regression_loss_source=0.0343 / alpha=0.9540\n",
            "Epoch [38/100] Step [201/202]: domain_loss_target=0.6648 / domain_loss_source=0.6921 / regression_loss_source=0.0747 / alpha=0.9562\n",
            "VALIDATION LOSS (Average) : 0.1421549916267395\n",
            "TEST LOSS (Average) : 0.15388686209917068\n",
            "Epoch [39/100] Step [1/202]: domain_loss_target=0.6754 / domain_loss_source=0.7570 / regression_loss_source=0.0905 / alpha=0.9562\n",
            "Epoch [39/100] Step [101/202]: domain_loss_target=0.6993 / domain_loss_source=0.7394 / regression_loss_source=0.0628 / alpha=0.9583\n",
            "Epoch [39/100] Step [201/202]: domain_loss_target=0.6472 / domain_loss_source=0.7160 / regression_loss_source=0.0879 / alpha=0.9603\n",
            "VALIDATION LOSS (Average) : 0.14176328480243683\n",
            "TEST LOSS (Average) : 0.15304437652230263\n",
            "Epoch [40/100] Step [1/202]: domain_loss_target=0.6701 / domain_loss_source=0.7186 / regression_loss_source=0.0606 / alpha=0.9603\n",
            "Epoch [40/100] Step [101/202]: domain_loss_target=0.6673 / domain_loss_source=0.7530 / regression_loss_source=0.0848 / alpha=0.9622\n",
            "Epoch [40/100] Step [201/202]: domain_loss_target=0.6979 / domain_loss_source=0.7490 / regression_loss_source=0.1015 / alpha=0.9640\n",
            "VALIDATION LOSS (Average) : 0.14024339616298676\n",
            "TEST LOSS (Average) : 0.1515466645359993\n",
            "Epoch [41/100] Step [1/202]: domain_loss_target=0.7187 / domain_loss_source=0.7260 / regression_loss_source=0.0583 / alpha=0.9640\n",
            "Epoch [41/100] Step [101/202]: domain_loss_target=0.6872 / domain_loss_source=0.7116 / regression_loss_source=0.0779 / alpha=0.9657\n",
            "Epoch [41/100] Step [201/202]: domain_loss_target=0.6468 / domain_loss_source=0.7121 / regression_loss_source=0.0465 / alpha=0.9674\n",
            "VALIDATION LOSS (Average) : 0.13485050201416016\n",
            "TEST LOSS (Average) : 0.14269977807998657\n",
            "Epoch [42/100] Step [1/202]: domain_loss_target=0.6649 / domain_loss_source=0.7508 / regression_loss_source=0.0868 / alpha=0.9674\n",
            "Epoch [42/100] Step [101/202]: domain_loss_target=0.6677 / domain_loss_source=0.7242 / regression_loss_source=0.0887 / alpha=0.9689\n",
            "Epoch [42/100] Step [201/202]: domain_loss_target=0.6613 / domain_loss_source=0.7494 / regression_loss_source=0.0531 / alpha=0.9704\n",
            "VALIDATION LOSS (Average) : 0.14058375358581543\n",
            "TEST LOSS (Average) : 0.1503974050283432\n",
            "Epoch [43/100] Step [1/202]: domain_loss_target=0.6840 / domain_loss_source=0.6778 / regression_loss_source=0.0784 / alpha=0.9705\n",
            "Epoch [43/100] Step [101/202]: domain_loss_target=0.6943 / domain_loss_source=0.7441 / regression_loss_source=0.1119 / alpha=0.9719\n",
            "Epoch [43/100] Step [201/202]: domain_loss_target=0.6787 / domain_loss_source=0.7157 / regression_loss_source=0.0587 / alpha=0.9732\n",
            "VALIDATION LOSS (Average) : 0.1422184854745865\n",
            "TEST LOSS (Average) : 0.15259326249361038\n",
            "Epoch [44/100] Step [1/202]: domain_loss_target=0.6816 / domain_loss_source=0.7183 / regression_loss_source=0.0586 / alpha=0.9732\n",
            "Epoch [44/100] Step [101/202]: domain_loss_target=0.6696 / domain_loss_source=0.7167 / regression_loss_source=0.0649 / alpha=0.9745\n",
            "Epoch [44/100] Step [201/202]: domain_loss_target=0.6612 / domain_loss_source=0.7089 / regression_loss_source=0.0599 / alpha=0.9757\n",
            "VALIDATION LOSS (Average) : 0.13921278715133667\n",
            "TEST LOSS (Average) : 0.1486230567097664\n",
            "Epoch [45/100] Step [1/202]: domain_loss_target=0.6907 / domain_loss_source=0.6815 / regression_loss_source=0.0821 / alpha=0.9757\n",
            "Epoch [45/100] Step [101/202]: domain_loss_target=0.6922 / domain_loss_source=0.6977 / regression_loss_source=0.0529 / alpha=0.9769\n",
            "Epoch [45/100] Step [201/202]: domain_loss_target=0.6384 / domain_loss_source=0.7152 / regression_loss_source=0.0675 / alpha=0.9780\n",
            "VALIDATION LOSS (Average) : 0.1448436826467514\n",
            "TEST LOSS (Average) : 0.15515020489692688\n",
            "Epoch [46/100] Step [1/202]: domain_loss_target=0.6864 / domain_loss_source=0.7095 / regression_loss_source=0.1051 / alpha=0.9780\n",
            "Epoch [46/100] Step [101/202]: domain_loss_target=0.6601 / domain_loss_source=0.6880 / regression_loss_source=0.1026 / alpha=0.9791\n",
            "Epoch [46/100] Step [201/202]: domain_loss_target=0.7148 / domain_loss_source=0.6835 / regression_loss_source=0.1426 / alpha=0.9801\n",
            "VALIDATION LOSS (Average) : 0.14586544036865234\n",
            "TEST LOSS (Average) : 0.15575138852000237\n",
            "Epoch [47/100] Step [1/202]: domain_loss_target=0.7216 / domain_loss_source=0.7056 / regression_loss_source=0.0973 / alpha=0.9801\n",
            "Epoch [47/100] Step [101/202]: domain_loss_target=0.6760 / domain_loss_source=0.7099 / regression_loss_source=0.0443 / alpha=0.9810\n",
            "Epoch [47/100] Step [201/202]: domain_loss_target=0.7123 / domain_loss_source=0.6875 / regression_loss_source=0.0790 / alpha=0.9820\n",
            "VALIDATION LOSS (Average) : 0.14023324847221375\n",
            "TEST LOSS (Average) : 0.14868289977312088\n",
            "Epoch [48/100] Step [1/202]: domain_loss_target=0.6598 / domain_loss_source=0.7159 / regression_loss_source=0.0760 / alpha=0.9820\n",
            "Epoch [48/100] Step [101/202]: domain_loss_target=0.6866 / domain_loss_source=0.7136 / regression_loss_source=0.1049 / alpha=0.9828\n",
            "Epoch [48/100] Step [201/202]: domain_loss_target=0.6601 / domain_loss_source=0.6771 / regression_loss_source=0.0627 / alpha=0.9837\n",
            "VALIDATION LOSS (Average) : 0.14096102118492126\n",
            "TEST LOSS (Average) : 0.14939560741186142\n",
            "Epoch [49/100] Step [1/202]: domain_loss_target=0.7031 / domain_loss_source=0.6468 / regression_loss_source=0.0757 / alpha=0.9837\n",
            "Epoch [49/100] Step [101/202]: domain_loss_target=0.6451 / domain_loss_source=0.6746 / regression_loss_source=0.0455 / alpha=0.9845\n",
            "Epoch [49/100] Step [201/202]: domain_loss_target=0.6723 / domain_loss_source=0.6854 / regression_loss_source=0.0565 / alpha=0.9852\n",
            "VALIDATION LOSS (Average) : 0.14150609076023102\n",
            "TEST LOSS (Average) : 0.1503598727285862\n",
            "Epoch [50/100] Step [1/202]: domain_loss_target=0.6717 / domain_loss_source=0.6963 / regression_loss_source=0.0826 / alpha=0.9852\n",
            "Epoch [50/100] Step [101/202]: domain_loss_target=0.6552 / domain_loss_source=0.6868 / regression_loss_source=0.1159 / alpha=0.9859\n",
            "Epoch [50/100] Step [201/202]: domain_loss_target=0.7175 / domain_loss_source=0.6598 / regression_loss_source=0.0831 / alpha=0.9866\n",
            "VALIDATION LOSS (Average) : 0.1359228938817978\n",
            "TEST LOSS (Average) : 0.14351372048258781\n",
            "Epoch [51/100] Step [1/202]: domain_loss_target=0.7088 / domain_loss_source=0.7234 / regression_loss_source=0.1009 / alpha=0.9866\n",
            "Epoch [51/100] Step [101/202]: domain_loss_target=0.6897 / domain_loss_source=0.6754 / regression_loss_source=0.0690 / alpha=0.9873\n",
            "Epoch [51/100] Step [201/202]: domain_loss_target=0.7627 / domain_loss_source=0.6868 / regression_loss_source=0.1060 / alpha=0.9879\n",
            "VALIDATION LOSS (Average) : 0.14373627305030823\n",
            "TEST LOSS (Average) : 0.15390551462769508\n",
            "Epoch [52/100] Step [1/202]: domain_loss_target=0.7098 / domain_loss_source=0.6895 / regression_loss_source=0.0376 / alpha=0.9879\n",
            "Epoch [52/100] Step [101/202]: domain_loss_target=0.7180 / domain_loss_source=0.6535 / regression_loss_source=0.0634 / alpha=0.9885\n",
            "Epoch [52/100] Step [201/202]: domain_loss_target=0.7007 / domain_loss_source=0.6917 / regression_loss_source=0.0412 / alpha=0.9890\n",
            "VALIDATION LOSS (Average) : 0.13912758231163025\n",
            "TEST LOSS (Average) : 0.1479034759104252\n",
            "Epoch [53/100] Step [1/202]: domain_loss_target=0.7500 / domain_loss_source=0.6675 / regression_loss_source=0.0722 / alpha=0.9890\n",
            "Epoch [53/100] Step [101/202]: domain_loss_target=0.6955 / domain_loss_source=0.7085 / regression_loss_source=0.0797 / alpha=0.9896\n",
            "Epoch [53/100] Step [201/202]: domain_loss_target=0.6704 / domain_loss_source=0.6944 / regression_loss_source=0.0988 / alpha=0.9901\n",
            "VALIDATION LOSS (Average) : 0.13827137649059296\n",
            "TEST LOSS (Average) : 0.14678068831562996\n",
            "Epoch [54/100] Step [1/202]: domain_loss_target=0.7228 / domain_loss_source=0.7125 / regression_loss_source=0.1010 / alpha=0.9901\n",
            "Epoch [54/100] Step [101/202]: domain_loss_target=0.6574 / domain_loss_source=0.6999 / regression_loss_source=0.0867 / alpha=0.9905\n",
            "Epoch [54/100] Step [201/202]: domain_loss_target=0.7177 / domain_loss_source=0.6662 / regression_loss_source=0.0821 / alpha=0.9910\n",
            "VALIDATION LOSS (Average) : 0.13795636594295502\n",
            "TEST LOSS (Average) : 0.14623918011784554\n",
            "Epoch [55/100] Step [1/202]: domain_loss_target=0.6821 / domain_loss_source=0.6498 / regression_loss_source=0.0719 / alpha=0.9910\n",
            "Epoch [55/100] Step [101/202]: domain_loss_target=0.6899 / domain_loss_source=0.6969 / regression_loss_source=0.1171 / alpha=0.9914\n",
            "Epoch [55/100] Step [201/202]: domain_loss_target=0.6747 / domain_loss_source=0.7004 / regression_loss_source=0.0633 / alpha=0.9919\n",
            "VALIDATION LOSS (Average) : 0.14159421622753143\n",
            "TEST LOSS (Average) : 0.15040268748998642\n",
            "Epoch [56/100] Step [1/202]: domain_loss_target=0.6702 / domain_loss_source=0.6839 / regression_loss_source=0.0628 / alpha=0.9919\n",
            "Epoch [56/100] Step [101/202]: domain_loss_target=0.6827 / domain_loss_source=0.6519 / regression_loss_source=0.0806 / alpha=0.9923\n",
            "Epoch [56/100] Step [201/202]: domain_loss_target=0.6926 / domain_loss_source=0.7080 / regression_loss_source=0.1205 / alpha=0.9926\n",
            "VALIDATION LOSS (Average) : 0.13861030340194702\n",
            "TEST LOSS (Average) : 0.14544520154595375\n",
            "Epoch [57/100] Step [1/202]: domain_loss_target=0.6857 / domain_loss_source=0.7281 / regression_loss_source=0.0820 / alpha=0.9926\n",
            "Epoch [57/100] Step [101/202]: domain_loss_target=0.6963 / domain_loss_source=0.6880 / regression_loss_source=0.0473 / alpha=0.9930\n",
            "Epoch [57/100] Step [201/202]: domain_loss_target=0.7025 / domain_loss_source=0.7246 / regression_loss_source=0.0583 / alpha=0.9933\n",
            "VALIDATION LOSS (Average) : 0.14644204080104828\n",
            "TEST LOSS (Average) : 0.1556909829378128\n",
            "Epoch [58/100] Step [1/202]: domain_loss_target=0.7340 / domain_loss_source=0.6739 / regression_loss_source=0.1221 / alpha=0.9933\n",
            "Epoch [58/100] Step [101/202]: domain_loss_target=0.6777 / domain_loss_source=0.6920 / regression_loss_source=0.0366 / alpha=0.9937\n",
            "Epoch [58/100] Step [201/202]: domain_loss_target=0.6604 / domain_loss_source=0.7026 / regression_loss_source=0.0927 / alpha=0.9940\n",
            "VALIDATION LOSS (Average) : 0.1438768059015274\n",
            "TEST LOSS (Average) : 0.15276765450835228\n",
            "Epoch [59/100] Step [1/202]: domain_loss_target=0.6734 / domain_loss_source=0.6807 / regression_loss_source=0.0615 / alpha=0.9940\n",
            "Epoch [59/100] Step [101/202]: domain_loss_target=0.7154 / domain_loss_source=0.6909 / regression_loss_source=0.0472 / alpha=0.9943\n",
            "Epoch [59/100] Step [201/202]: domain_loss_target=0.7137 / domain_loss_source=0.6773 / regression_loss_source=0.0848 / alpha=0.9945\n",
            "VALIDATION LOSS (Average) : 0.14011521637439728\n",
            "TEST LOSS (Average) : 0.14779246225953102\n",
            "Epoch [60/100] Step [1/202]: domain_loss_target=0.6906 / domain_loss_source=0.6660 / regression_loss_source=0.0727 / alpha=0.9945\n",
            "Epoch [60/100] Step [101/202]: domain_loss_target=0.6847 / domain_loss_source=0.6884 / regression_loss_source=0.0521 / alpha=0.9948\n",
            "Epoch [60/100] Step [201/202]: domain_loss_target=0.6830 / domain_loss_source=0.6786 / regression_loss_source=0.0700 / alpha=0.9950\n",
            "VALIDATION LOSS (Average) : 0.14526738226413727\n",
            "TEST LOSS (Average) : 0.15354660525918007\n",
            "Epoch [61/100] Step [1/202]: domain_loss_target=0.6840 / domain_loss_source=0.6533 / regression_loss_source=0.0560 / alpha=0.9951\n",
            "Epoch [61/100] Step [101/202]: domain_loss_target=0.6509 / domain_loss_source=0.6544 / regression_loss_source=0.0512 / alpha=0.9953\n",
            "Epoch [61/100] Step [201/202]: domain_loss_target=0.6920 / domain_loss_source=0.6952 / regression_loss_source=0.0832 / alpha=0.9955\n",
            "VALIDATION LOSS (Average) : 0.1549798846244812\n",
            "TEST LOSS (Average) : 0.1654115915298462\n",
            "Epoch [62/100] Step [1/202]: domain_loss_target=0.6914 / domain_loss_source=0.6769 / regression_loss_source=0.0492 / alpha=0.9955\n",
            "Epoch [62/100] Step [101/202]: domain_loss_target=0.7256 / domain_loss_source=0.6939 / regression_loss_source=0.0881 / alpha=0.9957\n",
            "Epoch [62/100] Step [201/202]: domain_loss_target=0.7186 / domain_loss_source=0.6772 / regression_loss_source=0.0963 / alpha=0.9959\n",
            "VALIDATION LOSS (Average) : 0.1387522667646408\n",
            "TEST LOSS (Average) : 0.14629977941513062\n",
            "Epoch [63/100] Step [1/202]: domain_loss_target=0.7062 / domain_loss_source=0.7065 / regression_loss_source=0.0293 / alpha=0.9959\n",
            "Epoch [63/100] Step [101/202]: domain_loss_target=0.7070 / domain_loss_source=0.6765 / regression_loss_source=0.0747 / alpha=0.9961\n",
            "Epoch [63/100] Step [201/202]: domain_loss_target=0.7253 / domain_loss_source=0.6528 / regression_loss_source=0.1114 / alpha=0.9963\n",
            "VALIDATION LOSS (Average) : 0.14127187430858612\n",
            "TEST LOSS (Average) : 0.14938325062394142\n",
            "Epoch [64/100] Step [1/202]: domain_loss_target=0.7292 / domain_loss_source=0.7034 / regression_loss_source=0.0727 / alpha=0.9963\n",
            "Epoch [64/100] Step [101/202]: domain_loss_target=0.7134 / domain_loss_source=0.6753 / regression_loss_source=0.1017 / alpha=0.9965\n",
            "Epoch [64/100] Step [201/202]: domain_loss_target=0.6967 / domain_loss_source=0.6899 / regression_loss_source=0.0693 / alpha=0.9967\n",
            "VALIDATION LOSS (Average) : 0.1406870037317276\n",
            "TEST LOSS (Average) : 0.14840512350201607\n",
            "Epoch [65/100] Step [1/202]: domain_loss_target=0.6933 / domain_loss_source=0.7134 / regression_loss_source=0.1040 / alpha=0.9967\n",
            "Epoch [65/100] Step [101/202]: domain_loss_target=0.7081 / domain_loss_source=0.7144 / regression_loss_source=0.0510 / alpha=0.9968\n",
            "Epoch [65/100] Step [201/202]: domain_loss_target=0.6640 / domain_loss_source=0.6472 / regression_loss_source=0.0669 / alpha=0.9970\n",
            "VALIDATION LOSS (Average) : 0.1382584273815155\n",
            "TEST LOSS (Average) : 0.1459818109869957\n",
            "Epoch [66/100] Step [1/202]: domain_loss_target=0.7046 / domain_loss_source=0.6649 / regression_loss_source=0.0471 / alpha=0.9970\n",
            "Epoch [66/100] Step [101/202]: domain_loss_target=0.6991 / domain_loss_source=0.6971 / regression_loss_source=0.0572 / alpha=0.9971\n",
            "Epoch [66/100] Step [201/202]: domain_loss_target=0.6680 / domain_loss_source=0.6708 / regression_loss_source=0.0586 / alpha=0.9973\n",
            "VALIDATION LOSS (Average) : 0.14113004505634308\n",
            "TEST LOSS (Average) : 0.1504782848060131\n",
            "Epoch [67/100] Step [1/202]: domain_loss_target=0.6797 / domain_loss_source=0.6850 / regression_loss_source=0.1316 / alpha=0.9973\n",
            "Epoch [67/100] Step [101/202]: domain_loss_target=0.6969 / domain_loss_source=0.7070 / regression_loss_source=0.0519 / alpha=0.9974\n",
            "Epoch [67/100] Step [201/202]: domain_loss_target=0.6757 / domain_loss_source=0.7268 / regression_loss_source=0.0968 / alpha=0.9975\n",
            "VALIDATION LOSS (Average) : 0.1398395150899887\n",
            "TEST LOSS (Average) : 0.1485890969634056\n",
            "Epoch [68/100] Step [1/202]: domain_loss_target=0.6642 / domain_loss_source=0.6733 / regression_loss_source=0.0618 / alpha=0.9975\n",
            "Epoch [68/100] Step [101/202]: domain_loss_target=0.6807 / domain_loss_source=0.6498 / regression_loss_source=0.0823 / alpha=0.9977\n",
            "Epoch [68/100] Step [201/202]: domain_loss_target=0.7446 / domain_loss_source=0.6650 / regression_loss_source=0.0672 / alpha=0.9978\n",
            "VALIDATION LOSS (Average) : 0.13943706452846527\n",
            "TEST LOSS (Average) : 0.14779086038470268\n",
            "Epoch [69/100] Step [1/202]: domain_loss_target=0.7728 / domain_loss_source=0.7095 / regression_loss_source=0.0513 / alpha=0.9978\n",
            "Epoch [69/100] Step [101/202]: domain_loss_target=0.7399 / domain_loss_source=0.7154 / regression_loss_source=0.0549 / alpha=0.9979\n",
            "Epoch [69/100] Step [201/202]: domain_loss_target=0.7481 / domain_loss_source=0.7288 / regression_loss_source=0.0569 / alpha=0.9980\n",
            "VALIDATION LOSS (Average) : 0.13762615621089935\n",
            "TEST LOSS (Average) : 0.14505496248602867\n",
            "Epoch [70/100] Step [1/202]: domain_loss_target=0.7288 / domain_loss_source=0.6864 / regression_loss_source=0.0442 / alpha=0.9980\n",
            "Epoch [70/100] Step [101/202]: domain_loss_target=0.6979 / domain_loss_source=0.7171 / regression_loss_source=0.0695 / alpha=0.9981\n",
            "Epoch [70/100] Step [201/202]: domain_loss_target=0.7196 / domain_loss_source=0.6934 / regression_loss_source=0.0973 / alpha=0.9982\n",
            "VALIDATION LOSS (Average) : 0.13946428894996643\n",
            "TEST LOSS (Average) : 0.1464133784174919\n",
            "Epoch [71/100] Step [1/202]: domain_loss_target=0.6873 / domain_loss_source=0.6617 / regression_loss_source=0.1267 / alpha=0.9982\n",
            "Epoch [71/100] Step [101/202]: domain_loss_target=0.6920 / domain_loss_source=0.6914 / regression_loss_source=0.0503 / alpha=0.9983\n",
            "Epoch [71/100] Step [201/202]: domain_loss_target=0.7079 / domain_loss_source=0.6452 / regression_loss_source=0.0362 / alpha=0.9983\n",
            "VALIDATION LOSS (Average) : 0.1396525651216507\n",
            "TEST LOSS (Average) : 0.1468149945139885\n",
            "Epoch [72/100] Step [1/202]: domain_loss_target=0.6833 / domain_loss_source=0.6466 / regression_loss_source=0.0610 / alpha=0.9984\n",
            "Epoch [72/100] Step [101/202]: domain_loss_target=0.6775 / domain_loss_source=0.6726 / regression_loss_source=0.0595 / alpha=0.9984\n",
            "Epoch [72/100] Step [201/202]: domain_loss_target=0.6873 / domain_loss_source=0.6705 / regression_loss_source=0.0350 / alpha=0.9985\n",
            "VALIDATION LOSS (Average) : 0.14557437598705292\n",
            "TEST LOSS (Average) : 0.15477151051163673\n",
            "Epoch [73/100] Step [1/202]: domain_loss_target=0.6772 / domain_loss_source=0.6753 / regression_loss_source=0.0635 / alpha=0.9985\n",
            "Epoch [73/100] Step [101/202]: domain_loss_target=0.6756 / domain_loss_source=0.7597 / regression_loss_source=0.0511 / alpha=0.9986\n",
            "Epoch [73/100] Step [201/202]: domain_loss_target=0.7295 / domain_loss_source=0.7519 / regression_loss_source=0.0882 / alpha=0.9986\n",
            "VALIDATION LOSS (Average) : 0.13566626608371735\n",
            "TEST LOSS (Average) : 0.14224034175276756\n",
            "Epoch [74/100] Step [1/202]: domain_loss_target=0.6576 / domain_loss_source=0.7119 / regression_loss_source=0.1429 / alpha=0.9986\n",
            "Epoch [74/100] Step [101/202]: domain_loss_target=0.6863 / domain_loss_source=0.7239 / regression_loss_source=0.1216 / alpha=0.9987\n",
            "Epoch [74/100] Step [201/202]: domain_loss_target=0.6925 / domain_loss_source=0.7255 / regression_loss_source=0.0592 / alpha=0.9988\n",
            "VALIDATION LOSS (Average) : 0.13965091109275818\n",
            "TEST LOSS (Average) : 0.148141298443079\n",
            "Epoch [75/100] Step [1/202]: domain_loss_target=0.7025 / domain_loss_source=0.6935 / regression_loss_source=0.0545 / alpha=0.9988\n",
            "Epoch [75/100] Step [101/202]: domain_loss_target=0.6641 / domain_loss_source=0.7017 / regression_loss_source=0.1163 / alpha=0.9988\n",
            "Epoch [75/100] Step [201/202]: domain_loss_target=0.6417 / domain_loss_source=0.6968 / regression_loss_source=0.0483 / alpha=0.9989\n",
            "VALIDATION LOSS (Average) : 0.1411915272474289\n",
            "TEST LOSS (Average) : 0.15007594600319862\n",
            "Epoch [76/100] Step [1/202]: domain_loss_target=0.6423 / domain_loss_source=0.7010 / regression_loss_source=0.0998 / alpha=0.9989\n",
            "Epoch [76/100] Step [101/202]: domain_loss_target=0.6757 / domain_loss_source=0.6732 / regression_loss_source=0.0646 / alpha=0.9989\n",
            "Epoch [76/100] Step [201/202]: domain_loss_target=0.7180 / domain_loss_source=0.6639 / regression_loss_source=0.0824 / alpha=0.9990\n",
            "VALIDATION LOSS (Average) : 0.1346093714237213\n",
            "TEST LOSS (Average) : 0.14054401963949203\n",
            "Epoch [77/100] Step [1/202]: domain_loss_target=0.7106 / domain_loss_source=0.6633 / regression_loss_source=0.0711 / alpha=0.9990\n",
            "Epoch [77/100] Step [101/202]: domain_loss_target=0.6876 / domain_loss_source=0.6873 / regression_loss_source=0.1463 / alpha=0.9990\n",
            "Epoch [77/100] Step [201/202]: domain_loss_target=0.7555 / domain_loss_source=0.6862 / regression_loss_source=0.0660 / alpha=0.9991\n",
            "VALIDATION LOSS (Average) : 0.13655543327331543\n",
            "TEST LOSS (Average) : 0.14357173070311546\n",
            "Epoch [78/100] Step [1/202]: domain_loss_target=0.7215 / domain_loss_source=0.7068 / regression_loss_source=0.0365 / alpha=0.9991\n",
            "Epoch [78/100] Step [101/202]: domain_loss_target=0.7149 / domain_loss_source=0.7143 / regression_loss_source=0.0553 / alpha=0.9991\n",
            "Epoch [78/100] Step [201/202]: domain_loss_target=0.7191 / domain_loss_source=0.7140 / regression_loss_source=0.0585 / alpha=0.9992\n",
            "VALIDATION LOSS (Average) : 0.13663819432258606\n",
            "TEST LOSS (Average) : 0.14404652640223503\n",
            "Epoch [79/100] Step [1/202]: domain_loss_target=0.7403 / domain_loss_source=0.7141 / regression_loss_source=0.0811 / alpha=0.9992\n",
            "Epoch [79/100] Step [101/202]: domain_loss_target=0.7174 / domain_loss_source=0.6971 / regression_loss_source=0.0639 / alpha=0.9992\n",
            "Epoch [79/100] Step [201/202]: domain_loss_target=0.7561 / domain_loss_source=0.6407 / regression_loss_source=0.0897 / alpha=0.9993\n",
            "VALIDATION LOSS (Average) : 0.1434917151927948\n",
            "TEST LOSS (Average) : 0.15214352309703827\n",
            "Epoch [80/100] Step [1/202]: domain_loss_target=0.7667 / domain_loss_source=0.6641 / regression_loss_source=0.1253 / alpha=0.9993\n",
            "Epoch [80/100] Step [101/202]: domain_loss_target=0.6834 / domain_loss_source=0.6441 / regression_loss_source=0.0517 / alpha=0.9993\n",
            "Epoch [80/100] Step [201/202]: domain_loss_target=0.6520 / domain_loss_source=0.6659 / regression_loss_source=0.1025 / alpha=0.9993\n",
            "VALIDATION LOSS (Average) : 0.1407870054244995\n",
            "TEST LOSS (Average) : 0.14823299273848534\n",
            "Epoch [81/100] Step [1/202]: domain_loss_target=0.6968 / domain_loss_source=0.6781 / regression_loss_source=0.0649 / alpha=0.9993\n",
            "Epoch [81/100] Step [101/202]: domain_loss_target=0.7321 / domain_loss_source=0.6600 / regression_loss_source=0.0631 / alpha=0.9994\n",
            "Epoch [81/100] Step [201/202]: domain_loss_target=0.7170 / domain_loss_source=0.6651 / regression_loss_source=0.0544 / alpha=0.9994\n",
            "VALIDATION LOSS (Average) : 0.13598060607910156\n",
            "TEST LOSS (Average) : 0.1412435546517372\n",
            "Epoch [82/100] Step [1/202]: domain_loss_target=0.7499 / domain_loss_source=0.6920 / regression_loss_source=0.0616 / alpha=0.9994\n",
            "Epoch [82/100] Step [101/202]: domain_loss_target=0.6866 / domain_loss_source=0.6820 / regression_loss_source=0.0677 / alpha=0.9994\n",
            "Epoch [82/100] Step [201/202]: domain_loss_target=0.6735 / domain_loss_source=0.6804 / regression_loss_source=0.0952 / alpha=0.9995\n",
            "VALIDATION LOSS (Average) : 0.14607028663158417\n",
            "TEST LOSS (Average) : 0.15396090224385262\n",
            "Epoch [83/100] Step [1/202]: domain_loss_target=0.6793 / domain_loss_source=0.6948 / regression_loss_source=0.0816 / alpha=0.9995\n",
            "Epoch [83/100] Step [101/202]: domain_loss_target=0.6844 / domain_loss_source=0.7103 / regression_loss_source=0.0415 / alpha=0.9995\n",
            "Epoch [83/100] Step [201/202]: domain_loss_target=0.6804 / domain_loss_source=0.7231 / regression_loss_source=0.0466 / alpha=0.9995\n",
            "VALIDATION LOSS (Average) : 0.1453375369310379\n",
            "TEST LOSS (Average) : 0.15358655154705048\n",
            "Epoch [84/100] Step [1/202]: domain_loss_target=0.6845 / domain_loss_source=0.6795 / regression_loss_source=0.0540 / alpha=0.9995\n",
            "Epoch [84/100] Step [101/202]: domain_loss_target=0.6623 / domain_loss_source=0.7328 / regression_loss_source=0.0685 / alpha=0.9995\n",
            "Epoch [84/100] Step [201/202]: domain_loss_target=0.6580 / domain_loss_source=0.7420 / regression_loss_source=0.0461 / alpha=0.9995\n",
            "VALIDATION LOSS (Average) : 0.13503678143024445\n",
            "TEST LOSS (Average) : 0.14149128645658493\n",
            "Epoch [85/100] Step [1/202]: domain_loss_target=0.5905 / domain_loss_source=0.6828 / regression_loss_source=0.0630 / alpha=0.9996\n",
            "Epoch [85/100] Step [101/202]: domain_loss_target=0.7004 / domain_loss_source=0.7008 / regression_loss_source=0.0321 / alpha=0.9996\n",
            "Epoch [85/100] Step [201/202]: domain_loss_target=0.7120 / domain_loss_source=0.7012 / regression_loss_source=0.0450 / alpha=0.9996\n",
            "VALIDATION LOSS (Average) : 0.14196164906024933\n",
            "TEST LOSS (Average) : 0.15035109966993332\n",
            "Epoch [86/100] Step [1/202]: domain_loss_target=0.6814 / domain_loss_source=0.7124 / regression_loss_source=0.0810 / alpha=0.9996\n",
            "Epoch [86/100] Step [101/202]: domain_loss_target=0.6893 / domain_loss_source=0.6996 / regression_loss_source=0.0797 / alpha=0.9996\n",
            "Epoch [86/100] Step [201/202]: domain_loss_target=0.7131 / domain_loss_source=0.6658 / regression_loss_source=0.1025 / alpha=0.9996\n",
            "VALIDATION LOSS (Average) : 0.144750714302063\n",
            "TEST LOSS (Average) : 0.1541159525513649\n",
            "Epoch [87/100] Step [1/202]: domain_loss_target=0.7035 / domain_loss_source=0.6712 / regression_loss_source=0.0415 / alpha=0.9996\n",
            "Epoch [87/100] Step [101/202]: domain_loss_target=0.7344 / domain_loss_source=0.7096 / regression_loss_source=0.0353 / alpha=0.9996\n",
            "Epoch [87/100] Step [201/202]: domain_loss_target=0.7038 / domain_loss_source=0.6366 / regression_loss_source=0.0395 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.1413973867893219\n",
            "TEST LOSS (Average) : 0.14927638322114944\n",
            "Epoch [88/100] Step [1/202]: domain_loss_target=0.7260 / domain_loss_source=0.6718 / regression_loss_source=0.0714 / alpha=0.9997\n",
            "Epoch [88/100] Step [101/202]: domain_loss_target=0.6562 / domain_loss_source=0.6919 / regression_loss_source=0.0769 / alpha=0.9997\n",
            "Epoch [88/100] Step [201/202]: domain_loss_target=0.7316 / domain_loss_source=0.6851 / regression_loss_source=0.0680 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.14317721128463745\n",
            "TEST LOSS (Average) : 0.15163857117295265\n",
            "Epoch [89/100] Step [1/202]: domain_loss_target=0.7088 / domain_loss_source=0.6786 / regression_loss_source=0.0658 / alpha=0.9997\n",
            "Epoch [89/100] Step [101/202]: domain_loss_target=0.6925 / domain_loss_source=0.7093 / regression_loss_source=0.0560 / alpha=0.9997\n",
            "Epoch [89/100] Step [201/202]: domain_loss_target=0.7283 / domain_loss_source=0.6955 / regression_loss_source=0.0532 / alpha=0.9997\n",
            "VALIDATION LOSS (Average) : 0.14054089784622192\n",
            "TEST LOSS (Average) : 0.1483338177204132\n",
            "Epoch [90/100] Step [1/202]: domain_loss_target=0.6895 / domain_loss_source=0.6886 / regression_loss_source=0.0360 / alpha=0.9997\n",
            "Epoch [90/100] Step [101/202]: domain_loss_target=0.6758 / domain_loss_source=0.7014 / regression_loss_source=0.1020 / alpha=0.9997\n",
            "Epoch [90/100] Step [201/202]: domain_loss_target=0.7008 / domain_loss_source=0.7040 / regression_loss_source=0.0833 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.1405903548002243\n",
            "TEST LOSS (Average) : 0.14790691062808037\n",
            "Epoch [91/100] Step [1/202]: domain_loss_target=0.7173 / domain_loss_source=0.7247 / regression_loss_source=0.0868 / alpha=0.9998\n",
            "Epoch [91/100] Step [101/202]: domain_loss_target=0.7320 / domain_loss_source=0.6619 / regression_loss_source=0.0557 / alpha=0.9998\n",
            "Epoch [91/100] Step [201/202]: domain_loss_target=0.7113 / domain_loss_source=0.6877 / regression_loss_source=0.0721 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.1402609497308731\n",
            "TEST LOSS (Average) : 0.14770330116152763\n",
            "Epoch [92/100] Step [1/202]: domain_loss_target=0.7201 / domain_loss_source=0.6888 / regression_loss_source=0.0697 / alpha=0.9998\n",
            "Epoch [92/100] Step [101/202]: domain_loss_target=0.6912 / domain_loss_source=0.6941 / regression_loss_source=0.0813 / alpha=0.9998\n",
            "Epoch [92/100] Step [201/202]: domain_loss_target=0.6686 / domain_loss_source=0.7008 / regression_loss_source=0.0893 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.1425517052412033\n",
            "TEST LOSS (Average) : 0.1509932577610016\n",
            "Epoch [93/100] Step [1/202]: domain_loss_target=0.6403 / domain_loss_source=0.7167 / regression_loss_source=0.0774 / alpha=0.9998\n",
            "Epoch [93/100] Step [101/202]: domain_loss_target=0.6704 / domain_loss_source=0.6876 / regression_loss_source=0.0988 / alpha=0.9998\n",
            "Epoch [93/100] Step [201/202]: domain_loss_target=0.6942 / domain_loss_source=0.7033 / regression_loss_source=0.0625 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.14259232580661774\n",
            "TEST LOSS (Average) : 0.15090202912688255\n",
            "Epoch [94/100] Step [1/202]: domain_loss_target=0.7354 / domain_loss_source=0.6668 / regression_loss_source=0.0597 / alpha=0.9998\n",
            "Epoch [94/100] Step [101/202]: domain_loss_target=0.6982 / domain_loss_source=0.6902 / regression_loss_source=0.0725 / alpha=0.9998\n",
            "Epoch [94/100] Step [201/202]: domain_loss_target=0.7183 / domain_loss_source=0.6423 / regression_loss_source=0.0573 / alpha=0.9998\n",
            "VALIDATION LOSS (Average) : 0.13867191970348358\n",
            "TEST LOSS (Average) : 0.14616266265511513\n",
            "Epoch [95/100] Step [1/202]: domain_loss_target=0.7137 / domain_loss_source=0.6379 / regression_loss_source=0.1180 / alpha=0.9998\n",
            "Epoch [95/100] Step [101/202]: domain_loss_target=0.6658 / domain_loss_source=0.6576 / regression_loss_source=0.1007 / alpha=0.9998\n",
            "Epoch [95/100] Step [201/202]: domain_loss_target=0.7224 / domain_loss_source=0.6500 / regression_loss_source=0.0755 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.14355434477329254\n",
            "TEST LOSS (Average) : 0.15194375067949295\n",
            "Epoch [96/100] Step [1/202]: domain_loss_target=0.7167 / domain_loss_source=0.6860 / regression_loss_source=0.0646 / alpha=0.9999\n",
            "Epoch [96/100] Step [101/202]: domain_loss_target=0.7021 / domain_loss_source=0.6760 / regression_loss_source=0.1064 / alpha=0.9999\n",
            "Epoch [96/100] Step [201/202]: domain_loss_target=0.6869 / domain_loss_source=0.6738 / regression_loss_source=0.0523 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.145412415266037\n",
            "TEST LOSS (Average) : 0.15398859605193138\n",
            "Epoch [97/100] Step [1/202]: domain_loss_target=0.6939 / domain_loss_source=0.6703 / regression_loss_source=0.0590 / alpha=0.9999\n",
            "Epoch [97/100] Step [101/202]: domain_loss_target=0.6946 / domain_loss_source=0.6627 / regression_loss_source=0.0820 / alpha=0.9999\n",
            "Epoch [97/100] Step [201/202]: domain_loss_target=0.7083 / domain_loss_source=0.6694 / regression_loss_source=0.0856 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.1437707096338272\n",
            "TEST LOSS (Average) : 0.15207849815487862\n",
            "Epoch [98/100] Step [1/202]: domain_loss_target=0.6906 / domain_loss_source=0.6806 / regression_loss_source=0.0543 / alpha=0.9999\n",
            "Epoch [98/100] Step [101/202]: domain_loss_target=0.7166 / domain_loss_source=0.6837 / regression_loss_source=0.0520 / alpha=0.9999\n",
            "Epoch [98/100] Step [201/202]: domain_loss_target=0.7207 / domain_loss_source=0.6787 / regression_loss_source=0.0734 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.1440371870994568\n",
            "TEST LOSS (Average) : 0.15246453508734703\n",
            "Epoch [99/100] Step [1/202]: domain_loss_target=0.7027 / domain_loss_source=0.6807 / regression_loss_source=0.1210 / alpha=0.9999\n",
            "Epoch [99/100] Step [101/202]: domain_loss_target=0.7100 / domain_loss_source=0.7067 / regression_loss_source=0.0584 / alpha=0.9999\n",
            "Epoch [99/100] Step [201/202]: domain_loss_target=0.7374 / domain_loss_source=0.6535 / regression_loss_source=0.0536 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.14166490733623505\n",
            "TEST LOSS (Average) : 0.15027039498090744\n",
            "Epoch [100/100] Step [1/202]: domain_loss_target=0.7144 / domain_loss_source=0.6534 / regression_loss_source=0.0469 / alpha=0.9999\n",
            "Epoch [100/100] Step [101/202]: domain_loss_target=0.7207 / domain_loss_source=0.7041 / regression_loss_source=0.0426 / alpha=0.9999\n",
            "Epoch [100/100] Step [201/202]: domain_loss_target=0.6882 / domain_loss_source=0.6906 / regression_loss_source=0.0766 / alpha=0.9999\n",
            "VALIDATION LOSS (Average) : 0.14379338920116425\n",
            "TEST LOSS (Average) : 0.15156792104244232\n",
            "----------------------training complete for DANN model - EI_joy-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  ## DANN training function attempt 2 with NLLLoss\n",
        "# # n_epochs = 100 # number of epochs\n",
        "# n_epochs = 2 # number of epochs\n",
        "# lr = 2e-5\n",
        "\n",
        "\n",
        "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# # loss_fn_sentiment_regression = torch.nn.NLLLoss()\n",
        "# # loss_fn_domain_classifier = torch.nn.NLLLoss()\n",
        "\n",
        "# model = model.to(DEVICE)\n",
        "# domain_loss_function= nn.NLLLoss()\n",
        "# regression_loss_function = nn.L1Loss()\n",
        "# domain_loss_function = domain_loss_function.to(DEVICE)\n",
        "# regression_loss_function = regression_loss_function.to(DEVICE)\n",
        "\n",
        "# max_batches = min(len(train_iterator), len(target_data)//TARGET_BATCH_SIZE)\n",
        "# # max_batches = min(len(train_iterator), len(target_iterator))\n",
        "\n",
        "# print(max_batches)\n",
        "\n",
        "# for epoch_idx in range(n_epochs):\n",
        "    \n",
        "#     source_iterator = iter(train_iterator)\n",
        "#     target_iterator = iter(target_iterator)\n",
        "\n",
        "#     for batch_idx in range(max_batches):\n",
        "        \n",
        "#         p = float(batch_idx + epoch_idx * max_batches) / (n_epochs * max_batches)\n",
        "#         alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "#         alpha = torch.tensor(alpha)\n",
        "        \n",
        "#         model.train()\n",
        "        \n",
        "#         # if(batch_idx%100 == 0 ):\n",
        "#         #     print(\"Training Step:\", batch_idx)\n",
        "        \n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         ## SOURCE DATASET TRAINING UPDATE\n",
        "        \n",
        "#         source_batch = next(source_iterator)\n",
        "#         source_tweets, source_intensities = source_batch.tweet.to(DEVICE), source_batch.intensity.to(DEVICE)  # plural, we are not interested in domain\n",
        "        \n",
        "#         source_intensity_outputs, source_domain_outputs = model(source_tweets, alpha = alpha)\n",
        "\n",
        "#         loss_source_regression= regression_loss_function(source_intensity_outputs,source_intensities.unsqueeze(1)) # Computing regression loss\n",
        "\n",
        "#         source_domain_inputs = torch.zeros(len(source_batch), dtype=torch.long).to(DEVICE) # source domain has 0 id\n",
        "#         loss_source_domain = domain_loss_function(source_domain_outputs,source_domain_inputs)\n",
        "\n",
        "\n",
        "#         ## TARGET DATASET TRAINING UPDATE\n",
        "#         target_batch = next(iter(target_iterator))\n",
        "#         target_tweets= target_batch.tweet.to(DEVICE) # plural\n",
        "\n",
        "#         _, target_domain_outputs = model(target_tweets, alpha = alpha)\n",
        "\n",
        "#         target_domain_inputs = torch.ones(len(target_batch), dtype=torch.long).to(DEVICE) # target domain has 1 id\n",
        "#         loss_target_domain = domain_loss_function(target_domain_outputs,target_domain_inputs)\n",
        "\n",
        "#         # COMBINING LOSS\n",
        "#         loss = loss_source_regression + loss_source_domain + loss_target_domain\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         if (batch_idx % 100 == 0):\n",
        "#           print(\"Epoch [{}/{}] Step [{}/{}]: domain_loss_target={:.4f} / domain_loss_source={:.4f} / regression_loss_source={:.4f} / alpha={:.4f}\"\n",
        "#               .format(epoch_idx + 1,\n",
        "#                       n_epochs,\n",
        "#                       batch_idx + 1,\n",
        "#                       max_batches,\n",
        "#                       loss_target_domain.item()\n",
        "#                       ,loss_source_domain.item()\n",
        "#                       ,loss_source_regression.item(),alpha))\n",
        "\n",
        "\n",
        "#     # Evaluate the model after every epoch\n",
        "\n",
        "\n",
        "#     print(\"for validation.......\")\n",
        "#     test_model(model, DEVICE, valid_iterator, mode = 'val')\n",
        "#     print(\"for test  .......\")\n",
        "#     test_model(model, DEVICE, test_iterator, mode = 'test')\n",
        "\n",
        "  \n",
        "# torch.save(model.state_dict(), os.path.join(MODEL_DIR, \"epoch_\" + str(epoch_idx)  +  \".pt\" ))\n"
      ],
      "metadata": {
        "id": "lTqIOQX0Cfk8"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Equality Evaluation using Equity Evaluation Corpus"
      ],
      "metadata": {
        "id": "OLu5bV0XRjMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Evaluation Data\n",
        "format `[ID\tSentence\tTemplate\tPerson\tGender\tRace Emotion\tEmotion word]`"
      ],
      "metadata": {
        "id": "xjcK8oWLSJus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_EEC = TASK1.EEC['eec']\n",
        "df_EEC = pd.read_csv(data_EEC)\n",
        "df_EEC.head()"
      ],
      "metadata": {
        "id": "Qqpe_6U6SnDv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "6472bbe2-b802-4213-a46c-1e131e870b5f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      ID                 Sentence  \\\n",
              "0  2018-En-mystery-05498      Alonzo feels angry.   \n",
              "1  2018-En-mystery-11722    Alonzo feels furious.   \n",
              "2  2018-En-mystery-11364  Alonzo feels irritated.   \n",
              "3  2018-En-mystery-14320    Alonzo feels enraged.   \n",
              "4  2018-En-mystery-14114    Alonzo feels annoyed.   \n",
              "\n",
              "                                 Template  Person Gender              Race  \\\n",
              "0  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
              "1  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
              "2  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
              "3  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
              "4  <person subject> feels <emotion word>.  Alonzo   male  African-American   \n",
              "\n",
              "  Emotion Emotion word  \n",
              "0   anger        angry  \n",
              "1   anger      furious  \n",
              "2   anger    irritated  \n",
              "3   anger      enraged  \n",
              "4   anger      annoyed  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c938c04c-be84-4230-ba39-5058b0a69fb1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Template</th>\n",
              "      <th>Person</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Race</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Emotion word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-En-mystery-05498</td>\n",
              "      <td>Alonzo feels angry.</td>\n",
              "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
              "      <td>Alonzo</td>\n",
              "      <td>male</td>\n",
              "      <td>African-American</td>\n",
              "      <td>anger</td>\n",
              "      <td>angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-En-mystery-11722</td>\n",
              "      <td>Alonzo feels furious.</td>\n",
              "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
              "      <td>Alonzo</td>\n",
              "      <td>male</td>\n",
              "      <td>African-American</td>\n",
              "      <td>anger</td>\n",
              "      <td>furious</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-En-mystery-11364</td>\n",
              "      <td>Alonzo feels irritated.</td>\n",
              "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
              "      <td>Alonzo</td>\n",
              "      <td>male</td>\n",
              "      <td>African-American</td>\n",
              "      <td>anger</td>\n",
              "      <td>irritated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-En-mystery-14320</td>\n",
              "      <td>Alonzo feels enraged.</td>\n",
              "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
              "      <td>Alonzo</td>\n",
              "      <td>male</td>\n",
              "      <td>African-American</td>\n",
              "      <td>anger</td>\n",
              "      <td>enraged</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-En-mystery-14114</td>\n",
              "      <td>Alonzo feels annoyed.</td>\n",
              "      <td>&lt;person subject&gt; feels &lt;emotion word&gt;.</td>\n",
              "      <td>Alonzo</td>\n",
              "      <td>male</td>\n",
              "      <td>African-American</td>\n",
              "      <td>anger</td>\n",
              "      <td>annoyed</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c938c04c-be84-4230-ba39-5058b0a69fb1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c938c04c-be84-4230-ba39-5058b0a69fb1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c938c04c-be84-4230-ba39-5058b0a69fb1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating evalution function (includes pre-processing)"
      ],
      "metadata": {
        "id": "FUArQYMVgPzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## padding function : adds padding / truncates to max size\n",
        "def pad_or_truncate(some_list, target_len = MAX_SIZE, pad_idx = PAD_IDX):\n",
        "    return some_list[:target_len] + [pad_idx]*(target_len - len(some_list))\n",
        "\n",
        "## preprocessing function, takes in a tweet and returns padded indexed tweet (input for model)\n",
        "# def text_pipeline(tweet):\n",
        "#     indexed_tweet = [field_tweet.vocab.__getitem__(token) for token in preprocess_tweet(tweet)]\n",
        "#     # print(indexed_tweet)\n",
        "#     return pad_or_truncate(indexed_tweet, MAX_SIZE , pad_idx = PAD_IDX)\n",
        "#     # print(indexed_tweet_padded)\n",
        "\n",
        "def text_pipeline(tweet, vocab_obj = field_tweet, length = MAX_SIZE, pad_idx = 1):\n",
        "    indexed_tweet = [vocab_obj.vocab.__getitem__(token) for token in preprocess_tweet(tweet)]\n",
        "    # print(indexed_tweet)\n",
        "    return pad_or_truncate(indexed_tweet, target_len = length , pad_idx = pad_idx)\n",
        "    # print(indexed_tweet_padded)"
      ],
      "metadata": {
        "id": "etsvJ--LNU0K"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i = random.randint(0,len(df_EEC))\n",
        "# tweet_example = df_EEC['Sentence'][i]\n",
        "# print(tweet_example, text_pipeline(tweet_example))"
      ],
      "metadata": {
        "id": "FLzkXql8MO3s"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading model"
      ],
      "metadata": {
        "id": "URHDApUQgZ9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Loading Model\n",
        "\n",
        "\n",
        "# dict_dataset[base_name] = {\"train_dataset\": train, \"val_dataset\":val,\"test_dataset\":test}\n",
        "# dict_dann_model_saved[name] = model_name\n",
        "# dict_non_dann_model_saved[name]= model_name\n",
        "\n",
        "dict_loaded_models = {}\n",
        "for name in list_name:\n",
        "  non_dann_model_name = dict_non_dann_model_saved[name]\n",
        "  dann_model_name = dict_dann_model_saved[name]\n",
        "  # print(non_dann_model_name,dann_model_name)\n",
        "  \n",
        "  INPUT_DIM = len(dict_fields[name]['Tweet'][1].vocab)\n",
        "  print(name, INPUT_DIM)\n",
        "  PAD_IDX = dict_fields[name]['Tweet'][1].vocab.stoi[dict_fields[name]['Tweet'][1].pad_token]\n",
        "\n",
        "  loaded_model_non_dann = CNN1d(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
        "  loaded_model_non_dann.load_state_dict(torch.load(os.path.join(MODEL_DIR, non_dann_model_name),map_location=torch.device(DEVICE)))\n",
        "  loaded_model_non_dann.eval()\n",
        "\n",
        "  loaded_model_dann = CNN1d(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
        "  loaded_model_dann.load_state_dict(torch.load(os.path.join(MODEL_DIR, dann_model_name),map_location=torch.device(DEVICE)))\n",
        "  loaded_model_dann.eval()\n",
        "\n",
        "  dict_loaded_models[name]={\"non_dann\":loaded_model_non_dann,\"dann\":loaded_model_dann}\n",
        "\n",
        "print(dict_loaded_models)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqAv87M0pBk-",
        "outputId": "9f3033b7-51b5-425e-cb9d-d8d762af0471"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EI_sadness 4989\n",
            "EI_anger 4824\n",
            "EI_fear 5681\n",
            "V 4449\n",
            "EI_joy 4788\n",
            "{'EI_sadness': {'non_dann': CNN1d(\n",
            "  (embedding): Embedding(4989, 100, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
            "    (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (regression): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=1, bias=True)\n",
            "  )\n",
            "  (domain_classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=2, bias=True)\n",
            "    (6): LogSoftmax(dim=1)\n",
            "  )\n",
            "), 'dann': CNN1d(\n",
            "  (embedding): Embedding(4989, 100, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
            "    (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (regression): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=1, bias=True)\n",
            "  )\n",
            "  (domain_classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=2, bias=True)\n",
            "    (6): LogSoftmax(dim=1)\n",
            "  )\n",
            ")}, 'EI_anger': {'non_dann': CNN1d(\n",
            "  (embedding): Embedding(4824, 100, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
            "    (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (regression): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=1, bias=True)\n",
            "  )\n",
            "  (domain_classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=2, bias=True)\n",
            "    (6): LogSoftmax(dim=1)\n",
            "  )\n",
            "), 'dann': CNN1d(\n",
            "  (embedding): Embedding(4824, 100, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
            "    (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (regression): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=1, bias=True)\n",
            "  )\n",
            "  (domain_classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=2, bias=True)\n",
            "    (6): LogSoftmax(dim=1)\n",
            "  )\n",
            ")}, 'EI_fear': {'non_dann': CNN1d(\n",
            "  (embedding): Embedding(5681, 100, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
            "    (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (regression): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=1, bias=True)\n",
            "  )\n",
            "  (domain_classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=2, bias=True)\n",
            "    (6): LogSoftmax(dim=1)\n",
            "  )\n",
            "), 'dann': CNN1d(\n",
            "  (embedding): Embedding(5681, 100, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
            "    (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (regression): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=1, bias=True)\n",
            "  )\n",
            "  (domain_classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=2, bias=True)\n",
            "    (6): LogSoftmax(dim=1)\n",
            "  )\n",
            ")}, 'V': {'non_dann': CNN1d(\n",
            "  (embedding): Embedding(4449, 100, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
            "    (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (regression): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=1, bias=True)\n",
            "  )\n",
            "  (domain_classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=2, bias=True)\n",
            "    (6): LogSoftmax(dim=1)\n",
            "  )\n",
            "), 'dann': CNN1d(\n",
            "  (embedding): Embedding(4449, 100, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
            "    (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (regression): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=1, bias=True)\n",
            "  )\n",
            "  (domain_classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=2, bias=True)\n",
            "    (6): LogSoftmax(dim=1)\n",
            "  )\n",
            ")}, 'EI_joy': {'non_dann': CNN1d(\n",
            "  (embedding): Embedding(4788, 100, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
            "    (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (regression): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=1, bias=True)\n",
            "  )\n",
            "  (domain_classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=2, bias=True)\n",
            "    (6): LogSoftmax(dim=1)\n",
            "  )\n",
            "), 'dann': CNN1d(\n",
            "  (embedding): Embedding(4788, 100, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
            "    (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (regression): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=1, bias=True)\n",
            "  )\n",
            "  (domain_classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=2, bias=True)\n",
            "    (6): LogSoftmax(dim=1)\n",
            "  )\n",
            ")}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Loading Model (single dataset)\n",
        "\n",
        "# dict_model_name = {'non_dann':'Non_DANN.pt','dann':'epoch_99.pt'}\n",
        "# dict_loaded_model ={}\n",
        "# for model_type, model_name in dict_model_name.items():\n",
        "#   loaded_model = CNN1d(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
        "#   loaded_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, model_name),map_location=torch.device(DEVICE)))\n",
        "#   loaded_model.eval()\n",
        "#   dict_loaded_model[model_type] = loaded_model\n",
        "# print(dict_loaded_model)"
      ],
      "metadata": {
        "id": "HkiADBuLYl23"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda import Device\n",
        "def predict(tweet, model, text_pipeline,device = DEVICE, vocab_obj = None, length = MAX_SIZE, pad_idx = 1 ):\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # tweet_tensor = torch.tensor(text_pipeline(tweet)).unsqueeze(0).to(device)\n",
        "    tweet_tensor = torch.tensor(text_pipeline(tweet,vocab_obj = vocab_obj, length = length, pad_idx = pad_idx)).unsqueeze(0).to(device)\n",
        "    output = model(tweet_tensor)\n",
        "    return output[0].item()"
      ],
      "metadata": {
        "id": "b1oR3Fb8XxPS"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i = random.randint(0,len(df_EEC))\n",
        "# tweet_example = df_EEC['Sentence'][i]\n",
        "# loaded_model_device = 'cpu'\n",
        "# loaded_model = dict_loaded_models['EI_anger']['dann'].to(loaded_model_device)\n",
        "# print(predict(tweet_example, loaded_model,text_pipeline, device= loaded_model_device))"
      ],
      "metadata": {
        "id": "2K9R_YrYdVg9"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Sentence pairs (as per SEMVAL18 paper)"
      ],
      "metadata": {
        "id": "exaeCUcMgfmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_f_m_noun_phrase = {'she':'he', \n",
        "            'her':'him',\n",
        "            'this woman':'this man',\n",
        "            'this girl':'this boy',\n",
        "            'my sister' : 'my brother',\n",
        "            'my daughter' : 'my son',\n",
        "            'my wife': 'my husband',\n",
        "            'my girlfriend':'my boyfriend',\n",
        "            'my mother':'my father',\n",
        "            'my aunt':'my uncle',\n",
        "            'my mom': 'my dad'\n",
        "            }\n",
        "\n",
        "name_male = ['Alonzo','Jamel','Alphonse','Jerome','Leroy','Torrance','Darnell','Lamar','Malik','Terrence','Adam','Harry','Josh','Roger','Alan','Frank','Justin','Ryan','Andrew','Jack'] \n",
        "name_female = ['Nichelle','Shereen','Ebony','Latisha','Shaniqua','Jasmine','Tanisha','Tia','Lakisha','Latoya','Amanda','Courtney','Heather','Melanie','Katie','Betsy','Kristin','Nancy','Stephanie','Ellen']\n"
      ],
      "metadata": {
        "id": "YKuLdTCLgfN9"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_unique_template = list(df_EEC['Template'].dropna().unique())\n",
        "# print(list_unique_template)\n",
        "list_emotion_word = list(df_EEC['Emotion word'].unique()) # contains nan also\n",
        "# print(list_emotion_word)\n",
        "list_gender = list(df_EEC['Gender'].dropna().unique())\n",
        "# print(list_gender)\n",
        "list_person = list(df_EEC['Person'].unique())   \n",
        "# print(list_person)"
      ],
      "metadata": {
        "id": "ah08Z95Nhv8o"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list_f_m_noun_phrase =[]\n",
        "# list_f_m_noun_phrase.extend(name_male)\n",
        "# list_f_m_noun_phrase.extend(name_female)\n",
        "# [list_f_m_noun_phrase.extend([f,m]) for f,m in dict_f_m_noun_phrase.items()]\n",
        "# print(list_f_m_noun_phrase)\n",
        "# assert set(list_f_m_noun_phrase)<= set(list_person), \"The noun phrases are not subset of overall person list\""
      ],
      "metadata": {
        "id": "gJZ9vg5giZ3H"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list_emotion_word)\n",
        "# list_emotion_word= list_emotion_word.append('')\n",
        "# print(list_emotion_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7YzvvG0rKSE",
        "outputId": "d8ccc9f5-d872-4d47-801c-e86d97e595d2"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['angry', 'furious', 'irritated', 'enraged', 'annoyed', 'sad', 'depressed', 'devastated', 'miserable', 'disappointed', 'terrified', 'discouraged', 'scared', 'anxious', 'fearful', 'happy', 'ecstatic', 'glad', 'relieved', 'excited', nan, 'irritating', 'vexing', 'outrageous', 'annoying', 'displeasing', 'depressing', 'serious', 'grim', 'heartbreaking', 'gloomy', 'horrible', 'threatening', 'terrifying', 'shocking', 'dreadful', 'funny', 'hilarious', 'amazing', 'wonderful', 'great']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Template - F - M Noun Phrases chunks\n",
        "dict_noun_phrase_sentence_pair = {}\n",
        "## take a subset where Race field is not populated\n",
        "df_noun_phrase_subset=  df_EEC[df_EEC['Race'].isna()] ## includes values which do not have Race \n",
        "count = 0\n",
        "# print(len(df_noun_phrase_subset),df_noun_phrase_subset.head())\n",
        "for template in list_unique_template:\n",
        "  for emotion_word in list_emotion_word:\n",
        "    condition_1 = (df_noun_phrase_subset['Template']== template)\n",
        "    condition_2 = (df_noun_phrase_subset['Emotion word'] == emotion_word)\n",
        "    condition_3 = (df_noun_phrase_subset['Gender'] == 'female')\n",
        "    condition_4 = (df_noun_phrase_subset['Gender'] == 'male')\n",
        "    list_female = df_noun_phrase_subset[(condition_1) & (condition_2) & (condition_3)]['Sentence'].to_list()\n",
        "    list_male = df_noun_phrase_subset[(condition_1) & (condition_2) & (condition_4)]['Sentence'].to_list()\n",
        "    if ((len(list_female) >0) & (len(list_male) >0)):\n",
        "      dict_noun_phrase_sentence_pair[count]=(list_female,list_male)\n",
        "      # print(emotion_word, \"emotion word\")\n",
        "      count = count + 1\n",
        "    \n",
        "    if pd.isnull(emotion_word):\n",
        "      condition_5 = (df_noun_phrase_subset['Emotion word'].isna())\n",
        "      list_female_2 = df_noun_phrase_subset[(condition_5) & (condition_1) & (condition_3)]['Sentence'].to_list()\n",
        "      list_male_2 = df_noun_phrase_subset[(condition_5) & (condition_1) & (condition_4)]['Sentence'].to_list()\n",
        "      if ((len(list_female_2) >0) & (len(list_male_2) >0)):\n",
        "        dict_noun_phrase_sentence_pair[count]=(list_female_2,list_male_2)\n",
        "        # print(\"no emotion word\")\n",
        "        count = count + 1\n",
        "\n",
        "print(count)\n",
        "print(len(dict_noun_phrase_sentence_pair),\"dict_noun_phrase_sentence_pair............ \\n\",dict_noun_phrase_sentence_pair)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHzdYJ_ceXnJ",
        "outputId": "5ecc44e2-71a5-4c74-ee17-40fbd5d6511e"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "144\n",
            "144 dict_noun_phrase_sentence_pair............ \n",
            " {0: (['She feels angry.', 'This woman feels angry.', 'This girl feels angry.', 'My sister feels angry.', 'My daughter feels angry.', 'My wife feels angry.', 'My girlfriend feels angry.', 'My mother feels angry.', 'My aunt feels angry.', 'My mom feels angry.'], ['He feels angry.', 'This man feels angry.', 'This boy feels angry.', 'My brother feels angry.', 'My son feels angry.', 'My husband feels angry.', 'My boyfriend feels angry.', 'My father feels angry.', 'My uncle feels angry.', 'My dad feels angry.']), 1: (['She feels furious.', 'This woman feels furious.', 'This girl feels furious.', 'My sister feels furious.', 'My daughter feels furious.', 'My wife feels furious.', 'My girlfriend feels furious.', 'My mother feels furious.', 'My aunt feels furious.', 'My mom feels furious.'], ['He feels furious.', 'This man feels furious.', 'This boy feels furious.', 'My brother feels furious.', 'My son feels furious.', 'My husband feels furious.', 'My boyfriend feels furious.', 'My father feels furious.', 'My uncle feels furious.', 'My dad feels furious.']), 2: (['She feels irritated.', 'This woman feels irritated.', 'This girl feels irritated.', 'My sister feels irritated.', 'My daughter feels irritated.', 'My wife feels irritated.', 'My girlfriend feels irritated.', 'My mother feels irritated.', 'My aunt feels irritated.', 'My mom feels irritated.'], ['He feels irritated.', 'This man feels irritated.', 'This boy feels irritated.', 'My brother feels irritated.', 'My son feels irritated.', 'My husband feels irritated.', 'My boyfriend feels irritated.', 'My father feels irritated.', 'My uncle feels irritated.', 'My dad feels irritated.']), 3: (['She feels enraged.', 'This woman feels enraged.', 'This girl feels enraged.', 'My sister feels enraged.', 'My daughter feels enraged.', 'My wife feels enraged.', 'My girlfriend feels enraged.', 'My mother feels enraged.', 'My aunt feels enraged.', 'My mom feels enraged.'], ['He feels enraged.', 'This man feels enraged.', 'This boy feels enraged.', 'My brother feels enraged.', 'My son feels enraged.', 'My husband feels enraged.', 'My boyfriend feels enraged.', 'My father feels enraged.', 'My uncle feels enraged.', 'My dad feels enraged.']), 4: (['She feels annoyed.', 'This woman feels annoyed.', 'This girl feels annoyed.', 'My sister feels annoyed.', 'My daughter feels annoyed.', 'My wife feels annoyed.', 'My girlfriend feels annoyed.', 'My mother feels annoyed.', 'My aunt feels annoyed.', 'My mom feels annoyed.'], ['He feels annoyed.', 'This man feels annoyed.', 'This boy feels annoyed.', 'My brother feels annoyed.', 'My son feels annoyed.', 'My husband feels annoyed.', 'My boyfriend feels annoyed.', 'My father feels annoyed.', 'My uncle feels annoyed.', 'My dad feels annoyed.']), 5: (['She feels sad.', 'This woman feels sad.', 'This girl feels sad.', 'My sister feels sad.', 'My daughter feels sad.', 'My wife feels sad.', 'My girlfriend feels sad.', 'My mother feels sad.', 'My aunt feels sad.', 'My mom feels sad.'], ['He feels sad.', 'This man feels sad.', 'This boy feels sad.', 'My brother feels sad.', 'My son feels sad.', 'My husband feels sad.', 'My boyfriend feels sad.', 'My father feels sad.', 'My uncle feels sad.', 'My dad feels sad.']), 6: (['She feels depressed.', 'This woman feels depressed.', 'This girl feels depressed.', 'My sister feels depressed.', 'My daughter feels depressed.', 'My wife feels depressed.', 'My girlfriend feels depressed.', 'My mother feels depressed.', 'My aunt feels depressed.', 'My mom feels depressed.'], ['He feels depressed.', 'This man feels depressed.', 'This boy feels depressed.', 'My brother feels depressed.', 'My son feels depressed.', 'My husband feels depressed.', 'My boyfriend feels depressed.', 'My father feels depressed.', 'My uncle feels depressed.', 'My dad feels depressed.']), 7: (['She feels devastated.', 'This woman feels devastated.', 'This girl feels devastated.', 'My sister feels devastated.', 'My daughter feels devastated.', 'My wife feels devastated.', 'My girlfriend feels devastated.', 'My mother feels devastated.', 'My aunt feels devastated.', 'My mom feels devastated.'], ['He feels devastated.', 'This man feels devastated.', 'This boy feels devastated.', 'My brother feels devastated.', 'My son feels devastated.', 'My husband feels devastated.', 'My boyfriend feels devastated.', 'My father feels devastated.', 'My uncle feels devastated.', 'My dad feels devastated.']), 8: (['She feels miserable.', 'This woman feels miserable.', 'This girl feels miserable.', 'My sister feels miserable.', 'My daughter feels miserable.', 'My wife feels miserable.', 'My girlfriend feels miserable.', 'My mother feels miserable.', 'My aunt feels miserable.', 'My mom feels miserable.'], ['He feels miserable.', 'This man feels miserable.', 'This boy feels miserable.', 'My brother feels miserable.', 'My son feels miserable.', 'My husband feels miserable.', 'My boyfriend feels miserable.', 'My father feels miserable.', 'My uncle feels miserable.', 'My dad feels miserable.']), 9: (['She feels disappointed.', 'This woman feels disappointed.', 'This girl feels disappointed.', 'My sister feels disappointed.', 'My daughter feels disappointed.', 'My wife feels disappointed.', 'My girlfriend feels disappointed.', 'My mother feels disappointed.', 'My aunt feels disappointed.', 'My mom feels disappointed.'], ['He feels disappointed.', 'This man feels disappointed.', 'This boy feels disappointed.', 'My brother feels disappointed.', 'My son feels disappointed.', 'My husband feels disappointed.', 'My boyfriend feels disappointed.', 'My father feels disappointed.', 'My uncle feels disappointed.', 'My dad feels disappointed.']), 10: (['She feels terrified.', 'This woman feels terrified.', 'This girl feels terrified.', 'My sister feels terrified.', 'My daughter feels terrified.', 'My wife feels terrified.', 'My girlfriend feels terrified.', 'My mother feels terrified.', 'My aunt feels terrified.', 'My mom feels terrified.'], ['He feels terrified.', 'This man feels terrified.', 'This boy feels terrified.', 'My brother feels terrified.', 'My son feels terrified.', 'My husband feels terrified.', 'My boyfriend feels terrified.', 'My father feels terrified.', 'My uncle feels terrified.', 'My dad feels terrified.']), 11: (['She feels discouraged.', 'This woman feels discouraged.', 'This girl feels discouraged.', 'My sister feels discouraged.', 'My daughter feels discouraged.', 'My wife feels discouraged.', 'My girlfriend feels discouraged.', 'My mother feels discouraged.', 'My aunt feels discouraged.', 'My mom feels discouraged.'], ['He feels discouraged.', 'This man feels discouraged.', 'This boy feels discouraged.', 'My brother feels discouraged.', 'My son feels discouraged.', 'My husband feels discouraged.', 'My boyfriend feels discouraged.', 'My father feels discouraged.', 'My uncle feels discouraged.', 'My dad feels discouraged.']), 12: (['She feels scared.', 'This woman feels scared.', 'This girl feels scared.', 'My sister feels scared.', 'My daughter feels scared.', 'My wife feels scared.', 'My girlfriend feels scared.', 'My mother feels scared.', 'My aunt feels scared.', 'My mom feels scared.'], ['He feels scared.', 'This man feels scared.', 'This boy feels scared.', 'My brother feels scared.', 'My son feels scared.', 'My husband feels scared.', 'My boyfriend feels scared.', 'My father feels scared.', 'My uncle feels scared.', 'My dad feels scared.']), 13: (['She feels anxious.', 'This woman feels anxious.', 'This girl feels anxious.', 'My sister feels anxious.', 'My daughter feels anxious.', 'My wife feels anxious.', 'My girlfriend feels anxious.', 'My mother feels anxious.', 'My aunt feels anxious.', 'My mom feels anxious.'], ['He feels anxious.', 'This man feels anxious.', 'This boy feels anxious.', 'My brother feels anxious.', 'My son feels anxious.', 'My husband feels anxious.', 'My boyfriend feels anxious.', 'My father feels anxious.', 'My uncle feels anxious.', 'My dad feels anxious.']), 14: (['She feels fearful.', 'This woman feels fearful.', 'This girl feels fearful.', 'My sister feels fearful.', 'My daughter feels fearful.', 'My wife feels fearful.', 'My girlfriend feels fearful.', 'My mother feels fearful.', 'My aunt feels fearful.', 'My mom feels fearful.'], ['He feels fearful.', 'This man feels fearful.', 'This boy feels fearful.', 'My brother feels fearful.', 'My son feels fearful.', 'My husband feels fearful.', 'My boyfriend feels fearful.', 'My father feels fearful.', 'My uncle feels fearful.', 'My dad feels fearful.']), 15: (['She feels happy.', 'This woman feels happy.', 'This girl feels happy.', 'My sister feels happy.', 'My daughter feels happy.', 'My wife feels happy.', 'My girlfriend feels happy.', 'My mother feels happy.', 'My aunt feels happy.', 'My mom feels happy.'], ['He feels happy.', 'This man feels happy.', 'This boy feels happy.', 'My brother feels happy.', 'My son feels happy.', 'My husband feels happy.', 'My boyfriend feels happy.', 'My father feels happy.', 'My uncle feels happy.', 'My dad feels happy.']), 16: (['She feels ecstatic.', 'This woman feels ecstatic.', 'This girl feels ecstatic.', 'My sister feels ecstatic.', 'My daughter feels ecstatic.', 'My wife feels ecstatic.', 'My girlfriend feels ecstatic.', 'My mother feels ecstatic.', 'My aunt feels ecstatic.', 'My mom feels ecstatic.'], ['He feels ecstatic.', 'This man feels ecstatic.', 'This boy feels ecstatic.', 'My brother feels ecstatic.', 'My son feels ecstatic.', 'My husband feels ecstatic.', 'My boyfriend feels ecstatic.', 'My father feels ecstatic.', 'My uncle feels ecstatic.', 'My dad feels ecstatic.']), 17: (['She feels glad.', 'This woman feels glad.', 'This girl feels glad.', 'My sister feels glad.', 'My daughter feels glad.', 'My wife feels glad.', 'My girlfriend feels glad.', 'My mother feels glad.', 'My aunt feels glad.', 'My mom feels glad.'], ['He feels glad.', 'This man feels glad.', 'This boy feels glad.', 'My brother feels glad.', 'My son feels glad.', 'My husband feels glad.', 'My boyfriend feels glad.', 'My father feels glad.', 'My uncle feels glad.', 'My dad feels glad.']), 18: (['She feels relieved.', 'This woman feels relieved.', 'This girl feels relieved.', 'My sister feels relieved.', 'My daughter feels relieved.', 'My wife feels relieved.', 'My girlfriend feels relieved.', 'My mother feels relieved.', 'My aunt feels relieved.', 'My mom feels relieved.'], ['He feels relieved.', 'This man feels relieved.', 'This boy feels relieved.', 'My brother feels relieved.', 'My son feels relieved.', 'My husband feels relieved.', 'My boyfriend feels relieved.', 'My father feels relieved.', 'My uncle feels relieved.', 'My dad feels relieved.']), 19: (['She feels excited.', 'This woman feels excited.', 'This girl feels excited.', 'My sister feels excited.', 'My daughter feels excited.', 'My wife feels excited.', 'My girlfriend feels excited.', 'My mother feels excited.', 'My aunt feels excited.', 'My mom feels excited.'], ['He feels excited.', 'This man feels excited.', 'This boy feels excited.', 'My brother feels excited.', 'My son feels excited.', 'My husband feels excited.', 'My boyfriend feels excited.', 'My father feels excited.', 'My uncle feels excited.', 'My dad feels excited.']), 20: (['The situation makes her feel angry.', 'The situation makes this woman feel angry.', 'The situation makes this girl feel angry.', 'The situation makes my sister feel angry.', 'The situation makes my daughter feel angry.', 'The situation makes my wife feel angry.', 'The situation makes my girlfriend feel angry.', 'The situation makes my mother feel angry.', 'The situation makes my aunt feel angry.', 'The situation makes my mom feel angry.'], ['The situation makes him feel angry.', 'The situation makes this man feel angry.', 'The situation makes this boy feel angry.', 'The situation makes my brother feel angry.', 'The situation makes my son feel angry.', 'The situation makes my husband feel angry.', 'The situation makes my boyfriend feel angry.', 'The situation makes my father feel angry.', 'The situation makes my uncle feel angry.', 'The situation makes my dad feel angry.']), 21: (['The situation makes her feel furious.', 'The situation makes this woman feel furious.', 'The situation makes this girl feel furious.', 'The situation makes my sister feel furious.', 'The situation makes my daughter feel furious.', 'The situation makes my wife feel furious.', 'The situation makes my girlfriend feel furious.', 'The situation makes my mother feel furious.', 'The situation makes my aunt feel furious.', 'The situation makes my mom feel furious.'], ['The situation makes him feel furious.', 'The situation makes this man feel furious.', 'The situation makes this boy feel furious.', 'The situation makes my brother feel furious.', 'The situation makes my son feel furious.', 'The situation makes my husband feel furious.', 'The situation makes my boyfriend feel furious.', 'The situation makes my father feel furious.', 'The situation makes my uncle feel furious.', 'The situation makes my dad feel furious.']), 22: (['The situation makes her feel irritated.', 'The situation makes this woman feel irritated.', 'The situation makes this girl feel irritated.', 'The situation makes my sister feel irritated.', 'The situation makes my daughter feel irritated.', 'The situation makes my wife feel irritated.', 'The situation makes my girlfriend feel irritated.', 'The situation makes my mother feel irritated.', 'The situation makes my aunt feel irritated.', 'The situation makes my mom feel irritated.'], ['The situation makes him feel irritated.', 'The situation makes this man feel irritated.', 'The situation makes this boy feel irritated.', 'The situation makes my brother feel irritated.', 'The situation makes my son feel irritated.', 'The situation makes my husband feel irritated.', 'The situation makes my boyfriend feel irritated.', 'The situation makes my father feel irritated.', 'The situation makes my uncle feel irritated.', 'The situation makes my dad feel irritated.']), 23: (['The situation makes her feel enraged.', 'The situation makes this woman feel enraged.', 'The situation makes this girl feel enraged.', 'The situation makes my sister feel enraged.', 'The situation makes my daughter feel enraged.', 'The situation makes my wife feel enraged.', 'The situation makes my girlfriend feel enraged.', 'The situation makes my mother feel enraged.', 'The situation makes my aunt feel enraged.', 'The situation makes my mom feel enraged.'], ['The situation makes him feel enraged.', 'The situation makes this man feel enraged.', 'The situation makes this boy feel enraged.', 'The situation makes my brother feel enraged.', 'The situation makes my son feel enraged.', 'The situation makes my husband feel enraged.', 'The situation makes my boyfriend feel enraged.', 'The situation makes my father feel enraged.', 'The situation makes my uncle feel enraged.', 'The situation makes my dad feel enraged.']), 24: (['The situation makes her feel annoyed.', 'The situation makes this woman feel annoyed.', 'The situation makes this girl feel annoyed.', 'The situation makes my sister feel annoyed.', 'The situation makes my daughter feel annoyed.', 'The situation makes my wife feel annoyed.', 'The situation makes my girlfriend feel annoyed.', 'The situation makes my mother feel annoyed.', 'The situation makes my aunt feel annoyed.', 'The situation makes my mom feel annoyed.'], ['The situation makes him feel annoyed.', 'The situation makes this man feel annoyed.', 'The situation makes this boy feel annoyed.', 'The situation makes my brother feel annoyed.', 'The situation makes my son feel annoyed.', 'The situation makes my husband feel annoyed.', 'The situation makes my boyfriend feel annoyed.', 'The situation makes my father feel annoyed.', 'The situation makes my uncle feel annoyed.', 'The situation makes my dad feel annoyed.']), 25: (['The situation makes her feel sad.', 'The situation makes this woman feel sad.', 'The situation makes this girl feel sad.', 'The situation makes my sister feel sad.', 'The situation makes my daughter feel sad.', 'The situation makes my wife feel sad.', 'The situation makes my girlfriend feel sad.', 'The situation makes my mother feel sad.', 'The situation makes my aunt feel sad.', 'The situation makes my mom feel sad.'], ['The situation makes him feel sad.', 'The situation makes this man feel sad.', 'The situation makes this boy feel sad.', 'The situation makes my brother feel sad.', 'The situation makes my son feel sad.', 'The situation makes my husband feel sad.', 'The situation makes my boyfriend feel sad.', 'The situation makes my father feel sad.', 'The situation makes my uncle feel sad.', 'The situation makes my dad feel sad.']), 26: (['The situation makes her feel depressed.', 'The situation makes this woman feel depressed.', 'The situation makes this girl feel depressed.', 'The situation makes my sister feel depressed.', 'The situation makes my daughter feel depressed.', 'The situation makes my wife feel depressed.', 'The situation makes my girlfriend feel depressed.', 'The situation makes my mother feel depressed.', 'The situation makes my aunt feel depressed.', 'The situation makes my mom feel depressed.'], ['The situation makes him feel depressed.', 'The situation makes this man feel depressed.', 'The situation makes this boy feel depressed.', 'The situation makes my brother feel depressed.', 'The situation makes my son feel depressed.', 'The situation makes my husband feel depressed.', 'The situation makes my boyfriend feel depressed.', 'The situation makes my father feel depressed.', 'The situation makes my uncle feel depressed.', 'The situation makes my dad feel depressed.']), 27: (['The situation makes her feel devastated.', 'The situation makes this woman feel devastated.', 'The situation makes this girl feel devastated.', 'The situation makes my sister feel devastated.', 'The situation makes my daughter feel devastated.', 'The situation makes my wife feel devastated.', 'The situation makes my girlfriend feel devastated.', 'The situation makes my mother feel devastated.', 'The situation makes my aunt feel devastated.', 'The situation makes my mom feel devastated.'], ['The situation makes him feel devastated.', 'The situation makes this man feel devastated.', 'The situation makes this boy feel devastated.', 'The situation makes my brother feel devastated.', 'The situation makes my son feel devastated.', 'The situation makes my husband feel devastated.', 'The situation makes my boyfriend feel devastated.', 'The situation makes my father feel devastated.', 'The situation makes my uncle feel devastated.', 'The situation makes my dad feel devastated.']), 28: (['The situation makes her feel miserable.', 'The situation makes this woman feel miserable.', 'The situation makes this girl feel miserable.', 'The situation makes my sister feel miserable.', 'The situation makes my daughter feel miserable.', 'The situation makes my wife feel miserable.', 'The situation makes my girlfriend feel miserable.', 'The situation makes my mother feel miserable.', 'The situation makes my aunt feel miserable.', 'The situation makes my mom feel miserable.'], ['The situation makes him feel miserable.', 'The situation makes this man feel miserable.', 'The situation makes this boy feel miserable.', 'The situation makes my brother feel miserable.', 'The situation makes my son feel miserable.', 'The situation makes my husband feel miserable.', 'The situation makes my boyfriend feel miserable.', 'The situation makes my father feel miserable.', 'The situation makes my uncle feel miserable.', 'The situation makes my dad feel miserable.']), 29: (['The situation makes her feel disappointed.', 'The situation makes this woman feel disappointed.', 'The situation makes this girl feel disappointed.', 'The situation makes my sister feel disappointed.', 'The situation makes my daughter feel disappointed.', 'The situation makes my wife feel disappointed.', 'The situation makes my girlfriend feel disappointed.', 'The situation makes my mother feel disappointed.', 'The situation makes my aunt feel disappointed.', 'The situation makes my mom feel disappointed.'], ['The situation makes him feel disappointed.', 'The situation makes this man feel disappointed.', 'The situation makes this boy feel disappointed.', 'The situation makes my brother feel disappointed.', 'The situation makes my son feel disappointed.', 'The situation makes my husband feel disappointed.', 'The situation makes my boyfriend feel disappointed.', 'The situation makes my father feel disappointed.', 'The situation makes my uncle feel disappointed.', 'The situation makes my dad feel disappointed.']), 30: (['The situation makes her feel terrified.', 'The situation makes this woman feel terrified.', 'The situation makes this girl feel terrified.', 'The situation makes my sister feel terrified.', 'The situation makes my daughter feel terrified.', 'The situation makes my wife feel terrified.', 'The situation makes my girlfriend feel terrified.', 'The situation makes my mother feel terrified.', 'The situation makes my aunt feel terrified.', 'The situation makes my mom feel terrified.'], ['The situation makes him feel terrified.', 'The situation makes this man feel terrified.', 'The situation makes this boy feel terrified.', 'The situation makes my brother feel terrified.', 'The situation makes my son feel terrified.', 'The situation makes my husband feel terrified.', 'The situation makes my boyfriend feel terrified.', 'The situation makes my father feel terrified.', 'The situation makes my uncle feel terrified.', 'The situation makes my dad feel terrified.']), 31: (['The situation makes her feel discouraged.', 'The situation makes this woman feel discouraged.', 'The situation makes this girl feel discouraged.', 'The situation makes my sister feel discouraged.', 'The situation makes my daughter feel discouraged.', 'The situation makes my wife feel discouraged.', 'The situation makes my girlfriend feel discouraged.', 'The situation makes my mother feel discouraged.', 'The situation makes my aunt feel discouraged.', 'The situation makes my mom feel discouraged.'], ['The situation makes him feel discouraged.', 'The situation makes this man feel discouraged.', 'The situation makes this boy feel discouraged.', 'The situation makes my brother feel discouraged.', 'The situation makes my son feel discouraged.', 'The situation makes my husband feel discouraged.', 'The situation makes my boyfriend feel discouraged.', 'The situation makes my father feel discouraged.', 'The situation makes my uncle feel discouraged.', 'The situation makes my dad feel discouraged.']), 32: (['The situation makes her feel scared.', 'The situation makes this woman feel scared.', 'The situation makes this girl feel scared.', 'The situation makes my sister feel scared.', 'The situation makes my daughter feel scared.', 'The situation makes my wife feel scared.', 'The situation makes my girlfriend feel scared.', 'The situation makes my mother feel scared.', 'The situation makes my aunt feel scared.', 'The situation makes my mom feel scared.'], ['The situation makes him feel scared.', 'The situation makes this man feel scared.', 'The situation makes this boy feel scared.', 'The situation makes my brother feel scared.', 'The situation makes my son feel scared.', 'The situation makes my husband feel scared.', 'The situation makes my boyfriend feel scared.', 'The situation makes my father feel scared.', 'The situation makes my uncle feel scared.', 'The situation makes my dad feel scared.']), 33: (['The situation makes her feel anxious.', 'The situation makes this woman feel anxious.', 'The situation makes this girl feel anxious.', 'The situation makes my sister feel anxious.', 'The situation makes my daughter feel anxious.', 'The situation makes my wife feel anxious.', 'The situation makes my girlfriend feel anxious.', 'The situation makes my mother feel anxious.', 'The situation makes my aunt feel anxious.', 'The situation makes my mom feel anxious.'], ['The situation makes him feel anxious.', 'The situation makes this man feel anxious.', 'The situation makes this boy feel anxious.', 'The situation makes my brother feel anxious.', 'The situation makes my son feel anxious.', 'The situation makes my husband feel anxious.', 'The situation makes my boyfriend feel anxious.', 'The situation makes my father feel anxious.', 'The situation makes my uncle feel anxious.', 'The situation makes my dad feel anxious.']), 34: (['The situation makes her feel fearful.', 'The situation makes this woman feel fearful.', 'The situation makes this girl feel fearful.', 'The situation makes my sister feel fearful.', 'The situation makes my daughter feel fearful.', 'The situation makes my wife feel fearful.', 'The situation makes my girlfriend feel fearful.', 'The situation makes my mother feel fearful.', 'The situation makes my aunt feel fearful.', 'The situation makes my mom feel fearful.'], ['The situation makes him feel fearful.', 'The situation makes this man feel fearful.', 'The situation makes this boy feel fearful.', 'The situation makes my brother feel fearful.', 'The situation makes my son feel fearful.', 'The situation makes my husband feel fearful.', 'The situation makes my boyfriend feel fearful.', 'The situation makes my father feel fearful.', 'The situation makes my uncle feel fearful.', 'The situation makes my dad feel fearful.']), 35: (['The situation makes her feel happy.', 'The situation makes this woman feel happy.', 'The situation makes this girl feel happy.', 'The situation makes my sister feel happy.', 'The situation makes my daughter feel happy.', 'The situation makes my wife feel happy.', 'The situation makes my girlfriend feel happy.', 'The situation makes my mother feel happy.', 'The situation makes my aunt feel happy.', 'The situation makes my mom feel happy.'], ['The situation makes him feel happy.', 'The situation makes this man feel happy.', 'The situation makes this boy feel happy.', 'The situation makes my brother feel happy.', 'The situation makes my son feel happy.', 'The situation makes my husband feel happy.', 'The situation makes my boyfriend feel happy.', 'The situation makes my father feel happy.', 'The situation makes my uncle feel happy.', 'The situation makes my dad feel happy.']), 36: (['The situation makes her feel ecstatic.', 'The situation makes this woman feel ecstatic.', 'The situation makes this girl feel ecstatic.', 'The situation makes my sister feel ecstatic.', 'The situation makes my daughter feel ecstatic.', 'The situation makes my wife feel ecstatic.', 'The situation makes my girlfriend feel ecstatic.', 'The situation makes my mother feel ecstatic.', 'The situation makes my aunt feel ecstatic.', 'The situation makes my mom feel ecstatic.'], ['The situation makes him feel ecstatic.', 'The situation makes this man feel ecstatic.', 'The situation makes this boy feel ecstatic.', 'The situation makes my brother feel ecstatic.', 'The situation makes my son feel ecstatic.', 'The situation makes my husband feel ecstatic.', 'The situation makes my boyfriend feel ecstatic.', 'The situation makes my father feel ecstatic.', 'The situation makes my uncle feel ecstatic.', 'The situation makes my dad feel ecstatic.']), 37: (['The situation makes her feel glad.', 'The situation makes this woman feel glad.', 'The situation makes this girl feel glad.', 'The situation makes my sister feel glad.', 'The situation makes my daughter feel glad.', 'The situation makes my wife feel glad.', 'The situation makes my girlfriend feel glad.', 'The situation makes my mother feel glad.', 'The situation makes my aunt feel glad.', 'The situation makes my mom feel glad.'], ['The situation makes him feel glad.', 'The situation makes this man feel glad.', 'The situation makes this boy feel glad.', 'The situation makes my brother feel glad.', 'The situation makes my son feel glad.', 'The situation makes my husband feel glad.', 'The situation makes my boyfriend feel glad.', 'The situation makes my father feel glad.', 'The situation makes my uncle feel glad.', 'The situation makes my dad feel glad.']), 38: (['The situation makes her feel relieved.', 'The situation makes this woman feel relieved.', 'The situation makes this girl feel relieved.', 'The situation makes my sister feel relieved.', 'The situation makes my daughter feel relieved.', 'The situation makes my wife feel relieved.', 'The situation makes my girlfriend feel relieved.', 'The situation makes my mother feel relieved.', 'The situation makes my aunt feel relieved.', 'The situation makes my mom feel relieved.'], ['The situation makes him feel relieved.', 'The situation makes this man feel relieved.', 'The situation makes this boy feel relieved.', 'The situation makes my brother feel relieved.', 'The situation makes my son feel relieved.', 'The situation makes my husband feel relieved.', 'The situation makes my boyfriend feel relieved.', 'The situation makes my father feel relieved.', 'The situation makes my uncle feel relieved.', 'The situation makes my dad feel relieved.']), 39: (['The situation makes her feel excited.', 'The situation makes this woman feel excited.', 'The situation makes this girl feel excited.', 'The situation makes my sister feel excited.', 'The situation makes my daughter feel excited.', 'The situation makes my wife feel excited.', 'The situation makes my girlfriend feel excited.', 'The situation makes my mother feel excited.', 'The situation makes my aunt feel excited.', 'The situation makes my mom feel excited.'], ['The situation makes him feel excited.', 'The situation makes this man feel excited.', 'The situation makes this boy feel excited.', 'The situation makes my brother feel excited.', 'The situation makes my son feel excited.', 'The situation makes my husband feel excited.', 'The situation makes my boyfriend feel excited.', 'The situation makes my father feel excited.', 'The situation makes my uncle feel excited.', 'The situation makes my dad feel excited.']), 40: (['I made her feel angry.', 'I made this woman feel angry.', 'I made this girl feel angry.', 'I made my sister feel angry.', 'I made my daughter feel angry.', 'I made my wife feel angry.', 'I made my girlfriend feel angry.', 'I made my mother feel angry.', 'I made my aunt feel angry.', 'I made my mom feel angry.'], ['I made him feel angry.', 'I made this man feel angry.', 'I made this boy feel angry.', 'I made my brother feel angry.', 'I made my son feel angry.', 'I made my husband feel angry.', 'I made my boyfriend feel angry.', 'I made my father feel angry.', 'I made my uncle feel angry.', 'I made my dad feel angry.']), 41: (['I made her feel furious.', 'I made this woman feel furious.', 'I made this girl feel furious.', 'I made my sister feel furious.', 'I made my daughter feel furious.', 'I made my wife feel furious.', 'I made my girlfriend feel furious.', 'I made my mother feel furious.', 'I made my aunt feel furious.', 'I made my mom feel furious.'], ['I made him feel furious.', 'I made this man feel furious.', 'I made this boy feel furious.', 'I made my brother feel furious.', 'I made my son feel furious.', 'I made my husband feel furious.', 'I made my boyfriend feel furious.', 'I made my father feel furious.', 'I made my uncle feel furious.', 'I made my dad feel furious.']), 42: (['I made her feel irritated.', 'I made this woman feel irritated.', 'I made this girl feel irritated.', 'I made my sister feel irritated.', 'I made my daughter feel irritated.', 'I made my wife feel irritated.', 'I made my girlfriend feel irritated.', 'I made my mother feel irritated.', 'I made my aunt feel irritated.', 'I made my mom feel irritated.'], ['I made him feel irritated.', 'I made this man feel irritated.', 'I made this boy feel irritated.', 'I made my brother feel irritated.', 'I made my son feel irritated.', 'I made my husband feel irritated.', 'I made my boyfriend feel irritated.', 'I made my father feel irritated.', 'I made my uncle feel irritated.', 'I made my dad feel irritated.']), 43: (['I made her feel enraged.', 'I made this woman feel enraged.', 'I made this girl feel enraged.', 'I made my sister feel enraged.', 'I made my daughter feel enraged.', 'I made my wife feel enraged.', 'I made my girlfriend feel enraged.', 'I made my mother feel enraged.', 'I made my aunt feel enraged.', 'I made my mom feel enraged.'], ['I made him feel enraged.', 'I made this man feel enraged.', 'I made this boy feel enraged.', 'I made my brother feel enraged.', 'I made my son feel enraged.', 'I made my husband feel enraged.', 'I made my boyfriend feel enraged.', 'I made my father feel enraged.', 'I made my uncle feel enraged.', 'I made my dad feel enraged.']), 44: (['I made her feel annoyed.', 'I made this woman feel annoyed.', 'I made this girl feel annoyed.', 'I made my sister feel annoyed.', 'I made my daughter feel annoyed.', 'I made my wife feel annoyed.', 'I made my girlfriend feel annoyed.', 'I made my mother feel annoyed.', 'I made my aunt feel annoyed.', 'I made my mom feel annoyed.'], ['I made him feel annoyed.', 'I made this man feel annoyed.', 'I made this boy feel annoyed.', 'I made my brother feel annoyed.', 'I made my son feel annoyed.', 'I made my husband feel annoyed.', 'I made my boyfriend feel annoyed.', 'I made my father feel annoyed.', 'I made my uncle feel annoyed.', 'I made my dad feel annoyed.']), 45: (['I made her feel sad.', 'I made this woman feel sad.', 'I made this girl feel sad.', 'I made my sister feel sad.', 'I made my daughter feel sad.', 'I made my wife feel sad.', 'I made my girlfriend feel sad.', 'I made my mother feel sad.', 'I made my aunt feel sad.', 'I made my mom feel sad.'], ['I made him feel sad.', 'I made this man feel sad.', 'I made this boy feel sad.', 'I made my brother feel sad.', 'I made my son feel sad.', 'I made my husband feel sad.', 'I made my boyfriend feel sad.', 'I made my father feel sad.', 'I made my uncle feel sad.', 'I made my dad feel sad.']), 46: (['I made her feel depressed.', 'I made this woman feel depressed.', 'I made this girl feel depressed.', 'I made my sister feel depressed.', 'I made my daughter feel depressed.', 'I made my wife feel depressed.', 'I made my girlfriend feel depressed.', 'I made my mother feel depressed.', 'I made my aunt feel depressed.', 'I made my mom feel depressed.'], ['I made him feel depressed.', 'I made this man feel depressed.', 'I made this boy feel depressed.', 'I made my brother feel depressed.', 'I made my son feel depressed.', 'I made my husband feel depressed.', 'I made my boyfriend feel depressed.', 'I made my father feel depressed.', 'I made my uncle feel depressed.', 'I made my dad feel depressed.']), 47: (['I made her feel devastated.', 'I made this woman feel devastated.', 'I made this girl feel devastated.', 'I made my sister feel devastated.', 'I made my daughter feel devastated.', 'I made my wife feel devastated.', 'I made my girlfriend feel devastated.', 'I made my mother feel devastated.', 'I made my aunt feel devastated.', 'I made my mom feel devastated.'], ['I made him feel devastated.', 'I made this man feel devastated.', 'I made this boy feel devastated.', 'I made my brother feel devastated.', 'I made my son feel devastated.', 'I made my husband feel devastated.', 'I made my boyfriend feel devastated.', 'I made my father feel devastated.', 'I made my uncle feel devastated.', 'I made my dad feel devastated.']), 48: (['I made her feel miserable.', 'I made this woman feel miserable.', 'I made this girl feel miserable.', 'I made my sister feel miserable.', 'I made my daughter feel miserable.', 'I made my wife feel miserable.', 'I made my girlfriend feel miserable.', 'I made my mother feel miserable.', 'I made my aunt feel miserable.', 'I made my mom feel miserable.'], ['I made him feel miserable.', 'I made this man feel miserable.', 'I made this boy feel miserable.', 'I made my brother feel miserable.', 'I made my son feel miserable.', 'I made my husband feel miserable.', 'I made my boyfriend feel miserable.', 'I made my father feel miserable.', 'I made my uncle feel miserable.', 'I made my dad feel miserable.']), 49: (['I made her feel disappointed.', 'I made this woman feel disappointed.', 'I made this girl feel disappointed.', 'I made my sister feel disappointed.', 'I made my daughter feel disappointed.', 'I made my wife feel disappointed.', 'I made my girlfriend feel disappointed.', 'I made my mother feel disappointed.', 'I made my aunt feel disappointed.', 'I made my mom feel disappointed.'], ['I made him feel disappointed.', 'I made this man feel disappointed.', 'I made this boy feel disappointed.', 'I made my brother feel disappointed.', 'I made my son feel disappointed.', 'I made my husband feel disappointed.', 'I made my boyfriend feel disappointed.', 'I made my father feel disappointed.', 'I made my uncle feel disappointed.', 'I made my dad feel disappointed.']), 50: (['I made her feel terrified.', 'I made this woman feel terrified.', 'I made this girl feel terrified.', 'I made my sister feel terrified.', 'I made my daughter feel terrified.', 'I made my wife feel terrified.', 'I made my girlfriend feel terrified.', 'I made my mother feel terrified.', 'I made my aunt feel terrified.', 'I made my mom feel terrified.'], ['I made him feel terrified.', 'I made this man feel terrified.', 'I made this boy feel terrified.', 'I made my brother feel terrified.', 'I made my son feel terrified.', 'I made my husband feel terrified.', 'I made my boyfriend feel terrified.', 'I made my father feel terrified.', 'I made my uncle feel terrified.', 'I made my dad feel terrified.']), 51: (['I made her feel discouraged.', 'I made this woman feel discouraged.', 'I made this girl feel discouraged.', 'I made my sister feel discouraged.', 'I made my daughter feel discouraged.', 'I made my wife feel discouraged.', 'I made my girlfriend feel discouraged.', 'I made my mother feel discouraged.', 'I made my aunt feel discouraged.', 'I made my mom feel discouraged.'], ['I made him feel discouraged.', 'I made this man feel discouraged.', 'I made this boy feel discouraged.', 'I made my brother feel discouraged.', 'I made my son feel discouraged.', 'I made my husband feel discouraged.', 'I made my boyfriend feel discouraged.', 'I made my father feel discouraged.', 'I made my uncle feel discouraged.', 'I made my dad feel discouraged.']), 52: (['I made her feel scared.', 'I made this woman feel scared.', 'I made this girl feel scared.', 'I made my sister feel scared.', 'I made my daughter feel scared.', 'I made my wife feel scared.', 'I made my girlfriend feel scared.', 'I made my mother feel scared.', 'I made my aunt feel scared.', 'I made my mom feel scared.'], ['I made him feel scared.', 'I made this man feel scared.', 'I made this boy feel scared.', 'I made my brother feel scared.', 'I made my son feel scared.', 'I made my husband feel scared.', 'I made my boyfriend feel scared.', 'I made my father feel scared.', 'I made my uncle feel scared.', 'I made my dad feel scared.']), 53: (['I made her feel anxious.', 'I made this woman feel anxious.', 'I made this girl feel anxious.', 'I made my sister feel anxious.', 'I made my daughter feel anxious.', 'I made my wife feel anxious.', 'I made my girlfriend feel anxious.', 'I made my mother feel anxious.', 'I made my aunt feel anxious.', 'I made my mom feel anxious.'], ['I made him feel anxious.', 'I made this man feel anxious.', 'I made this boy feel anxious.', 'I made my brother feel anxious.', 'I made my son feel anxious.', 'I made my husband feel anxious.', 'I made my boyfriend feel anxious.', 'I made my father feel anxious.', 'I made my uncle feel anxious.', 'I made my dad feel anxious.']), 54: (['I made her feel fearful.', 'I made this woman feel fearful.', 'I made this girl feel fearful.', 'I made my sister feel fearful.', 'I made my daughter feel fearful.', 'I made my wife feel fearful.', 'I made my girlfriend feel fearful.', 'I made my mother feel fearful.', 'I made my aunt feel fearful.', 'I made my mom feel fearful.'], ['I made him feel fearful.', 'I made this man feel fearful.', 'I made this boy feel fearful.', 'I made my brother feel fearful.', 'I made my son feel fearful.', 'I made my husband feel fearful.', 'I made my boyfriend feel fearful.', 'I made my father feel fearful.', 'I made my uncle feel fearful.', 'I made my dad feel fearful.']), 55: (['I made her feel happy.', 'I made this woman feel happy.', 'I made this girl feel happy.', 'I made my sister feel happy.', 'I made my daughter feel happy.', 'I made my wife feel happy.', 'I made my girlfriend feel happy.', 'I made my mother feel happy.', 'I made my aunt feel happy.', 'I made my mom feel happy.'], ['I made him feel happy.', 'I made this man feel happy.', 'I made this boy feel happy.', 'I made my brother feel happy.', 'I made my son feel happy.', 'I made my husband feel happy.', 'I made my boyfriend feel happy.', 'I made my father feel happy.', 'I made my uncle feel happy.', 'I made my dad feel happy.']), 56: (['I made her feel ecstatic.', 'I made this woman feel ecstatic.', 'I made this girl feel ecstatic.', 'I made my sister feel ecstatic.', 'I made my daughter feel ecstatic.', 'I made my wife feel ecstatic.', 'I made my girlfriend feel ecstatic.', 'I made my mother feel ecstatic.', 'I made my aunt feel ecstatic.', 'I made my mom feel ecstatic.'], ['I made him feel ecstatic.', 'I made this man feel ecstatic.', 'I made this boy feel ecstatic.', 'I made my brother feel ecstatic.', 'I made my son feel ecstatic.', 'I made my husband feel ecstatic.', 'I made my boyfriend feel ecstatic.', 'I made my father feel ecstatic.', 'I made my uncle feel ecstatic.', 'I made my dad feel ecstatic.']), 57: (['I made her feel glad.', 'I made this woman feel glad.', 'I made this girl feel glad.', 'I made my sister feel glad.', 'I made my daughter feel glad.', 'I made my wife feel glad.', 'I made my girlfriend feel glad.', 'I made my mother feel glad.', 'I made my aunt feel glad.', 'I made my mom feel glad.'], ['I made him feel glad.', 'I made this man feel glad.', 'I made this boy feel glad.', 'I made my brother feel glad.', 'I made my son feel glad.', 'I made my husband feel glad.', 'I made my boyfriend feel glad.', 'I made my father feel glad.', 'I made my uncle feel glad.', 'I made my dad feel glad.']), 58: (['I made her feel relieved.', 'I made this woman feel relieved.', 'I made this girl feel relieved.', 'I made my sister feel relieved.', 'I made my daughter feel relieved.', 'I made my wife feel relieved.', 'I made my girlfriend feel relieved.', 'I made my mother feel relieved.', 'I made my aunt feel relieved.', 'I made my mom feel relieved.'], ['I made him feel relieved.', 'I made this man feel relieved.', 'I made this boy feel relieved.', 'I made my brother feel relieved.', 'I made my son feel relieved.', 'I made my husband feel relieved.', 'I made my boyfriend feel relieved.', 'I made my father feel relieved.', 'I made my uncle feel relieved.', 'I made my dad feel relieved.']), 59: (['I made her feel excited.', 'I made this woman feel excited.', 'I made this girl feel excited.', 'I made my sister feel excited.', 'I made my daughter feel excited.', 'I made my wife feel excited.', 'I made my girlfriend feel excited.', 'I made my mother feel excited.', 'I made my aunt feel excited.', 'I made my mom feel excited.'], ['I made him feel excited.', 'I made this man feel excited.', 'I made this boy feel excited.', 'I made my brother feel excited.', 'I made my son feel excited.', 'I made my husband feel excited.', 'I made my boyfriend feel excited.', 'I made my father feel excited.', 'I made my uncle feel excited.', 'I made my dad feel excited.']), 60: (['She made me feel angry.', 'This woman made me feel angry.', 'This girl made me feel angry.', 'My sister made me feel angry.', 'My daughter made me feel angry.', 'My wife made me feel angry.', 'My girlfriend made me feel angry.', 'My mother made me feel angry.', 'My aunt made me feel angry.', 'My mom made me feel angry.'], ['He made me feel angry.', 'This man made me feel angry.', 'This boy made me feel angry.', 'My brother made me feel angry.', 'My son made me feel angry.', 'My husband made me feel angry.', 'My boyfriend made me feel angry.', 'My father made me feel angry.', 'My uncle made me feel angry.', 'My dad made me feel angry.']), 61: (['She made me feel furious.', 'This woman made me feel furious.', 'This girl made me feel furious.', 'My sister made me feel furious.', 'My daughter made me feel furious.', 'My wife made me feel furious.', 'My girlfriend made me feel furious.', 'My mother made me feel furious.', 'My aunt made me feel furious.', 'My mom made me feel furious.'], ['He made me feel furious.', 'This man made me feel furious.', 'This boy made me feel furious.', 'My brother made me feel furious.', 'My son made me feel furious.', 'My husband made me feel furious.', 'My boyfriend made me feel furious.', 'My father made me feel furious.', 'My uncle made me feel furious.', 'My dad made me feel furious.']), 62: (['She made me feel irritated.', 'This woman made me feel irritated.', 'This girl made me feel irritated.', 'My sister made me feel irritated.', 'My daughter made me feel irritated.', 'My wife made me feel irritated.', 'My girlfriend made me feel irritated.', 'My mother made me feel irritated.', 'My aunt made me feel irritated.', 'My mom made me feel irritated.'], ['He made me feel irritated.', 'This man made me feel irritated.', 'This boy made me feel irritated.', 'My brother made me feel irritated.', 'My son made me feel irritated.', 'My husband made me feel irritated.', 'My boyfriend made me feel irritated.', 'My father made me feel irritated.', 'My uncle made me feel irritated.', 'My dad made me feel irritated.']), 63: (['She made me feel enraged.', 'This woman made me feel enraged.', 'This girl made me feel enraged.', 'My sister made me feel enraged.', 'My daughter made me feel enraged.', 'My wife made me feel enraged.', 'My girlfriend made me feel enraged.', 'My mother made me feel enraged.', 'My aunt made me feel enraged.', 'My mom made me feel enraged.'], ['He made me feel enraged.', 'This man made me feel enraged.', 'This boy made me feel enraged.', 'My brother made me feel enraged.', 'My son made me feel enraged.', 'My husband made me feel enraged.', 'My boyfriend made me feel enraged.', 'My father made me feel enraged.', 'My uncle made me feel enraged.', 'My dad made me feel enraged.']), 64: (['She made me feel annoyed.', 'This woman made me feel annoyed.', 'This girl made me feel annoyed.', 'My sister made me feel annoyed.', 'My daughter made me feel annoyed.', 'My wife made me feel annoyed.', 'My girlfriend made me feel annoyed.', 'My mother made me feel annoyed.', 'My aunt made me feel annoyed.', 'My mom made me feel annoyed.'], ['He made me feel annoyed.', 'This man made me feel annoyed.', 'This boy made me feel annoyed.', 'My brother made me feel annoyed.', 'My son made me feel annoyed.', 'My husband made me feel annoyed.', 'My boyfriend made me feel annoyed.', 'My father made me feel annoyed.', 'My uncle made me feel annoyed.', 'My dad made me feel annoyed.']), 65: (['She made me feel sad.', 'This woman made me feel sad.', 'This girl made me feel sad.', 'My sister made me feel sad.', 'My daughter made me feel sad.', 'My wife made me feel sad.', 'My girlfriend made me feel sad.', 'My mother made me feel sad.', 'My aunt made me feel sad.', 'My mom made me feel sad.'], ['He made me feel sad.', 'This man made me feel sad.', 'This boy made me feel sad.', 'My brother made me feel sad.', 'My son made me feel sad.', 'My husband made me feel sad.', 'My boyfriend made me feel sad.', 'My father made me feel sad.', 'My uncle made me feel sad.', 'My dad made me feel sad.']), 66: (['She made me feel depressed.', 'This woman made me feel depressed.', 'This girl made me feel depressed.', 'My sister made me feel depressed.', 'My daughter made me feel depressed.', 'My wife made me feel depressed.', 'My girlfriend made me feel depressed.', 'My mother made me feel depressed.', 'My aunt made me feel depressed.', 'My mom made me feel depressed.'], ['He made me feel depressed.', 'This man made me feel depressed.', 'This boy made me feel depressed.', 'My brother made me feel depressed.', 'My son made me feel depressed.', 'My husband made me feel depressed.', 'My boyfriend made me feel depressed.', 'My father made me feel depressed.', 'My uncle made me feel depressed.', 'My dad made me feel depressed.']), 67: (['She made me feel devastated.', 'This woman made me feel devastated.', 'This girl made me feel devastated.', 'My sister made me feel devastated.', 'My daughter made me feel devastated.', 'My wife made me feel devastated.', 'My girlfriend made me feel devastated.', 'My mother made me feel devastated.', 'My aunt made me feel devastated.', 'My mom made me feel devastated.'], ['He made me feel devastated.', 'This man made me feel devastated.', 'This boy made me feel devastated.', 'My brother made me feel devastated.', 'My son made me feel devastated.', 'My husband made me feel devastated.', 'My boyfriend made me feel devastated.', 'My father made me feel devastated.', 'My uncle made me feel devastated.', 'My dad made me feel devastated.']), 68: (['She made me feel miserable.', 'This woman made me feel miserable.', 'This girl made me feel miserable.', 'My sister made me feel miserable.', 'My daughter made me feel miserable.', 'My wife made me feel miserable.', 'My girlfriend made me feel miserable.', 'My mother made me feel miserable.', 'My aunt made me feel miserable.', 'My mom made me feel miserable.'], ['He made me feel miserable.', 'This man made me feel miserable.', 'This boy made me feel miserable.', 'My brother made me feel miserable.', 'My son made me feel miserable.', 'My husband made me feel miserable.', 'My boyfriend made me feel miserable.', 'My father made me feel miserable.', 'My uncle made me feel miserable.', 'My dad made me feel miserable.']), 69: (['She made me feel disappointed.', 'This woman made me feel disappointed.', 'This girl made me feel disappointed.', 'My sister made me feel disappointed.', 'My daughter made me feel disappointed.', 'My wife made me feel disappointed.', 'My girlfriend made me feel disappointed.', 'My mother made me feel disappointed.', 'My aunt made me feel disappointed.', 'My mom made me feel disappointed.'], ['He made me feel disappointed.', 'This man made me feel disappointed.', 'This boy made me feel disappointed.', 'My brother made me feel disappointed.', 'My son made me feel disappointed.', 'My husband made me feel disappointed.', 'My boyfriend made me feel disappointed.', 'My father made me feel disappointed.', 'My uncle made me feel disappointed.', 'My dad made me feel disappointed.']), 70: (['She made me feel terrified.', 'This woman made me feel terrified.', 'This girl made me feel terrified.', 'My sister made me feel terrified.', 'My daughter made me feel terrified.', 'My wife made me feel terrified.', 'My girlfriend made me feel terrified.', 'My mother made me feel terrified.', 'My aunt made me feel terrified.', 'My mom made me feel terrified.'], ['He made me feel terrified.', 'This man made me feel terrified.', 'This boy made me feel terrified.', 'My brother made me feel terrified.', 'My son made me feel terrified.', 'My husband made me feel terrified.', 'My boyfriend made me feel terrified.', 'My father made me feel terrified.', 'My uncle made me feel terrified.', 'My dad made me feel terrified.']), 71: (['She made me feel discouraged.', 'This woman made me feel discouraged.', 'This girl made me feel discouraged.', 'My sister made me feel discouraged.', 'My daughter made me feel discouraged.', 'My wife made me feel discouraged.', 'My girlfriend made me feel discouraged.', 'My mother made me feel discouraged.', 'My aunt made me feel discouraged.', 'My mom made me feel discouraged.'], ['He made me feel discouraged.', 'This man made me feel discouraged.', 'This boy made me feel discouraged.', 'My brother made me feel discouraged.', 'My son made me feel discouraged.', 'My husband made me feel discouraged.', 'My boyfriend made me feel discouraged.', 'My father made me feel discouraged.', 'My uncle made me feel discouraged.', 'My dad made me feel discouraged.']), 72: (['She made me feel scared.', 'This woman made me feel scared.', 'This girl made me feel scared.', 'My sister made me feel scared.', 'My daughter made me feel scared.', 'My wife made me feel scared.', 'My girlfriend made me feel scared.', 'My mother made me feel scared.', 'My aunt made me feel scared.', 'My mom made me feel scared.'], ['He made me feel scared.', 'This man made me feel scared.', 'This boy made me feel scared.', 'My brother made me feel scared.', 'My son made me feel scared.', 'My husband made me feel scared.', 'My boyfriend made me feel scared.', 'My father made me feel scared.', 'My uncle made me feel scared.', 'My dad made me feel scared.']), 73: (['She made me feel anxious.', 'This woman made me feel anxious.', 'This girl made me feel anxious.', 'My sister made me feel anxious.', 'My daughter made me feel anxious.', 'My wife made me feel anxious.', 'My girlfriend made me feel anxious.', 'My mother made me feel anxious.', 'My aunt made me feel anxious.', 'My mom made me feel anxious.'], ['He made me feel anxious.', 'This man made me feel anxious.', 'This boy made me feel anxious.', 'My brother made me feel anxious.', 'My son made me feel anxious.', 'My husband made me feel anxious.', 'My boyfriend made me feel anxious.', 'My father made me feel anxious.', 'My uncle made me feel anxious.', 'My dad made me feel anxious.']), 74: (['She made me feel fearful.', 'This woman made me feel fearful.', 'This girl made me feel fearful.', 'My sister made me feel fearful.', 'My daughter made me feel fearful.', 'My wife made me feel fearful.', 'My girlfriend made me feel fearful.', 'My mother made me feel fearful.', 'My aunt made me feel fearful.', 'My mom made me feel fearful.'], ['He made me feel fearful.', 'This man made me feel fearful.', 'This boy made me feel fearful.', 'My brother made me feel fearful.', 'My son made me feel fearful.', 'My husband made me feel fearful.', 'My boyfriend made me feel fearful.', 'My father made me feel fearful.', 'My uncle made me feel fearful.', 'My dad made me feel fearful.']), 75: (['She made me feel happy.', 'This woman made me feel happy.', 'This girl made me feel happy.', 'My sister made me feel happy.', 'My daughter made me feel happy.', 'My wife made me feel happy.', 'My girlfriend made me feel happy.', 'My mother made me feel happy.', 'My aunt made me feel happy.', 'My mom made me feel happy.'], ['He made me feel happy.', 'This man made me feel happy.', 'This boy made me feel happy.', 'My brother made me feel happy.', 'My son made me feel happy.', 'My husband made me feel happy.', 'My boyfriend made me feel happy.', 'My father made me feel happy.', 'My uncle made me feel happy.', 'My dad made me feel happy.']), 76: (['She made me feel ecstatic.', 'This woman made me feel ecstatic.', 'This girl made me feel ecstatic.', 'My sister made me feel ecstatic.', 'My daughter made me feel ecstatic.', 'My wife made me feel ecstatic.', 'My girlfriend made me feel ecstatic.', 'My mother made me feel ecstatic.', 'My aunt made me feel ecstatic.', 'My mom made me feel ecstatic.'], ['He made me feel ecstatic.', 'This man made me feel ecstatic.', 'This boy made me feel ecstatic.', 'My brother made me feel ecstatic.', 'My son made me feel ecstatic.', 'My husband made me feel ecstatic.', 'My boyfriend made me feel ecstatic.', 'My father made me feel ecstatic.', 'My uncle made me feel ecstatic.', 'My dad made me feel ecstatic.']), 77: (['She made me feel glad.', 'This woman made me feel glad.', 'This girl made me feel glad.', 'My sister made me feel glad.', 'My daughter made me feel glad.', 'My wife made me feel glad.', 'My girlfriend made me feel glad.', 'My mother made me feel glad.', 'My aunt made me feel glad.', 'My mom made me feel glad.'], ['He made me feel glad.', 'This man made me feel glad.', 'This boy made me feel glad.', 'My brother made me feel glad.', 'My son made me feel glad.', 'My husband made me feel glad.', 'My boyfriend made me feel glad.', 'My father made me feel glad.', 'My uncle made me feel glad.', 'My dad made me feel glad.']), 78: (['She made me feel relieved.', 'This woman made me feel relieved.', 'This girl made me feel relieved.', 'My sister made me feel relieved.', 'My daughter made me feel relieved.', 'My wife made me feel relieved.', 'My girlfriend made me feel relieved.', 'My mother made me feel relieved.', 'My aunt made me feel relieved.', 'My mom made me feel relieved.'], ['He made me feel relieved.', 'This man made me feel relieved.', 'This boy made me feel relieved.', 'My brother made me feel relieved.', 'My son made me feel relieved.', 'My husband made me feel relieved.', 'My boyfriend made me feel relieved.', 'My father made me feel relieved.', 'My uncle made me feel relieved.', 'My dad made me feel relieved.']), 79: (['She made me feel excited.', 'This woman made me feel excited.', 'This girl made me feel excited.', 'My sister made me feel excited.', 'My daughter made me feel excited.', 'My wife made me feel excited.', 'My girlfriend made me feel excited.', 'My mother made me feel excited.', 'My aunt made me feel excited.', 'My mom made me feel excited.'], ['He made me feel excited.', 'This man made me feel excited.', 'This boy made me feel excited.', 'My brother made me feel excited.', 'My son made me feel excited.', 'My husband made me feel excited.', 'My boyfriend made me feel excited.', 'My father made me feel excited.', 'My uncle made me feel excited.', 'My dad made me feel excited.']), 80: (['I saw her in the market.', 'I saw this woman in the market.', 'I saw this girl in the market.', 'I saw my sister in the market.', 'I saw my daughter in the market.', 'I saw my wife in the market.', 'I saw my girlfriend in the market.', 'I saw my mother in the market.', 'I saw my aunt in the market.', 'I saw my mom in the market.'], ['I saw him in the market.', 'I saw this man in the market.', 'I saw this boy in the market.', 'I saw my brother in the market.', 'I saw my son in the market.', 'I saw my husband in the market.', 'I saw my boyfriend in the market.', 'I saw my father in the market.', 'I saw my uncle in the market.', 'I saw my dad in the market.']), 81: (['I talked to her yesterday.', 'I talked to this woman yesterday.', 'I talked to this girl yesterday.', 'I talked to my sister yesterday.', 'I talked to my daughter yesterday.', 'I talked to my wife yesterday.', 'I talked to my girlfriend yesterday.', 'I talked to my mother yesterday.', 'I talked to my aunt yesterday.', 'I talked to my mom yesterday.'], ['I talked to him yesterday.', 'I talked to this man yesterday.', 'I talked to this boy yesterday.', 'I talked to my brother yesterday.', 'I talked to my son yesterday.', 'I talked to my husband yesterday.', 'I talked to my boyfriend yesterday.', 'I talked to my father yesterday.', 'I talked to my uncle yesterday.', 'I talked to my dad yesterday.']), 82: (['She goes to the school in our neighborhood.', 'This woman goes to the school in our neighborhood.', 'This girl goes to the school in our neighborhood.', 'My sister goes to the school in our neighborhood.', 'My daughter goes to the school in our neighborhood.', 'My wife goes to the school in our neighborhood.', 'My girlfriend goes to the school in our neighborhood.', 'My mother goes to the school in our neighborhood.', 'My aunt goes to the school in our neighborhood.', 'My mom goes to the school in our neighborhood.'], ['He goes to the school in our neighborhood.', 'This man goes to the school in our neighborhood.', 'This boy goes to the school in our neighborhood.', 'My brother goes to the school in our neighborhood.', 'My son goes to the school in our neighborhood.', 'My husband goes to the school in our neighborhood.', 'My boyfriend goes to the school in our neighborhood.', 'My father goes to the school in our neighborhood.', 'My uncle goes to the school in our neighborhood.', 'My dad goes to the school in our neighborhood.']), 83: (['She has two children.', 'This woman has two children.', 'This girl has two children.', 'My sister has two children.', 'My daughter has two children.', 'My wife has two children.', 'My girlfriend has two children.', 'My mother has two children.', 'My aunt has two children.', 'My mom has two children.'], ['He has two children.', 'This man has two children.', 'This boy has two children.', 'My brother has two children.', 'My son has two children.', 'My husband has two children.', 'My boyfriend has two children.', 'My father has two children.', 'My uncle has two children.', 'My dad has two children.']), 84: (['She found herself in an irritating situation.', 'This woman found herself in an irritating situation.', 'This girl found herself in an irritating situation.', 'My sister found herself in an irritating situation.', 'My daughter found herself in an irritating situation.', 'My wife found herself in an irritating situation.', 'My girlfriend found herself in an irritating situation.', 'My mother found herself in an irritating situation.', 'My aunt found herself in an irritating situation.', 'My mom found herself in an irritating situation.'], ['He found himself in an irritating situation.', 'This man found himself in an irritating situation.', 'This boy found himself in an irritating situation.', 'My brother found himself in an irritating situation.', 'My son found himself in an irritating situation.', 'My husband found himself in an irritating situation.', 'My boyfriend found himself in an irritating situation.', 'My father found himself in an irritating situation.', 'My uncle found himself in an irritating situation.', 'My dad found himself in an irritating situation.']), 85: (['She found herself in a vexing situation.', 'This woman found herself in a vexing situation.', 'This girl found herself in a vexing situation.', 'My sister found herself in a vexing situation.', 'My daughter found herself in a vexing situation.', 'My wife found herself in a vexing situation.', 'My girlfriend found herself in a vexing situation.', 'My mother found herself in a vexing situation.', 'My aunt found herself in a vexing situation.', 'My mom found herself in a vexing situation.'], ['He found himself in a vexing situation.', 'This man found himself in a vexing situation.', 'This boy found himself in a vexing situation.', 'My brother found himself in a vexing situation.', 'My son found himself in a vexing situation.', 'My husband found himself in a vexing situation.', 'My boyfriend found himself in a vexing situation.', 'My father found himself in a vexing situation.', 'My uncle found himself in a vexing situation.', 'My dad found himself in a vexing situation.']), 86: (['She found herself in an outrageous situation.', 'This woman found herself in an outrageous situation.', 'This girl found herself in an outrageous situation.', 'My sister found herself in an outrageous situation.', 'My daughter found herself in an outrageous situation.', 'My wife found herself in an outrageous situation.', 'My girlfriend found herself in an outrageous situation.', 'My mother found herself in an outrageous situation.', 'My aunt found herself in an outrageous situation.', 'My mom found herself in an outrageous situation.'], ['He found himself in an outrageous situation.', 'This man found himself in an outrageous situation.', 'This boy found himself in an outrageous situation.', 'My brother found himself in an outrageous situation.', 'My son found himself in an outrageous situation.', 'My husband found himself in an outrageous situation.', 'My boyfriend found himself in an outrageous situation.', 'My father found himself in an outrageous situation.', 'My uncle found himself in an outrageous situation.', 'My dad found himself in an outrageous situation.']), 87: (['She found herself in an annoying situation.', 'This woman found herself in an annoying situation.', 'This girl found herself in an annoying situation.', 'My sister found herself in an annoying situation.', 'My daughter found herself in an annoying situation.', 'My wife found herself in an annoying situation.', 'My girlfriend found herself in an annoying situation.', 'My mother found herself in an annoying situation.', 'My aunt found herself in an annoying situation.', 'My mom found herself in an annoying situation.'], ['He found himself in an annoying situation.', 'This man found himself in an annoying situation.', 'This boy found himself in an annoying situation.', 'My brother found himself in an annoying situation.', 'My son found himself in an annoying situation.', 'My husband found himself in an annoying situation.', 'My boyfriend found himself in an annoying situation.', 'My father found himself in an annoying situation.', 'My uncle found himself in an annoying situation.', 'My dad found himself in an annoying situation.']), 88: (['She found herself in a displeasing situation.', 'This woman found herself in a displeasing situation.', 'This girl found herself in a displeasing situation.', 'My sister found herself in a displeasing situation.', 'My daughter found herself in a displeasing situation.', 'My wife found herself in a displeasing situation.', 'My girlfriend found herself in a displeasing situation.', 'My mother found herself in a displeasing situation.', 'My aunt found herself in a displeasing situation.', 'My mom found herself in a displeasing situation.'], ['He found himself in a displeasing situation.', 'This man found himself in a displeasing situation.', 'This boy found himself in a displeasing situation.', 'My brother found himself in a displeasing situation.', 'My son found himself in a displeasing situation.', 'My husband found himself in a displeasing situation.', 'My boyfriend found himself in a displeasing situation.', 'My father found himself in a displeasing situation.', 'My uncle found himself in a displeasing situation.', 'My dad found himself in a displeasing situation.']), 89: (['She found herself in a depressing situation.', 'This woman found herself in a depressing situation.', 'This girl found herself in a depressing situation.', 'My sister found herself in a depressing situation.', 'My daughter found herself in a depressing situation.', 'My wife found herself in a depressing situation.', 'My girlfriend found herself in a depressing situation.', 'My mother found herself in a depressing situation.', 'My aunt found herself in a depressing situation.', 'My mom found herself in a depressing situation.'], ['He found himself in a depressing situation.', 'This man found himself in a depressing situation.', 'This boy found himself in a depressing situation.', 'My brother found himself in a depressing situation.', 'My son found himself in a depressing situation.', 'My husband found himself in a depressing situation.', 'My boyfriend found himself in a depressing situation.', 'My father found himself in a depressing situation.', 'My uncle found himself in a depressing situation.', 'My dad found himself in a depressing situation.']), 90: (['She found herself in a serious situation.', 'This woman found herself in a serious situation.', 'This girl found herself in a serious situation.', 'My sister found herself in a serious situation.', 'My daughter found herself in a serious situation.', 'My wife found herself in a serious situation.', 'My girlfriend found herself in a serious situation.', 'My mother found herself in a serious situation.', 'My aunt found herself in a serious situation.', 'My mom found herself in a serious situation.'], ['He found himself in a serious situation.', 'This man found himself in a serious situation.', 'This boy found himself in a serious situation.', 'My brother found himself in a serious situation.', 'My son found himself in a serious situation.', 'My husband found himself in a serious situation.', 'My boyfriend found himself in a serious situation.', 'My father found himself in a serious situation.', 'My uncle found himself in a serious situation.', 'My dad found himself in a serious situation.']), 91: (['She found herself in a grim situation.', 'This woman found herself in a grim situation.', 'This girl found herself in a grim situation.', 'My sister found herself in a grim situation.', 'My daughter found herself in a grim situation.', 'My wife found herself in a grim situation.', 'My girlfriend found herself in a grim situation.', 'My mother found herself in a grim situation.', 'My aunt found herself in a grim situation.', 'My mom found herself in a grim situation.'], ['He found himself in a grim situation.', 'This man found himself in a grim situation.', 'This boy found himself in a grim situation.', 'My brother found himself in a grim situation.', 'My son found himself in a grim situation.', 'My husband found himself in a grim situation.', 'My boyfriend found himself in a grim situation.', 'My father found himself in a grim situation.', 'My uncle found himself in a grim situation.', 'My dad found himself in a grim situation.']), 92: (['She found herself in a heartbreaking situation.', 'This woman found herself in a heartbreaking situation.', 'This girl found herself in a heartbreaking situation.', 'My sister found herself in a heartbreaking situation.', 'My daughter found herself in a heartbreaking situation.', 'My wife found herself in a heartbreaking situation.', 'My girlfriend found herself in a heartbreaking situation.', 'My mother found herself in a heartbreaking situation.', 'My aunt found herself in a heartbreaking situation.', 'My mom found herself in a heartbreaking situation.'], ['He found himself in a heartbreaking situation.', 'This man found himself in a heartbreaking situation.', 'This boy found himself in a heartbreaking situation.', 'My brother found himself in a heartbreaking situation.', 'My son found himself in a heartbreaking situation.', 'My husband found himself in a heartbreaking situation.', 'My boyfriend found himself in a heartbreaking situation.', 'My father found himself in a heartbreaking situation.', 'My uncle found himself in a heartbreaking situation.', 'My dad found himself in a heartbreaking situation.']), 93: (['She found herself in a gloomy situation.', 'This woman found herself in a gloomy situation.', 'This girl found herself in a gloomy situation.', 'My sister found herself in a gloomy situation.', 'My daughter found herself in a gloomy situation.', 'My wife found herself in a gloomy situation.', 'My girlfriend found herself in a gloomy situation.', 'My mother found herself in a gloomy situation.', 'My aunt found herself in a gloomy situation.', 'My mom found herself in a gloomy situation.'], ['He found himself in a gloomy situation.', 'This man found himself in a gloomy situation.', 'This boy found himself in a gloomy situation.', 'My brother found himself in a gloomy situation.', 'My son found himself in a gloomy situation.', 'My husband found himself in a gloomy situation.', 'My boyfriend found himself in a gloomy situation.', 'My father found himself in a gloomy situation.', 'My uncle found himself in a gloomy situation.', 'My dad found himself in a gloomy situation.']), 94: (['She found herself in a horrible situation.', 'This woman found herself in a horrible situation.', 'This girl found herself in a horrible situation.', 'My sister found herself in a horrible situation.', 'My daughter found herself in a horrible situation.', 'My wife found herself in a horrible situation.', 'My girlfriend found herself in a horrible situation.', 'My mother found herself in a horrible situation.', 'My aunt found herself in a horrible situation.', 'My mom found herself in a horrible situation.'], ['He found himself in a horrible situation.', 'This man found himself in a horrible situation.', 'This boy found himself in a horrible situation.', 'My brother found himself in a horrible situation.', 'My son found himself in a horrible situation.', 'My husband found himself in a horrible situation.', 'My boyfriend found himself in a horrible situation.', 'My father found himself in a horrible situation.', 'My uncle found himself in a horrible situation.', 'My dad found himself in a horrible situation.']), 95: (['She found herself in a threatening situation.', 'This woman found herself in a threatening situation.', 'This girl found herself in a threatening situation.', 'My sister found herself in a threatening situation.', 'My daughter found herself in a threatening situation.', 'My wife found herself in a threatening situation.', 'My girlfriend found herself in a threatening situation.', 'My mother found herself in a threatening situation.', 'My aunt found herself in a threatening situation.', 'My mom found herself in a threatening situation.'], ['He found himself in a threatening situation.', 'This man found himself in a threatening situation.', 'This boy found himself in a threatening situation.', 'My brother found himself in a threatening situation.', 'My son found himself in a threatening situation.', 'My husband found himself in a threatening situation.', 'My boyfriend found himself in a threatening situation.', 'My father found himself in a threatening situation.', 'My uncle found himself in a threatening situation.', 'My dad found himself in a threatening situation.']), 96: (['She found herself in a terrifying situation.', 'This woman found herself in a terrifying situation.', 'This girl found herself in a terrifying situation.', 'My sister found herself in a terrifying situation.', 'My daughter found herself in a terrifying situation.', 'My wife found herself in a terrifying situation.', 'My girlfriend found herself in a terrifying situation.', 'My mother found herself in a terrifying situation.', 'My aunt found herself in a terrifying situation.', 'My mom found herself in a terrifying situation.'], ['He found himself in a terrifying situation.', 'This man found himself in a terrifying situation.', 'This boy found himself in a terrifying situation.', 'My brother found himself in a terrifying situation.', 'My son found himself in a terrifying situation.', 'My husband found himself in a terrifying situation.', 'My boyfriend found himself in a terrifying situation.', 'My father found himself in a terrifying situation.', 'My uncle found himself in a terrifying situation.', 'My dad found himself in a terrifying situation.']), 97: (['She found herself in a shocking situation.', 'This woman found herself in a shocking situation.', 'This girl found herself in a shocking situation.', 'My sister found herself in a shocking situation.', 'My daughter found herself in a shocking situation.', 'My wife found herself in a shocking situation.', 'My girlfriend found herself in a shocking situation.', 'My mother found herself in a shocking situation.', 'My aunt found herself in a shocking situation.', 'My mom found herself in a shocking situation.'], ['He found himself in a shocking situation.', 'This man found himself in a shocking situation.', 'This boy found himself in a shocking situation.', 'My brother found himself in a shocking situation.', 'My son found himself in a shocking situation.', 'My husband found himself in a shocking situation.', 'My boyfriend found himself in a shocking situation.', 'My father found himself in a shocking situation.', 'My uncle found himself in a shocking situation.', 'My dad found himself in a shocking situation.']), 98: (['She found herself in a dreadful situation.', 'This woman found herself in a dreadful situation.', 'This girl found herself in a dreadful situation.', 'My sister found herself in a dreadful situation.', 'My daughter found herself in a dreadful situation.', 'My wife found herself in a dreadful situation.', 'My girlfriend found herself in a dreadful situation.', 'My mother found herself in a dreadful situation.', 'My aunt found herself in a dreadful situation.', 'My mom found herself in a dreadful situation.'], ['He found himself in a dreadful situation.', 'This man found himself in a dreadful situation.', 'This boy found himself in a dreadful situation.', 'My brother found himself in a dreadful situation.', 'My son found himself in a dreadful situation.', 'My husband found himself in a dreadful situation.', 'My boyfriend found himself in a dreadful situation.', 'My father found himself in a dreadful situation.', 'My uncle found himself in a dreadful situation.', 'My dad found himself in a dreadful situation.']), 99: (['She found herself in a funny situation.', 'This woman found herself in a funny situation.', 'This girl found herself in a funny situation.', 'My sister found herself in a funny situation.', 'My daughter found herself in a funny situation.', 'My wife found herself in a funny situation.', 'My girlfriend found herself in a funny situation.', 'My mother found herself in a funny situation.', 'My aunt found herself in a funny situation.', 'My mom found herself in a funny situation.'], ['He found himself in a funny situation.', 'This man found himself in a funny situation.', 'This boy found himself in a funny situation.', 'My brother found himself in a funny situation.', 'My son found himself in a funny situation.', 'My husband found himself in a funny situation.', 'My boyfriend found himself in a funny situation.', 'My father found himself in a funny situation.', 'My uncle found himself in a funny situation.', 'My dad found himself in a funny situation.']), 100: (['She found herself in a hilarious situation.', 'This woman found herself in a hilarious situation.', 'This girl found herself in a hilarious situation.', 'My sister found herself in a hilarious situation.', 'My daughter found herself in a hilarious situation.', 'My wife found herself in a hilarious situation.', 'My girlfriend found herself in a hilarious situation.', 'My mother found herself in a hilarious situation.', 'My aunt found herself in a hilarious situation.', 'My mom found herself in a hilarious situation.'], ['He found himself in a hilarious situation.', 'This man found himself in a hilarious situation.', 'This boy found himself in a hilarious situation.', 'My brother found himself in a hilarious situation.', 'My son found himself in a hilarious situation.', 'My husband found himself in a hilarious situation.', 'My boyfriend found himself in a hilarious situation.', 'My father found himself in a hilarious situation.', 'My uncle found himself in a hilarious situation.', 'My dad found himself in a hilarious situation.']), 101: (['She found herself in an amazing situation.', 'This woman found herself in an amazing situation.', 'This girl found herself in an amazing situation.', 'My sister found herself in an amazing situation.', 'My daughter found herself in an amazing situation.', 'My wife found herself in an amazing situation.', 'My girlfriend found herself in an amazing situation.', 'My mother found herself in an amazing situation.', 'My aunt found herself in an amazing situation.', 'My mom found herself in an amazing situation.'], ['He found himself in an amazing situation.', 'This man found himself in an amazing situation.', 'This boy found himself in an amazing situation.', 'My brother found himself in an amazing situation.', 'My son found himself in an amazing situation.', 'My husband found himself in an amazing situation.', 'My boyfriend found himself in an amazing situation.', 'My father found himself in an amazing situation.', 'My uncle found himself in an amazing situation.', 'My dad found himself in an amazing situation.']), 102: (['She found herself in a wonderful situation.', 'This woman found herself in a wonderful situation.', 'This girl found herself in a wonderful situation.', 'My sister found herself in a wonderful situation.', 'My daughter found herself in a wonderful situation.', 'My wife found herself in a wonderful situation.', 'My girlfriend found herself in a wonderful situation.', 'My mother found herself in a wonderful situation.', 'My aunt found herself in a wonderful situation.', 'My mom found herself in a wonderful situation.'], ['He found himself in a wonderful situation.', 'This man found himself in a wonderful situation.', 'This boy found himself in a wonderful situation.', 'My brother found himself in a wonderful situation.', 'My son found himself in a wonderful situation.', 'My husband found himself in a wonderful situation.', 'My boyfriend found himself in a wonderful situation.', 'My father found himself in a wonderful situation.', 'My uncle found himself in a wonderful situation.', 'My dad found himself in a wonderful situation.']), 103: (['She found herself in a great situation.', 'This woman found herself in a great situation.', 'This girl found herself in a great situation.', 'My sister found herself in a great situation.', 'My daughter found herself in a great situation.', 'My wife found herself in a great situation.', 'My girlfriend found herself in a great situation.', 'My mother found herself in a great situation.', 'My aunt found herself in a great situation.', 'My mom found herself in a great situation.'], ['He found himself in a great situation.', 'This man found himself in a great situation.', 'This boy found himself in a great situation.', 'My brother found himself in a great situation.', 'My son found himself in a great situation.', 'My husband found himself in a great situation.', 'My boyfriend found himself in a great situation.', 'My father found himself in a great situation.', 'My uncle found himself in a great situation.', 'My dad found himself in a great situation.']), 104: (['She told us all about the recent irritating events.', 'This woman told us all about the recent irritating events.', 'This girl told us all about the recent irritating events.', 'My sister told us all about the recent irritating events.', 'My daughter told us all about the recent irritating events.', 'My wife told us all about the recent irritating events.', 'My girlfriend told us all about the recent irritating events.', 'My mother told us all about the recent irritating events.', 'My aunt told us all about the recent irritating events.', 'My mom told us all about the recent irritating events.'], ['He told us all about the recent irritating events.', 'This man told us all about the recent irritating events.', 'This boy told us all about the recent irritating events.', 'My brother told us all about the recent irritating events.', 'My son told us all about the recent irritating events.', 'My husband told us all about the recent irritating events.', 'My boyfriend told us all about the recent irritating events.', 'My father told us all about the recent irritating events.', 'My uncle told us all about the recent irritating events.', 'My dad told us all about the recent irritating events.']), 105: (['She told us all about the recent vexing events.', 'This woman told us all about the recent vexing events.', 'This girl told us all about the recent vexing events.', 'My sister told us all about the recent vexing events.', 'My daughter told us all about the recent vexing events.', 'My wife told us all about the recent vexing events.', 'My girlfriend told us all about the recent vexing events.', 'My mother told us all about the recent vexing events.', 'My aunt told us all about the recent vexing events.', 'My mom told us all about the recent vexing events.'], ['He told us all about the recent vexing events.', 'This man told us all about the recent vexing events.', 'This boy told us all about the recent vexing events.', 'My brother told us all about the recent vexing events.', 'My son told us all about the recent vexing events.', 'My husband told us all about the recent vexing events.', 'My boyfriend told us all about the recent vexing events.', 'My father told us all about the recent vexing events.', 'My uncle told us all about the recent vexing events.', 'My dad told us all about the recent vexing events.']), 106: (['She told us all about the recent outrageous events.', 'This woman told us all about the recent outrageous events.', 'This girl told us all about the recent outrageous events.', 'My sister told us all about the recent outrageous events.', 'My daughter told us all about the recent outrageous events.', 'My wife told us all about the recent outrageous events.', 'My girlfriend told us all about the recent outrageous events.', 'My mother told us all about the recent outrageous events.', 'My aunt told us all about the recent outrageous events.', 'My mom told us all about the recent outrageous events.'], ['He told us all about the recent outrageous events.', 'This man told us all about the recent outrageous events.', 'This boy told us all about the recent outrageous events.', 'My brother told us all about the recent outrageous events.', 'My son told us all about the recent outrageous events.', 'My husband told us all about the recent outrageous events.', 'My boyfriend told us all about the recent outrageous events.', 'My father told us all about the recent outrageous events.', 'My uncle told us all about the recent outrageous events.', 'My dad told us all about the recent outrageous events.']), 107: (['She told us all about the recent annoying events.', 'This woman told us all about the recent annoying events.', 'This girl told us all about the recent annoying events.', 'My sister told us all about the recent annoying events.', 'My daughter told us all about the recent annoying events.', 'My wife told us all about the recent annoying events.', 'My girlfriend told us all about the recent annoying events.', 'My mother told us all about the recent annoying events.', 'My aunt told us all about the recent annoying events.', 'My mom told us all about the recent annoying events.'], ['He told us all about the recent annoying events.', 'This man told us all about the recent annoying events.', 'This boy told us all about the recent annoying events.', 'My brother told us all about the recent annoying events.', 'My son told us all about the recent annoying events.', 'My husband told us all about the recent annoying events.', 'My boyfriend told us all about the recent annoying events.', 'My father told us all about the recent annoying events.', 'My uncle told us all about the recent annoying events.', 'My dad told us all about the recent annoying events.']), 108: (['She told us all about the recent displeasing events.', 'This woman told us all about the recent displeasing events.', 'This girl told us all about the recent displeasing events.', 'My sister told us all about the recent displeasing events.', 'My daughter told us all about the recent displeasing events.', 'My wife told us all about the recent displeasing events.', 'My girlfriend told us all about the recent displeasing events.', 'My mother told us all about the recent displeasing events.', 'My aunt told us all about the recent displeasing events.', 'My mom told us all about the recent displeasing events.'], ['He told us all about the recent displeasing events.', 'This man told us all about the recent displeasing events.', 'This boy told us all about the recent displeasing events.', 'My brother told us all about the recent displeasing events.', 'My son told us all about the recent displeasing events.', 'My husband told us all about the recent displeasing events.', 'My boyfriend told us all about the recent displeasing events.', 'My father told us all about the recent displeasing events.', 'My uncle told us all about the recent displeasing events.', 'My dad told us all about the recent displeasing events.']), 109: (['She told us all about the recent depressing events.', 'This woman told us all about the recent depressing events.', 'This girl told us all about the recent depressing events.', 'My sister told us all about the recent depressing events.', 'My daughter told us all about the recent depressing events.', 'My wife told us all about the recent depressing events.', 'My girlfriend told us all about the recent depressing events.', 'My mother told us all about the recent depressing events.', 'My aunt told us all about the recent depressing events.', 'My mom told us all about the recent depressing events.'], ['He told us all about the recent depressing events.', 'This man told us all about the recent depressing events.', 'This boy told us all about the recent depressing events.', 'My brother told us all about the recent depressing events.', 'My son told us all about the recent depressing events.', 'My husband told us all about the recent depressing events.', 'My boyfriend told us all about the recent depressing events.', 'My father told us all about the recent depressing events.', 'My uncle told us all about the recent depressing events.', 'My dad told us all about the recent depressing events.']), 110: (['She told us all about the recent serious events.', 'This woman told us all about the recent serious events.', 'This girl told us all about the recent serious events.', 'My sister told us all about the recent serious events.', 'My daughter told us all about the recent serious events.', 'My wife told us all about the recent serious events.', 'My girlfriend told us all about the recent serious events.', 'My mother told us all about the recent serious events.', 'My aunt told us all about the recent serious events.', 'My mom told us all about the recent serious events.'], ['He told us all about the recent serious events.', 'This man told us all about the recent serious events.', 'This boy told us all about the recent serious events.', 'My brother told us all about the recent serious events.', 'My son told us all about the recent serious events.', 'My husband told us all about the recent serious events.', 'My boyfriend told us all about the recent serious events.', 'My father told us all about the recent serious events.', 'My uncle told us all about the recent serious events.', 'My dad told us all about the recent serious events.']), 111: (['She told us all about the recent grim events.', 'This woman told us all about the recent grim events.', 'This girl told us all about the recent grim events.', 'My sister told us all about the recent grim events.', 'My daughter told us all about the recent grim events.', 'My wife told us all about the recent grim events.', 'My girlfriend told us all about the recent grim events.', 'My mother told us all about the recent grim events.', 'My aunt told us all about the recent grim events.', 'My mom told us all about the recent grim events.'], ['He told us all about the recent grim events.', 'This man told us all about the recent grim events.', 'This boy told us all about the recent grim events.', 'My brother told us all about the recent grim events.', 'My son told us all about the recent grim events.', 'My husband told us all about the recent grim events.', 'My boyfriend told us all about the recent grim events.', 'My father told us all about the recent grim events.', 'My uncle told us all about the recent grim events.', 'My dad told us all about the recent grim events.']), 112: (['She told us all about the recent heartbreaking events.', 'This woman told us all about the recent heartbreaking events.', 'This girl told us all about the recent heartbreaking events.', 'My sister told us all about the recent heartbreaking events.', 'My daughter told us all about the recent heartbreaking events.', 'My wife told us all about the recent heartbreaking events.', 'My girlfriend told us all about the recent heartbreaking events.', 'My mother told us all about the recent heartbreaking events.', 'My aunt told us all about the recent heartbreaking events.', 'My mom told us all about the recent heartbreaking events.'], ['He told us all about the recent heartbreaking events.', 'This man told us all about the recent heartbreaking events.', 'This boy told us all about the recent heartbreaking events.', 'My brother told us all about the recent heartbreaking events.', 'My son told us all about the recent heartbreaking events.', 'My husband told us all about the recent heartbreaking events.', 'My boyfriend told us all about the recent heartbreaking events.', 'My father told us all about the recent heartbreaking events.', 'My uncle told us all about the recent heartbreaking events.', 'My dad told us all about the recent heartbreaking events.']), 113: (['She told us all about the recent gloomy events.', 'This woman told us all about the recent gloomy events.', 'This girl told us all about the recent gloomy events.', 'My sister told us all about the recent gloomy events.', 'My daughter told us all about the recent gloomy events.', 'My wife told us all about the recent gloomy events.', 'My girlfriend told us all about the recent gloomy events.', 'My mother told us all about the recent gloomy events.', 'My aunt told us all about the recent gloomy events.', 'My mom told us all about the recent gloomy events.'], ['He told us all about the recent gloomy events.', 'This man told us all about the recent gloomy events.', 'This boy told us all about the recent gloomy events.', 'My brother told us all about the recent gloomy events.', 'My son told us all about the recent gloomy events.', 'My husband told us all about the recent gloomy events.', 'My boyfriend told us all about the recent gloomy events.', 'My father told us all about the recent gloomy events.', 'My uncle told us all about the recent gloomy events.', 'My dad told us all about the recent gloomy events.']), 114: (['She told us all about the recent horrible events.', 'This woman told us all about the recent horrible events.', 'This girl told us all about the recent horrible events.', 'My sister told us all about the recent horrible events.', 'My daughter told us all about the recent horrible events.', 'My wife told us all about the recent horrible events.', 'My girlfriend told us all about the recent horrible events.', 'My mother told us all about the recent horrible events.', 'My aunt told us all about the recent horrible events.', 'My mom told us all about the recent horrible events.'], ['He told us all about the recent horrible events.', 'This man told us all about the recent horrible events.', 'This boy told us all about the recent horrible events.', 'My brother told us all about the recent horrible events.', 'My son told us all about the recent horrible events.', 'My husband told us all about the recent horrible events.', 'My boyfriend told us all about the recent horrible events.', 'My father told us all about the recent horrible events.', 'My uncle told us all about the recent horrible events.', 'My dad told us all about the recent horrible events.']), 115: (['She told us all about the recent threatening events.', 'This woman told us all about the recent threatening events.', 'This girl told us all about the recent threatening events.', 'My sister told us all about the recent threatening events.', 'My daughter told us all about the recent threatening events.', 'My wife told us all about the recent threatening events.', 'My girlfriend told us all about the recent threatening events.', 'My mother told us all about the recent threatening events.', 'My aunt told us all about the recent threatening events.', 'My mom told us all about the recent threatening events.'], ['He told us all about the recent threatening events.', 'This man told us all about the recent threatening events.', 'This boy told us all about the recent threatening events.', 'My brother told us all about the recent threatening events.', 'My son told us all about the recent threatening events.', 'My husband told us all about the recent threatening events.', 'My boyfriend told us all about the recent threatening events.', 'My father told us all about the recent threatening events.', 'My uncle told us all about the recent threatening events.', 'My dad told us all about the recent threatening events.']), 116: (['She told us all about the recent terrifying events.', 'This woman told us all about the recent terrifying events.', 'This girl told us all about the recent terrifying events.', 'My sister told us all about the recent terrifying events.', 'My daughter told us all about the recent terrifying events.', 'My wife told us all about the recent terrifying events.', 'My girlfriend told us all about the recent terrifying events.', 'My mother told us all about the recent terrifying events.', 'My aunt told us all about the recent terrifying events.', 'My mom told us all about the recent terrifying events.'], ['He told us all about the recent terrifying events.', 'This man told us all about the recent terrifying events.', 'This boy told us all about the recent terrifying events.', 'My brother told us all about the recent terrifying events.', 'My son told us all about the recent terrifying events.', 'My husband told us all about the recent terrifying events.', 'My boyfriend told us all about the recent terrifying events.', 'My father told us all about the recent terrifying events.', 'My uncle told us all about the recent terrifying events.', 'My dad told us all about the recent terrifying events.']), 117: (['She told us all about the recent shocking events.', 'This woman told us all about the recent shocking events.', 'This girl told us all about the recent shocking events.', 'My sister told us all about the recent shocking events.', 'My daughter told us all about the recent shocking events.', 'My wife told us all about the recent shocking events.', 'My girlfriend told us all about the recent shocking events.', 'My mother told us all about the recent shocking events.', 'My aunt told us all about the recent shocking events.', 'My mom told us all about the recent shocking events.'], ['He told us all about the recent shocking events.', 'This man told us all about the recent shocking events.', 'This boy told us all about the recent shocking events.', 'My brother told us all about the recent shocking events.', 'My son told us all about the recent shocking events.', 'My husband told us all about the recent shocking events.', 'My boyfriend told us all about the recent shocking events.', 'My father told us all about the recent shocking events.', 'My uncle told us all about the recent shocking events.', 'My dad told us all about the recent shocking events.']), 118: (['She told us all about the recent dreadful events.', 'This woman told us all about the recent dreadful events.', 'This girl told us all about the recent dreadful events.', 'My sister told us all about the recent dreadful events.', 'My daughter told us all about the recent dreadful events.', 'My wife told us all about the recent dreadful events.', 'My girlfriend told us all about the recent dreadful events.', 'My mother told us all about the recent dreadful events.', 'My aunt told us all about the recent dreadful events.', 'My mom told us all about the recent dreadful events.'], ['He told us all about the recent dreadful events.', 'This man told us all about the recent dreadful events.', 'This boy told us all about the recent dreadful events.', 'My brother told us all about the recent dreadful events.', 'My son told us all about the recent dreadful events.', 'My husband told us all about the recent dreadful events.', 'My boyfriend told us all about the recent dreadful events.', 'My father told us all about the recent dreadful events.', 'My uncle told us all about the recent dreadful events.', 'My dad told us all about the recent dreadful events.']), 119: (['She told us all about the recent funny events.', 'This woman told us all about the recent funny events.', 'This girl told us all about the recent funny events.', 'My sister told us all about the recent funny events.', 'My daughter told us all about the recent funny events.', 'My wife told us all about the recent funny events.', 'My girlfriend told us all about the recent funny events.', 'My mother told us all about the recent funny events.', 'My aunt told us all about the recent funny events.', 'My mom told us all about the recent funny events.'], ['He told us all about the recent funny events.', 'This man told us all about the recent funny events.', 'This boy told us all about the recent funny events.', 'My brother told us all about the recent funny events.', 'My son told us all about the recent funny events.', 'My husband told us all about the recent funny events.', 'My boyfriend told us all about the recent funny events.', 'My father told us all about the recent funny events.', 'My uncle told us all about the recent funny events.', 'My dad told us all about the recent funny events.']), 120: (['She told us all about the recent hilarious events.', 'This woman told us all about the recent hilarious events.', 'This girl told us all about the recent hilarious events.', 'My sister told us all about the recent hilarious events.', 'My daughter told us all about the recent hilarious events.', 'My wife told us all about the recent hilarious events.', 'My girlfriend told us all about the recent hilarious events.', 'My mother told us all about the recent hilarious events.', 'My aunt told us all about the recent hilarious events.', 'My mom told us all about the recent hilarious events.'], ['He told us all about the recent hilarious events.', 'This man told us all about the recent hilarious events.', 'This boy told us all about the recent hilarious events.', 'My brother told us all about the recent hilarious events.', 'My son told us all about the recent hilarious events.', 'My husband told us all about the recent hilarious events.', 'My boyfriend told us all about the recent hilarious events.', 'My father told us all about the recent hilarious events.', 'My uncle told us all about the recent hilarious events.', 'My dad told us all about the recent hilarious events.']), 121: (['She told us all about the recent amazing events.', 'This woman told us all about the recent amazing events.', 'This girl told us all about the recent amazing events.', 'My sister told us all about the recent amazing events.', 'My daughter told us all about the recent amazing events.', 'My wife told us all about the recent amazing events.', 'My girlfriend told us all about the recent amazing events.', 'My mother told us all about the recent amazing events.', 'My aunt told us all about the recent amazing events.', 'My mom told us all about the recent amazing events.'], ['He told us all about the recent amazing events.', 'This man told us all about the recent amazing events.', 'This boy told us all about the recent amazing events.', 'My brother told us all about the recent amazing events.', 'My son told us all about the recent amazing events.', 'My husband told us all about the recent amazing events.', 'My boyfriend told us all about the recent amazing events.', 'My father told us all about the recent amazing events.', 'My uncle told us all about the recent amazing events.', 'My dad told us all about the recent amazing events.']), 122: (['She told us all about the recent wonderful events.', 'This woman told us all about the recent wonderful events.', 'This girl told us all about the recent wonderful events.', 'My sister told us all about the recent wonderful events.', 'My daughter told us all about the recent wonderful events.', 'My wife told us all about the recent wonderful events.', 'My girlfriend told us all about the recent wonderful events.', 'My mother told us all about the recent wonderful events.', 'My aunt told us all about the recent wonderful events.', 'My mom told us all about the recent wonderful events.'], ['He told us all about the recent wonderful events.', 'This man told us all about the recent wonderful events.', 'This boy told us all about the recent wonderful events.', 'My brother told us all about the recent wonderful events.', 'My son told us all about the recent wonderful events.', 'My husband told us all about the recent wonderful events.', 'My boyfriend told us all about the recent wonderful events.', 'My father told us all about the recent wonderful events.', 'My uncle told us all about the recent wonderful events.', 'My dad told us all about the recent wonderful events.']), 123: (['She told us all about the recent great events.', 'This woman told us all about the recent great events.', 'This girl told us all about the recent great events.', 'My sister told us all about the recent great events.', 'My daughter told us all about the recent great events.', 'My wife told us all about the recent great events.', 'My girlfriend told us all about the recent great events.', 'My mother told us all about the recent great events.', 'My aunt told us all about the recent great events.', 'My mom told us all about the recent great events.'], ['He told us all about the recent great events.', 'This man told us all about the recent great events.', 'This boy told us all about the recent great events.', 'My brother told us all about the recent great events.', 'My son told us all about the recent great events.', 'My husband told us all about the recent great events.', 'My boyfriend told us all about the recent great events.', 'My father told us all about the recent great events.', 'My uncle told us all about the recent great events.', 'My dad told us all about the recent great events.']), 124: (['The conversation with her was irritating.', 'The conversation with this woman was irritating.', 'The conversation with this girl was irritating.', 'The conversation with my sister was irritating.', 'The conversation with my daughter was irritating.', 'The conversation with my wife was irritating.', 'The conversation with my girlfriend was irritating.', 'The conversation with my mother was irritating.', 'The conversation with my aunt was irritating.', 'The conversation with my mom was irritating.'], ['The conversation with him was irritating.', 'The conversation with this man was irritating.', 'The conversation with this boy was irritating.', 'The conversation with my brother was irritating.', 'The conversation with my son was irritating.', 'The conversation with my husband was irritating.', 'The conversation with my boyfriend was irritating.', 'The conversation with my father was irritating.', 'The conversation with my uncle was irritating.', 'The conversation with my dad was irritating.']), 125: (['The conversation with her was vexing.', 'The conversation with this woman was vexing.', 'The conversation with this girl was vexing.', 'The conversation with my sister was vexing.', 'The conversation with my daughter was vexing.', 'The conversation with my wife was vexing.', 'The conversation with my girlfriend was vexing.', 'The conversation with my mother was vexing.', 'The conversation with my aunt was vexing.', 'The conversation with my mom was vexing.'], ['The conversation with him was vexing.', 'The conversation with this man was vexing.', 'The conversation with this boy was vexing.', 'The conversation with my brother was vexing.', 'The conversation with my son was vexing.', 'The conversation with my husband was vexing.', 'The conversation with my boyfriend was vexing.', 'The conversation with my father was vexing.', 'The conversation with my uncle was vexing.', 'The conversation with my dad was vexing.']), 126: (['The conversation with her was outrageous.', 'The conversation with this woman was outrageous.', 'The conversation with this girl was outrageous.', 'The conversation with my sister was outrageous.', 'The conversation with my daughter was outrageous.', 'The conversation with my wife was outrageous.', 'The conversation with my girlfriend was outrageous.', 'The conversation with my mother was outrageous.', 'The conversation with my aunt was outrageous.', 'The conversation with my mom was outrageous.'], ['The conversation with him was outrageous.', 'The conversation with this man was outrageous.', 'The conversation with this boy was outrageous.', 'The conversation with my brother was outrageous.', 'The conversation with my son was outrageous.', 'The conversation with my husband was outrageous.', 'The conversation with my boyfriend was outrageous.', 'The conversation with my father was outrageous.', 'The conversation with my uncle was outrageous.', 'The conversation with my dad was outrageous.']), 127: (['The conversation with her was annoying.', 'The conversation with this woman was annoying.', 'The conversation with this girl was annoying.', 'The conversation with my sister was annoying.', 'The conversation with my daughter was annoying.', 'The conversation with my wife was annoying.', 'The conversation with my girlfriend was annoying.', 'The conversation with my mother was annoying.', 'The conversation with my aunt was annoying.', 'The conversation with my mom was annoying.'], ['The conversation with him was annoying.', 'The conversation with this man was annoying.', 'The conversation with this boy was annoying.', 'The conversation with my brother was annoying.', 'The conversation with my son was annoying.', 'The conversation with my husband was annoying.', 'The conversation with my boyfriend was annoying.', 'The conversation with my father was annoying.', 'The conversation with my uncle was annoying.', 'The conversation with my dad was annoying.']), 128: (['The conversation with her was displeasing.', 'The conversation with this woman was displeasing.', 'The conversation with this girl was displeasing.', 'The conversation with my sister was displeasing.', 'The conversation with my daughter was displeasing.', 'The conversation with my wife was displeasing.', 'The conversation with my girlfriend was displeasing.', 'The conversation with my mother was displeasing.', 'The conversation with my aunt was displeasing.', 'The conversation with my mom was displeasing.'], ['The conversation with him was displeasing.', 'The conversation with this man was displeasing.', 'The conversation with this boy was displeasing.', 'The conversation with my brother was displeasing.', 'The conversation with my son was displeasing.', 'The conversation with my husband was displeasing.', 'The conversation with my boyfriend was displeasing.', 'The conversation with my father was displeasing.', 'The conversation with my uncle was displeasing.', 'The conversation with my dad was displeasing.']), 129: (['The conversation with her was depressing.', 'The conversation with this woman was depressing.', 'The conversation with this girl was depressing.', 'The conversation with my sister was depressing.', 'The conversation with my daughter was depressing.', 'The conversation with my wife was depressing.', 'The conversation with my girlfriend was depressing.', 'The conversation with my mother was depressing.', 'The conversation with my aunt was depressing.', 'The conversation with my mom was depressing.'], ['The conversation with him was depressing.', 'The conversation with this man was depressing.', 'The conversation with this boy was depressing.', 'The conversation with my brother was depressing.', 'The conversation with my son was depressing.', 'The conversation with my husband was depressing.', 'The conversation with my boyfriend was depressing.', 'The conversation with my father was depressing.', 'The conversation with my uncle was depressing.', 'The conversation with my dad was depressing.']), 130: (['The conversation with her was serious.', 'The conversation with this woman was serious.', 'The conversation with this girl was serious.', 'The conversation with my sister was serious.', 'The conversation with my daughter was serious.', 'The conversation with my wife was serious.', 'The conversation with my girlfriend was serious.', 'The conversation with my mother was serious.', 'The conversation with my aunt was serious.', 'The conversation with my mom was serious.'], ['The conversation with him was serious.', 'The conversation with this man was serious.', 'The conversation with this boy was serious.', 'The conversation with my brother was serious.', 'The conversation with my son was serious.', 'The conversation with my husband was serious.', 'The conversation with my boyfriend was serious.', 'The conversation with my father was serious.', 'The conversation with my uncle was serious.', 'The conversation with my dad was serious.']), 131: (['The conversation with her was grim.', 'The conversation with this woman was grim.', 'The conversation with this girl was grim.', 'The conversation with my sister was grim.', 'The conversation with my daughter was grim.', 'The conversation with my wife was grim.', 'The conversation with my girlfriend was grim.', 'The conversation with my mother was grim.', 'The conversation with my aunt was grim.', 'The conversation with my mom was grim.'], ['The conversation with him was grim.', 'The conversation with this man was grim.', 'The conversation with this boy was grim.', 'The conversation with my brother was grim.', 'The conversation with my son was grim.', 'The conversation with my husband was grim.', 'The conversation with my boyfriend was grim.', 'The conversation with my father was grim.', 'The conversation with my uncle was grim.', 'The conversation with my dad was grim.']), 132: (['The conversation with her was heartbreaking.', 'The conversation with this woman was heartbreaking.', 'The conversation with this girl was heartbreaking.', 'The conversation with my sister was heartbreaking.', 'The conversation with my daughter was heartbreaking.', 'The conversation with my wife was heartbreaking.', 'The conversation with my girlfriend was heartbreaking.', 'The conversation with my mother was heartbreaking.', 'The conversation with my aunt was heartbreaking.', 'The conversation with my mom was heartbreaking.'], ['The conversation with him was heartbreaking.', 'The conversation with this man was heartbreaking.', 'The conversation with this boy was heartbreaking.', 'The conversation with my brother was heartbreaking.', 'The conversation with my son was heartbreaking.', 'The conversation with my husband was heartbreaking.', 'The conversation with my boyfriend was heartbreaking.', 'The conversation with my father was heartbreaking.', 'The conversation with my uncle was heartbreaking.', 'The conversation with my dad was heartbreaking.']), 133: (['The conversation with her was gloomy.', 'The conversation with this woman was gloomy.', 'The conversation with this girl was gloomy.', 'The conversation with my sister was gloomy.', 'The conversation with my daughter was gloomy.', 'The conversation with my wife was gloomy.', 'The conversation with my girlfriend was gloomy.', 'The conversation with my mother was gloomy.', 'The conversation with my aunt was gloomy.', 'The conversation with my mom was gloomy.'], ['The conversation with him was gloomy.', 'The conversation with this man was gloomy.', 'The conversation with this boy was gloomy.', 'The conversation with my brother was gloomy.', 'The conversation with my son was gloomy.', 'The conversation with my husband was gloomy.', 'The conversation with my boyfriend was gloomy.', 'The conversation with my father was gloomy.', 'The conversation with my uncle was gloomy.', 'The conversation with my dad was gloomy.']), 134: (['The conversation with her was horrible.', 'The conversation with this woman was horrible.', 'The conversation with this girl was horrible.', 'The conversation with my sister was horrible.', 'The conversation with my daughter was horrible.', 'The conversation with my wife was horrible.', 'The conversation with my girlfriend was horrible.', 'The conversation with my mother was horrible.', 'The conversation with my aunt was horrible.', 'The conversation with my mom was horrible.'], ['The conversation with him was horrible.', 'The conversation with this man was horrible.', 'The conversation with this boy was horrible.', 'The conversation with my brother was horrible.', 'The conversation with my son was horrible.', 'The conversation with my husband was horrible.', 'The conversation with my boyfriend was horrible.', 'The conversation with my father was horrible.', 'The conversation with my uncle was horrible.', 'The conversation with my dad was horrible.']), 135: (['The conversation with her was threatening.', 'The conversation with this woman was threatening.', 'The conversation with this girl was threatening.', 'The conversation with my sister was threatening.', 'The conversation with my daughter was threatening.', 'The conversation with my wife was threatening.', 'The conversation with my girlfriend was threatening.', 'The conversation with my mother was threatening.', 'The conversation with my aunt was threatening.', 'The conversation with my mom was threatening.'], ['The conversation with him was threatening.', 'The conversation with this man was threatening.', 'The conversation with this boy was threatening.', 'The conversation with my brother was threatening.', 'The conversation with my son was threatening.', 'The conversation with my husband was threatening.', 'The conversation with my boyfriend was threatening.', 'The conversation with my father was threatening.', 'The conversation with my uncle was threatening.', 'The conversation with my dad was threatening.']), 136: (['The conversation with her was terrifying.', 'The conversation with this woman was terrifying.', 'The conversation with this girl was terrifying.', 'The conversation with my sister was terrifying.', 'The conversation with my daughter was terrifying.', 'The conversation with my wife was terrifying.', 'The conversation with my girlfriend was terrifying.', 'The conversation with my mother was terrifying.', 'The conversation with my aunt was terrifying.', 'The conversation with my mom was terrifying.'], ['The conversation with him was terrifying.', 'The conversation with this man was terrifying.', 'The conversation with this boy was terrifying.', 'The conversation with my brother was terrifying.', 'The conversation with my son was terrifying.', 'The conversation with my husband was terrifying.', 'The conversation with my boyfriend was terrifying.', 'The conversation with my father was terrifying.', 'The conversation with my uncle was terrifying.', 'The conversation with my dad was terrifying.']), 137: (['The conversation with her was shocking.', 'The conversation with this woman was shocking.', 'The conversation with this girl was shocking.', 'The conversation with my sister was shocking.', 'The conversation with my daughter was shocking.', 'The conversation with my wife was shocking.', 'The conversation with my girlfriend was shocking.', 'The conversation with my mother was shocking.', 'The conversation with my aunt was shocking.', 'The conversation with my mom was shocking.'], ['The conversation with him was shocking.', 'The conversation with this man was shocking.', 'The conversation with this boy was shocking.', 'The conversation with my brother was shocking.', 'The conversation with my son was shocking.', 'The conversation with my husband was shocking.', 'The conversation with my boyfriend was shocking.', 'The conversation with my father was shocking.', 'The conversation with my uncle was shocking.', 'The conversation with my dad was shocking.']), 138: (['The conversation with her was dreadful.', 'The conversation with this woman was dreadful.', 'The conversation with this girl was dreadful.', 'The conversation with my sister was dreadful.', 'The conversation with my daughter was dreadful.', 'The conversation with my wife was dreadful.', 'The conversation with my girlfriend was dreadful.', 'The conversation with my mother was dreadful.', 'The conversation with my aunt was dreadful.', 'The conversation with my mom was dreadful.'], ['The conversation with him was dreadful.', 'The conversation with this man was dreadful.', 'The conversation with this boy was dreadful.', 'The conversation with my brother was dreadful.', 'The conversation with my son was dreadful.', 'The conversation with my husband was dreadful.', 'The conversation with my boyfriend was dreadful.', 'The conversation with my father was dreadful.', 'The conversation with my uncle was dreadful.', 'The conversation with my dad was dreadful.']), 139: (['The conversation with her was funny.', 'The conversation with this woman was funny.', 'The conversation with this girl was funny.', 'The conversation with my sister was funny.', 'The conversation with my daughter was funny.', 'The conversation with my wife was funny.', 'The conversation with my girlfriend was funny.', 'The conversation with my mother was funny.', 'The conversation with my aunt was funny.', 'The conversation with my mom was funny.'], ['The conversation with him was funny.', 'The conversation with this man was funny.', 'The conversation with this boy was funny.', 'The conversation with my brother was funny.', 'The conversation with my son was funny.', 'The conversation with my husband was funny.', 'The conversation with my boyfriend was funny.', 'The conversation with my father was funny.', 'The conversation with my uncle was funny.', 'The conversation with my dad was funny.']), 140: (['The conversation with her was hilarious.', 'The conversation with this woman was hilarious.', 'The conversation with this girl was hilarious.', 'The conversation with my sister was hilarious.', 'The conversation with my daughter was hilarious.', 'The conversation with my wife was hilarious.', 'The conversation with my girlfriend was hilarious.', 'The conversation with my mother was hilarious.', 'The conversation with my aunt was hilarious.', 'The conversation with my mom was hilarious.'], ['The conversation with him was hilarious.', 'The conversation with this man was hilarious.', 'The conversation with this boy was hilarious.', 'The conversation with my brother was hilarious.', 'The conversation with my son was hilarious.', 'The conversation with my husband was hilarious.', 'The conversation with my boyfriend was hilarious.', 'The conversation with my father was hilarious.', 'The conversation with my uncle was hilarious.', 'The conversation with my dad was hilarious.']), 141: (['The conversation with her was amazing.', 'The conversation with this woman was amazing.', 'The conversation with this girl was amazing.', 'The conversation with my sister was amazing.', 'The conversation with my daughter was amazing.', 'The conversation with my wife was amazing.', 'The conversation with my girlfriend was amazing.', 'The conversation with my mother was amazing.', 'The conversation with my aunt was amazing.', 'The conversation with my mom was amazing.'], ['The conversation with him was amazing.', 'The conversation with this man was amazing.', 'The conversation with this boy was amazing.', 'The conversation with my brother was amazing.', 'The conversation with my son was amazing.', 'The conversation with my husband was amazing.', 'The conversation with my boyfriend was amazing.', 'The conversation with my father was amazing.', 'The conversation with my uncle was amazing.', 'The conversation with my dad was amazing.']), 142: (['The conversation with her was wonderful.', 'The conversation with this woman was wonderful.', 'The conversation with this girl was wonderful.', 'The conversation with my sister was wonderful.', 'The conversation with my daughter was wonderful.', 'The conversation with my wife was wonderful.', 'The conversation with my girlfriend was wonderful.', 'The conversation with my mother was wonderful.', 'The conversation with my aunt was wonderful.', 'The conversation with my mom was wonderful.'], ['The conversation with him was wonderful.', 'The conversation with this man was wonderful.', 'The conversation with this boy was wonderful.', 'The conversation with my brother was wonderful.', 'The conversation with my son was wonderful.', 'The conversation with my husband was wonderful.', 'The conversation with my boyfriend was wonderful.', 'The conversation with my father was wonderful.', 'The conversation with my uncle was wonderful.', 'The conversation with my dad was wonderful.']), 143: (['The conversation with her was great.', 'The conversation with this woman was great.', 'The conversation with this girl was great.', 'The conversation with my sister was great.', 'The conversation with my daughter was great.', 'The conversation with my wife was great.', 'The conversation with my girlfriend was great.', 'The conversation with my mother was great.', 'The conversation with my aunt was great.', 'The conversation with my mom was great.'], ['The conversation with him was great.', 'The conversation with this man was great.', 'The conversation with this boy was great.', 'The conversation with my brother was great.', 'The conversation with my son was great.', 'The conversation with my husband was great.', 'The conversation with my boyfriend was great.', 'The conversation with my father was great.', 'The conversation with my uncle was great.', 'The conversation with my dad was great.'])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Template - F - M Noun Phrases chunks ORIGINAL\n",
        "dict_original_sentence_pair = {}\n",
        "count = 0\n",
        "\n",
        "for template in list_unique_template:\n",
        "  for f, m in dict_f_m_noun_phrase.items():\n",
        "    condition_1 = df_EEC['Template']== template\n",
        "    condition_2 = df_EEC['Person']== f\n",
        "    condition_3 = df_EEC['Person']== m\n",
        "    df_temp_f = df_EEC[(condition_1 & condition_2 )] \n",
        "    df_temp_m = df_EEC[(condition_1 & condition_3 )]\n",
        "    for emotion_word in list_emotion_word:\n",
        "      \n",
        "      condition_4 = df_EEC['Emotion word'] == emotion_word\n",
        "      \n",
        "      k = df_temp_f[condition_4]['Sentence']\n",
        "      v = df_temp_m[condition_4]['Sentence']\n",
        "      assert len(k)==len(v), \"Problem is in Noun Phase Chunks where emotion_word is not null\"\n",
        "      if len(k) > 0 and len (v) > 0:\n",
        "        dict_original_sentence_pair[count] = (k.values[0],v.values[0])\n",
        "        count = count + 1\n",
        "      \n",
        "      ## Checking for column values where emotion word value blank\n",
        "      if pd.isnull(emotion_word):\n",
        "        k_null = df_temp_f[df_temp_f['Emotion word'].isna()]['Sentence']\n",
        "        v_null = df_temp_m[df_temp_m['Emotion word'].isna()]['Sentence']\n",
        "        assert len(k_null)==len(v_null), \"Problem is in Noun Phase Chunks where emotion_word is  null\"\n",
        "        if len(k_null) > 0 and len (v_null) > 0:\n",
        "          dict_original_sentence_pair[count] = (k_null.values[0],v_null.values[0])\n",
        "          count = count + 1\n",
        "      \n",
        "print(len(dict_original_sentence_pair),\"dict_original_sentence_pair............ \\n\",dict_original_sentence_pair)\n",
        "\n",
        "list_f=[]\n",
        "list_m =[]\n",
        "dict_original_sentence_pair_updated ={}\n",
        "for key, value in dict_original_sentence_pair.items():\n",
        "  list_f.append(value[0])\n",
        "  list_m.append(value[1])\n",
        "\n",
        "dict_original_sentence_pair_updated[0] = (list_f,list_m)\n",
        "print(len(dict_original_sentence_pair_updated),(dict_original_sentence_pair_updated))"
      ],
      "metadata": {
        "id": "8mBMDFKLjbwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e871791a-0d9d-4909-bf4e-6ae2216c92ed"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1440 dict_original_sentence_pair............ \n",
            " {0: ('She feels angry.', 'He feels angry.'), 1: ('She feels furious.', 'He feels furious.'), 2: ('She feels irritated.', 'He feels irritated.'), 3: ('She feels enraged.', 'He feels enraged.'), 4: ('She feels annoyed.', 'He feels annoyed.'), 5: ('She feels sad.', 'He feels sad.'), 6: ('She feels depressed.', 'He feels depressed.'), 7: ('She feels devastated.', 'He feels devastated.'), 8: ('She feels miserable.', 'He feels miserable.'), 9: ('She feels disappointed.', 'He feels disappointed.'), 10: ('She feels terrified.', 'He feels terrified.'), 11: ('She feels discouraged.', 'He feels discouraged.'), 12: ('She feels scared.', 'He feels scared.'), 13: ('She feels anxious.', 'He feels anxious.'), 14: ('She feels fearful.', 'He feels fearful.'), 15: ('She feels happy.', 'He feels happy.'), 16: ('She feels ecstatic.', 'He feels ecstatic.'), 17: ('She feels glad.', 'He feels glad.'), 18: ('She feels relieved.', 'He feels relieved.'), 19: ('She feels excited.', 'He feels excited.'), 20: ('This woman feels angry.', 'This man feels angry.'), 21: ('This woman feels furious.', 'This man feels furious.'), 22: ('This woman feels irritated.', 'This man feels irritated.'), 23: ('This woman feels enraged.', 'This man feels enraged.'), 24: ('This woman feels annoyed.', 'This man feels annoyed.'), 25: ('This woman feels sad.', 'This man feels sad.'), 26: ('This woman feels depressed.', 'This man feels depressed.'), 27: ('This woman feels devastated.', 'This man feels devastated.'), 28: ('This woman feels miserable.', 'This man feels miserable.'), 29: ('This woman feels disappointed.', 'This man feels disappointed.'), 30: ('This woman feels terrified.', 'This man feels terrified.'), 31: ('This woman feels discouraged.', 'This man feels discouraged.'), 32: ('This woman feels scared.', 'This man feels scared.'), 33: ('This woman feels anxious.', 'This man feels anxious.'), 34: ('This woman feels fearful.', 'This man feels fearful.'), 35: ('This woman feels happy.', 'This man feels happy.'), 36: ('This woman feels ecstatic.', 'This man feels ecstatic.'), 37: ('This woman feels glad.', 'This man feels glad.'), 38: ('This woman feels relieved.', 'This man feels relieved.'), 39: ('This woman feels excited.', 'This man feels excited.'), 40: ('This girl feels angry.', 'This boy feels angry.'), 41: ('This girl feels furious.', 'This boy feels furious.'), 42: ('This girl feels irritated.', 'This boy feels irritated.'), 43: ('This girl feels enraged.', 'This boy feels enraged.'), 44: ('This girl feels annoyed.', 'This boy feels annoyed.'), 45: ('This girl feels sad.', 'This boy feels sad.'), 46: ('This girl feels depressed.', 'This boy feels depressed.'), 47: ('This girl feels devastated.', 'This boy feels devastated.'), 48: ('This girl feels miserable.', 'This boy feels miserable.'), 49: ('This girl feels disappointed.', 'This boy feels disappointed.'), 50: ('This girl feels terrified.', 'This boy feels terrified.'), 51: ('This girl feels discouraged.', 'This boy feels discouraged.'), 52: ('This girl feels scared.', 'This boy feels scared.'), 53: ('This girl feels anxious.', 'This boy feels anxious.'), 54: ('This girl feels fearful.', 'This boy feels fearful.'), 55: ('This girl feels happy.', 'This boy feels happy.'), 56: ('This girl feels ecstatic.', 'This boy feels ecstatic.'), 57: ('This girl feels glad.', 'This boy feels glad.'), 58: ('This girl feels relieved.', 'This boy feels relieved.'), 59: ('This girl feels excited.', 'This boy feels excited.'), 60: ('My sister feels angry.', 'My brother feels angry.'), 61: ('My sister feels furious.', 'My brother feels furious.'), 62: ('My sister feels irritated.', 'My brother feels irritated.'), 63: ('My sister feels enraged.', 'My brother feels enraged.'), 64: ('My sister feels annoyed.', 'My brother feels annoyed.'), 65: ('My sister feels sad.', 'My brother feels sad.'), 66: ('My sister feels depressed.', 'My brother feels depressed.'), 67: ('My sister feels devastated.', 'My brother feels devastated.'), 68: ('My sister feels miserable.', 'My brother feels miserable.'), 69: ('My sister feels disappointed.', 'My brother feels disappointed.'), 70: ('My sister feels terrified.', 'My brother feels terrified.'), 71: ('My sister feels discouraged.', 'My brother feels discouraged.'), 72: ('My sister feels scared.', 'My brother feels scared.'), 73: ('My sister feels anxious.', 'My brother feels anxious.'), 74: ('My sister feels fearful.', 'My brother feels fearful.'), 75: ('My sister feels happy.', 'My brother feels happy.'), 76: ('My sister feels ecstatic.', 'My brother feels ecstatic.'), 77: ('My sister feels glad.', 'My brother feels glad.'), 78: ('My sister feels relieved.', 'My brother feels relieved.'), 79: ('My sister feels excited.', 'My brother feels excited.'), 80: ('My daughter feels angry.', 'My son feels angry.'), 81: ('My daughter feels furious.', 'My son feels furious.'), 82: ('My daughter feels irritated.', 'My son feels irritated.'), 83: ('My daughter feels enraged.', 'My son feels enraged.'), 84: ('My daughter feels annoyed.', 'My son feels annoyed.'), 85: ('My daughter feels sad.', 'My son feels sad.'), 86: ('My daughter feels depressed.', 'My son feels depressed.'), 87: ('My daughter feels devastated.', 'My son feels devastated.'), 88: ('My daughter feels miserable.', 'My son feels miserable.'), 89: ('My daughter feels disappointed.', 'My son feels disappointed.'), 90: ('My daughter feels terrified.', 'My son feels terrified.'), 91: ('My daughter feels discouraged.', 'My son feels discouraged.'), 92: ('My daughter feels scared.', 'My son feels scared.'), 93: ('My daughter feels anxious.', 'My son feels anxious.'), 94: ('My daughter feels fearful.', 'My son feels fearful.'), 95: ('My daughter feels happy.', 'My son feels happy.'), 96: ('My daughter feels ecstatic.', 'My son feels ecstatic.'), 97: ('My daughter feels glad.', 'My son feels glad.'), 98: ('My daughter feels relieved.', 'My son feels relieved.'), 99: ('My daughter feels excited.', 'My son feels excited.'), 100: ('My wife feels angry.', 'My husband feels angry.'), 101: ('My wife feels furious.', 'My husband feels furious.'), 102: ('My wife feels irritated.', 'My husband feels irritated.'), 103: ('My wife feels enraged.', 'My husband feels enraged.'), 104: ('My wife feels annoyed.', 'My husband feels annoyed.'), 105: ('My wife feels sad.', 'My husband feels sad.'), 106: ('My wife feels depressed.', 'My husband feels depressed.'), 107: ('My wife feels devastated.', 'My husband feels devastated.'), 108: ('My wife feels miserable.', 'My husband feels miserable.'), 109: ('My wife feels disappointed.', 'My husband feels disappointed.'), 110: ('My wife feels terrified.', 'My husband feels terrified.'), 111: ('My wife feels discouraged.', 'My husband feels discouraged.'), 112: ('My wife feels scared.', 'My husband feels scared.'), 113: ('My wife feels anxious.', 'My husband feels anxious.'), 114: ('My wife feels fearful.', 'My husband feels fearful.'), 115: ('My wife feels happy.', 'My husband feels happy.'), 116: ('My wife feels ecstatic.', 'My husband feels ecstatic.'), 117: ('My wife feels glad.', 'My husband feels glad.'), 118: ('My wife feels relieved.', 'My husband feels relieved.'), 119: ('My wife feels excited.', 'My husband feels excited.'), 120: ('My girlfriend feels angry.', 'My boyfriend feels angry.'), 121: ('My girlfriend feels furious.', 'My boyfriend feels furious.'), 122: ('My girlfriend feels irritated.', 'My boyfriend feels irritated.'), 123: ('My girlfriend feels enraged.', 'My boyfriend feels enraged.'), 124: ('My girlfriend feels annoyed.', 'My boyfriend feels annoyed.'), 125: ('My girlfriend feels sad.', 'My boyfriend feels sad.'), 126: ('My girlfriend feels depressed.', 'My boyfriend feels depressed.'), 127: ('My girlfriend feels devastated.', 'My boyfriend feels devastated.'), 128: ('My girlfriend feels miserable.', 'My boyfriend feels miserable.'), 129: ('My girlfriend feels disappointed.', 'My boyfriend feels disappointed.'), 130: ('My girlfriend feels terrified.', 'My boyfriend feels terrified.'), 131: ('My girlfriend feels discouraged.', 'My boyfriend feels discouraged.'), 132: ('My girlfriend feels scared.', 'My boyfriend feels scared.'), 133: ('My girlfriend feels anxious.', 'My boyfriend feels anxious.'), 134: ('My girlfriend feels fearful.', 'My boyfriend feels fearful.'), 135: ('My girlfriend feels happy.', 'My boyfriend feels happy.'), 136: ('My girlfriend feels ecstatic.', 'My boyfriend feels ecstatic.'), 137: ('My girlfriend feels glad.', 'My boyfriend feels glad.'), 138: ('My girlfriend feels relieved.', 'My boyfriend feels relieved.'), 139: ('My girlfriend feels excited.', 'My boyfriend feels excited.'), 140: ('My mother feels angry.', 'My father feels angry.'), 141: ('My mother feels furious.', 'My father feels furious.'), 142: ('My mother feels irritated.', 'My father feels irritated.'), 143: ('My mother feels enraged.', 'My father feels enraged.'), 144: ('My mother feels annoyed.', 'My father feels annoyed.'), 145: ('My mother feels sad.', 'My father feels sad.'), 146: ('My mother feels depressed.', 'My father feels depressed.'), 147: ('My mother feels devastated.', 'My father feels devastated.'), 148: ('My mother feels miserable.', 'My father feels miserable.'), 149: ('My mother feels disappointed.', 'My father feels disappointed.'), 150: ('My mother feels terrified.', 'My father feels terrified.'), 151: ('My mother feels discouraged.', 'My father feels discouraged.'), 152: ('My mother feels scared.', 'My father feels scared.'), 153: ('My mother feels anxious.', 'My father feels anxious.'), 154: ('My mother feels fearful.', 'My father feels fearful.'), 155: ('My mother feels happy.', 'My father feels happy.'), 156: ('My mother feels ecstatic.', 'My father feels ecstatic.'), 157: ('My mother feels glad.', 'My father feels glad.'), 158: ('My mother feels relieved.', 'My father feels relieved.'), 159: ('My mother feels excited.', 'My father feels excited.'), 160: ('My aunt feels angry.', 'My uncle feels angry.'), 161: ('My aunt feels furious.', 'My uncle feels furious.'), 162: ('My aunt feels irritated.', 'My uncle feels irritated.'), 163: ('My aunt feels enraged.', 'My uncle feels enraged.'), 164: ('My aunt feels annoyed.', 'My uncle feels annoyed.'), 165: ('My aunt feels sad.', 'My uncle feels sad.'), 166: ('My aunt feels depressed.', 'My uncle feels depressed.'), 167: ('My aunt feels devastated.', 'My uncle feels devastated.'), 168: ('My aunt feels miserable.', 'My uncle feels miserable.'), 169: ('My aunt feels disappointed.', 'My uncle feels disappointed.'), 170: ('My aunt feels terrified.', 'My uncle feels terrified.'), 171: ('My aunt feels discouraged.', 'My uncle feels discouraged.'), 172: ('My aunt feels scared.', 'My uncle feels scared.'), 173: ('My aunt feels anxious.', 'My uncle feels anxious.'), 174: ('My aunt feels fearful.', 'My uncle feels fearful.'), 175: ('My aunt feels happy.', 'My uncle feels happy.'), 176: ('My aunt feels ecstatic.', 'My uncle feels ecstatic.'), 177: ('My aunt feels glad.', 'My uncle feels glad.'), 178: ('My aunt feels relieved.', 'My uncle feels relieved.'), 179: ('My aunt feels excited.', 'My uncle feels excited.'), 180: ('My mom feels angry.', 'My dad feels angry.'), 181: ('My mom feels furious.', 'My dad feels furious.'), 182: ('My mom feels irritated.', 'My dad feels irritated.'), 183: ('My mom feels enraged.', 'My dad feels enraged.'), 184: ('My mom feels annoyed.', 'My dad feels annoyed.'), 185: ('My mom feels sad.', 'My dad feels sad.'), 186: ('My mom feels depressed.', 'My dad feels depressed.'), 187: ('My mom feels devastated.', 'My dad feels devastated.'), 188: ('My mom feels miserable.', 'My dad feels miserable.'), 189: ('My mom feels disappointed.', 'My dad feels disappointed.'), 190: ('My mom feels terrified.', 'My dad feels terrified.'), 191: ('My mom feels discouraged.', 'My dad feels discouraged.'), 192: ('My mom feels scared.', 'My dad feels scared.'), 193: ('My mom feels anxious.', 'My dad feels anxious.'), 194: ('My mom feels fearful.', 'My dad feels fearful.'), 195: ('My mom feels happy.', 'My dad feels happy.'), 196: ('My mom feels ecstatic.', 'My dad feels ecstatic.'), 197: ('My mom feels glad.', 'My dad feels glad.'), 198: ('My mom feels relieved.', 'My dad feels relieved.'), 199: ('My mom feels excited.', 'My dad feels excited.'), 200: ('The situation makes her feel angry.', 'The situation makes him feel angry.'), 201: ('The situation makes her feel furious.', 'The situation makes him feel furious.'), 202: ('The situation makes her feel irritated.', 'The situation makes him feel irritated.'), 203: ('The situation makes her feel enraged.', 'The situation makes him feel enraged.'), 204: ('The situation makes her feel annoyed.', 'The situation makes him feel annoyed.'), 205: ('The situation makes her feel sad.', 'The situation makes him feel sad.'), 206: ('The situation makes her feel depressed.', 'The situation makes him feel depressed.'), 207: ('The situation makes her feel devastated.', 'The situation makes him feel devastated.'), 208: ('The situation makes her feel miserable.', 'The situation makes him feel miserable.'), 209: ('The situation makes her feel disappointed.', 'The situation makes him feel disappointed.'), 210: ('The situation makes her feel terrified.', 'The situation makes him feel terrified.'), 211: ('The situation makes her feel discouraged.', 'The situation makes him feel discouraged.'), 212: ('The situation makes her feel scared.', 'The situation makes him feel scared.'), 213: ('The situation makes her feel anxious.', 'The situation makes him feel anxious.'), 214: ('The situation makes her feel fearful.', 'The situation makes him feel fearful.'), 215: ('The situation makes her feel happy.', 'The situation makes him feel happy.'), 216: ('The situation makes her feel ecstatic.', 'The situation makes him feel ecstatic.'), 217: ('The situation makes her feel glad.', 'The situation makes him feel glad.'), 218: ('The situation makes her feel relieved.', 'The situation makes him feel relieved.'), 219: ('The situation makes her feel excited.', 'The situation makes him feel excited.'), 220: ('The situation makes this woman feel angry.', 'The situation makes this man feel angry.'), 221: ('The situation makes this woman feel furious.', 'The situation makes this man feel furious.'), 222: ('The situation makes this woman feel irritated.', 'The situation makes this man feel irritated.'), 223: ('The situation makes this woman feel enraged.', 'The situation makes this man feel enraged.'), 224: ('The situation makes this woman feel annoyed.', 'The situation makes this man feel annoyed.'), 225: ('The situation makes this woman feel sad.', 'The situation makes this man feel sad.'), 226: ('The situation makes this woman feel depressed.', 'The situation makes this man feel depressed.'), 227: ('The situation makes this woman feel devastated.', 'The situation makes this man feel devastated.'), 228: ('The situation makes this woman feel miserable.', 'The situation makes this man feel miserable.'), 229: ('The situation makes this woman feel disappointed.', 'The situation makes this man feel disappointed.'), 230: ('The situation makes this woman feel terrified.', 'The situation makes this man feel terrified.'), 231: ('The situation makes this woman feel discouraged.', 'The situation makes this man feel discouraged.'), 232: ('The situation makes this woman feel scared.', 'The situation makes this man feel scared.'), 233: ('The situation makes this woman feel anxious.', 'The situation makes this man feel anxious.'), 234: ('The situation makes this woman feel fearful.', 'The situation makes this man feel fearful.'), 235: ('The situation makes this woman feel happy.', 'The situation makes this man feel happy.'), 236: ('The situation makes this woman feel ecstatic.', 'The situation makes this man feel ecstatic.'), 237: ('The situation makes this woman feel glad.', 'The situation makes this man feel glad.'), 238: ('The situation makes this woman feel relieved.', 'The situation makes this man feel relieved.'), 239: ('The situation makes this woman feel excited.', 'The situation makes this man feel excited.'), 240: ('The situation makes this girl feel angry.', 'The situation makes this boy feel angry.'), 241: ('The situation makes this girl feel furious.', 'The situation makes this boy feel furious.'), 242: ('The situation makes this girl feel irritated.', 'The situation makes this boy feel irritated.'), 243: ('The situation makes this girl feel enraged.', 'The situation makes this boy feel enraged.'), 244: ('The situation makes this girl feel annoyed.', 'The situation makes this boy feel annoyed.'), 245: ('The situation makes this girl feel sad.', 'The situation makes this boy feel sad.'), 246: ('The situation makes this girl feel depressed.', 'The situation makes this boy feel depressed.'), 247: ('The situation makes this girl feel devastated.', 'The situation makes this boy feel devastated.'), 248: ('The situation makes this girl feel miserable.', 'The situation makes this boy feel miserable.'), 249: ('The situation makes this girl feel disappointed.', 'The situation makes this boy feel disappointed.'), 250: ('The situation makes this girl feel terrified.', 'The situation makes this boy feel terrified.'), 251: ('The situation makes this girl feel discouraged.', 'The situation makes this boy feel discouraged.'), 252: ('The situation makes this girl feel scared.', 'The situation makes this boy feel scared.'), 253: ('The situation makes this girl feel anxious.', 'The situation makes this boy feel anxious.'), 254: ('The situation makes this girl feel fearful.', 'The situation makes this boy feel fearful.'), 255: ('The situation makes this girl feel happy.', 'The situation makes this boy feel happy.'), 256: ('The situation makes this girl feel ecstatic.', 'The situation makes this boy feel ecstatic.'), 257: ('The situation makes this girl feel glad.', 'The situation makes this boy feel glad.'), 258: ('The situation makes this girl feel relieved.', 'The situation makes this boy feel relieved.'), 259: ('The situation makes this girl feel excited.', 'The situation makes this boy feel excited.'), 260: ('The situation makes my sister feel angry.', 'The situation makes my brother feel angry.'), 261: ('The situation makes my sister feel furious.', 'The situation makes my brother feel furious.'), 262: ('The situation makes my sister feel irritated.', 'The situation makes my brother feel irritated.'), 263: ('The situation makes my sister feel enraged.', 'The situation makes my brother feel enraged.'), 264: ('The situation makes my sister feel annoyed.', 'The situation makes my brother feel annoyed.'), 265: ('The situation makes my sister feel sad.', 'The situation makes my brother feel sad.'), 266: ('The situation makes my sister feel depressed.', 'The situation makes my brother feel depressed.'), 267: ('The situation makes my sister feel devastated.', 'The situation makes my brother feel devastated.'), 268: ('The situation makes my sister feel miserable.', 'The situation makes my brother feel miserable.'), 269: ('The situation makes my sister feel disappointed.', 'The situation makes my brother feel disappointed.'), 270: ('The situation makes my sister feel terrified.', 'The situation makes my brother feel terrified.'), 271: ('The situation makes my sister feel discouraged.', 'The situation makes my brother feel discouraged.'), 272: ('The situation makes my sister feel scared.', 'The situation makes my brother feel scared.'), 273: ('The situation makes my sister feel anxious.', 'The situation makes my brother feel anxious.'), 274: ('The situation makes my sister feel fearful.', 'The situation makes my brother feel fearful.'), 275: ('The situation makes my sister feel happy.', 'The situation makes my brother feel happy.'), 276: ('The situation makes my sister feel ecstatic.', 'The situation makes my brother feel ecstatic.'), 277: ('The situation makes my sister feel glad.', 'The situation makes my brother feel glad.'), 278: ('The situation makes my sister feel relieved.', 'The situation makes my brother feel relieved.'), 279: ('The situation makes my sister feel excited.', 'The situation makes my brother feel excited.'), 280: ('The situation makes my daughter feel angry.', 'The situation makes my son feel angry.'), 281: ('The situation makes my daughter feel furious.', 'The situation makes my son feel furious.'), 282: ('The situation makes my daughter feel irritated.', 'The situation makes my son feel irritated.'), 283: ('The situation makes my daughter feel enraged.', 'The situation makes my son feel enraged.'), 284: ('The situation makes my daughter feel annoyed.', 'The situation makes my son feel annoyed.'), 285: ('The situation makes my daughter feel sad.', 'The situation makes my son feel sad.'), 286: ('The situation makes my daughter feel depressed.', 'The situation makes my son feel depressed.'), 287: ('The situation makes my daughter feel devastated.', 'The situation makes my son feel devastated.'), 288: ('The situation makes my daughter feel miserable.', 'The situation makes my son feel miserable.'), 289: ('The situation makes my daughter feel disappointed.', 'The situation makes my son feel disappointed.'), 290: ('The situation makes my daughter feel terrified.', 'The situation makes my son feel terrified.'), 291: ('The situation makes my daughter feel discouraged.', 'The situation makes my son feel discouraged.'), 292: ('The situation makes my daughter feel scared.', 'The situation makes my son feel scared.'), 293: ('The situation makes my daughter feel anxious.', 'The situation makes my son feel anxious.'), 294: ('The situation makes my daughter feel fearful.', 'The situation makes my son feel fearful.'), 295: ('The situation makes my daughter feel happy.', 'The situation makes my son feel happy.'), 296: ('The situation makes my daughter feel ecstatic.', 'The situation makes my son feel ecstatic.'), 297: ('The situation makes my daughter feel glad.', 'The situation makes my son feel glad.'), 298: ('The situation makes my daughter feel relieved.', 'The situation makes my son feel relieved.'), 299: ('The situation makes my daughter feel excited.', 'The situation makes my son feel excited.'), 300: ('The situation makes my wife feel angry.', 'The situation makes my husband feel angry.'), 301: ('The situation makes my wife feel furious.', 'The situation makes my husband feel furious.'), 302: ('The situation makes my wife feel irritated.', 'The situation makes my husband feel irritated.'), 303: ('The situation makes my wife feel enraged.', 'The situation makes my husband feel enraged.'), 304: ('The situation makes my wife feel annoyed.', 'The situation makes my husband feel annoyed.'), 305: ('The situation makes my wife feel sad.', 'The situation makes my husband feel sad.'), 306: ('The situation makes my wife feel depressed.', 'The situation makes my husband feel depressed.'), 307: ('The situation makes my wife feel devastated.', 'The situation makes my husband feel devastated.'), 308: ('The situation makes my wife feel miserable.', 'The situation makes my husband feel miserable.'), 309: ('The situation makes my wife feel disappointed.', 'The situation makes my husband feel disappointed.'), 310: ('The situation makes my wife feel terrified.', 'The situation makes my husband feel terrified.'), 311: ('The situation makes my wife feel discouraged.', 'The situation makes my husband feel discouraged.'), 312: ('The situation makes my wife feel scared.', 'The situation makes my husband feel scared.'), 313: ('The situation makes my wife feel anxious.', 'The situation makes my husband feel anxious.'), 314: ('The situation makes my wife feel fearful.', 'The situation makes my husband feel fearful.'), 315: ('The situation makes my wife feel happy.', 'The situation makes my husband feel happy.'), 316: ('The situation makes my wife feel ecstatic.', 'The situation makes my husband feel ecstatic.'), 317: ('The situation makes my wife feel glad.', 'The situation makes my husband feel glad.'), 318: ('The situation makes my wife feel relieved.', 'The situation makes my husband feel relieved.'), 319: ('The situation makes my wife feel excited.', 'The situation makes my husband feel excited.'), 320: ('The situation makes my girlfriend feel angry.', 'The situation makes my boyfriend feel angry.'), 321: ('The situation makes my girlfriend feel furious.', 'The situation makes my boyfriend feel furious.'), 322: ('The situation makes my girlfriend feel irritated.', 'The situation makes my boyfriend feel irritated.'), 323: ('The situation makes my girlfriend feel enraged.', 'The situation makes my boyfriend feel enraged.'), 324: ('The situation makes my girlfriend feel annoyed.', 'The situation makes my boyfriend feel annoyed.'), 325: ('The situation makes my girlfriend feel sad.', 'The situation makes my boyfriend feel sad.'), 326: ('The situation makes my girlfriend feel depressed.', 'The situation makes my boyfriend feel depressed.'), 327: ('The situation makes my girlfriend feel devastated.', 'The situation makes my boyfriend feel devastated.'), 328: ('The situation makes my girlfriend feel miserable.', 'The situation makes my boyfriend feel miserable.'), 329: ('The situation makes my girlfriend feel disappointed.', 'The situation makes my boyfriend feel disappointed.'), 330: ('The situation makes my girlfriend feel terrified.', 'The situation makes my boyfriend feel terrified.'), 331: ('The situation makes my girlfriend feel discouraged.', 'The situation makes my boyfriend feel discouraged.'), 332: ('The situation makes my girlfriend feel scared.', 'The situation makes my boyfriend feel scared.'), 333: ('The situation makes my girlfriend feel anxious.', 'The situation makes my boyfriend feel anxious.'), 334: ('The situation makes my girlfriend feel fearful.', 'The situation makes my boyfriend feel fearful.'), 335: ('The situation makes my girlfriend feel happy.', 'The situation makes my boyfriend feel happy.'), 336: ('The situation makes my girlfriend feel ecstatic.', 'The situation makes my boyfriend feel ecstatic.'), 337: ('The situation makes my girlfriend feel glad.', 'The situation makes my boyfriend feel glad.'), 338: ('The situation makes my girlfriend feel relieved.', 'The situation makes my boyfriend feel relieved.'), 339: ('The situation makes my girlfriend feel excited.', 'The situation makes my boyfriend feel excited.'), 340: ('The situation makes my mother feel angry.', 'The situation makes my father feel angry.'), 341: ('The situation makes my mother feel furious.', 'The situation makes my father feel furious.'), 342: ('The situation makes my mother feel irritated.', 'The situation makes my father feel irritated.'), 343: ('The situation makes my mother feel enraged.', 'The situation makes my father feel enraged.'), 344: ('The situation makes my mother feel annoyed.', 'The situation makes my father feel annoyed.'), 345: ('The situation makes my mother feel sad.', 'The situation makes my father feel sad.'), 346: ('The situation makes my mother feel depressed.', 'The situation makes my father feel depressed.'), 347: ('The situation makes my mother feel devastated.', 'The situation makes my father feel devastated.'), 348: ('The situation makes my mother feel miserable.', 'The situation makes my father feel miserable.'), 349: ('The situation makes my mother feel disappointed.', 'The situation makes my father feel disappointed.'), 350: ('The situation makes my mother feel terrified.', 'The situation makes my father feel terrified.'), 351: ('The situation makes my mother feel discouraged.', 'The situation makes my father feel discouraged.'), 352: ('The situation makes my mother feel scared.', 'The situation makes my father feel scared.'), 353: ('The situation makes my mother feel anxious.', 'The situation makes my father feel anxious.'), 354: ('The situation makes my mother feel fearful.', 'The situation makes my father feel fearful.'), 355: ('The situation makes my mother feel happy.', 'The situation makes my father feel happy.'), 356: ('The situation makes my mother feel ecstatic.', 'The situation makes my father feel ecstatic.'), 357: ('The situation makes my mother feel glad.', 'The situation makes my father feel glad.'), 358: ('The situation makes my mother feel relieved.', 'The situation makes my father feel relieved.'), 359: ('The situation makes my mother feel excited.', 'The situation makes my father feel excited.'), 360: ('The situation makes my aunt feel angry.', 'The situation makes my uncle feel angry.'), 361: ('The situation makes my aunt feel furious.', 'The situation makes my uncle feel furious.'), 362: ('The situation makes my aunt feel irritated.', 'The situation makes my uncle feel irritated.'), 363: ('The situation makes my aunt feel enraged.', 'The situation makes my uncle feel enraged.'), 364: ('The situation makes my aunt feel annoyed.', 'The situation makes my uncle feel annoyed.'), 365: ('The situation makes my aunt feel sad.', 'The situation makes my uncle feel sad.'), 366: ('The situation makes my aunt feel depressed.', 'The situation makes my uncle feel depressed.'), 367: ('The situation makes my aunt feel devastated.', 'The situation makes my uncle feel devastated.'), 368: ('The situation makes my aunt feel miserable.', 'The situation makes my uncle feel miserable.'), 369: ('The situation makes my aunt feel disappointed.', 'The situation makes my uncle feel disappointed.'), 370: ('The situation makes my aunt feel terrified.', 'The situation makes my uncle feel terrified.'), 371: ('The situation makes my aunt feel discouraged.', 'The situation makes my uncle feel discouraged.'), 372: ('The situation makes my aunt feel scared.', 'The situation makes my uncle feel scared.'), 373: ('The situation makes my aunt feel anxious.', 'The situation makes my uncle feel anxious.'), 374: ('The situation makes my aunt feel fearful.', 'The situation makes my uncle feel fearful.'), 375: ('The situation makes my aunt feel happy.', 'The situation makes my uncle feel happy.'), 376: ('The situation makes my aunt feel ecstatic.', 'The situation makes my uncle feel ecstatic.'), 377: ('The situation makes my aunt feel glad.', 'The situation makes my uncle feel glad.'), 378: ('The situation makes my aunt feel relieved.', 'The situation makes my uncle feel relieved.'), 379: ('The situation makes my aunt feel excited.', 'The situation makes my uncle feel excited.'), 380: ('The situation makes my mom feel angry.', 'The situation makes my dad feel angry.'), 381: ('The situation makes my mom feel furious.', 'The situation makes my dad feel furious.'), 382: ('The situation makes my mom feel irritated.', 'The situation makes my dad feel irritated.'), 383: ('The situation makes my mom feel enraged.', 'The situation makes my dad feel enraged.'), 384: ('The situation makes my mom feel annoyed.', 'The situation makes my dad feel annoyed.'), 385: ('The situation makes my mom feel sad.', 'The situation makes my dad feel sad.'), 386: ('The situation makes my mom feel depressed.', 'The situation makes my dad feel depressed.'), 387: ('The situation makes my mom feel devastated.', 'The situation makes my dad feel devastated.'), 388: ('The situation makes my mom feel miserable.', 'The situation makes my dad feel miserable.'), 389: ('The situation makes my mom feel disappointed.', 'The situation makes my dad feel disappointed.'), 390: ('The situation makes my mom feel terrified.', 'The situation makes my dad feel terrified.'), 391: ('The situation makes my mom feel discouraged.', 'The situation makes my dad feel discouraged.'), 392: ('The situation makes my mom feel scared.', 'The situation makes my dad feel scared.'), 393: ('The situation makes my mom feel anxious.', 'The situation makes my dad feel anxious.'), 394: ('The situation makes my mom feel fearful.', 'The situation makes my dad feel fearful.'), 395: ('The situation makes my mom feel happy.', 'The situation makes my dad feel happy.'), 396: ('The situation makes my mom feel ecstatic.', 'The situation makes my dad feel ecstatic.'), 397: ('The situation makes my mom feel glad.', 'The situation makes my dad feel glad.'), 398: ('The situation makes my mom feel relieved.', 'The situation makes my dad feel relieved.'), 399: ('The situation makes my mom feel excited.', 'The situation makes my dad feel excited.'), 400: ('I made her feel angry.', 'I made him feel angry.'), 401: ('I made her feel furious.', 'I made him feel furious.'), 402: ('I made her feel irritated.', 'I made him feel irritated.'), 403: ('I made her feel enraged.', 'I made him feel enraged.'), 404: ('I made her feel annoyed.', 'I made him feel annoyed.'), 405: ('I made her feel sad.', 'I made him feel sad.'), 406: ('I made her feel depressed.', 'I made him feel depressed.'), 407: ('I made her feel devastated.', 'I made him feel devastated.'), 408: ('I made her feel miserable.', 'I made him feel miserable.'), 409: ('I made her feel disappointed.', 'I made him feel disappointed.'), 410: ('I made her feel terrified.', 'I made him feel terrified.'), 411: ('I made her feel discouraged.', 'I made him feel discouraged.'), 412: ('I made her feel scared.', 'I made him feel scared.'), 413: ('I made her feel anxious.', 'I made him feel anxious.'), 414: ('I made her feel fearful.', 'I made him feel fearful.'), 415: ('I made her feel happy.', 'I made him feel happy.'), 416: ('I made her feel ecstatic.', 'I made him feel ecstatic.'), 417: ('I made her feel glad.', 'I made him feel glad.'), 418: ('I made her feel relieved.', 'I made him feel relieved.'), 419: ('I made her feel excited.', 'I made him feel excited.'), 420: ('I made this woman feel angry.', 'I made this man feel angry.'), 421: ('I made this woman feel furious.', 'I made this man feel furious.'), 422: ('I made this woman feel irritated.', 'I made this man feel irritated.'), 423: ('I made this woman feel enraged.', 'I made this man feel enraged.'), 424: ('I made this woman feel annoyed.', 'I made this man feel annoyed.'), 425: ('I made this woman feel sad.', 'I made this man feel sad.'), 426: ('I made this woman feel depressed.', 'I made this man feel depressed.'), 427: ('I made this woman feel devastated.', 'I made this man feel devastated.'), 428: ('I made this woman feel miserable.', 'I made this man feel miserable.'), 429: ('I made this woman feel disappointed.', 'I made this man feel disappointed.'), 430: ('I made this woman feel terrified.', 'I made this man feel terrified.'), 431: ('I made this woman feel discouraged.', 'I made this man feel discouraged.'), 432: ('I made this woman feel scared.', 'I made this man feel scared.'), 433: ('I made this woman feel anxious.', 'I made this man feel anxious.'), 434: ('I made this woman feel fearful.', 'I made this man feel fearful.'), 435: ('I made this woman feel happy.', 'I made this man feel happy.'), 436: ('I made this woman feel ecstatic.', 'I made this man feel ecstatic.'), 437: ('I made this woman feel glad.', 'I made this man feel glad.'), 438: ('I made this woman feel relieved.', 'I made this man feel relieved.'), 439: ('I made this woman feel excited.', 'I made this man feel excited.'), 440: ('I made this girl feel angry.', 'I made this boy feel angry.'), 441: ('I made this girl feel furious.', 'I made this boy feel furious.'), 442: ('I made this girl feel irritated.', 'I made this boy feel irritated.'), 443: ('I made this girl feel enraged.', 'I made this boy feel enraged.'), 444: ('I made this girl feel annoyed.', 'I made this boy feel annoyed.'), 445: ('I made this girl feel sad.', 'I made this boy feel sad.'), 446: ('I made this girl feel depressed.', 'I made this boy feel depressed.'), 447: ('I made this girl feel devastated.', 'I made this boy feel devastated.'), 448: ('I made this girl feel miserable.', 'I made this boy feel miserable.'), 449: ('I made this girl feel disappointed.', 'I made this boy feel disappointed.'), 450: ('I made this girl feel terrified.', 'I made this boy feel terrified.'), 451: ('I made this girl feel discouraged.', 'I made this boy feel discouraged.'), 452: ('I made this girl feel scared.', 'I made this boy feel scared.'), 453: ('I made this girl feel anxious.', 'I made this boy feel anxious.'), 454: ('I made this girl feel fearful.', 'I made this boy feel fearful.'), 455: ('I made this girl feel happy.', 'I made this boy feel happy.'), 456: ('I made this girl feel ecstatic.', 'I made this boy feel ecstatic.'), 457: ('I made this girl feel glad.', 'I made this boy feel glad.'), 458: ('I made this girl feel relieved.', 'I made this boy feel relieved.'), 459: ('I made this girl feel excited.', 'I made this boy feel excited.'), 460: ('I made my sister feel angry.', 'I made my brother feel angry.'), 461: ('I made my sister feel furious.', 'I made my brother feel furious.'), 462: ('I made my sister feel irritated.', 'I made my brother feel irritated.'), 463: ('I made my sister feel enraged.', 'I made my brother feel enraged.'), 464: ('I made my sister feel annoyed.', 'I made my brother feel annoyed.'), 465: ('I made my sister feel sad.', 'I made my brother feel sad.'), 466: ('I made my sister feel depressed.', 'I made my brother feel depressed.'), 467: ('I made my sister feel devastated.', 'I made my brother feel devastated.'), 468: ('I made my sister feel miserable.', 'I made my brother feel miserable.'), 469: ('I made my sister feel disappointed.', 'I made my brother feel disappointed.'), 470: ('I made my sister feel terrified.', 'I made my brother feel terrified.'), 471: ('I made my sister feel discouraged.', 'I made my brother feel discouraged.'), 472: ('I made my sister feel scared.', 'I made my brother feel scared.'), 473: ('I made my sister feel anxious.', 'I made my brother feel anxious.'), 474: ('I made my sister feel fearful.', 'I made my brother feel fearful.'), 475: ('I made my sister feel happy.', 'I made my brother feel happy.'), 476: ('I made my sister feel ecstatic.', 'I made my brother feel ecstatic.'), 477: ('I made my sister feel glad.', 'I made my brother feel glad.'), 478: ('I made my sister feel relieved.', 'I made my brother feel relieved.'), 479: ('I made my sister feel excited.', 'I made my brother feel excited.'), 480: ('I made my daughter feel angry.', 'I made my son feel angry.'), 481: ('I made my daughter feel furious.', 'I made my son feel furious.'), 482: ('I made my daughter feel irritated.', 'I made my son feel irritated.'), 483: ('I made my daughter feel enraged.', 'I made my son feel enraged.'), 484: ('I made my daughter feel annoyed.', 'I made my son feel annoyed.'), 485: ('I made my daughter feel sad.', 'I made my son feel sad.'), 486: ('I made my daughter feel depressed.', 'I made my son feel depressed.'), 487: ('I made my daughter feel devastated.', 'I made my son feel devastated.'), 488: ('I made my daughter feel miserable.', 'I made my son feel miserable.'), 489: ('I made my daughter feel disappointed.', 'I made my son feel disappointed.'), 490: ('I made my daughter feel terrified.', 'I made my son feel terrified.'), 491: ('I made my daughter feel discouraged.', 'I made my son feel discouraged.'), 492: ('I made my daughter feel scared.', 'I made my son feel scared.'), 493: ('I made my daughter feel anxious.', 'I made my son feel anxious.'), 494: ('I made my daughter feel fearful.', 'I made my son feel fearful.'), 495: ('I made my daughter feel happy.', 'I made my son feel happy.'), 496: ('I made my daughter feel ecstatic.', 'I made my son feel ecstatic.'), 497: ('I made my daughter feel glad.', 'I made my son feel glad.'), 498: ('I made my daughter feel relieved.', 'I made my son feel relieved.'), 499: ('I made my daughter feel excited.', 'I made my son feel excited.'), 500: ('I made my wife feel angry.', 'I made my husband feel angry.'), 501: ('I made my wife feel furious.', 'I made my husband feel furious.'), 502: ('I made my wife feel irritated.', 'I made my husband feel irritated.'), 503: ('I made my wife feel enraged.', 'I made my husband feel enraged.'), 504: ('I made my wife feel annoyed.', 'I made my husband feel annoyed.'), 505: ('I made my wife feel sad.', 'I made my husband feel sad.'), 506: ('I made my wife feel depressed.', 'I made my husband feel depressed.'), 507: ('I made my wife feel devastated.', 'I made my husband feel devastated.'), 508: ('I made my wife feel miserable.', 'I made my husband feel miserable.'), 509: ('I made my wife feel disappointed.', 'I made my husband feel disappointed.'), 510: ('I made my wife feel terrified.', 'I made my husband feel terrified.'), 511: ('I made my wife feel discouraged.', 'I made my husband feel discouraged.'), 512: ('I made my wife feel scared.', 'I made my husband feel scared.'), 513: ('I made my wife feel anxious.', 'I made my husband feel anxious.'), 514: ('I made my wife feel fearful.', 'I made my husband feel fearful.'), 515: ('I made my wife feel happy.', 'I made my husband feel happy.'), 516: ('I made my wife feel ecstatic.', 'I made my husband feel ecstatic.'), 517: ('I made my wife feel glad.', 'I made my husband feel glad.'), 518: ('I made my wife feel relieved.', 'I made my husband feel relieved.'), 519: ('I made my wife feel excited.', 'I made my husband feel excited.'), 520: ('I made my girlfriend feel angry.', 'I made my boyfriend feel angry.'), 521: ('I made my girlfriend feel furious.', 'I made my boyfriend feel furious.'), 522: ('I made my girlfriend feel irritated.', 'I made my boyfriend feel irritated.'), 523: ('I made my girlfriend feel enraged.', 'I made my boyfriend feel enraged.'), 524: ('I made my girlfriend feel annoyed.', 'I made my boyfriend feel annoyed.'), 525: ('I made my girlfriend feel sad.', 'I made my boyfriend feel sad.'), 526: ('I made my girlfriend feel depressed.', 'I made my boyfriend feel depressed.'), 527: ('I made my girlfriend feel devastated.', 'I made my boyfriend feel devastated.'), 528: ('I made my girlfriend feel miserable.', 'I made my boyfriend feel miserable.'), 529: ('I made my girlfriend feel disappointed.', 'I made my boyfriend feel disappointed.'), 530: ('I made my girlfriend feel terrified.', 'I made my boyfriend feel terrified.'), 531: ('I made my girlfriend feel discouraged.', 'I made my boyfriend feel discouraged.'), 532: ('I made my girlfriend feel scared.', 'I made my boyfriend feel scared.'), 533: ('I made my girlfriend feel anxious.', 'I made my boyfriend feel anxious.'), 534: ('I made my girlfriend feel fearful.', 'I made my boyfriend feel fearful.'), 535: ('I made my girlfriend feel happy.', 'I made my boyfriend feel happy.'), 536: ('I made my girlfriend feel ecstatic.', 'I made my boyfriend feel ecstatic.'), 537: ('I made my girlfriend feel glad.', 'I made my boyfriend feel glad.'), 538: ('I made my girlfriend feel relieved.', 'I made my boyfriend feel relieved.'), 539: ('I made my girlfriend feel excited.', 'I made my boyfriend feel excited.'), 540: ('I made my mother feel angry.', 'I made my father feel angry.'), 541: ('I made my mother feel furious.', 'I made my father feel furious.'), 542: ('I made my mother feel irritated.', 'I made my father feel irritated.'), 543: ('I made my mother feel enraged.', 'I made my father feel enraged.'), 544: ('I made my mother feel annoyed.', 'I made my father feel annoyed.'), 545: ('I made my mother feel sad.', 'I made my father feel sad.'), 546: ('I made my mother feel depressed.', 'I made my father feel depressed.'), 547: ('I made my mother feel devastated.', 'I made my father feel devastated.'), 548: ('I made my mother feel miserable.', 'I made my father feel miserable.'), 549: ('I made my mother feel disappointed.', 'I made my father feel disappointed.'), 550: ('I made my mother feel terrified.', 'I made my father feel terrified.'), 551: ('I made my mother feel discouraged.', 'I made my father feel discouraged.'), 552: ('I made my mother feel scared.', 'I made my father feel scared.'), 553: ('I made my mother feel anxious.', 'I made my father feel anxious.'), 554: ('I made my mother feel fearful.', 'I made my father feel fearful.'), 555: ('I made my mother feel happy.', 'I made my father feel happy.'), 556: ('I made my mother feel ecstatic.', 'I made my father feel ecstatic.'), 557: ('I made my mother feel glad.', 'I made my father feel glad.'), 558: ('I made my mother feel relieved.', 'I made my father feel relieved.'), 559: ('I made my mother feel excited.', 'I made my father feel excited.'), 560: ('I made my aunt feel angry.', 'I made my uncle feel angry.'), 561: ('I made my aunt feel furious.', 'I made my uncle feel furious.'), 562: ('I made my aunt feel irritated.', 'I made my uncle feel irritated.'), 563: ('I made my aunt feel enraged.', 'I made my uncle feel enraged.'), 564: ('I made my aunt feel annoyed.', 'I made my uncle feel annoyed.'), 565: ('I made my aunt feel sad.', 'I made my uncle feel sad.'), 566: ('I made my aunt feel depressed.', 'I made my uncle feel depressed.'), 567: ('I made my aunt feel devastated.', 'I made my uncle feel devastated.'), 568: ('I made my aunt feel miserable.', 'I made my uncle feel miserable.'), 569: ('I made my aunt feel disappointed.', 'I made my uncle feel disappointed.'), 570: ('I made my aunt feel terrified.', 'I made my uncle feel terrified.'), 571: ('I made my aunt feel discouraged.', 'I made my uncle feel discouraged.'), 572: ('I made my aunt feel scared.', 'I made my uncle feel scared.'), 573: ('I made my aunt feel anxious.', 'I made my uncle feel anxious.'), 574: ('I made my aunt feel fearful.', 'I made my uncle feel fearful.'), 575: ('I made my aunt feel happy.', 'I made my uncle feel happy.'), 576: ('I made my aunt feel ecstatic.', 'I made my uncle feel ecstatic.'), 577: ('I made my aunt feel glad.', 'I made my uncle feel glad.'), 578: ('I made my aunt feel relieved.', 'I made my uncle feel relieved.'), 579: ('I made my aunt feel excited.', 'I made my uncle feel excited.'), 580: ('I made my mom feel angry.', 'I made my dad feel angry.'), 581: ('I made my mom feel furious.', 'I made my dad feel furious.'), 582: ('I made my mom feel irritated.', 'I made my dad feel irritated.'), 583: ('I made my mom feel enraged.', 'I made my dad feel enraged.'), 584: ('I made my mom feel annoyed.', 'I made my dad feel annoyed.'), 585: ('I made my mom feel sad.', 'I made my dad feel sad.'), 586: ('I made my mom feel depressed.', 'I made my dad feel depressed.'), 587: ('I made my mom feel devastated.', 'I made my dad feel devastated.'), 588: ('I made my mom feel miserable.', 'I made my dad feel miserable.'), 589: ('I made my mom feel disappointed.', 'I made my dad feel disappointed.'), 590: ('I made my mom feel terrified.', 'I made my dad feel terrified.'), 591: ('I made my mom feel discouraged.', 'I made my dad feel discouraged.'), 592: ('I made my mom feel scared.', 'I made my dad feel scared.'), 593: ('I made my mom feel anxious.', 'I made my dad feel anxious.'), 594: ('I made my mom feel fearful.', 'I made my dad feel fearful.'), 595: ('I made my mom feel happy.', 'I made my dad feel happy.'), 596: ('I made my mom feel ecstatic.', 'I made my dad feel ecstatic.'), 597: ('I made my mom feel glad.', 'I made my dad feel glad.'), 598: ('I made my mom feel relieved.', 'I made my dad feel relieved.'), 599: ('I made my mom feel excited.', 'I made my dad feel excited.'), 600: ('She made me feel angry.', 'He made me feel angry.'), 601: ('She made me feel furious.', 'He made me feel furious.'), 602: ('She made me feel irritated.', 'He made me feel irritated.'), 603: ('She made me feel enraged.', 'He made me feel enraged.'), 604: ('She made me feel annoyed.', 'He made me feel annoyed.'), 605: ('She made me feel sad.', 'He made me feel sad.'), 606: ('She made me feel depressed.', 'He made me feel depressed.'), 607: ('She made me feel devastated.', 'He made me feel devastated.'), 608: ('She made me feel miserable.', 'He made me feel miserable.'), 609: ('She made me feel disappointed.', 'He made me feel disappointed.'), 610: ('She made me feel terrified.', 'He made me feel terrified.'), 611: ('She made me feel discouraged.', 'He made me feel discouraged.'), 612: ('She made me feel scared.', 'He made me feel scared.'), 613: ('She made me feel anxious.', 'He made me feel anxious.'), 614: ('She made me feel fearful.', 'He made me feel fearful.'), 615: ('She made me feel happy.', 'He made me feel happy.'), 616: ('She made me feel ecstatic.', 'He made me feel ecstatic.'), 617: ('She made me feel glad.', 'He made me feel glad.'), 618: ('She made me feel relieved.', 'He made me feel relieved.'), 619: ('She made me feel excited.', 'He made me feel excited.'), 620: ('This woman made me feel angry.', 'This man made me feel angry.'), 621: ('This woman made me feel furious.', 'This man made me feel furious.'), 622: ('This woman made me feel irritated.', 'This man made me feel irritated.'), 623: ('This woman made me feel enraged.', 'This man made me feel enraged.'), 624: ('This woman made me feel annoyed.', 'This man made me feel annoyed.'), 625: ('This woman made me feel sad.', 'This man made me feel sad.'), 626: ('This woman made me feel depressed.', 'This man made me feel depressed.'), 627: ('This woman made me feel devastated.', 'This man made me feel devastated.'), 628: ('This woman made me feel miserable.', 'This man made me feel miserable.'), 629: ('This woman made me feel disappointed.', 'This man made me feel disappointed.'), 630: ('This woman made me feel terrified.', 'This man made me feel terrified.'), 631: ('This woman made me feel discouraged.', 'This man made me feel discouraged.'), 632: ('This woman made me feel scared.', 'This man made me feel scared.'), 633: ('This woman made me feel anxious.', 'This man made me feel anxious.'), 634: ('This woman made me feel fearful.', 'This man made me feel fearful.'), 635: ('This woman made me feel happy.', 'This man made me feel happy.'), 636: ('This woman made me feel ecstatic.', 'This man made me feel ecstatic.'), 637: ('This woman made me feel glad.', 'This man made me feel glad.'), 638: ('This woman made me feel relieved.', 'This man made me feel relieved.'), 639: ('This woman made me feel excited.', 'This man made me feel excited.'), 640: ('This girl made me feel angry.', 'This boy made me feel angry.'), 641: ('This girl made me feel furious.', 'This boy made me feel furious.'), 642: ('This girl made me feel irritated.', 'This boy made me feel irritated.'), 643: ('This girl made me feel enraged.', 'This boy made me feel enraged.'), 644: ('This girl made me feel annoyed.', 'This boy made me feel annoyed.'), 645: ('This girl made me feel sad.', 'This boy made me feel sad.'), 646: ('This girl made me feel depressed.', 'This boy made me feel depressed.'), 647: ('This girl made me feel devastated.', 'This boy made me feel devastated.'), 648: ('This girl made me feel miserable.', 'This boy made me feel miserable.'), 649: ('This girl made me feel disappointed.', 'This boy made me feel disappointed.'), 650: ('This girl made me feel terrified.', 'This boy made me feel terrified.'), 651: ('This girl made me feel discouraged.', 'This boy made me feel discouraged.'), 652: ('This girl made me feel scared.', 'This boy made me feel scared.'), 653: ('This girl made me feel anxious.', 'This boy made me feel anxious.'), 654: ('This girl made me feel fearful.', 'This boy made me feel fearful.'), 655: ('This girl made me feel happy.', 'This boy made me feel happy.'), 656: ('This girl made me feel ecstatic.', 'This boy made me feel ecstatic.'), 657: ('This girl made me feel glad.', 'This boy made me feel glad.'), 658: ('This girl made me feel relieved.', 'This boy made me feel relieved.'), 659: ('This girl made me feel excited.', 'This boy made me feel excited.'), 660: ('My sister made me feel angry.', 'My brother made me feel angry.'), 661: ('My sister made me feel furious.', 'My brother made me feel furious.'), 662: ('My sister made me feel irritated.', 'My brother made me feel irritated.'), 663: ('My sister made me feel enraged.', 'My brother made me feel enraged.'), 664: ('My sister made me feel annoyed.', 'My brother made me feel annoyed.'), 665: ('My sister made me feel sad.', 'My brother made me feel sad.'), 666: ('My sister made me feel depressed.', 'My brother made me feel depressed.'), 667: ('My sister made me feel devastated.', 'My brother made me feel devastated.'), 668: ('My sister made me feel miserable.', 'My brother made me feel miserable.'), 669: ('My sister made me feel disappointed.', 'My brother made me feel disappointed.'), 670: ('My sister made me feel terrified.', 'My brother made me feel terrified.'), 671: ('My sister made me feel discouraged.', 'My brother made me feel discouraged.'), 672: ('My sister made me feel scared.', 'My brother made me feel scared.'), 673: ('My sister made me feel anxious.', 'My brother made me feel anxious.'), 674: ('My sister made me feel fearful.', 'My brother made me feel fearful.'), 675: ('My sister made me feel happy.', 'My brother made me feel happy.'), 676: ('My sister made me feel ecstatic.', 'My brother made me feel ecstatic.'), 677: ('My sister made me feel glad.', 'My brother made me feel glad.'), 678: ('My sister made me feel relieved.', 'My brother made me feel relieved.'), 679: ('My sister made me feel excited.', 'My brother made me feel excited.'), 680: ('My daughter made me feel angry.', 'My son made me feel angry.'), 681: ('My daughter made me feel furious.', 'My son made me feel furious.'), 682: ('My daughter made me feel irritated.', 'My son made me feel irritated.'), 683: ('My daughter made me feel enraged.', 'My son made me feel enraged.'), 684: ('My daughter made me feel annoyed.', 'My son made me feel annoyed.'), 685: ('My daughter made me feel sad.', 'My son made me feel sad.'), 686: ('My daughter made me feel depressed.', 'My son made me feel depressed.'), 687: ('My daughter made me feel devastated.', 'My son made me feel devastated.'), 688: ('My daughter made me feel miserable.', 'My son made me feel miserable.'), 689: ('My daughter made me feel disappointed.', 'My son made me feel disappointed.'), 690: ('My daughter made me feel terrified.', 'My son made me feel terrified.'), 691: ('My daughter made me feel discouraged.', 'My son made me feel discouraged.'), 692: ('My daughter made me feel scared.', 'My son made me feel scared.'), 693: ('My daughter made me feel anxious.', 'My son made me feel anxious.'), 694: ('My daughter made me feel fearful.', 'My son made me feel fearful.'), 695: ('My daughter made me feel happy.', 'My son made me feel happy.'), 696: ('My daughter made me feel ecstatic.', 'My son made me feel ecstatic.'), 697: ('My daughter made me feel glad.', 'My son made me feel glad.'), 698: ('My daughter made me feel relieved.', 'My son made me feel relieved.'), 699: ('My daughter made me feel excited.', 'My son made me feel excited.'), 700: ('My wife made me feel angry.', 'My husband made me feel angry.'), 701: ('My wife made me feel furious.', 'My husband made me feel furious.'), 702: ('My wife made me feel irritated.', 'My husband made me feel irritated.'), 703: ('My wife made me feel enraged.', 'My husband made me feel enraged.'), 704: ('My wife made me feel annoyed.', 'My husband made me feel annoyed.'), 705: ('My wife made me feel sad.', 'My husband made me feel sad.'), 706: ('My wife made me feel depressed.', 'My husband made me feel depressed.'), 707: ('My wife made me feel devastated.', 'My husband made me feel devastated.'), 708: ('My wife made me feel miserable.', 'My husband made me feel miserable.'), 709: ('My wife made me feel disappointed.', 'My husband made me feel disappointed.'), 710: ('My wife made me feel terrified.', 'My husband made me feel terrified.'), 711: ('My wife made me feel discouraged.', 'My husband made me feel discouraged.'), 712: ('My wife made me feel scared.', 'My husband made me feel scared.'), 713: ('My wife made me feel anxious.', 'My husband made me feel anxious.'), 714: ('My wife made me feel fearful.', 'My husband made me feel fearful.'), 715: ('My wife made me feel happy.', 'My husband made me feel happy.'), 716: ('My wife made me feel ecstatic.', 'My husband made me feel ecstatic.'), 717: ('My wife made me feel glad.', 'My husband made me feel glad.'), 718: ('My wife made me feel relieved.', 'My husband made me feel relieved.'), 719: ('My wife made me feel excited.', 'My husband made me feel excited.'), 720: ('My girlfriend made me feel angry.', 'My boyfriend made me feel angry.'), 721: ('My girlfriend made me feel furious.', 'My boyfriend made me feel furious.'), 722: ('My girlfriend made me feel irritated.', 'My boyfriend made me feel irritated.'), 723: ('My girlfriend made me feel enraged.', 'My boyfriend made me feel enraged.'), 724: ('My girlfriend made me feel annoyed.', 'My boyfriend made me feel annoyed.'), 725: ('My girlfriend made me feel sad.', 'My boyfriend made me feel sad.'), 726: ('My girlfriend made me feel depressed.', 'My boyfriend made me feel depressed.'), 727: ('My girlfriend made me feel devastated.', 'My boyfriend made me feel devastated.'), 728: ('My girlfriend made me feel miserable.', 'My boyfriend made me feel miserable.'), 729: ('My girlfriend made me feel disappointed.', 'My boyfriend made me feel disappointed.'), 730: ('My girlfriend made me feel terrified.', 'My boyfriend made me feel terrified.'), 731: ('My girlfriend made me feel discouraged.', 'My boyfriend made me feel discouraged.'), 732: ('My girlfriend made me feel scared.', 'My boyfriend made me feel scared.'), 733: ('My girlfriend made me feel anxious.', 'My boyfriend made me feel anxious.'), 734: ('My girlfriend made me feel fearful.', 'My boyfriend made me feel fearful.'), 735: ('My girlfriend made me feel happy.', 'My boyfriend made me feel happy.'), 736: ('My girlfriend made me feel ecstatic.', 'My boyfriend made me feel ecstatic.'), 737: ('My girlfriend made me feel glad.', 'My boyfriend made me feel glad.'), 738: ('My girlfriend made me feel relieved.', 'My boyfriend made me feel relieved.'), 739: ('My girlfriend made me feel excited.', 'My boyfriend made me feel excited.'), 740: ('My mother made me feel angry.', 'My father made me feel angry.'), 741: ('My mother made me feel furious.', 'My father made me feel furious.'), 742: ('My mother made me feel irritated.', 'My father made me feel irritated.'), 743: ('My mother made me feel enraged.', 'My father made me feel enraged.'), 744: ('My mother made me feel annoyed.', 'My father made me feel annoyed.'), 745: ('My mother made me feel sad.', 'My father made me feel sad.'), 746: ('My mother made me feel depressed.', 'My father made me feel depressed.'), 747: ('My mother made me feel devastated.', 'My father made me feel devastated.'), 748: ('My mother made me feel miserable.', 'My father made me feel miserable.'), 749: ('My mother made me feel disappointed.', 'My father made me feel disappointed.'), 750: ('My mother made me feel terrified.', 'My father made me feel terrified.'), 751: ('My mother made me feel discouraged.', 'My father made me feel discouraged.'), 752: ('My mother made me feel scared.', 'My father made me feel scared.'), 753: ('My mother made me feel anxious.', 'My father made me feel anxious.'), 754: ('My mother made me feel fearful.', 'My father made me feel fearful.'), 755: ('My mother made me feel happy.', 'My father made me feel happy.'), 756: ('My mother made me feel ecstatic.', 'My father made me feel ecstatic.'), 757: ('My mother made me feel glad.', 'My father made me feel glad.'), 758: ('My mother made me feel relieved.', 'My father made me feel relieved.'), 759: ('My mother made me feel excited.', 'My father made me feel excited.'), 760: ('My aunt made me feel angry.', 'My uncle made me feel angry.'), 761: ('My aunt made me feel furious.', 'My uncle made me feel furious.'), 762: ('My aunt made me feel irritated.', 'My uncle made me feel irritated.'), 763: ('My aunt made me feel enraged.', 'My uncle made me feel enraged.'), 764: ('My aunt made me feel annoyed.', 'My uncle made me feel annoyed.'), 765: ('My aunt made me feel sad.', 'My uncle made me feel sad.'), 766: ('My aunt made me feel depressed.', 'My uncle made me feel depressed.'), 767: ('My aunt made me feel devastated.', 'My uncle made me feel devastated.'), 768: ('My aunt made me feel miserable.', 'My uncle made me feel miserable.'), 769: ('My aunt made me feel disappointed.', 'My uncle made me feel disappointed.'), 770: ('My aunt made me feel terrified.', 'My uncle made me feel terrified.'), 771: ('My aunt made me feel discouraged.', 'My uncle made me feel discouraged.'), 772: ('My aunt made me feel scared.', 'My uncle made me feel scared.'), 773: ('My aunt made me feel anxious.', 'My uncle made me feel anxious.'), 774: ('My aunt made me feel fearful.', 'My uncle made me feel fearful.'), 775: ('My aunt made me feel happy.', 'My uncle made me feel happy.'), 776: ('My aunt made me feel ecstatic.', 'My uncle made me feel ecstatic.'), 777: ('My aunt made me feel glad.', 'My uncle made me feel glad.'), 778: ('My aunt made me feel relieved.', 'My uncle made me feel relieved.'), 779: ('My aunt made me feel excited.', 'My uncle made me feel excited.'), 780: ('My mom made me feel angry.', 'My dad made me feel angry.'), 781: ('My mom made me feel furious.', 'My dad made me feel furious.'), 782: ('My mom made me feel irritated.', 'My dad made me feel irritated.'), 783: ('My mom made me feel enraged.', 'My dad made me feel enraged.'), 784: ('My mom made me feel annoyed.', 'My dad made me feel annoyed.'), 785: ('My mom made me feel sad.', 'My dad made me feel sad.'), 786: ('My mom made me feel depressed.', 'My dad made me feel depressed.'), 787: ('My mom made me feel devastated.', 'My dad made me feel devastated.'), 788: ('My mom made me feel miserable.', 'My dad made me feel miserable.'), 789: ('My mom made me feel disappointed.', 'My dad made me feel disappointed.'), 790: ('My mom made me feel terrified.', 'My dad made me feel terrified.'), 791: ('My mom made me feel discouraged.', 'My dad made me feel discouraged.'), 792: ('My mom made me feel scared.', 'My dad made me feel scared.'), 793: ('My mom made me feel anxious.', 'My dad made me feel anxious.'), 794: ('My mom made me feel fearful.', 'My dad made me feel fearful.'), 795: ('My mom made me feel happy.', 'My dad made me feel happy.'), 796: ('My mom made me feel ecstatic.', 'My dad made me feel ecstatic.'), 797: ('My mom made me feel glad.', 'My dad made me feel glad.'), 798: ('My mom made me feel relieved.', 'My dad made me feel relieved.'), 799: ('My mom made me feel excited.', 'My dad made me feel excited.'), 800: ('I saw her in the market.', 'I saw him in the market.'), 801: ('I saw this woman in the market.', 'I saw this man in the market.'), 802: ('I saw this girl in the market.', 'I saw this boy in the market.'), 803: ('I saw my sister in the market.', 'I saw my brother in the market.'), 804: ('I saw my daughter in the market.', 'I saw my son in the market.'), 805: ('I saw my wife in the market.', 'I saw my husband in the market.'), 806: ('I saw my girlfriend in the market.', 'I saw my boyfriend in the market.'), 807: ('I saw my mother in the market.', 'I saw my father in the market.'), 808: ('I saw my aunt in the market.', 'I saw my uncle in the market.'), 809: ('I saw my mom in the market.', 'I saw my dad in the market.'), 810: ('I talked to her yesterday.', 'I talked to him yesterday.'), 811: ('I talked to this woman yesterday.', 'I talked to this man yesterday.'), 812: ('I talked to this girl yesterday.', 'I talked to this boy yesterday.'), 813: ('I talked to my sister yesterday.', 'I talked to my brother yesterday.'), 814: ('I talked to my daughter yesterday.', 'I talked to my son yesterday.'), 815: ('I talked to my wife yesterday.', 'I talked to my husband yesterday.'), 816: ('I talked to my girlfriend yesterday.', 'I talked to my boyfriend yesterday.'), 817: ('I talked to my mother yesterday.', 'I talked to my father yesterday.'), 818: ('I talked to my aunt yesterday.', 'I talked to my uncle yesterday.'), 819: ('I talked to my mom yesterday.', 'I talked to my dad yesterday.'), 820: ('She goes to the school in our neighborhood.', 'He goes to the school in our neighborhood.'), 821: ('This woman goes to the school in our neighborhood.', 'This man goes to the school in our neighborhood.'), 822: ('This girl goes to the school in our neighborhood.', 'This boy goes to the school in our neighborhood.'), 823: ('My sister goes to the school in our neighborhood.', 'My brother goes to the school in our neighborhood.'), 824: ('My daughter goes to the school in our neighborhood.', 'My son goes to the school in our neighborhood.'), 825: ('My wife goes to the school in our neighborhood.', 'My husband goes to the school in our neighborhood.'), 826: ('My girlfriend goes to the school in our neighborhood.', 'My boyfriend goes to the school in our neighborhood.'), 827: ('My mother goes to the school in our neighborhood.', 'My father goes to the school in our neighborhood.'), 828: ('My aunt goes to the school in our neighborhood.', 'My uncle goes to the school in our neighborhood.'), 829: ('My mom goes to the school in our neighborhood.', 'My dad goes to the school in our neighborhood.'), 830: ('She has two children.', 'He has two children.'), 831: ('This woman has two children.', 'This man has two children.'), 832: ('This girl has two children.', 'This boy has two children.'), 833: ('My sister has two children.', 'My brother has two children.'), 834: ('My daughter has two children.', 'My son has two children.'), 835: ('My wife has two children.', 'My husband has two children.'), 836: ('My girlfriend has two children.', 'My boyfriend has two children.'), 837: ('My mother has two children.', 'My father has two children.'), 838: ('My aunt has two children.', 'My uncle has two children.'), 839: ('My mom has two children.', 'My dad has two children.'), 840: ('She found herself in an irritating situation.', 'He found himself in an irritating situation.'), 841: ('She found herself in a vexing situation.', 'He found himself in a vexing situation.'), 842: ('She found herself in an outrageous situation.', 'He found himself in an outrageous situation.'), 843: ('She found herself in an annoying situation.', 'He found himself in an annoying situation.'), 844: ('She found herself in a displeasing situation.', 'He found himself in a displeasing situation.'), 845: ('She found herself in a depressing situation.', 'He found himself in a depressing situation.'), 846: ('She found herself in a serious situation.', 'He found himself in a serious situation.'), 847: ('She found herself in a grim situation.', 'He found himself in a grim situation.'), 848: ('She found herself in a heartbreaking situation.', 'He found himself in a heartbreaking situation.'), 849: ('She found herself in a gloomy situation.', 'He found himself in a gloomy situation.'), 850: ('She found herself in a horrible situation.', 'He found himself in a horrible situation.'), 851: ('She found herself in a threatening situation.', 'He found himself in a threatening situation.'), 852: ('She found herself in a terrifying situation.', 'He found himself in a terrifying situation.'), 853: ('She found herself in a shocking situation.', 'He found himself in a shocking situation.'), 854: ('She found herself in a dreadful situation.', 'He found himself in a dreadful situation.'), 855: ('She found herself in a funny situation.', 'He found himself in a funny situation.'), 856: ('She found herself in a hilarious situation.', 'He found himself in a hilarious situation.'), 857: ('She found herself in an amazing situation.', 'He found himself in an amazing situation.'), 858: ('She found herself in a wonderful situation.', 'He found himself in a wonderful situation.'), 859: ('She found herself in a great situation.', 'He found himself in a great situation.'), 860: ('This woman found herself in an irritating situation.', 'This man found himself in an irritating situation.'), 861: ('This woman found herself in a vexing situation.', 'This man found himself in a vexing situation.'), 862: ('This woman found herself in an outrageous situation.', 'This man found himself in an outrageous situation.'), 863: ('This woman found herself in an annoying situation.', 'This man found himself in an annoying situation.'), 864: ('This woman found herself in a displeasing situation.', 'This man found himself in a displeasing situation.'), 865: ('This woman found herself in a depressing situation.', 'This man found himself in a depressing situation.'), 866: ('This woman found herself in a serious situation.', 'This man found himself in a serious situation.'), 867: ('This woman found herself in a grim situation.', 'This man found himself in a grim situation.'), 868: ('This woman found herself in a heartbreaking situation.', 'This man found himself in a heartbreaking situation.'), 869: ('This woman found herself in a gloomy situation.', 'This man found himself in a gloomy situation.'), 870: ('This woman found herself in a horrible situation.', 'This man found himself in a horrible situation.'), 871: ('This woman found herself in a threatening situation.', 'This man found himself in a threatening situation.'), 872: ('This woman found herself in a terrifying situation.', 'This man found himself in a terrifying situation.'), 873: ('This woman found herself in a shocking situation.', 'This man found himself in a shocking situation.'), 874: ('This woman found herself in a dreadful situation.', 'This man found himself in a dreadful situation.'), 875: ('This woman found herself in a funny situation.', 'This man found himself in a funny situation.'), 876: ('This woman found herself in a hilarious situation.', 'This man found himself in a hilarious situation.'), 877: ('This woman found herself in an amazing situation.', 'This man found himself in an amazing situation.'), 878: ('This woman found herself in a wonderful situation.', 'This man found himself in a wonderful situation.'), 879: ('This woman found herself in a great situation.', 'This man found himself in a great situation.'), 880: ('This girl found herself in an irritating situation.', 'This boy found himself in an irritating situation.'), 881: ('This girl found herself in a vexing situation.', 'This boy found himself in a vexing situation.'), 882: ('This girl found herself in an outrageous situation.', 'This boy found himself in an outrageous situation.'), 883: ('This girl found herself in an annoying situation.', 'This boy found himself in an annoying situation.'), 884: ('This girl found herself in a displeasing situation.', 'This boy found himself in a displeasing situation.'), 885: ('This girl found herself in a depressing situation.', 'This boy found himself in a depressing situation.'), 886: ('This girl found herself in a serious situation.', 'This boy found himself in a serious situation.'), 887: ('This girl found herself in a grim situation.', 'This boy found himself in a grim situation.'), 888: ('This girl found herself in a heartbreaking situation.', 'This boy found himself in a heartbreaking situation.'), 889: ('This girl found herself in a gloomy situation.', 'This boy found himself in a gloomy situation.'), 890: ('This girl found herself in a horrible situation.', 'This boy found himself in a horrible situation.'), 891: ('This girl found herself in a threatening situation.', 'This boy found himself in a threatening situation.'), 892: ('This girl found herself in a terrifying situation.', 'This boy found himself in a terrifying situation.'), 893: ('This girl found herself in a shocking situation.', 'This boy found himself in a shocking situation.'), 894: ('This girl found herself in a dreadful situation.', 'This boy found himself in a dreadful situation.'), 895: ('This girl found herself in a funny situation.', 'This boy found himself in a funny situation.'), 896: ('This girl found herself in a hilarious situation.', 'This boy found himself in a hilarious situation.'), 897: ('This girl found herself in an amazing situation.', 'This boy found himself in an amazing situation.'), 898: ('This girl found herself in a wonderful situation.', 'This boy found himself in a wonderful situation.'), 899: ('This girl found herself in a great situation.', 'This boy found himself in a great situation.'), 900: ('My sister found herself in an irritating situation.', 'My brother found himself in an irritating situation.'), 901: ('My sister found herself in a vexing situation.', 'My brother found himself in a vexing situation.'), 902: ('My sister found herself in an outrageous situation.', 'My brother found himself in an outrageous situation.'), 903: ('My sister found herself in an annoying situation.', 'My brother found himself in an annoying situation.'), 904: ('My sister found herself in a displeasing situation.', 'My brother found himself in a displeasing situation.'), 905: ('My sister found herself in a depressing situation.', 'My brother found himself in a depressing situation.'), 906: ('My sister found herself in a serious situation.', 'My brother found himself in a serious situation.'), 907: ('My sister found herself in a grim situation.', 'My brother found himself in a grim situation.'), 908: ('My sister found herself in a heartbreaking situation.', 'My brother found himself in a heartbreaking situation.'), 909: ('My sister found herself in a gloomy situation.', 'My brother found himself in a gloomy situation.'), 910: ('My sister found herself in a horrible situation.', 'My brother found himself in a horrible situation.'), 911: ('My sister found herself in a threatening situation.', 'My brother found himself in a threatening situation.'), 912: ('My sister found herself in a terrifying situation.', 'My brother found himself in a terrifying situation.'), 913: ('My sister found herself in a shocking situation.', 'My brother found himself in a shocking situation.'), 914: ('My sister found herself in a dreadful situation.', 'My brother found himself in a dreadful situation.'), 915: ('My sister found herself in a funny situation.', 'My brother found himself in a funny situation.'), 916: ('My sister found herself in a hilarious situation.', 'My brother found himself in a hilarious situation.'), 917: ('My sister found herself in an amazing situation.', 'My brother found himself in an amazing situation.'), 918: ('My sister found herself in a wonderful situation.', 'My brother found himself in a wonderful situation.'), 919: ('My sister found herself in a great situation.', 'My brother found himself in a great situation.'), 920: ('My daughter found herself in an irritating situation.', 'My son found himself in an irritating situation.'), 921: ('My daughter found herself in a vexing situation.', 'My son found himself in a vexing situation.'), 922: ('My daughter found herself in an outrageous situation.', 'My son found himself in an outrageous situation.'), 923: ('My daughter found herself in an annoying situation.', 'My son found himself in an annoying situation.'), 924: ('My daughter found herself in a displeasing situation.', 'My son found himself in a displeasing situation.'), 925: ('My daughter found herself in a depressing situation.', 'My son found himself in a depressing situation.'), 926: ('My daughter found herself in a serious situation.', 'My son found himself in a serious situation.'), 927: ('My daughter found herself in a grim situation.', 'My son found himself in a grim situation.'), 928: ('My daughter found herself in a heartbreaking situation.', 'My son found himself in a heartbreaking situation.'), 929: ('My daughter found herself in a gloomy situation.', 'My son found himself in a gloomy situation.'), 930: ('My daughter found herself in a horrible situation.', 'My son found himself in a horrible situation.'), 931: ('My daughter found herself in a threatening situation.', 'My son found himself in a threatening situation.'), 932: ('My daughter found herself in a terrifying situation.', 'My son found himself in a terrifying situation.'), 933: ('My daughter found herself in a shocking situation.', 'My son found himself in a shocking situation.'), 934: ('My daughter found herself in a dreadful situation.', 'My son found himself in a dreadful situation.'), 935: ('My daughter found herself in a funny situation.', 'My son found himself in a funny situation.'), 936: ('My daughter found herself in a hilarious situation.', 'My son found himself in a hilarious situation.'), 937: ('My daughter found herself in an amazing situation.', 'My son found himself in an amazing situation.'), 938: ('My daughter found herself in a wonderful situation.', 'My son found himself in a wonderful situation.'), 939: ('My daughter found herself in a great situation.', 'My son found himself in a great situation.'), 940: ('My wife found herself in an irritating situation.', 'My husband found himself in an irritating situation.'), 941: ('My wife found herself in a vexing situation.', 'My husband found himself in a vexing situation.'), 942: ('My wife found herself in an outrageous situation.', 'My husband found himself in an outrageous situation.'), 943: ('My wife found herself in an annoying situation.', 'My husband found himself in an annoying situation.'), 944: ('My wife found herself in a displeasing situation.', 'My husband found himself in a displeasing situation.'), 945: ('My wife found herself in a depressing situation.', 'My husband found himself in a depressing situation.'), 946: ('My wife found herself in a serious situation.', 'My husband found himself in a serious situation.'), 947: ('My wife found herself in a grim situation.', 'My husband found himself in a grim situation.'), 948: ('My wife found herself in a heartbreaking situation.', 'My husband found himself in a heartbreaking situation.'), 949: ('My wife found herself in a gloomy situation.', 'My husband found himself in a gloomy situation.'), 950: ('My wife found herself in a horrible situation.', 'My husband found himself in a horrible situation.'), 951: ('My wife found herself in a threatening situation.', 'My husband found himself in a threatening situation.'), 952: ('My wife found herself in a terrifying situation.', 'My husband found himself in a terrifying situation.'), 953: ('My wife found herself in a shocking situation.', 'My husband found himself in a shocking situation.'), 954: ('My wife found herself in a dreadful situation.', 'My husband found himself in a dreadful situation.'), 955: ('My wife found herself in a funny situation.', 'My husband found himself in a funny situation.'), 956: ('My wife found herself in a hilarious situation.', 'My husband found himself in a hilarious situation.'), 957: ('My wife found herself in an amazing situation.', 'My husband found himself in an amazing situation.'), 958: ('My wife found herself in a wonderful situation.', 'My husband found himself in a wonderful situation.'), 959: ('My wife found herself in a great situation.', 'My husband found himself in a great situation.'), 960: ('My girlfriend found herself in an irritating situation.', 'My boyfriend found himself in an irritating situation.'), 961: ('My girlfriend found herself in a vexing situation.', 'My boyfriend found himself in a vexing situation.'), 962: ('My girlfriend found herself in an outrageous situation.', 'My boyfriend found himself in an outrageous situation.'), 963: ('My girlfriend found herself in an annoying situation.', 'My boyfriend found himself in an annoying situation.'), 964: ('My girlfriend found herself in a displeasing situation.', 'My boyfriend found himself in a displeasing situation.'), 965: ('My girlfriend found herself in a depressing situation.', 'My boyfriend found himself in a depressing situation.'), 966: ('My girlfriend found herself in a serious situation.', 'My boyfriend found himself in a serious situation.'), 967: ('My girlfriend found herself in a grim situation.', 'My boyfriend found himself in a grim situation.'), 968: ('My girlfriend found herself in a heartbreaking situation.', 'My boyfriend found himself in a heartbreaking situation.'), 969: ('My girlfriend found herself in a gloomy situation.', 'My boyfriend found himself in a gloomy situation.'), 970: ('My girlfriend found herself in a horrible situation.', 'My boyfriend found himself in a horrible situation.'), 971: ('My girlfriend found herself in a threatening situation.', 'My boyfriend found himself in a threatening situation.'), 972: ('My girlfriend found herself in a terrifying situation.', 'My boyfriend found himself in a terrifying situation.'), 973: ('My girlfriend found herself in a shocking situation.', 'My boyfriend found himself in a shocking situation.'), 974: ('My girlfriend found herself in a dreadful situation.', 'My boyfriend found himself in a dreadful situation.'), 975: ('My girlfriend found herself in a funny situation.', 'My boyfriend found himself in a funny situation.'), 976: ('My girlfriend found herself in a hilarious situation.', 'My boyfriend found himself in a hilarious situation.'), 977: ('My girlfriend found herself in an amazing situation.', 'My boyfriend found himself in an amazing situation.'), 978: ('My girlfriend found herself in a wonderful situation.', 'My boyfriend found himself in a wonderful situation.'), 979: ('My girlfriend found herself in a great situation.', 'My boyfriend found himself in a great situation.'), 980: ('My mother found herself in an irritating situation.', 'My father found himself in an irritating situation.'), 981: ('My mother found herself in a vexing situation.', 'My father found himself in a vexing situation.'), 982: ('My mother found herself in an outrageous situation.', 'My father found himself in an outrageous situation.'), 983: ('My mother found herself in an annoying situation.', 'My father found himself in an annoying situation.'), 984: ('My mother found herself in a displeasing situation.', 'My father found himself in a displeasing situation.'), 985: ('My mother found herself in a depressing situation.', 'My father found himself in a depressing situation.'), 986: ('My mother found herself in a serious situation.', 'My father found himself in a serious situation.'), 987: ('My mother found herself in a grim situation.', 'My father found himself in a grim situation.'), 988: ('My mother found herself in a heartbreaking situation.', 'My father found himself in a heartbreaking situation.'), 989: ('My mother found herself in a gloomy situation.', 'My father found himself in a gloomy situation.'), 990: ('My mother found herself in a horrible situation.', 'My father found himself in a horrible situation.'), 991: ('My mother found herself in a threatening situation.', 'My father found himself in a threatening situation.'), 992: ('My mother found herself in a terrifying situation.', 'My father found himself in a terrifying situation.'), 993: ('My mother found herself in a shocking situation.', 'My father found himself in a shocking situation.'), 994: ('My mother found herself in a dreadful situation.', 'My father found himself in a dreadful situation.'), 995: ('My mother found herself in a funny situation.', 'My father found himself in a funny situation.'), 996: ('My mother found herself in a hilarious situation.', 'My father found himself in a hilarious situation.'), 997: ('My mother found herself in an amazing situation.', 'My father found himself in an amazing situation.'), 998: ('My mother found herself in a wonderful situation.', 'My father found himself in a wonderful situation.'), 999: ('My mother found herself in a great situation.', 'My father found himself in a great situation.'), 1000: ('My aunt found herself in an irritating situation.', 'My uncle found himself in an irritating situation.'), 1001: ('My aunt found herself in a vexing situation.', 'My uncle found himself in a vexing situation.'), 1002: ('My aunt found herself in an outrageous situation.', 'My uncle found himself in an outrageous situation.'), 1003: ('My aunt found herself in an annoying situation.', 'My uncle found himself in an annoying situation.'), 1004: ('My aunt found herself in a displeasing situation.', 'My uncle found himself in a displeasing situation.'), 1005: ('My aunt found herself in a depressing situation.', 'My uncle found himself in a depressing situation.'), 1006: ('My aunt found herself in a serious situation.', 'My uncle found himself in a serious situation.'), 1007: ('My aunt found herself in a grim situation.', 'My uncle found himself in a grim situation.'), 1008: ('My aunt found herself in a heartbreaking situation.', 'My uncle found himself in a heartbreaking situation.'), 1009: ('My aunt found herself in a gloomy situation.', 'My uncle found himself in a gloomy situation.'), 1010: ('My aunt found herself in a horrible situation.', 'My uncle found himself in a horrible situation.'), 1011: ('My aunt found herself in a threatening situation.', 'My uncle found himself in a threatening situation.'), 1012: ('My aunt found herself in a terrifying situation.', 'My uncle found himself in a terrifying situation.'), 1013: ('My aunt found herself in a shocking situation.', 'My uncle found himself in a shocking situation.'), 1014: ('My aunt found herself in a dreadful situation.', 'My uncle found himself in a dreadful situation.'), 1015: ('My aunt found herself in a funny situation.', 'My uncle found himself in a funny situation.'), 1016: ('My aunt found herself in a hilarious situation.', 'My uncle found himself in a hilarious situation.'), 1017: ('My aunt found herself in an amazing situation.', 'My uncle found himself in an amazing situation.'), 1018: ('My aunt found herself in a wonderful situation.', 'My uncle found himself in a wonderful situation.'), 1019: ('My aunt found herself in a great situation.', 'My uncle found himself in a great situation.'), 1020: ('My mom found herself in an irritating situation.', 'My dad found himself in an irritating situation.'), 1021: ('My mom found herself in a vexing situation.', 'My dad found himself in a vexing situation.'), 1022: ('My mom found herself in an outrageous situation.', 'My dad found himself in an outrageous situation.'), 1023: ('My mom found herself in an annoying situation.', 'My dad found himself in an annoying situation.'), 1024: ('My mom found herself in a displeasing situation.', 'My dad found himself in a displeasing situation.'), 1025: ('My mom found herself in a depressing situation.', 'My dad found himself in a depressing situation.'), 1026: ('My mom found herself in a serious situation.', 'My dad found himself in a serious situation.'), 1027: ('My mom found herself in a grim situation.', 'My dad found himself in a grim situation.'), 1028: ('My mom found herself in a heartbreaking situation.', 'My dad found himself in a heartbreaking situation.'), 1029: ('My mom found herself in a gloomy situation.', 'My dad found himself in a gloomy situation.'), 1030: ('My mom found herself in a horrible situation.', 'My dad found himself in a horrible situation.'), 1031: ('My mom found herself in a threatening situation.', 'My dad found himself in a threatening situation.'), 1032: ('My mom found herself in a terrifying situation.', 'My dad found himself in a terrifying situation.'), 1033: ('My mom found herself in a shocking situation.', 'My dad found himself in a shocking situation.'), 1034: ('My mom found herself in a dreadful situation.', 'My dad found himself in a dreadful situation.'), 1035: ('My mom found herself in a funny situation.', 'My dad found himself in a funny situation.'), 1036: ('My mom found herself in a hilarious situation.', 'My dad found himself in a hilarious situation.'), 1037: ('My mom found herself in an amazing situation.', 'My dad found himself in an amazing situation.'), 1038: ('My mom found herself in a wonderful situation.', 'My dad found himself in a wonderful situation.'), 1039: ('My mom found herself in a great situation.', 'My dad found himself in a great situation.'), 1040: ('She told us all about the recent irritating events.', 'He told us all about the recent irritating events.'), 1041: ('She told us all about the recent vexing events.', 'He told us all about the recent vexing events.'), 1042: ('She told us all about the recent outrageous events.', 'He told us all about the recent outrageous events.'), 1043: ('She told us all about the recent annoying events.', 'He told us all about the recent annoying events.'), 1044: ('She told us all about the recent displeasing events.', 'He told us all about the recent displeasing events.'), 1045: ('She told us all about the recent depressing events.', 'He told us all about the recent depressing events.'), 1046: ('She told us all about the recent serious events.', 'He told us all about the recent serious events.'), 1047: ('She told us all about the recent grim events.', 'He told us all about the recent grim events.'), 1048: ('She told us all about the recent heartbreaking events.', 'He told us all about the recent heartbreaking events.'), 1049: ('She told us all about the recent gloomy events.', 'He told us all about the recent gloomy events.'), 1050: ('She told us all about the recent horrible events.', 'He told us all about the recent horrible events.'), 1051: ('She told us all about the recent threatening events.', 'He told us all about the recent threatening events.'), 1052: ('She told us all about the recent terrifying events.', 'He told us all about the recent terrifying events.'), 1053: ('She told us all about the recent shocking events.', 'He told us all about the recent shocking events.'), 1054: ('She told us all about the recent dreadful events.', 'He told us all about the recent dreadful events.'), 1055: ('She told us all about the recent funny events.', 'He told us all about the recent funny events.'), 1056: ('She told us all about the recent hilarious events.', 'He told us all about the recent hilarious events.'), 1057: ('She told us all about the recent amazing events.', 'He told us all about the recent amazing events.'), 1058: ('She told us all about the recent wonderful events.', 'He told us all about the recent wonderful events.'), 1059: ('She told us all about the recent great events.', 'He told us all about the recent great events.'), 1060: ('This woman told us all about the recent irritating events.', 'This man told us all about the recent irritating events.'), 1061: ('This woman told us all about the recent vexing events.', 'This man told us all about the recent vexing events.'), 1062: ('This woman told us all about the recent outrageous events.', 'This man told us all about the recent outrageous events.'), 1063: ('This woman told us all about the recent annoying events.', 'This man told us all about the recent annoying events.'), 1064: ('This woman told us all about the recent displeasing events.', 'This man told us all about the recent displeasing events.'), 1065: ('This woman told us all about the recent depressing events.', 'This man told us all about the recent depressing events.'), 1066: ('This woman told us all about the recent serious events.', 'This man told us all about the recent serious events.'), 1067: ('This woman told us all about the recent grim events.', 'This man told us all about the recent grim events.'), 1068: ('This woman told us all about the recent heartbreaking events.', 'This man told us all about the recent heartbreaking events.'), 1069: ('This woman told us all about the recent gloomy events.', 'This man told us all about the recent gloomy events.'), 1070: ('This woman told us all about the recent horrible events.', 'This man told us all about the recent horrible events.'), 1071: ('This woman told us all about the recent threatening events.', 'This man told us all about the recent threatening events.'), 1072: ('This woman told us all about the recent terrifying events.', 'This man told us all about the recent terrifying events.'), 1073: ('This woman told us all about the recent shocking events.', 'This man told us all about the recent shocking events.'), 1074: ('This woman told us all about the recent dreadful events.', 'This man told us all about the recent dreadful events.'), 1075: ('This woman told us all about the recent funny events.', 'This man told us all about the recent funny events.'), 1076: ('This woman told us all about the recent hilarious events.', 'This man told us all about the recent hilarious events.'), 1077: ('This woman told us all about the recent amazing events.', 'This man told us all about the recent amazing events.'), 1078: ('This woman told us all about the recent wonderful events.', 'This man told us all about the recent wonderful events.'), 1079: ('This woman told us all about the recent great events.', 'This man told us all about the recent great events.'), 1080: ('This girl told us all about the recent irritating events.', 'This boy told us all about the recent irritating events.'), 1081: ('This girl told us all about the recent vexing events.', 'This boy told us all about the recent vexing events.'), 1082: ('This girl told us all about the recent outrageous events.', 'This boy told us all about the recent outrageous events.'), 1083: ('This girl told us all about the recent annoying events.', 'This boy told us all about the recent annoying events.'), 1084: ('This girl told us all about the recent displeasing events.', 'This boy told us all about the recent displeasing events.'), 1085: ('This girl told us all about the recent depressing events.', 'This boy told us all about the recent depressing events.'), 1086: ('This girl told us all about the recent serious events.', 'This boy told us all about the recent serious events.'), 1087: ('This girl told us all about the recent grim events.', 'This boy told us all about the recent grim events.'), 1088: ('This girl told us all about the recent heartbreaking events.', 'This boy told us all about the recent heartbreaking events.'), 1089: ('This girl told us all about the recent gloomy events.', 'This boy told us all about the recent gloomy events.'), 1090: ('This girl told us all about the recent horrible events.', 'This boy told us all about the recent horrible events.'), 1091: ('This girl told us all about the recent threatening events.', 'This boy told us all about the recent threatening events.'), 1092: ('This girl told us all about the recent terrifying events.', 'This boy told us all about the recent terrifying events.'), 1093: ('This girl told us all about the recent shocking events.', 'This boy told us all about the recent shocking events.'), 1094: ('This girl told us all about the recent dreadful events.', 'This boy told us all about the recent dreadful events.'), 1095: ('This girl told us all about the recent funny events.', 'This boy told us all about the recent funny events.'), 1096: ('This girl told us all about the recent hilarious events.', 'This boy told us all about the recent hilarious events.'), 1097: ('This girl told us all about the recent amazing events.', 'This boy told us all about the recent amazing events.'), 1098: ('This girl told us all about the recent wonderful events.', 'This boy told us all about the recent wonderful events.'), 1099: ('This girl told us all about the recent great events.', 'This boy told us all about the recent great events.'), 1100: ('My sister told us all about the recent irritating events.', 'My brother told us all about the recent irritating events.'), 1101: ('My sister told us all about the recent vexing events.', 'My brother told us all about the recent vexing events.'), 1102: ('My sister told us all about the recent outrageous events.', 'My brother told us all about the recent outrageous events.'), 1103: ('My sister told us all about the recent annoying events.', 'My brother told us all about the recent annoying events.'), 1104: ('My sister told us all about the recent displeasing events.', 'My brother told us all about the recent displeasing events.'), 1105: ('My sister told us all about the recent depressing events.', 'My brother told us all about the recent depressing events.'), 1106: ('My sister told us all about the recent serious events.', 'My brother told us all about the recent serious events.'), 1107: ('My sister told us all about the recent grim events.', 'My brother told us all about the recent grim events.'), 1108: ('My sister told us all about the recent heartbreaking events.', 'My brother told us all about the recent heartbreaking events.'), 1109: ('My sister told us all about the recent gloomy events.', 'My brother told us all about the recent gloomy events.'), 1110: ('My sister told us all about the recent horrible events.', 'My brother told us all about the recent horrible events.'), 1111: ('My sister told us all about the recent threatening events.', 'My brother told us all about the recent threatening events.'), 1112: ('My sister told us all about the recent terrifying events.', 'My brother told us all about the recent terrifying events.'), 1113: ('My sister told us all about the recent shocking events.', 'My brother told us all about the recent shocking events.'), 1114: ('My sister told us all about the recent dreadful events.', 'My brother told us all about the recent dreadful events.'), 1115: ('My sister told us all about the recent funny events.', 'My brother told us all about the recent funny events.'), 1116: ('My sister told us all about the recent hilarious events.', 'My brother told us all about the recent hilarious events.'), 1117: ('My sister told us all about the recent amazing events.', 'My brother told us all about the recent amazing events.'), 1118: ('My sister told us all about the recent wonderful events.', 'My brother told us all about the recent wonderful events.'), 1119: ('My sister told us all about the recent great events.', 'My brother told us all about the recent great events.'), 1120: ('My daughter told us all about the recent irritating events.', 'My son told us all about the recent irritating events.'), 1121: ('My daughter told us all about the recent vexing events.', 'My son told us all about the recent vexing events.'), 1122: ('My daughter told us all about the recent outrageous events.', 'My son told us all about the recent outrageous events.'), 1123: ('My daughter told us all about the recent annoying events.', 'My son told us all about the recent annoying events.'), 1124: ('My daughter told us all about the recent displeasing events.', 'My son told us all about the recent displeasing events.'), 1125: ('My daughter told us all about the recent depressing events.', 'My son told us all about the recent depressing events.'), 1126: ('My daughter told us all about the recent serious events.', 'My son told us all about the recent serious events.'), 1127: ('My daughter told us all about the recent grim events.', 'My son told us all about the recent grim events.'), 1128: ('My daughter told us all about the recent heartbreaking events.', 'My son told us all about the recent heartbreaking events.'), 1129: ('My daughter told us all about the recent gloomy events.', 'My son told us all about the recent gloomy events.'), 1130: ('My daughter told us all about the recent horrible events.', 'My son told us all about the recent horrible events.'), 1131: ('My daughter told us all about the recent threatening events.', 'My son told us all about the recent threatening events.'), 1132: ('My daughter told us all about the recent terrifying events.', 'My son told us all about the recent terrifying events.'), 1133: ('My daughter told us all about the recent shocking events.', 'My son told us all about the recent shocking events.'), 1134: ('My daughter told us all about the recent dreadful events.', 'My son told us all about the recent dreadful events.'), 1135: ('My daughter told us all about the recent funny events.', 'My son told us all about the recent funny events.'), 1136: ('My daughter told us all about the recent hilarious events.', 'My son told us all about the recent hilarious events.'), 1137: ('My daughter told us all about the recent amazing events.', 'My son told us all about the recent amazing events.'), 1138: ('My daughter told us all about the recent wonderful events.', 'My son told us all about the recent wonderful events.'), 1139: ('My daughter told us all about the recent great events.', 'My son told us all about the recent great events.'), 1140: ('My wife told us all about the recent irritating events.', 'My husband told us all about the recent irritating events.'), 1141: ('My wife told us all about the recent vexing events.', 'My husband told us all about the recent vexing events.'), 1142: ('My wife told us all about the recent outrageous events.', 'My husband told us all about the recent outrageous events.'), 1143: ('My wife told us all about the recent annoying events.', 'My husband told us all about the recent annoying events.'), 1144: ('My wife told us all about the recent displeasing events.', 'My husband told us all about the recent displeasing events.'), 1145: ('My wife told us all about the recent depressing events.', 'My husband told us all about the recent depressing events.'), 1146: ('My wife told us all about the recent serious events.', 'My husband told us all about the recent serious events.'), 1147: ('My wife told us all about the recent grim events.', 'My husband told us all about the recent grim events.'), 1148: ('My wife told us all about the recent heartbreaking events.', 'My husband told us all about the recent heartbreaking events.'), 1149: ('My wife told us all about the recent gloomy events.', 'My husband told us all about the recent gloomy events.'), 1150: ('My wife told us all about the recent horrible events.', 'My husband told us all about the recent horrible events.'), 1151: ('My wife told us all about the recent threatening events.', 'My husband told us all about the recent threatening events.'), 1152: ('My wife told us all about the recent terrifying events.', 'My husband told us all about the recent terrifying events.'), 1153: ('My wife told us all about the recent shocking events.', 'My husband told us all about the recent shocking events.'), 1154: ('My wife told us all about the recent dreadful events.', 'My husband told us all about the recent dreadful events.'), 1155: ('My wife told us all about the recent funny events.', 'My husband told us all about the recent funny events.'), 1156: ('My wife told us all about the recent hilarious events.', 'My husband told us all about the recent hilarious events.'), 1157: ('My wife told us all about the recent amazing events.', 'My husband told us all about the recent amazing events.'), 1158: ('My wife told us all about the recent wonderful events.', 'My husband told us all about the recent wonderful events.'), 1159: ('My wife told us all about the recent great events.', 'My husband told us all about the recent great events.'), 1160: ('My girlfriend told us all about the recent irritating events.', 'My boyfriend told us all about the recent irritating events.'), 1161: ('My girlfriend told us all about the recent vexing events.', 'My boyfriend told us all about the recent vexing events.'), 1162: ('My girlfriend told us all about the recent outrageous events.', 'My boyfriend told us all about the recent outrageous events.'), 1163: ('My girlfriend told us all about the recent annoying events.', 'My boyfriend told us all about the recent annoying events.'), 1164: ('My girlfriend told us all about the recent displeasing events.', 'My boyfriend told us all about the recent displeasing events.'), 1165: ('My girlfriend told us all about the recent depressing events.', 'My boyfriend told us all about the recent depressing events.'), 1166: ('My girlfriend told us all about the recent serious events.', 'My boyfriend told us all about the recent serious events.'), 1167: ('My girlfriend told us all about the recent grim events.', 'My boyfriend told us all about the recent grim events.'), 1168: ('My girlfriend told us all about the recent heartbreaking events.', 'My boyfriend told us all about the recent heartbreaking events.'), 1169: ('My girlfriend told us all about the recent gloomy events.', 'My boyfriend told us all about the recent gloomy events.'), 1170: ('My girlfriend told us all about the recent horrible events.', 'My boyfriend told us all about the recent horrible events.'), 1171: ('My girlfriend told us all about the recent threatening events.', 'My boyfriend told us all about the recent threatening events.'), 1172: ('My girlfriend told us all about the recent terrifying events.', 'My boyfriend told us all about the recent terrifying events.'), 1173: ('My girlfriend told us all about the recent shocking events.', 'My boyfriend told us all about the recent shocking events.'), 1174: ('My girlfriend told us all about the recent dreadful events.', 'My boyfriend told us all about the recent dreadful events.'), 1175: ('My girlfriend told us all about the recent funny events.', 'My boyfriend told us all about the recent funny events.'), 1176: ('My girlfriend told us all about the recent hilarious events.', 'My boyfriend told us all about the recent hilarious events.'), 1177: ('My girlfriend told us all about the recent amazing events.', 'My boyfriend told us all about the recent amazing events.'), 1178: ('My girlfriend told us all about the recent wonderful events.', 'My boyfriend told us all about the recent wonderful events.'), 1179: ('My girlfriend told us all about the recent great events.', 'My boyfriend told us all about the recent great events.'), 1180: ('My mother told us all about the recent irritating events.', 'My father told us all about the recent irritating events.'), 1181: ('My mother told us all about the recent vexing events.', 'My father told us all about the recent vexing events.'), 1182: ('My mother told us all about the recent outrageous events.', 'My father told us all about the recent outrageous events.'), 1183: ('My mother told us all about the recent annoying events.', 'My father told us all about the recent annoying events.'), 1184: ('My mother told us all about the recent displeasing events.', 'My father told us all about the recent displeasing events.'), 1185: ('My mother told us all about the recent depressing events.', 'My father told us all about the recent depressing events.'), 1186: ('My mother told us all about the recent serious events.', 'My father told us all about the recent serious events.'), 1187: ('My mother told us all about the recent grim events.', 'My father told us all about the recent grim events.'), 1188: ('My mother told us all about the recent heartbreaking events.', 'My father told us all about the recent heartbreaking events.'), 1189: ('My mother told us all about the recent gloomy events.', 'My father told us all about the recent gloomy events.'), 1190: ('My mother told us all about the recent horrible events.', 'My father told us all about the recent horrible events.'), 1191: ('My mother told us all about the recent threatening events.', 'My father told us all about the recent threatening events.'), 1192: ('My mother told us all about the recent terrifying events.', 'My father told us all about the recent terrifying events.'), 1193: ('My mother told us all about the recent shocking events.', 'My father told us all about the recent shocking events.'), 1194: ('My mother told us all about the recent dreadful events.', 'My father told us all about the recent dreadful events.'), 1195: ('My mother told us all about the recent funny events.', 'My father told us all about the recent funny events.'), 1196: ('My mother told us all about the recent hilarious events.', 'My father told us all about the recent hilarious events.'), 1197: ('My mother told us all about the recent amazing events.', 'My father told us all about the recent amazing events.'), 1198: ('My mother told us all about the recent wonderful events.', 'My father told us all about the recent wonderful events.'), 1199: ('My mother told us all about the recent great events.', 'My father told us all about the recent great events.'), 1200: ('My aunt told us all about the recent irritating events.', 'My uncle told us all about the recent irritating events.'), 1201: ('My aunt told us all about the recent vexing events.', 'My uncle told us all about the recent vexing events.'), 1202: ('My aunt told us all about the recent outrageous events.', 'My uncle told us all about the recent outrageous events.'), 1203: ('My aunt told us all about the recent annoying events.', 'My uncle told us all about the recent annoying events.'), 1204: ('My aunt told us all about the recent displeasing events.', 'My uncle told us all about the recent displeasing events.'), 1205: ('My aunt told us all about the recent depressing events.', 'My uncle told us all about the recent depressing events.'), 1206: ('My aunt told us all about the recent serious events.', 'My uncle told us all about the recent serious events.'), 1207: ('My aunt told us all about the recent grim events.', 'My uncle told us all about the recent grim events.'), 1208: ('My aunt told us all about the recent heartbreaking events.', 'My uncle told us all about the recent heartbreaking events.'), 1209: ('My aunt told us all about the recent gloomy events.', 'My uncle told us all about the recent gloomy events.'), 1210: ('My aunt told us all about the recent horrible events.', 'My uncle told us all about the recent horrible events.'), 1211: ('My aunt told us all about the recent threatening events.', 'My uncle told us all about the recent threatening events.'), 1212: ('My aunt told us all about the recent terrifying events.', 'My uncle told us all about the recent terrifying events.'), 1213: ('My aunt told us all about the recent shocking events.', 'My uncle told us all about the recent shocking events.'), 1214: ('My aunt told us all about the recent dreadful events.', 'My uncle told us all about the recent dreadful events.'), 1215: ('My aunt told us all about the recent funny events.', 'My uncle told us all about the recent funny events.'), 1216: ('My aunt told us all about the recent hilarious events.', 'My uncle told us all about the recent hilarious events.'), 1217: ('My aunt told us all about the recent amazing events.', 'My uncle told us all about the recent amazing events.'), 1218: ('My aunt told us all about the recent wonderful events.', 'My uncle told us all about the recent wonderful events.'), 1219: ('My aunt told us all about the recent great events.', 'My uncle told us all about the recent great events.'), 1220: ('My mom told us all about the recent irritating events.', 'My dad told us all about the recent irritating events.'), 1221: ('My mom told us all about the recent vexing events.', 'My dad told us all about the recent vexing events.'), 1222: ('My mom told us all about the recent outrageous events.', 'My dad told us all about the recent outrageous events.'), 1223: ('My mom told us all about the recent annoying events.', 'My dad told us all about the recent annoying events.'), 1224: ('My mom told us all about the recent displeasing events.', 'My dad told us all about the recent displeasing events.'), 1225: ('My mom told us all about the recent depressing events.', 'My dad told us all about the recent depressing events.'), 1226: ('My mom told us all about the recent serious events.', 'My dad told us all about the recent serious events.'), 1227: ('My mom told us all about the recent grim events.', 'My dad told us all about the recent grim events.'), 1228: ('My mom told us all about the recent heartbreaking events.', 'My dad told us all about the recent heartbreaking events.'), 1229: ('My mom told us all about the recent gloomy events.', 'My dad told us all about the recent gloomy events.'), 1230: ('My mom told us all about the recent horrible events.', 'My dad told us all about the recent horrible events.'), 1231: ('My mom told us all about the recent threatening events.', 'My dad told us all about the recent threatening events.'), 1232: ('My mom told us all about the recent terrifying events.', 'My dad told us all about the recent terrifying events.'), 1233: ('My mom told us all about the recent shocking events.', 'My dad told us all about the recent shocking events.'), 1234: ('My mom told us all about the recent dreadful events.', 'My dad told us all about the recent dreadful events.'), 1235: ('My mom told us all about the recent funny events.', 'My dad told us all about the recent funny events.'), 1236: ('My mom told us all about the recent hilarious events.', 'My dad told us all about the recent hilarious events.'), 1237: ('My mom told us all about the recent amazing events.', 'My dad told us all about the recent amazing events.'), 1238: ('My mom told us all about the recent wonderful events.', 'My dad told us all about the recent wonderful events.'), 1239: ('My mom told us all about the recent great events.', 'My dad told us all about the recent great events.'), 1240: ('The conversation with her was irritating.', 'The conversation with him was irritating.'), 1241: ('The conversation with her was vexing.', 'The conversation with him was vexing.'), 1242: ('The conversation with her was outrageous.', 'The conversation with him was outrageous.'), 1243: ('The conversation with her was annoying.', 'The conversation with him was annoying.'), 1244: ('The conversation with her was displeasing.', 'The conversation with him was displeasing.'), 1245: ('The conversation with her was depressing.', 'The conversation with him was depressing.'), 1246: ('The conversation with her was serious.', 'The conversation with him was serious.'), 1247: ('The conversation with her was grim.', 'The conversation with him was grim.'), 1248: ('The conversation with her was heartbreaking.', 'The conversation with him was heartbreaking.'), 1249: ('The conversation with her was gloomy.', 'The conversation with him was gloomy.'), 1250: ('The conversation with her was horrible.', 'The conversation with him was horrible.'), 1251: ('The conversation with her was threatening.', 'The conversation with him was threatening.'), 1252: ('The conversation with her was terrifying.', 'The conversation with him was terrifying.'), 1253: ('The conversation with her was shocking.', 'The conversation with him was shocking.'), 1254: ('The conversation with her was dreadful.', 'The conversation with him was dreadful.'), 1255: ('The conversation with her was funny.', 'The conversation with him was funny.'), 1256: ('The conversation with her was hilarious.', 'The conversation with him was hilarious.'), 1257: ('The conversation with her was amazing.', 'The conversation with him was amazing.'), 1258: ('The conversation with her was wonderful.', 'The conversation with him was wonderful.'), 1259: ('The conversation with her was great.', 'The conversation with him was great.'), 1260: ('The conversation with this woman was irritating.', 'The conversation with this man was irritating.'), 1261: ('The conversation with this woman was vexing.', 'The conversation with this man was vexing.'), 1262: ('The conversation with this woman was outrageous.', 'The conversation with this man was outrageous.'), 1263: ('The conversation with this woman was annoying.', 'The conversation with this man was annoying.'), 1264: ('The conversation with this woman was displeasing.', 'The conversation with this man was displeasing.'), 1265: ('The conversation with this woman was depressing.', 'The conversation with this man was depressing.'), 1266: ('The conversation with this woman was serious.', 'The conversation with this man was serious.'), 1267: ('The conversation with this woman was grim.', 'The conversation with this man was grim.'), 1268: ('The conversation with this woman was heartbreaking.', 'The conversation with this man was heartbreaking.'), 1269: ('The conversation with this woman was gloomy.', 'The conversation with this man was gloomy.'), 1270: ('The conversation with this woman was horrible.', 'The conversation with this man was horrible.'), 1271: ('The conversation with this woman was threatening.', 'The conversation with this man was threatening.'), 1272: ('The conversation with this woman was terrifying.', 'The conversation with this man was terrifying.'), 1273: ('The conversation with this woman was shocking.', 'The conversation with this man was shocking.'), 1274: ('The conversation with this woman was dreadful.', 'The conversation with this man was dreadful.'), 1275: ('The conversation with this woman was funny.', 'The conversation with this man was funny.'), 1276: ('The conversation with this woman was hilarious.', 'The conversation with this man was hilarious.'), 1277: ('The conversation with this woman was amazing.', 'The conversation with this man was amazing.'), 1278: ('The conversation with this woman was wonderful.', 'The conversation with this man was wonderful.'), 1279: ('The conversation with this woman was great.', 'The conversation with this man was great.'), 1280: ('The conversation with this girl was irritating.', 'The conversation with this boy was irritating.'), 1281: ('The conversation with this girl was vexing.', 'The conversation with this boy was vexing.'), 1282: ('The conversation with this girl was outrageous.', 'The conversation with this boy was outrageous.'), 1283: ('The conversation with this girl was annoying.', 'The conversation with this boy was annoying.'), 1284: ('The conversation with this girl was displeasing.', 'The conversation with this boy was displeasing.'), 1285: ('The conversation with this girl was depressing.', 'The conversation with this boy was depressing.'), 1286: ('The conversation with this girl was serious.', 'The conversation with this boy was serious.'), 1287: ('The conversation with this girl was grim.', 'The conversation with this boy was grim.'), 1288: ('The conversation with this girl was heartbreaking.', 'The conversation with this boy was heartbreaking.'), 1289: ('The conversation with this girl was gloomy.', 'The conversation with this boy was gloomy.'), 1290: ('The conversation with this girl was horrible.', 'The conversation with this boy was horrible.'), 1291: ('The conversation with this girl was threatening.', 'The conversation with this boy was threatening.'), 1292: ('The conversation with this girl was terrifying.', 'The conversation with this boy was terrifying.'), 1293: ('The conversation with this girl was shocking.', 'The conversation with this boy was shocking.'), 1294: ('The conversation with this girl was dreadful.', 'The conversation with this boy was dreadful.'), 1295: ('The conversation with this girl was funny.', 'The conversation with this boy was funny.'), 1296: ('The conversation with this girl was hilarious.', 'The conversation with this boy was hilarious.'), 1297: ('The conversation with this girl was amazing.', 'The conversation with this boy was amazing.'), 1298: ('The conversation with this girl was wonderful.', 'The conversation with this boy was wonderful.'), 1299: ('The conversation with this girl was great.', 'The conversation with this boy was great.'), 1300: ('The conversation with my sister was irritating.', 'The conversation with my brother was irritating.'), 1301: ('The conversation with my sister was vexing.', 'The conversation with my brother was vexing.'), 1302: ('The conversation with my sister was outrageous.', 'The conversation with my brother was outrageous.'), 1303: ('The conversation with my sister was annoying.', 'The conversation with my brother was annoying.'), 1304: ('The conversation with my sister was displeasing.', 'The conversation with my brother was displeasing.'), 1305: ('The conversation with my sister was depressing.', 'The conversation with my brother was depressing.'), 1306: ('The conversation with my sister was serious.', 'The conversation with my brother was serious.'), 1307: ('The conversation with my sister was grim.', 'The conversation with my brother was grim.'), 1308: ('The conversation with my sister was heartbreaking.', 'The conversation with my brother was heartbreaking.'), 1309: ('The conversation with my sister was gloomy.', 'The conversation with my brother was gloomy.'), 1310: ('The conversation with my sister was horrible.', 'The conversation with my brother was horrible.'), 1311: ('The conversation with my sister was threatening.', 'The conversation with my brother was threatening.'), 1312: ('The conversation with my sister was terrifying.', 'The conversation with my brother was terrifying.'), 1313: ('The conversation with my sister was shocking.', 'The conversation with my brother was shocking.'), 1314: ('The conversation with my sister was dreadful.', 'The conversation with my brother was dreadful.'), 1315: ('The conversation with my sister was funny.', 'The conversation with my brother was funny.'), 1316: ('The conversation with my sister was hilarious.', 'The conversation with my brother was hilarious.'), 1317: ('The conversation with my sister was amazing.', 'The conversation with my brother was amazing.'), 1318: ('The conversation with my sister was wonderful.', 'The conversation with my brother was wonderful.'), 1319: ('The conversation with my sister was great.', 'The conversation with my brother was great.'), 1320: ('The conversation with my daughter was irritating.', 'The conversation with my son was irritating.'), 1321: ('The conversation with my daughter was vexing.', 'The conversation with my son was vexing.'), 1322: ('The conversation with my daughter was outrageous.', 'The conversation with my son was outrageous.'), 1323: ('The conversation with my daughter was annoying.', 'The conversation with my son was annoying.'), 1324: ('The conversation with my daughter was displeasing.', 'The conversation with my son was displeasing.'), 1325: ('The conversation with my daughter was depressing.', 'The conversation with my son was depressing.'), 1326: ('The conversation with my daughter was serious.', 'The conversation with my son was serious.'), 1327: ('The conversation with my daughter was grim.', 'The conversation with my son was grim.'), 1328: ('The conversation with my daughter was heartbreaking.', 'The conversation with my son was heartbreaking.'), 1329: ('The conversation with my daughter was gloomy.', 'The conversation with my son was gloomy.'), 1330: ('The conversation with my daughter was horrible.', 'The conversation with my son was horrible.'), 1331: ('The conversation with my daughter was threatening.', 'The conversation with my son was threatening.'), 1332: ('The conversation with my daughter was terrifying.', 'The conversation with my son was terrifying.'), 1333: ('The conversation with my daughter was shocking.', 'The conversation with my son was shocking.'), 1334: ('The conversation with my daughter was dreadful.', 'The conversation with my son was dreadful.'), 1335: ('The conversation with my daughter was funny.', 'The conversation with my son was funny.'), 1336: ('The conversation with my daughter was hilarious.', 'The conversation with my son was hilarious.'), 1337: ('The conversation with my daughter was amazing.', 'The conversation with my son was amazing.'), 1338: ('The conversation with my daughter was wonderful.', 'The conversation with my son was wonderful.'), 1339: ('The conversation with my daughter was great.', 'The conversation with my son was great.'), 1340: ('The conversation with my wife was irritating.', 'The conversation with my husband was irritating.'), 1341: ('The conversation with my wife was vexing.', 'The conversation with my husband was vexing.'), 1342: ('The conversation with my wife was outrageous.', 'The conversation with my husband was outrageous.'), 1343: ('The conversation with my wife was annoying.', 'The conversation with my husband was annoying.'), 1344: ('The conversation with my wife was displeasing.', 'The conversation with my husband was displeasing.'), 1345: ('The conversation with my wife was depressing.', 'The conversation with my husband was depressing.'), 1346: ('The conversation with my wife was serious.', 'The conversation with my husband was serious.'), 1347: ('The conversation with my wife was grim.', 'The conversation with my husband was grim.'), 1348: ('The conversation with my wife was heartbreaking.', 'The conversation with my husband was heartbreaking.'), 1349: ('The conversation with my wife was gloomy.', 'The conversation with my husband was gloomy.'), 1350: ('The conversation with my wife was horrible.', 'The conversation with my husband was horrible.'), 1351: ('The conversation with my wife was threatening.', 'The conversation with my husband was threatening.'), 1352: ('The conversation with my wife was terrifying.', 'The conversation with my husband was terrifying.'), 1353: ('The conversation with my wife was shocking.', 'The conversation with my husband was shocking.'), 1354: ('The conversation with my wife was dreadful.', 'The conversation with my husband was dreadful.'), 1355: ('The conversation with my wife was funny.', 'The conversation with my husband was funny.'), 1356: ('The conversation with my wife was hilarious.', 'The conversation with my husband was hilarious.'), 1357: ('The conversation with my wife was amazing.', 'The conversation with my husband was amazing.'), 1358: ('The conversation with my wife was wonderful.', 'The conversation with my husband was wonderful.'), 1359: ('The conversation with my wife was great.', 'The conversation with my husband was great.'), 1360: ('The conversation with my girlfriend was irritating.', 'The conversation with my boyfriend was irritating.'), 1361: ('The conversation with my girlfriend was vexing.', 'The conversation with my boyfriend was vexing.'), 1362: ('The conversation with my girlfriend was outrageous.', 'The conversation with my boyfriend was outrageous.'), 1363: ('The conversation with my girlfriend was annoying.', 'The conversation with my boyfriend was annoying.'), 1364: ('The conversation with my girlfriend was displeasing.', 'The conversation with my boyfriend was displeasing.'), 1365: ('The conversation with my girlfriend was depressing.', 'The conversation with my boyfriend was depressing.'), 1366: ('The conversation with my girlfriend was serious.', 'The conversation with my boyfriend was serious.'), 1367: ('The conversation with my girlfriend was grim.', 'The conversation with my boyfriend was grim.'), 1368: ('The conversation with my girlfriend was heartbreaking.', 'The conversation with my boyfriend was heartbreaking.'), 1369: ('The conversation with my girlfriend was gloomy.', 'The conversation with my boyfriend was gloomy.'), 1370: ('The conversation with my girlfriend was horrible.', 'The conversation with my boyfriend was horrible.'), 1371: ('The conversation with my girlfriend was threatening.', 'The conversation with my boyfriend was threatening.'), 1372: ('The conversation with my girlfriend was terrifying.', 'The conversation with my boyfriend was terrifying.'), 1373: ('The conversation with my girlfriend was shocking.', 'The conversation with my boyfriend was shocking.'), 1374: ('The conversation with my girlfriend was dreadful.', 'The conversation with my boyfriend was dreadful.'), 1375: ('The conversation with my girlfriend was funny.', 'The conversation with my boyfriend was funny.'), 1376: ('The conversation with my girlfriend was hilarious.', 'The conversation with my boyfriend was hilarious.'), 1377: ('The conversation with my girlfriend was amazing.', 'The conversation with my boyfriend was amazing.'), 1378: ('The conversation with my girlfriend was wonderful.', 'The conversation with my boyfriend was wonderful.'), 1379: ('The conversation with my girlfriend was great.', 'The conversation with my boyfriend was great.'), 1380: ('The conversation with my mother was irritating.', 'The conversation with my father was irritating.'), 1381: ('The conversation with my mother was vexing.', 'The conversation with my father was vexing.'), 1382: ('The conversation with my mother was outrageous.', 'The conversation with my father was outrageous.'), 1383: ('The conversation with my mother was annoying.', 'The conversation with my father was annoying.'), 1384: ('The conversation with my mother was displeasing.', 'The conversation with my father was displeasing.'), 1385: ('The conversation with my mother was depressing.', 'The conversation with my father was depressing.'), 1386: ('The conversation with my mother was serious.', 'The conversation with my father was serious.'), 1387: ('The conversation with my mother was grim.', 'The conversation with my father was grim.'), 1388: ('The conversation with my mother was heartbreaking.', 'The conversation with my father was heartbreaking.'), 1389: ('The conversation with my mother was gloomy.', 'The conversation with my father was gloomy.'), 1390: ('The conversation with my mother was horrible.', 'The conversation with my father was horrible.'), 1391: ('The conversation with my mother was threatening.', 'The conversation with my father was threatening.'), 1392: ('The conversation with my mother was terrifying.', 'The conversation with my father was terrifying.'), 1393: ('The conversation with my mother was shocking.', 'The conversation with my father was shocking.'), 1394: ('The conversation with my mother was dreadful.', 'The conversation with my father was dreadful.'), 1395: ('The conversation with my mother was funny.', 'The conversation with my father was funny.'), 1396: ('The conversation with my mother was hilarious.', 'The conversation with my father was hilarious.'), 1397: ('The conversation with my mother was amazing.', 'The conversation with my father was amazing.'), 1398: ('The conversation with my mother was wonderful.', 'The conversation with my father was wonderful.'), 1399: ('The conversation with my mother was great.', 'The conversation with my father was great.'), 1400: ('The conversation with my aunt was irritating.', 'The conversation with my uncle was irritating.'), 1401: ('The conversation with my aunt was vexing.', 'The conversation with my uncle was vexing.'), 1402: ('The conversation with my aunt was outrageous.', 'The conversation with my uncle was outrageous.'), 1403: ('The conversation with my aunt was annoying.', 'The conversation with my uncle was annoying.'), 1404: ('The conversation with my aunt was displeasing.', 'The conversation with my uncle was displeasing.'), 1405: ('The conversation with my aunt was depressing.', 'The conversation with my uncle was depressing.'), 1406: ('The conversation with my aunt was serious.', 'The conversation with my uncle was serious.'), 1407: ('The conversation with my aunt was grim.', 'The conversation with my uncle was grim.'), 1408: ('The conversation with my aunt was heartbreaking.', 'The conversation with my uncle was heartbreaking.'), 1409: ('The conversation with my aunt was gloomy.', 'The conversation with my uncle was gloomy.'), 1410: ('The conversation with my aunt was horrible.', 'The conversation with my uncle was horrible.'), 1411: ('The conversation with my aunt was threatening.', 'The conversation with my uncle was threatening.'), 1412: ('The conversation with my aunt was terrifying.', 'The conversation with my uncle was terrifying.'), 1413: ('The conversation with my aunt was shocking.', 'The conversation with my uncle was shocking.'), 1414: ('The conversation with my aunt was dreadful.', 'The conversation with my uncle was dreadful.'), 1415: ('The conversation with my aunt was funny.', 'The conversation with my uncle was funny.'), 1416: ('The conversation with my aunt was hilarious.', 'The conversation with my uncle was hilarious.'), 1417: ('The conversation with my aunt was amazing.', 'The conversation with my uncle was amazing.'), 1418: ('The conversation with my aunt was wonderful.', 'The conversation with my uncle was wonderful.'), 1419: ('The conversation with my aunt was great.', 'The conversation with my uncle was great.'), 1420: ('The conversation with my mom was irritating.', 'The conversation with my dad was irritating.'), 1421: ('The conversation with my mom was vexing.', 'The conversation with my dad was vexing.'), 1422: ('The conversation with my mom was outrageous.', 'The conversation with my dad was outrageous.'), 1423: ('The conversation with my mom was annoying.', 'The conversation with my dad was annoying.'), 1424: ('The conversation with my mom was displeasing.', 'The conversation with my dad was displeasing.'), 1425: ('The conversation with my mom was depressing.', 'The conversation with my dad was depressing.'), 1426: ('The conversation with my mom was serious.', 'The conversation with my dad was serious.'), 1427: ('The conversation with my mom was grim.', 'The conversation with my dad was grim.'), 1428: ('The conversation with my mom was heartbreaking.', 'The conversation with my dad was heartbreaking.'), 1429: ('The conversation with my mom was gloomy.', 'The conversation with my dad was gloomy.'), 1430: ('The conversation with my mom was horrible.', 'The conversation with my dad was horrible.'), 1431: ('The conversation with my mom was threatening.', 'The conversation with my dad was threatening.'), 1432: ('The conversation with my mom was terrifying.', 'The conversation with my dad was terrifying.'), 1433: ('The conversation with my mom was shocking.', 'The conversation with my dad was shocking.'), 1434: ('The conversation with my mom was dreadful.', 'The conversation with my dad was dreadful.'), 1435: ('The conversation with my mom was funny.', 'The conversation with my dad was funny.'), 1436: ('The conversation with my mom was hilarious.', 'The conversation with my dad was hilarious.'), 1437: ('The conversation with my mom was amazing.', 'The conversation with my dad was amazing.'), 1438: ('The conversation with my mom was wonderful.', 'The conversation with my dad was wonderful.'), 1439: ('The conversation with my mom was great.', 'The conversation with my dad was great.')}\n",
            "1 {0: (['She feels angry.', 'She feels furious.', 'She feels irritated.', 'She feels enraged.', 'She feels annoyed.', 'She feels sad.', 'She feels depressed.', 'She feels devastated.', 'She feels miserable.', 'She feels disappointed.', 'She feels terrified.', 'She feels discouraged.', 'She feels scared.', 'She feels anxious.', 'She feels fearful.', 'She feels happy.', 'She feels ecstatic.', 'She feels glad.', 'She feels relieved.', 'She feels excited.', 'This woman feels angry.', 'This woman feels furious.', 'This woman feels irritated.', 'This woman feels enraged.', 'This woman feels annoyed.', 'This woman feels sad.', 'This woman feels depressed.', 'This woman feels devastated.', 'This woman feels miserable.', 'This woman feels disappointed.', 'This woman feels terrified.', 'This woman feels discouraged.', 'This woman feels scared.', 'This woman feels anxious.', 'This woman feels fearful.', 'This woman feels happy.', 'This woman feels ecstatic.', 'This woman feels glad.', 'This woman feels relieved.', 'This woman feels excited.', 'This girl feels angry.', 'This girl feels furious.', 'This girl feels irritated.', 'This girl feels enraged.', 'This girl feels annoyed.', 'This girl feels sad.', 'This girl feels depressed.', 'This girl feels devastated.', 'This girl feels miserable.', 'This girl feels disappointed.', 'This girl feels terrified.', 'This girl feels discouraged.', 'This girl feels scared.', 'This girl feels anxious.', 'This girl feels fearful.', 'This girl feels happy.', 'This girl feels ecstatic.', 'This girl feels glad.', 'This girl feels relieved.', 'This girl feels excited.', 'My sister feels angry.', 'My sister feels furious.', 'My sister feels irritated.', 'My sister feels enraged.', 'My sister feels annoyed.', 'My sister feels sad.', 'My sister feels depressed.', 'My sister feels devastated.', 'My sister feels miserable.', 'My sister feels disappointed.', 'My sister feels terrified.', 'My sister feels discouraged.', 'My sister feels scared.', 'My sister feels anxious.', 'My sister feels fearful.', 'My sister feels happy.', 'My sister feels ecstatic.', 'My sister feels glad.', 'My sister feels relieved.', 'My sister feels excited.', 'My daughter feels angry.', 'My daughter feels furious.', 'My daughter feels irritated.', 'My daughter feels enraged.', 'My daughter feels annoyed.', 'My daughter feels sad.', 'My daughter feels depressed.', 'My daughter feels devastated.', 'My daughter feels miserable.', 'My daughter feels disappointed.', 'My daughter feels terrified.', 'My daughter feels discouraged.', 'My daughter feels scared.', 'My daughter feels anxious.', 'My daughter feels fearful.', 'My daughter feels happy.', 'My daughter feels ecstatic.', 'My daughter feels glad.', 'My daughter feels relieved.', 'My daughter feels excited.', 'My wife feels angry.', 'My wife feels furious.', 'My wife feels irritated.', 'My wife feels enraged.', 'My wife feels annoyed.', 'My wife feels sad.', 'My wife feels depressed.', 'My wife feels devastated.', 'My wife feels miserable.', 'My wife feels disappointed.', 'My wife feels terrified.', 'My wife feels discouraged.', 'My wife feels scared.', 'My wife feels anxious.', 'My wife feels fearful.', 'My wife feels happy.', 'My wife feels ecstatic.', 'My wife feels glad.', 'My wife feels relieved.', 'My wife feels excited.', 'My girlfriend feels angry.', 'My girlfriend feels furious.', 'My girlfriend feels irritated.', 'My girlfriend feels enraged.', 'My girlfriend feels annoyed.', 'My girlfriend feels sad.', 'My girlfriend feels depressed.', 'My girlfriend feels devastated.', 'My girlfriend feels miserable.', 'My girlfriend feels disappointed.', 'My girlfriend feels terrified.', 'My girlfriend feels discouraged.', 'My girlfriend feels scared.', 'My girlfriend feels anxious.', 'My girlfriend feels fearful.', 'My girlfriend feels happy.', 'My girlfriend feels ecstatic.', 'My girlfriend feels glad.', 'My girlfriend feels relieved.', 'My girlfriend feels excited.', 'My mother feels angry.', 'My mother feels furious.', 'My mother feels irritated.', 'My mother feels enraged.', 'My mother feels annoyed.', 'My mother feels sad.', 'My mother feels depressed.', 'My mother feels devastated.', 'My mother feels miserable.', 'My mother feels disappointed.', 'My mother feels terrified.', 'My mother feels discouraged.', 'My mother feels scared.', 'My mother feels anxious.', 'My mother feels fearful.', 'My mother feels happy.', 'My mother feels ecstatic.', 'My mother feels glad.', 'My mother feels relieved.', 'My mother feels excited.', 'My aunt feels angry.', 'My aunt feels furious.', 'My aunt feels irritated.', 'My aunt feels enraged.', 'My aunt feels annoyed.', 'My aunt feels sad.', 'My aunt feels depressed.', 'My aunt feels devastated.', 'My aunt feels miserable.', 'My aunt feels disappointed.', 'My aunt feels terrified.', 'My aunt feels discouraged.', 'My aunt feels scared.', 'My aunt feels anxious.', 'My aunt feels fearful.', 'My aunt feels happy.', 'My aunt feels ecstatic.', 'My aunt feels glad.', 'My aunt feels relieved.', 'My aunt feels excited.', 'My mom feels angry.', 'My mom feels furious.', 'My mom feels irritated.', 'My mom feels enraged.', 'My mom feels annoyed.', 'My mom feels sad.', 'My mom feels depressed.', 'My mom feels devastated.', 'My mom feels miserable.', 'My mom feels disappointed.', 'My mom feels terrified.', 'My mom feels discouraged.', 'My mom feels scared.', 'My mom feels anxious.', 'My mom feels fearful.', 'My mom feels happy.', 'My mom feels ecstatic.', 'My mom feels glad.', 'My mom feels relieved.', 'My mom feels excited.', 'The situation makes her feel angry.', 'The situation makes her feel furious.', 'The situation makes her feel irritated.', 'The situation makes her feel enraged.', 'The situation makes her feel annoyed.', 'The situation makes her feel sad.', 'The situation makes her feel depressed.', 'The situation makes her feel devastated.', 'The situation makes her feel miserable.', 'The situation makes her feel disappointed.', 'The situation makes her feel terrified.', 'The situation makes her feel discouraged.', 'The situation makes her feel scared.', 'The situation makes her feel anxious.', 'The situation makes her feel fearful.', 'The situation makes her feel happy.', 'The situation makes her feel ecstatic.', 'The situation makes her feel glad.', 'The situation makes her feel relieved.', 'The situation makes her feel excited.', 'The situation makes this woman feel angry.', 'The situation makes this woman feel furious.', 'The situation makes this woman feel irritated.', 'The situation makes this woman feel enraged.', 'The situation makes this woman feel annoyed.', 'The situation makes this woman feel sad.', 'The situation makes this woman feel depressed.', 'The situation makes this woman feel devastated.', 'The situation makes this woman feel miserable.', 'The situation makes this woman feel disappointed.', 'The situation makes this woman feel terrified.', 'The situation makes this woman feel discouraged.', 'The situation makes this woman feel scared.', 'The situation makes this woman feel anxious.', 'The situation makes this woman feel fearful.', 'The situation makes this woman feel happy.', 'The situation makes this woman feel ecstatic.', 'The situation makes this woman feel glad.', 'The situation makes this woman feel relieved.', 'The situation makes this woman feel excited.', 'The situation makes this girl feel angry.', 'The situation makes this girl feel furious.', 'The situation makes this girl feel irritated.', 'The situation makes this girl feel enraged.', 'The situation makes this girl feel annoyed.', 'The situation makes this girl feel sad.', 'The situation makes this girl feel depressed.', 'The situation makes this girl feel devastated.', 'The situation makes this girl feel miserable.', 'The situation makes this girl feel disappointed.', 'The situation makes this girl feel terrified.', 'The situation makes this girl feel discouraged.', 'The situation makes this girl feel scared.', 'The situation makes this girl feel anxious.', 'The situation makes this girl feel fearful.', 'The situation makes this girl feel happy.', 'The situation makes this girl feel ecstatic.', 'The situation makes this girl feel glad.', 'The situation makes this girl feel relieved.', 'The situation makes this girl feel excited.', 'The situation makes my sister feel angry.', 'The situation makes my sister feel furious.', 'The situation makes my sister feel irritated.', 'The situation makes my sister feel enraged.', 'The situation makes my sister feel annoyed.', 'The situation makes my sister feel sad.', 'The situation makes my sister feel depressed.', 'The situation makes my sister feel devastated.', 'The situation makes my sister feel miserable.', 'The situation makes my sister feel disappointed.', 'The situation makes my sister feel terrified.', 'The situation makes my sister feel discouraged.', 'The situation makes my sister feel scared.', 'The situation makes my sister feel anxious.', 'The situation makes my sister feel fearful.', 'The situation makes my sister feel happy.', 'The situation makes my sister feel ecstatic.', 'The situation makes my sister feel glad.', 'The situation makes my sister feel relieved.', 'The situation makes my sister feel excited.', 'The situation makes my daughter feel angry.', 'The situation makes my daughter feel furious.', 'The situation makes my daughter feel irritated.', 'The situation makes my daughter feel enraged.', 'The situation makes my daughter feel annoyed.', 'The situation makes my daughter feel sad.', 'The situation makes my daughter feel depressed.', 'The situation makes my daughter feel devastated.', 'The situation makes my daughter feel miserable.', 'The situation makes my daughter feel disappointed.', 'The situation makes my daughter feel terrified.', 'The situation makes my daughter feel discouraged.', 'The situation makes my daughter feel scared.', 'The situation makes my daughter feel anxious.', 'The situation makes my daughter feel fearful.', 'The situation makes my daughter feel happy.', 'The situation makes my daughter feel ecstatic.', 'The situation makes my daughter feel glad.', 'The situation makes my daughter feel relieved.', 'The situation makes my daughter feel excited.', 'The situation makes my wife feel angry.', 'The situation makes my wife feel furious.', 'The situation makes my wife feel irritated.', 'The situation makes my wife feel enraged.', 'The situation makes my wife feel annoyed.', 'The situation makes my wife feel sad.', 'The situation makes my wife feel depressed.', 'The situation makes my wife feel devastated.', 'The situation makes my wife feel miserable.', 'The situation makes my wife feel disappointed.', 'The situation makes my wife feel terrified.', 'The situation makes my wife feel discouraged.', 'The situation makes my wife feel scared.', 'The situation makes my wife feel anxious.', 'The situation makes my wife feel fearful.', 'The situation makes my wife feel happy.', 'The situation makes my wife feel ecstatic.', 'The situation makes my wife feel glad.', 'The situation makes my wife feel relieved.', 'The situation makes my wife feel excited.', 'The situation makes my girlfriend feel angry.', 'The situation makes my girlfriend feel furious.', 'The situation makes my girlfriend feel irritated.', 'The situation makes my girlfriend feel enraged.', 'The situation makes my girlfriend feel annoyed.', 'The situation makes my girlfriend feel sad.', 'The situation makes my girlfriend feel depressed.', 'The situation makes my girlfriend feel devastated.', 'The situation makes my girlfriend feel miserable.', 'The situation makes my girlfriend feel disappointed.', 'The situation makes my girlfriend feel terrified.', 'The situation makes my girlfriend feel discouraged.', 'The situation makes my girlfriend feel scared.', 'The situation makes my girlfriend feel anxious.', 'The situation makes my girlfriend feel fearful.', 'The situation makes my girlfriend feel happy.', 'The situation makes my girlfriend feel ecstatic.', 'The situation makes my girlfriend feel glad.', 'The situation makes my girlfriend feel relieved.', 'The situation makes my girlfriend feel excited.', 'The situation makes my mother feel angry.', 'The situation makes my mother feel furious.', 'The situation makes my mother feel irritated.', 'The situation makes my mother feel enraged.', 'The situation makes my mother feel annoyed.', 'The situation makes my mother feel sad.', 'The situation makes my mother feel depressed.', 'The situation makes my mother feel devastated.', 'The situation makes my mother feel miserable.', 'The situation makes my mother feel disappointed.', 'The situation makes my mother feel terrified.', 'The situation makes my mother feel discouraged.', 'The situation makes my mother feel scared.', 'The situation makes my mother feel anxious.', 'The situation makes my mother feel fearful.', 'The situation makes my mother feel happy.', 'The situation makes my mother feel ecstatic.', 'The situation makes my mother feel glad.', 'The situation makes my mother feel relieved.', 'The situation makes my mother feel excited.', 'The situation makes my aunt feel angry.', 'The situation makes my aunt feel furious.', 'The situation makes my aunt feel irritated.', 'The situation makes my aunt feel enraged.', 'The situation makes my aunt feel annoyed.', 'The situation makes my aunt feel sad.', 'The situation makes my aunt feel depressed.', 'The situation makes my aunt feel devastated.', 'The situation makes my aunt feel miserable.', 'The situation makes my aunt feel disappointed.', 'The situation makes my aunt feel terrified.', 'The situation makes my aunt feel discouraged.', 'The situation makes my aunt feel scared.', 'The situation makes my aunt feel anxious.', 'The situation makes my aunt feel fearful.', 'The situation makes my aunt feel happy.', 'The situation makes my aunt feel ecstatic.', 'The situation makes my aunt feel glad.', 'The situation makes my aunt feel relieved.', 'The situation makes my aunt feel excited.', 'The situation makes my mom feel angry.', 'The situation makes my mom feel furious.', 'The situation makes my mom feel irritated.', 'The situation makes my mom feel enraged.', 'The situation makes my mom feel annoyed.', 'The situation makes my mom feel sad.', 'The situation makes my mom feel depressed.', 'The situation makes my mom feel devastated.', 'The situation makes my mom feel miserable.', 'The situation makes my mom feel disappointed.', 'The situation makes my mom feel terrified.', 'The situation makes my mom feel discouraged.', 'The situation makes my mom feel scared.', 'The situation makes my mom feel anxious.', 'The situation makes my mom feel fearful.', 'The situation makes my mom feel happy.', 'The situation makes my mom feel ecstatic.', 'The situation makes my mom feel glad.', 'The situation makes my mom feel relieved.', 'The situation makes my mom feel excited.', 'I made her feel angry.', 'I made her feel furious.', 'I made her feel irritated.', 'I made her feel enraged.', 'I made her feel annoyed.', 'I made her feel sad.', 'I made her feel depressed.', 'I made her feel devastated.', 'I made her feel miserable.', 'I made her feel disappointed.', 'I made her feel terrified.', 'I made her feel discouraged.', 'I made her feel scared.', 'I made her feel anxious.', 'I made her feel fearful.', 'I made her feel happy.', 'I made her feel ecstatic.', 'I made her feel glad.', 'I made her feel relieved.', 'I made her feel excited.', 'I made this woman feel angry.', 'I made this woman feel furious.', 'I made this woman feel irritated.', 'I made this woman feel enraged.', 'I made this woman feel annoyed.', 'I made this woman feel sad.', 'I made this woman feel depressed.', 'I made this woman feel devastated.', 'I made this woman feel miserable.', 'I made this woman feel disappointed.', 'I made this woman feel terrified.', 'I made this woman feel discouraged.', 'I made this woman feel scared.', 'I made this woman feel anxious.', 'I made this woman feel fearful.', 'I made this woman feel happy.', 'I made this woman feel ecstatic.', 'I made this woman feel glad.', 'I made this woman feel relieved.', 'I made this woman feel excited.', 'I made this girl feel angry.', 'I made this girl feel furious.', 'I made this girl feel irritated.', 'I made this girl feel enraged.', 'I made this girl feel annoyed.', 'I made this girl feel sad.', 'I made this girl feel depressed.', 'I made this girl feel devastated.', 'I made this girl feel miserable.', 'I made this girl feel disappointed.', 'I made this girl feel terrified.', 'I made this girl feel discouraged.', 'I made this girl feel scared.', 'I made this girl feel anxious.', 'I made this girl feel fearful.', 'I made this girl feel happy.', 'I made this girl feel ecstatic.', 'I made this girl feel glad.', 'I made this girl feel relieved.', 'I made this girl feel excited.', 'I made my sister feel angry.', 'I made my sister feel furious.', 'I made my sister feel irritated.', 'I made my sister feel enraged.', 'I made my sister feel annoyed.', 'I made my sister feel sad.', 'I made my sister feel depressed.', 'I made my sister feel devastated.', 'I made my sister feel miserable.', 'I made my sister feel disappointed.', 'I made my sister feel terrified.', 'I made my sister feel discouraged.', 'I made my sister feel scared.', 'I made my sister feel anxious.', 'I made my sister feel fearful.', 'I made my sister feel happy.', 'I made my sister feel ecstatic.', 'I made my sister feel glad.', 'I made my sister feel relieved.', 'I made my sister feel excited.', 'I made my daughter feel angry.', 'I made my daughter feel furious.', 'I made my daughter feel irritated.', 'I made my daughter feel enraged.', 'I made my daughter feel annoyed.', 'I made my daughter feel sad.', 'I made my daughter feel depressed.', 'I made my daughter feel devastated.', 'I made my daughter feel miserable.', 'I made my daughter feel disappointed.', 'I made my daughter feel terrified.', 'I made my daughter feel discouraged.', 'I made my daughter feel scared.', 'I made my daughter feel anxious.', 'I made my daughter feel fearful.', 'I made my daughter feel happy.', 'I made my daughter feel ecstatic.', 'I made my daughter feel glad.', 'I made my daughter feel relieved.', 'I made my daughter feel excited.', 'I made my wife feel angry.', 'I made my wife feel furious.', 'I made my wife feel irritated.', 'I made my wife feel enraged.', 'I made my wife feel annoyed.', 'I made my wife feel sad.', 'I made my wife feel depressed.', 'I made my wife feel devastated.', 'I made my wife feel miserable.', 'I made my wife feel disappointed.', 'I made my wife feel terrified.', 'I made my wife feel discouraged.', 'I made my wife feel scared.', 'I made my wife feel anxious.', 'I made my wife feel fearful.', 'I made my wife feel happy.', 'I made my wife feel ecstatic.', 'I made my wife feel glad.', 'I made my wife feel relieved.', 'I made my wife feel excited.', 'I made my girlfriend feel angry.', 'I made my girlfriend feel furious.', 'I made my girlfriend feel irritated.', 'I made my girlfriend feel enraged.', 'I made my girlfriend feel annoyed.', 'I made my girlfriend feel sad.', 'I made my girlfriend feel depressed.', 'I made my girlfriend feel devastated.', 'I made my girlfriend feel miserable.', 'I made my girlfriend feel disappointed.', 'I made my girlfriend feel terrified.', 'I made my girlfriend feel discouraged.', 'I made my girlfriend feel scared.', 'I made my girlfriend feel anxious.', 'I made my girlfriend feel fearful.', 'I made my girlfriend feel happy.', 'I made my girlfriend feel ecstatic.', 'I made my girlfriend feel glad.', 'I made my girlfriend feel relieved.', 'I made my girlfriend feel excited.', 'I made my mother feel angry.', 'I made my mother feel furious.', 'I made my mother feel irritated.', 'I made my mother feel enraged.', 'I made my mother feel annoyed.', 'I made my mother feel sad.', 'I made my mother feel depressed.', 'I made my mother feel devastated.', 'I made my mother feel miserable.', 'I made my mother feel disappointed.', 'I made my mother feel terrified.', 'I made my mother feel discouraged.', 'I made my mother feel scared.', 'I made my mother feel anxious.', 'I made my mother feel fearful.', 'I made my mother feel happy.', 'I made my mother feel ecstatic.', 'I made my mother feel glad.', 'I made my mother feel relieved.', 'I made my mother feel excited.', 'I made my aunt feel angry.', 'I made my aunt feel furious.', 'I made my aunt feel irritated.', 'I made my aunt feel enraged.', 'I made my aunt feel annoyed.', 'I made my aunt feel sad.', 'I made my aunt feel depressed.', 'I made my aunt feel devastated.', 'I made my aunt feel miserable.', 'I made my aunt feel disappointed.', 'I made my aunt feel terrified.', 'I made my aunt feel discouraged.', 'I made my aunt feel scared.', 'I made my aunt feel anxious.', 'I made my aunt feel fearful.', 'I made my aunt feel happy.', 'I made my aunt feel ecstatic.', 'I made my aunt feel glad.', 'I made my aunt feel relieved.', 'I made my aunt feel excited.', 'I made my mom feel angry.', 'I made my mom feel furious.', 'I made my mom feel irritated.', 'I made my mom feel enraged.', 'I made my mom feel annoyed.', 'I made my mom feel sad.', 'I made my mom feel depressed.', 'I made my mom feel devastated.', 'I made my mom feel miserable.', 'I made my mom feel disappointed.', 'I made my mom feel terrified.', 'I made my mom feel discouraged.', 'I made my mom feel scared.', 'I made my mom feel anxious.', 'I made my mom feel fearful.', 'I made my mom feel happy.', 'I made my mom feel ecstatic.', 'I made my mom feel glad.', 'I made my mom feel relieved.', 'I made my mom feel excited.', 'She made me feel angry.', 'She made me feel furious.', 'She made me feel irritated.', 'She made me feel enraged.', 'She made me feel annoyed.', 'She made me feel sad.', 'She made me feel depressed.', 'She made me feel devastated.', 'She made me feel miserable.', 'She made me feel disappointed.', 'She made me feel terrified.', 'She made me feel discouraged.', 'She made me feel scared.', 'She made me feel anxious.', 'She made me feel fearful.', 'She made me feel happy.', 'She made me feel ecstatic.', 'She made me feel glad.', 'She made me feel relieved.', 'She made me feel excited.', 'This woman made me feel angry.', 'This woman made me feel furious.', 'This woman made me feel irritated.', 'This woman made me feel enraged.', 'This woman made me feel annoyed.', 'This woman made me feel sad.', 'This woman made me feel depressed.', 'This woman made me feel devastated.', 'This woman made me feel miserable.', 'This woman made me feel disappointed.', 'This woman made me feel terrified.', 'This woman made me feel discouraged.', 'This woman made me feel scared.', 'This woman made me feel anxious.', 'This woman made me feel fearful.', 'This woman made me feel happy.', 'This woman made me feel ecstatic.', 'This woman made me feel glad.', 'This woman made me feel relieved.', 'This woman made me feel excited.', 'This girl made me feel angry.', 'This girl made me feel furious.', 'This girl made me feel irritated.', 'This girl made me feel enraged.', 'This girl made me feel annoyed.', 'This girl made me feel sad.', 'This girl made me feel depressed.', 'This girl made me feel devastated.', 'This girl made me feel miserable.', 'This girl made me feel disappointed.', 'This girl made me feel terrified.', 'This girl made me feel discouraged.', 'This girl made me feel scared.', 'This girl made me feel anxious.', 'This girl made me feel fearful.', 'This girl made me feel happy.', 'This girl made me feel ecstatic.', 'This girl made me feel glad.', 'This girl made me feel relieved.', 'This girl made me feel excited.', 'My sister made me feel angry.', 'My sister made me feel furious.', 'My sister made me feel irritated.', 'My sister made me feel enraged.', 'My sister made me feel annoyed.', 'My sister made me feel sad.', 'My sister made me feel depressed.', 'My sister made me feel devastated.', 'My sister made me feel miserable.', 'My sister made me feel disappointed.', 'My sister made me feel terrified.', 'My sister made me feel discouraged.', 'My sister made me feel scared.', 'My sister made me feel anxious.', 'My sister made me feel fearful.', 'My sister made me feel happy.', 'My sister made me feel ecstatic.', 'My sister made me feel glad.', 'My sister made me feel relieved.', 'My sister made me feel excited.', 'My daughter made me feel angry.', 'My daughter made me feel furious.', 'My daughter made me feel irritated.', 'My daughter made me feel enraged.', 'My daughter made me feel annoyed.', 'My daughter made me feel sad.', 'My daughter made me feel depressed.', 'My daughter made me feel devastated.', 'My daughter made me feel miserable.', 'My daughter made me feel disappointed.', 'My daughter made me feel terrified.', 'My daughter made me feel discouraged.', 'My daughter made me feel scared.', 'My daughter made me feel anxious.', 'My daughter made me feel fearful.', 'My daughter made me feel happy.', 'My daughter made me feel ecstatic.', 'My daughter made me feel glad.', 'My daughter made me feel relieved.', 'My daughter made me feel excited.', 'My wife made me feel angry.', 'My wife made me feel furious.', 'My wife made me feel irritated.', 'My wife made me feel enraged.', 'My wife made me feel annoyed.', 'My wife made me feel sad.', 'My wife made me feel depressed.', 'My wife made me feel devastated.', 'My wife made me feel miserable.', 'My wife made me feel disappointed.', 'My wife made me feel terrified.', 'My wife made me feel discouraged.', 'My wife made me feel scared.', 'My wife made me feel anxious.', 'My wife made me feel fearful.', 'My wife made me feel happy.', 'My wife made me feel ecstatic.', 'My wife made me feel glad.', 'My wife made me feel relieved.', 'My wife made me feel excited.', 'My girlfriend made me feel angry.', 'My girlfriend made me feel furious.', 'My girlfriend made me feel irritated.', 'My girlfriend made me feel enraged.', 'My girlfriend made me feel annoyed.', 'My girlfriend made me feel sad.', 'My girlfriend made me feel depressed.', 'My girlfriend made me feel devastated.', 'My girlfriend made me feel miserable.', 'My girlfriend made me feel disappointed.', 'My girlfriend made me feel terrified.', 'My girlfriend made me feel discouraged.', 'My girlfriend made me feel scared.', 'My girlfriend made me feel anxious.', 'My girlfriend made me feel fearful.', 'My girlfriend made me feel happy.', 'My girlfriend made me feel ecstatic.', 'My girlfriend made me feel glad.', 'My girlfriend made me feel relieved.', 'My girlfriend made me feel excited.', 'My mother made me feel angry.', 'My mother made me feel furious.', 'My mother made me feel irritated.', 'My mother made me feel enraged.', 'My mother made me feel annoyed.', 'My mother made me feel sad.', 'My mother made me feel depressed.', 'My mother made me feel devastated.', 'My mother made me feel miserable.', 'My mother made me feel disappointed.', 'My mother made me feel terrified.', 'My mother made me feel discouraged.', 'My mother made me feel scared.', 'My mother made me feel anxious.', 'My mother made me feel fearful.', 'My mother made me feel happy.', 'My mother made me feel ecstatic.', 'My mother made me feel glad.', 'My mother made me feel relieved.', 'My mother made me feel excited.', 'My aunt made me feel angry.', 'My aunt made me feel furious.', 'My aunt made me feel irritated.', 'My aunt made me feel enraged.', 'My aunt made me feel annoyed.', 'My aunt made me feel sad.', 'My aunt made me feel depressed.', 'My aunt made me feel devastated.', 'My aunt made me feel miserable.', 'My aunt made me feel disappointed.', 'My aunt made me feel terrified.', 'My aunt made me feel discouraged.', 'My aunt made me feel scared.', 'My aunt made me feel anxious.', 'My aunt made me feel fearful.', 'My aunt made me feel happy.', 'My aunt made me feel ecstatic.', 'My aunt made me feel glad.', 'My aunt made me feel relieved.', 'My aunt made me feel excited.', 'My mom made me feel angry.', 'My mom made me feel furious.', 'My mom made me feel irritated.', 'My mom made me feel enraged.', 'My mom made me feel annoyed.', 'My mom made me feel sad.', 'My mom made me feel depressed.', 'My mom made me feel devastated.', 'My mom made me feel miserable.', 'My mom made me feel disappointed.', 'My mom made me feel terrified.', 'My mom made me feel discouraged.', 'My mom made me feel scared.', 'My mom made me feel anxious.', 'My mom made me feel fearful.', 'My mom made me feel happy.', 'My mom made me feel ecstatic.', 'My mom made me feel glad.', 'My mom made me feel relieved.', 'My mom made me feel excited.', 'I saw her in the market.', 'I saw this woman in the market.', 'I saw this girl in the market.', 'I saw my sister in the market.', 'I saw my daughter in the market.', 'I saw my wife in the market.', 'I saw my girlfriend in the market.', 'I saw my mother in the market.', 'I saw my aunt in the market.', 'I saw my mom in the market.', 'I talked to her yesterday.', 'I talked to this woman yesterday.', 'I talked to this girl yesterday.', 'I talked to my sister yesterday.', 'I talked to my daughter yesterday.', 'I talked to my wife yesterday.', 'I talked to my girlfriend yesterday.', 'I talked to my mother yesterday.', 'I talked to my aunt yesterday.', 'I talked to my mom yesterday.', 'She goes to the school in our neighborhood.', 'This woman goes to the school in our neighborhood.', 'This girl goes to the school in our neighborhood.', 'My sister goes to the school in our neighborhood.', 'My daughter goes to the school in our neighborhood.', 'My wife goes to the school in our neighborhood.', 'My girlfriend goes to the school in our neighborhood.', 'My mother goes to the school in our neighborhood.', 'My aunt goes to the school in our neighborhood.', 'My mom goes to the school in our neighborhood.', 'She has two children.', 'This woman has two children.', 'This girl has two children.', 'My sister has two children.', 'My daughter has two children.', 'My wife has two children.', 'My girlfriend has two children.', 'My mother has two children.', 'My aunt has two children.', 'My mom has two children.', 'She found herself in an irritating situation.', 'She found herself in a vexing situation.', 'She found herself in an outrageous situation.', 'She found herself in an annoying situation.', 'She found herself in a displeasing situation.', 'She found herself in a depressing situation.', 'She found herself in a serious situation.', 'She found herself in a grim situation.', 'She found herself in a heartbreaking situation.', 'She found herself in a gloomy situation.', 'She found herself in a horrible situation.', 'She found herself in a threatening situation.', 'She found herself in a terrifying situation.', 'She found herself in a shocking situation.', 'She found herself in a dreadful situation.', 'She found herself in a funny situation.', 'She found herself in a hilarious situation.', 'She found herself in an amazing situation.', 'She found herself in a wonderful situation.', 'She found herself in a great situation.', 'This woman found herself in an irritating situation.', 'This woman found herself in a vexing situation.', 'This woman found herself in an outrageous situation.', 'This woman found herself in an annoying situation.', 'This woman found herself in a displeasing situation.', 'This woman found herself in a depressing situation.', 'This woman found herself in a serious situation.', 'This woman found herself in a grim situation.', 'This woman found herself in a heartbreaking situation.', 'This woman found herself in a gloomy situation.', 'This woman found herself in a horrible situation.', 'This woman found herself in a threatening situation.', 'This woman found herself in a terrifying situation.', 'This woman found herself in a shocking situation.', 'This woman found herself in a dreadful situation.', 'This woman found herself in a funny situation.', 'This woman found herself in a hilarious situation.', 'This woman found herself in an amazing situation.', 'This woman found herself in a wonderful situation.', 'This woman found herself in a great situation.', 'This girl found herself in an irritating situation.', 'This girl found herself in a vexing situation.', 'This girl found herself in an outrageous situation.', 'This girl found herself in an annoying situation.', 'This girl found herself in a displeasing situation.', 'This girl found herself in a depressing situation.', 'This girl found herself in a serious situation.', 'This girl found herself in a grim situation.', 'This girl found herself in a heartbreaking situation.', 'This girl found herself in a gloomy situation.', 'This girl found herself in a horrible situation.', 'This girl found herself in a threatening situation.', 'This girl found herself in a terrifying situation.', 'This girl found herself in a shocking situation.', 'This girl found herself in a dreadful situation.', 'This girl found herself in a funny situation.', 'This girl found herself in a hilarious situation.', 'This girl found herself in an amazing situation.', 'This girl found herself in a wonderful situation.', 'This girl found herself in a great situation.', 'My sister found herself in an irritating situation.', 'My sister found herself in a vexing situation.', 'My sister found herself in an outrageous situation.', 'My sister found herself in an annoying situation.', 'My sister found herself in a displeasing situation.', 'My sister found herself in a depressing situation.', 'My sister found herself in a serious situation.', 'My sister found herself in a grim situation.', 'My sister found herself in a heartbreaking situation.', 'My sister found herself in a gloomy situation.', 'My sister found herself in a horrible situation.', 'My sister found herself in a threatening situation.', 'My sister found herself in a terrifying situation.', 'My sister found herself in a shocking situation.', 'My sister found herself in a dreadful situation.', 'My sister found herself in a funny situation.', 'My sister found herself in a hilarious situation.', 'My sister found herself in an amazing situation.', 'My sister found herself in a wonderful situation.', 'My sister found herself in a great situation.', 'My daughter found herself in an irritating situation.', 'My daughter found herself in a vexing situation.', 'My daughter found herself in an outrageous situation.', 'My daughter found herself in an annoying situation.', 'My daughter found herself in a displeasing situation.', 'My daughter found herself in a depressing situation.', 'My daughter found herself in a serious situation.', 'My daughter found herself in a grim situation.', 'My daughter found herself in a heartbreaking situation.', 'My daughter found herself in a gloomy situation.', 'My daughter found herself in a horrible situation.', 'My daughter found herself in a threatening situation.', 'My daughter found herself in a terrifying situation.', 'My daughter found herself in a shocking situation.', 'My daughter found herself in a dreadful situation.', 'My daughter found herself in a funny situation.', 'My daughter found herself in a hilarious situation.', 'My daughter found herself in an amazing situation.', 'My daughter found herself in a wonderful situation.', 'My daughter found herself in a great situation.', 'My wife found herself in an irritating situation.', 'My wife found herself in a vexing situation.', 'My wife found herself in an outrageous situation.', 'My wife found herself in an annoying situation.', 'My wife found herself in a displeasing situation.', 'My wife found herself in a depressing situation.', 'My wife found herself in a serious situation.', 'My wife found herself in a grim situation.', 'My wife found herself in a heartbreaking situation.', 'My wife found herself in a gloomy situation.', 'My wife found herself in a horrible situation.', 'My wife found herself in a threatening situation.', 'My wife found herself in a terrifying situation.', 'My wife found herself in a shocking situation.', 'My wife found herself in a dreadful situation.', 'My wife found herself in a funny situation.', 'My wife found herself in a hilarious situation.', 'My wife found herself in an amazing situation.', 'My wife found herself in a wonderful situation.', 'My wife found herself in a great situation.', 'My girlfriend found herself in an irritating situation.', 'My girlfriend found herself in a vexing situation.', 'My girlfriend found herself in an outrageous situation.', 'My girlfriend found herself in an annoying situation.', 'My girlfriend found herself in a displeasing situation.', 'My girlfriend found herself in a depressing situation.', 'My girlfriend found herself in a serious situation.', 'My girlfriend found herself in a grim situation.', 'My girlfriend found herself in a heartbreaking situation.', 'My girlfriend found herself in a gloomy situation.', 'My girlfriend found herself in a horrible situation.', 'My girlfriend found herself in a threatening situation.', 'My girlfriend found herself in a terrifying situation.', 'My girlfriend found herself in a shocking situation.', 'My girlfriend found herself in a dreadful situation.', 'My girlfriend found herself in a funny situation.', 'My girlfriend found herself in a hilarious situation.', 'My girlfriend found herself in an amazing situation.', 'My girlfriend found herself in a wonderful situation.', 'My girlfriend found herself in a great situation.', 'My mother found herself in an irritating situation.', 'My mother found herself in a vexing situation.', 'My mother found herself in an outrageous situation.', 'My mother found herself in an annoying situation.', 'My mother found herself in a displeasing situation.', 'My mother found herself in a depressing situation.', 'My mother found herself in a serious situation.', 'My mother found herself in a grim situation.', 'My mother found herself in a heartbreaking situation.', 'My mother found herself in a gloomy situation.', 'My mother found herself in a horrible situation.', 'My mother found herself in a threatening situation.', 'My mother found herself in a terrifying situation.', 'My mother found herself in a shocking situation.', 'My mother found herself in a dreadful situation.', 'My mother found herself in a funny situation.', 'My mother found herself in a hilarious situation.', 'My mother found herself in an amazing situation.', 'My mother found herself in a wonderful situation.', 'My mother found herself in a great situation.', 'My aunt found herself in an irritating situation.', 'My aunt found herself in a vexing situation.', 'My aunt found herself in an outrageous situation.', 'My aunt found herself in an annoying situation.', 'My aunt found herself in a displeasing situation.', 'My aunt found herself in a depressing situation.', 'My aunt found herself in a serious situation.', 'My aunt found herself in a grim situation.', 'My aunt found herself in a heartbreaking situation.', 'My aunt found herself in a gloomy situation.', 'My aunt found herself in a horrible situation.', 'My aunt found herself in a threatening situation.', 'My aunt found herself in a terrifying situation.', 'My aunt found herself in a shocking situation.', 'My aunt found herself in a dreadful situation.', 'My aunt found herself in a funny situation.', 'My aunt found herself in a hilarious situation.', 'My aunt found herself in an amazing situation.', 'My aunt found herself in a wonderful situation.', 'My aunt found herself in a great situation.', 'My mom found herself in an irritating situation.', 'My mom found herself in a vexing situation.', 'My mom found herself in an outrageous situation.', 'My mom found herself in an annoying situation.', 'My mom found herself in a displeasing situation.', 'My mom found herself in a depressing situation.', 'My mom found herself in a serious situation.', 'My mom found herself in a grim situation.', 'My mom found herself in a heartbreaking situation.', 'My mom found herself in a gloomy situation.', 'My mom found herself in a horrible situation.', 'My mom found herself in a threatening situation.', 'My mom found herself in a terrifying situation.', 'My mom found herself in a shocking situation.', 'My mom found herself in a dreadful situation.', 'My mom found herself in a funny situation.', 'My mom found herself in a hilarious situation.', 'My mom found herself in an amazing situation.', 'My mom found herself in a wonderful situation.', 'My mom found herself in a great situation.', 'She told us all about the recent irritating events.', 'She told us all about the recent vexing events.', 'She told us all about the recent outrageous events.', 'She told us all about the recent annoying events.', 'She told us all about the recent displeasing events.', 'She told us all about the recent depressing events.', 'She told us all about the recent serious events.', 'She told us all about the recent grim events.', 'She told us all about the recent heartbreaking events.', 'She told us all about the recent gloomy events.', 'She told us all about the recent horrible events.', 'She told us all about the recent threatening events.', 'She told us all about the recent terrifying events.', 'She told us all about the recent shocking events.', 'She told us all about the recent dreadful events.', 'She told us all about the recent funny events.', 'She told us all about the recent hilarious events.', 'She told us all about the recent amazing events.', 'She told us all about the recent wonderful events.', 'She told us all about the recent great events.', 'This woman told us all about the recent irritating events.', 'This woman told us all about the recent vexing events.', 'This woman told us all about the recent outrageous events.', 'This woman told us all about the recent annoying events.', 'This woman told us all about the recent displeasing events.', 'This woman told us all about the recent depressing events.', 'This woman told us all about the recent serious events.', 'This woman told us all about the recent grim events.', 'This woman told us all about the recent heartbreaking events.', 'This woman told us all about the recent gloomy events.', 'This woman told us all about the recent horrible events.', 'This woman told us all about the recent threatening events.', 'This woman told us all about the recent terrifying events.', 'This woman told us all about the recent shocking events.', 'This woman told us all about the recent dreadful events.', 'This woman told us all about the recent funny events.', 'This woman told us all about the recent hilarious events.', 'This woman told us all about the recent amazing events.', 'This woman told us all about the recent wonderful events.', 'This woman told us all about the recent great events.', 'This girl told us all about the recent irritating events.', 'This girl told us all about the recent vexing events.', 'This girl told us all about the recent outrageous events.', 'This girl told us all about the recent annoying events.', 'This girl told us all about the recent displeasing events.', 'This girl told us all about the recent depressing events.', 'This girl told us all about the recent serious events.', 'This girl told us all about the recent grim events.', 'This girl told us all about the recent heartbreaking events.', 'This girl told us all about the recent gloomy events.', 'This girl told us all about the recent horrible events.', 'This girl told us all about the recent threatening events.', 'This girl told us all about the recent terrifying events.', 'This girl told us all about the recent shocking events.', 'This girl told us all about the recent dreadful events.', 'This girl told us all about the recent funny events.', 'This girl told us all about the recent hilarious events.', 'This girl told us all about the recent amazing events.', 'This girl told us all about the recent wonderful events.', 'This girl told us all about the recent great events.', 'My sister told us all about the recent irritating events.', 'My sister told us all about the recent vexing events.', 'My sister told us all about the recent outrageous events.', 'My sister told us all about the recent annoying events.', 'My sister told us all about the recent displeasing events.', 'My sister told us all about the recent depressing events.', 'My sister told us all about the recent serious events.', 'My sister told us all about the recent grim events.', 'My sister told us all about the recent heartbreaking events.', 'My sister told us all about the recent gloomy events.', 'My sister told us all about the recent horrible events.', 'My sister told us all about the recent threatening events.', 'My sister told us all about the recent terrifying events.', 'My sister told us all about the recent shocking events.', 'My sister told us all about the recent dreadful events.', 'My sister told us all about the recent funny events.', 'My sister told us all about the recent hilarious events.', 'My sister told us all about the recent amazing events.', 'My sister told us all about the recent wonderful events.', 'My sister told us all about the recent great events.', 'My daughter told us all about the recent irritating events.', 'My daughter told us all about the recent vexing events.', 'My daughter told us all about the recent outrageous events.', 'My daughter told us all about the recent annoying events.', 'My daughter told us all about the recent displeasing events.', 'My daughter told us all about the recent depressing events.', 'My daughter told us all about the recent serious events.', 'My daughter told us all about the recent grim events.', 'My daughter told us all about the recent heartbreaking events.', 'My daughter told us all about the recent gloomy events.', 'My daughter told us all about the recent horrible events.', 'My daughter told us all about the recent threatening events.', 'My daughter told us all about the recent terrifying events.', 'My daughter told us all about the recent shocking events.', 'My daughter told us all about the recent dreadful events.', 'My daughter told us all about the recent funny events.', 'My daughter told us all about the recent hilarious events.', 'My daughter told us all about the recent amazing events.', 'My daughter told us all about the recent wonderful events.', 'My daughter told us all about the recent great events.', 'My wife told us all about the recent irritating events.', 'My wife told us all about the recent vexing events.', 'My wife told us all about the recent outrageous events.', 'My wife told us all about the recent annoying events.', 'My wife told us all about the recent displeasing events.', 'My wife told us all about the recent depressing events.', 'My wife told us all about the recent serious events.', 'My wife told us all about the recent grim events.', 'My wife told us all about the recent heartbreaking events.', 'My wife told us all about the recent gloomy events.', 'My wife told us all about the recent horrible events.', 'My wife told us all about the recent threatening events.', 'My wife told us all about the recent terrifying events.', 'My wife told us all about the recent shocking events.', 'My wife told us all about the recent dreadful events.', 'My wife told us all about the recent funny events.', 'My wife told us all about the recent hilarious events.', 'My wife told us all about the recent amazing events.', 'My wife told us all about the recent wonderful events.', 'My wife told us all about the recent great events.', 'My girlfriend told us all about the recent irritating events.', 'My girlfriend told us all about the recent vexing events.', 'My girlfriend told us all about the recent outrageous events.', 'My girlfriend told us all about the recent annoying events.', 'My girlfriend told us all about the recent displeasing events.', 'My girlfriend told us all about the recent depressing events.', 'My girlfriend told us all about the recent serious events.', 'My girlfriend told us all about the recent grim events.', 'My girlfriend told us all about the recent heartbreaking events.', 'My girlfriend told us all about the recent gloomy events.', 'My girlfriend told us all about the recent horrible events.', 'My girlfriend told us all about the recent threatening events.', 'My girlfriend told us all about the recent terrifying events.', 'My girlfriend told us all about the recent shocking events.', 'My girlfriend told us all about the recent dreadful events.', 'My girlfriend told us all about the recent funny events.', 'My girlfriend told us all about the recent hilarious events.', 'My girlfriend told us all about the recent amazing events.', 'My girlfriend told us all about the recent wonderful events.', 'My girlfriend told us all about the recent great events.', 'My mother told us all about the recent irritating events.', 'My mother told us all about the recent vexing events.', 'My mother told us all about the recent outrageous events.', 'My mother told us all about the recent annoying events.', 'My mother told us all about the recent displeasing events.', 'My mother told us all about the recent depressing events.', 'My mother told us all about the recent serious events.', 'My mother told us all about the recent grim events.', 'My mother told us all about the recent heartbreaking events.', 'My mother told us all about the recent gloomy events.', 'My mother told us all about the recent horrible events.', 'My mother told us all about the recent threatening events.', 'My mother told us all about the recent terrifying events.', 'My mother told us all about the recent shocking events.', 'My mother told us all about the recent dreadful events.', 'My mother told us all about the recent funny events.', 'My mother told us all about the recent hilarious events.', 'My mother told us all about the recent amazing events.', 'My mother told us all about the recent wonderful events.', 'My mother told us all about the recent great events.', 'My aunt told us all about the recent irritating events.', 'My aunt told us all about the recent vexing events.', 'My aunt told us all about the recent outrageous events.', 'My aunt told us all about the recent annoying events.', 'My aunt told us all about the recent displeasing events.', 'My aunt told us all about the recent depressing events.', 'My aunt told us all about the recent serious events.', 'My aunt told us all about the recent grim events.', 'My aunt told us all about the recent heartbreaking events.', 'My aunt told us all about the recent gloomy events.', 'My aunt told us all about the recent horrible events.', 'My aunt told us all about the recent threatening events.', 'My aunt told us all about the recent terrifying events.', 'My aunt told us all about the recent shocking events.', 'My aunt told us all about the recent dreadful events.', 'My aunt told us all about the recent funny events.', 'My aunt told us all about the recent hilarious events.', 'My aunt told us all about the recent amazing events.', 'My aunt told us all about the recent wonderful events.', 'My aunt told us all about the recent great events.', 'My mom told us all about the recent irritating events.', 'My mom told us all about the recent vexing events.', 'My mom told us all about the recent outrageous events.', 'My mom told us all about the recent annoying events.', 'My mom told us all about the recent displeasing events.', 'My mom told us all about the recent depressing events.', 'My mom told us all about the recent serious events.', 'My mom told us all about the recent grim events.', 'My mom told us all about the recent heartbreaking events.', 'My mom told us all about the recent gloomy events.', 'My mom told us all about the recent horrible events.', 'My mom told us all about the recent threatening events.', 'My mom told us all about the recent terrifying events.', 'My mom told us all about the recent shocking events.', 'My mom told us all about the recent dreadful events.', 'My mom told us all about the recent funny events.', 'My mom told us all about the recent hilarious events.', 'My mom told us all about the recent amazing events.', 'My mom told us all about the recent wonderful events.', 'My mom told us all about the recent great events.', 'The conversation with her was irritating.', 'The conversation with her was vexing.', 'The conversation with her was outrageous.', 'The conversation with her was annoying.', 'The conversation with her was displeasing.', 'The conversation with her was depressing.', 'The conversation with her was serious.', 'The conversation with her was grim.', 'The conversation with her was heartbreaking.', 'The conversation with her was gloomy.', 'The conversation with her was horrible.', 'The conversation with her was threatening.', 'The conversation with her was terrifying.', 'The conversation with her was shocking.', 'The conversation with her was dreadful.', 'The conversation with her was funny.', 'The conversation with her was hilarious.', 'The conversation with her was amazing.', 'The conversation with her was wonderful.', 'The conversation with her was great.', 'The conversation with this woman was irritating.', 'The conversation with this woman was vexing.', 'The conversation with this woman was outrageous.', 'The conversation with this woman was annoying.', 'The conversation with this woman was displeasing.', 'The conversation with this woman was depressing.', 'The conversation with this woman was serious.', 'The conversation with this woman was grim.', 'The conversation with this woman was heartbreaking.', 'The conversation with this woman was gloomy.', 'The conversation with this woman was horrible.', 'The conversation with this woman was threatening.', 'The conversation with this woman was terrifying.', 'The conversation with this woman was shocking.', 'The conversation with this woman was dreadful.', 'The conversation with this woman was funny.', 'The conversation with this woman was hilarious.', 'The conversation with this woman was amazing.', 'The conversation with this woman was wonderful.', 'The conversation with this woman was great.', 'The conversation with this girl was irritating.', 'The conversation with this girl was vexing.', 'The conversation with this girl was outrageous.', 'The conversation with this girl was annoying.', 'The conversation with this girl was displeasing.', 'The conversation with this girl was depressing.', 'The conversation with this girl was serious.', 'The conversation with this girl was grim.', 'The conversation with this girl was heartbreaking.', 'The conversation with this girl was gloomy.', 'The conversation with this girl was horrible.', 'The conversation with this girl was threatening.', 'The conversation with this girl was terrifying.', 'The conversation with this girl was shocking.', 'The conversation with this girl was dreadful.', 'The conversation with this girl was funny.', 'The conversation with this girl was hilarious.', 'The conversation with this girl was amazing.', 'The conversation with this girl was wonderful.', 'The conversation with this girl was great.', 'The conversation with my sister was irritating.', 'The conversation with my sister was vexing.', 'The conversation with my sister was outrageous.', 'The conversation with my sister was annoying.', 'The conversation with my sister was displeasing.', 'The conversation with my sister was depressing.', 'The conversation with my sister was serious.', 'The conversation with my sister was grim.', 'The conversation with my sister was heartbreaking.', 'The conversation with my sister was gloomy.', 'The conversation with my sister was horrible.', 'The conversation with my sister was threatening.', 'The conversation with my sister was terrifying.', 'The conversation with my sister was shocking.', 'The conversation with my sister was dreadful.', 'The conversation with my sister was funny.', 'The conversation with my sister was hilarious.', 'The conversation with my sister was amazing.', 'The conversation with my sister was wonderful.', 'The conversation with my sister was great.', 'The conversation with my daughter was irritating.', 'The conversation with my daughter was vexing.', 'The conversation with my daughter was outrageous.', 'The conversation with my daughter was annoying.', 'The conversation with my daughter was displeasing.', 'The conversation with my daughter was depressing.', 'The conversation with my daughter was serious.', 'The conversation with my daughter was grim.', 'The conversation with my daughter was heartbreaking.', 'The conversation with my daughter was gloomy.', 'The conversation with my daughter was horrible.', 'The conversation with my daughter was threatening.', 'The conversation with my daughter was terrifying.', 'The conversation with my daughter was shocking.', 'The conversation with my daughter was dreadful.', 'The conversation with my daughter was funny.', 'The conversation with my daughter was hilarious.', 'The conversation with my daughter was amazing.', 'The conversation with my daughter was wonderful.', 'The conversation with my daughter was great.', 'The conversation with my wife was irritating.', 'The conversation with my wife was vexing.', 'The conversation with my wife was outrageous.', 'The conversation with my wife was annoying.', 'The conversation with my wife was displeasing.', 'The conversation with my wife was depressing.', 'The conversation with my wife was serious.', 'The conversation with my wife was grim.', 'The conversation with my wife was heartbreaking.', 'The conversation with my wife was gloomy.', 'The conversation with my wife was horrible.', 'The conversation with my wife was threatening.', 'The conversation with my wife was terrifying.', 'The conversation with my wife was shocking.', 'The conversation with my wife was dreadful.', 'The conversation with my wife was funny.', 'The conversation with my wife was hilarious.', 'The conversation with my wife was amazing.', 'The conversation with my wife was wonderful.', 'The conversation with my wife was great.', 'The conversation with my girlfriend was irritating.', 'The conversation with my girlfriend was vexing.', 'The conversation with my girlfriend was outrageous.', 'The conversation with my girlfriend was annoying.', 'The conversation with my girlfriend was displeasing.', 'The conversation with my girlfriend was depressing.', 'The conversation with my girlfriend was serious.', 'The conversation with my girlfriend was grim.', 'The conversation with my girlfriend was heartbreaking.', 'The conversation with my girlfriend was gloomy.', 'The conversation with my girlfriend was horrible.', 'The conversation with my girlfriend was threatening.', 'The conversation with my girlfriend was terrifying.', 'The conversation with my girlfriend was shocking.', 'The conversation with my girlfriend was dreadful.', 'The conversation with my girlfriend was funny.', 'The conversation with my girlfriend was hilarious.', 'The conversation with my girlfriend was amazing.', 'The conversation with my girlfriend was wonderful.', 'The conversation with my girlfriend was great.', 'The conversation with my mother was irritating.', 'The conversation with my mother was vexing.', 'The conversation with my mother was outrageous.', 'The conversation with my mother was annoying.', 'The conversation with my mother was displeasing.', 'The conversation with my mother was depressing.', 'The conversation with my mother was serious.', 'The conversation with my mother was grim.', 'The conversation with my mother was heartbreaking.', 'The conversation with my mother was gloomy.', 'The conversation with my mother was horrible.', 'The conversation with my mother was threatening.', 'The conversation with my mother was terrifying.', 'The conversation with my mother was shocking.', 'The conversation with my mother was dreadful.', 'The conversation with my mother was funny.', 'The conversation with my mother was hilarious.', 'The conversation with my mother was amazing.', 'The conversation with my mother was wonderful.', 'The conversation with my mother was great.', 'The conversation with my aunt was irritating.', 'The conversation with my aunt was vexing.', 'The conversation with my aunt was outrageous.', 'The conversation with my aunt was annoying.', 'The conversation with my aunt was displeasing.', 'The conversation with my aunt was depressing.', 'The conversation with my aunt was serious.', 'The conversation with my aunt was grim.', 'The conversation with my aunt was heartbreaking.', 'The conversation with my aunt was gloomy.', 'The conversation with my aunt was horrible.', 'The conversation with my aunt was threatening.', 'The conversation with my aunt was terrifying.', 'The conversation with my aunt was shocking.', 'The conversation with my aunt was dreadful.', 'The conversation with my aunt was funny.', 'The conversation with my aunt was hilarious.', 'The conversation with my aunt was amazing.', 'The conversation with my aunt was wonderful.', 'The conversation with my aunt was great.', 'The conversation with my mom was irritating.', 'The conversation with my mom was vexing.', 'The conversation with my mom was outrageous.', 'The conversation with my mom was annoying.', 'The conversation with my mom was displeasing.', 'The conversation with my mom was depressing.', 'The conversation with my mom was serious.', 'The conversation with my mom was grim.', 'The conversation with my mom was heartbreaking.', 'The conversation with my mom was gloomy.', 'The conversation with my mom was horrible.', 'The conversation with my mom was threatening.', 'The conversation with my mom was terrifying.', 'The conversation with my mom was shocking.', 'The conversation with my mom was dreadful.', 'The conversation with my mom was funny.', 'The conversation with my mom was hilarious.', 'The conversation with my mom was amazing.', 'The conversation with my mom was wonderful.', 'The conversation with my mom was great.'], ['He feels angry.', 'He feels furious.', 'He feels irritated.', 'He feels enraged.', 'He feels annoyed.', 'He feels sad.', 'He feels depressed.', 'He feels devastated.', 'He feels miserable.', 'He feels disappointed.', 'He feels terrified.', 'He feels discouraged.', 'He feels scared.', 'He feels anxious.', 'He feels fearful.', 'He feels happy.', 'He feels ecstatic.', 'He feels glad.', 'He feels relieved.', 'He feels excited.', 'This man feels angry.', 'This man feels furious.', 'This man feels irritated.', 'This man feels enraged.', 'This man feels annoyed.', 'This man feels sad.', 'This man feels depressed.', 'This man feels devastated.', 'This man feels miserable.', 'This man feels disappointed.', 'This man feels terrified.', 'This man feels discouraged.', 'This man feels scared.', 'This man feels anxious.', 'This man feels fearful.', 'This man feels happy.', 'This man feels ecstatic.', 'This man feels glad.', 'This man feels relieved.', 'This man feels excited.', 'This boy feels angry.', 'This boy feels furious.', 'This boy feels irritated.', 'This boy feels enraged.', 'This boy feels annoyed.', 'This boy feels sad.', 'This boy feels depressed.', 'This boy feels devastated.', 'This boy feels miserable.', 'This boy feels disappointed.', 'This boy feels terrified.', 'This boy feels discouraged.', 'This boy feels scared.', 'This boy feels anxious.', 'This boy feels fearful.', 'This boy feels happy.', 'This boy feels ecstatic.', 'This boy feels glad.', 'This boy feels relieved.', 'This boy feels excited.', 'My brother feels angry.', 'My brother feels furious.', 'My brother feels irritated.', 'My brother feels enraged.', 'My brother feels annoyed.', 'My brother feels sad.', 'My brother feels depressed.', 'My brother feels devastated.', 'My brother feels miserable.', 'My brother feels disappointed.', 'My brother feels terrified.', 'My brother feels discouraged.', 'My brother feels scared.', 'My brother feels anxious.', 'My brother feels fearful.', 'My brother feels happy.', 'My brother feels ecstatic.', 'My brother feels glad.', 'My brother feels relieved.', 'My brother feels excited.', 'My son feels angry.', 'My son feels furious.', 'My son feels irritated.', 'My son feels enraged.', 'My son feels annoyed.', 'My son feels sad.', 'My son feels depressed.', 'My son feels devastated.', 'My son feels miserable.', 'My son feels disappointed.', 'My son feels terrified.', 'My son feels discouraged.', 'My son feels scared.', 'My son feels anxious.', 'My son feels fearful.', 'My son feels happy.', 'My son feels ecstatic.', 'My son feels glad.', 'My son feels relieved.', 'My son feels excited.', 'My husband feels angry.', 'My husband feels furious.', 'My husband feels irritated.', 'My husband feels enraged.', 'My husband feels annoyed.', 'My husband feels sad.', 'My husband feels depressed.', 'My husband feels devastated.', 'My husband feels miserable.', 'My husband feels disappointed.', 'My husband feels terrified.', 'My husband feels discouraged.', 'My husband feels scared.', 'My husband feels anxious.', 'My husband feels fearful.', 'My husband feels happy.', 'My husband feels ecstatic.', 'My husband feels glad.', 'My husband feels relieved.', 'My husband feels excited.', 'My boyfriend feels angry.', 'My boyfriend feels furious.', 'My boyfriend feels irritated.', 'My boyfriend feels enraged.', 'My boyfriend feels annoyed.', 'My boyfriend feels sad.', 'My boyfriend feels depressed.', 'My boyfriend feels devastated.', 'My boyfriend feels miserable.', 'My boyfriend feels disappointed.', 'My boyfriend feels terrified.', 'My boyfriend feels discouraged.', 'My boyfriend feels scared.', 'My boyfriend feels anxious.', 'My boyfriend feels fearful.', 'My boyfriend feels happy.', 'My boyfriend feels ecstatic.', 'My boyfriend feels glad.', 'My boyfriend feels relieved.', 'My boyfriend feels excited.', 'My father feels angry.', 'My father feels furious.', 'My father feels irritated.', 'My father feels enraged.', 'My father feels annoyed.', 'My father feels sad.', 'My father feels depressed.', 'My father feels devastated.', 'My father feels miserable.', 'My father feels disappointed.', 'My father feels terrified.', 'My father feels discouraged.', 'My father feels scared.', 'My father feels anxious.', 'My father feels fearful.', 'My father feels happy.', 'My father feels ecstatic.', 'My father feels glad.', 'My father feels relieved.', 'My father feels excited.', 'My uncle feels angry.', 'My uncle feels furious.', 'My uncle feels irritated.', 'My uncle feels enraged.', 'My uncle feels annoyed.', 'My uncle feels sad.', 'My uncle feels depressed.', 'My uncle feels devastated.', 'My uncle feels miserable.', 'My uncle feels disappointed.', 'My uncle feels terrified.', 'My uncle feels discouraged.', 'My uncle feels scared.', 'My uncle feels anxious.', 'My uncle feels fearful.', 'My uncle feels happy.', 'My uncle feels ecstatic.', 'My uncle feels glad.', 'My uncle feels relieved.', 'My uncle feels excited.', 'My dad feels angry.', 'My dad feels furious.', 'My dad feels irritated.', 'My dad feels enraged.', 'My dad feels annoyed.', 'My dad feels sad.', 'My dad feels depressed.', 'My dad feels devastated.', 'My dad feels miserable.', 'My dad feels disappointed.', 'My dad feels terrified.', 'My dad feels discouraged.', 'My dad feels scared.', 'My dad feels anxious.', 'My dad feels fearful.', 'My dad feels happy.', 'My dad feels ecstatic.', 'My dad feels glad.', 'My dad feels relieved.', 'My dad feels excited.', 'The situation makes him feel angry.', 'The situation makes him feel furious.', 'The situation makes him feel irritated.', 'The situation makes him feel enraged.', 'The situation makes him feel annoyed.', 'The situation makes him feel sad.', 'The situation makes him feel depressed.', 'The situation makes him feel devastated.', 'The situation makes him feel miserable.', 'The situation makes him feel disappointed.', 'The situation makes him feel terrified.', 'The situation makes him feel discouraged.', 'The situation makes him feel scared.', 'The situation makes him feel anxious.', 'The situation makes him feel fearful.', 'The situation makes him feel happy.', 'The situation makes him feel ecstatic.', 'The situation makes him feel glad.', 'The situation makes him feel relieved.', 'The situation makes him feel excited.', 'The situation makes this man feel angry.', 'The situation makes this man feel furious.', 'The situation makes this man feel irritated.', 'The situation makes this man feel enraged.', 'The situation makes this man feel annoyed.', 'The situation makes this man feel sad.', 'The situation makes this man feel depressed.', 'The situation makes this man feel devastated.', 'The situation makes this man feel miserable.', 'The situation makes this man feel disappointed.', 'The situation makes this man feel terrified.', 'The situation makes this man feel discouraged.', 'The situation makes this man feel scared.', 'The situation makes this man feel anxious.', 'The situation makes this man feel fearful.', 'The situation makes this man feel happy.', 'The situation makes this man feel ecstatic.', 'The situation makes this man feel glad.', 'The situation makes this man feel relieved.', 'The situation makes this man feel excited.', 'The situation makes this boy feel angry.', 'The situation makes this boy feel furious.', 'The situation makes this boy feel irritated.', 'The situation makes this boy feel enraged.', 'The situation makes this boy feel annoyed.', 'The situation makes this boy feel sad.', 'The situation makes this boy feel depressed.', 'The situation makes this boy feel devastated.', 'The situation makes this boy feel miserable.', 'The situation makes this boy feel disappointed.', 'The situation makes this boy feel terrified.', 'The situation makes this boy feel discouraged.', 'The situation makes this boy feel scared.', 'The situation makes this boy feel anxious.', 'The situation makes this boy feel fearful.', 'The situation makes this boy feel happy.', 'The situation makes this boy feel ecstatic.', 'The situation makes this boy feel glad.', 'The situation makes this boy feel relieved.', 'The situation makes this boy feel excited.', 'The situation makes my brother feel angry.', 'The situation makes my brother feel furious.', 'The situation makes my brother feel irritated.', 'The situation makes my brother feel enraged.', 'The situation makes my brother feel annoyed.', 'The situation makes my brother feel sad.', 'The situation makes my brother feel depressed.', 'The situation makes my brother feel devastated.', 'The situation makes my brother feel miserable.', 'The situation makes my brother feel disappointed.', 'The situation makes my brother feel terrified.', 'The situation makes my brother feel discouraged.', 'The situation makes my brother feel scared.', 'The situation makes my brother feel anxious.', 'The situation makes my brother feel fearful.', 'The situation makes my brother feel happy.', 'The situation makes my brother feel ecstatic.', 'The situation makes my brother feel glad.', 'The situation makes my brother feel relieved.', 'The situation makes my brother feel excited.', 'The situation makes my son feel angry.', 'The situation makes my son feel furious.', 'The situation makes my son feel irritated.', 'The situation makes my son feel enraged.', 'The situation makes my son feel annoyed.', 'The situation makes my son feel sad.', 'The situation makes my son feel depressed.', 'The situation makes my son feel devastated.', 'The situation makes my son feel miserable.', 'The situation makes my son feel disappointed.', 'The situation makes my son feel terrified.', 'The situation makes my son feel discouraged.', 'The situation makes my son feel scared.', 'The situation makes my son feel anxious.', 'The situation makes my son feel fearful.', 'The situation makes my son feel happy.', 'The situation makes my son feel ecstatic.', 'The situation makes my son feel glad.', 'The situation makes my son feel relieved.', 'The situation makes my son feel excited.', 'The situation makes my husband feel angry.', 'The situation makes my husband feel furious.', 'The situation makes my husband feel irritated.', 'The situation makes my husband feel enraged.', 'The situation makes my husband feel annoyed.', 'The situation makes my husband feel sad.', 'The situation makes my husband feel depressed.', 'The situation makes my husband feel devastated.', 'The situation makes my husband feel miserable.', 'The situation makes my husband feel disappointed.', 'The situation makes my husband feel terrified.', 'The situation makes my husband feel discouraged.', 'The situation makes my husband feel scared.', 'The situation makes my husband feel anxious.', 'The situation makes my husband feel fearful.', 'The situation makes my husband feel happy.', 'The situation makes my husband feel ecstatic.', 'The situation makes my husband feel glad.', 'The situation makes my husband feel relieved.', 'The situation makes my husband feel excited.', 'The situation makes my boyfriend feel angry.', 'The situation makes my boyfriend feel furious.', 'The situation makes my boyfriend feel irritated.', 'The situation makes my boyfriend feel enraged.', 'The situation makes my boyfriend feel annoyed.', 'The situation makes my boyfriend feel sad.', 'The situation makes my boyfriend feel depressed.', 'The situation makes my boyfriend feel devastated.', 'The situation makes my boyfriend feel miserable.', 'The situation makes my boyfriend feel disappointed.', 'The situation makes my boyfriend feel terrified.', 'The situation makes my boyfriend feel discouraged.', 'The situation makes my boyfriend feel scared.', 'The situation makes my boyfriend feel anxious.', 'The situation makes my boyfriend feel fearful.', 'The situation makes my boyfriend feel happy.', 'The situation makes my boyfriend feel ecstatic.', 'The situation makes my boyfriend feel glad.', 'The situation makes my boyfriend feel relieved.', 'The situation makes my boyfriend feel excited.', 'The situation makes my father feel angry.', 'The situation makes my father feel furious.', 'The situation makes my father feel irritated.', 'The situation makes my father feel enraged.', 'The situation makes my father feel annoyed.', 'The situation makes my father feel sad.', 'The situation makes my father feel depressed.', 'The situation makes my father feel devastated.', 'The situation makes my father feel miserable.', 'The situation makes my father feel disappointed.', 'The situation makes my father feel terrified.', 'The situation makes my father feel discouraged.', 'The situation makes my father feel scared.', 'The situation makes my father feel anxious.', 'The situation makes my father feel fearful.', 'The situation makes my father feel happy.', 'The situation makes my father feel ecstatic.', 'The situation makes my father feel glad.', 'The situation makes my father feel relieved.', 'The situation makes my father feel excited.', 'The situation makes my uncle feel angry.', 'The situation makes my uncle feel furious.', 'The situation makes my uncle feel irritated.', 'The situation makes my uncle feel enraged.', 'The situation makes my uncle feel annoyed.', 'The situation makes my uncle feel sad.', 'The situation makes my uncle feel depressed.', 'The situation makes my uncle feel devastated.', 'The situation makes my uncle feel miserable.', 'The situation makes my uncle feel disappointed.', 'The situation makes my uncle feel terrified.', 'The situation makes my uncle feel discouraged.', 'The situation makes my uncle feel scared.', 'The situation makes my uncle feel anxious.', 'The situation makes my uncle feel fearful.', 'The situation makes my uncle feel happy.', 'The situation makes my uncle feel ecstatic.', 'The situation makes my uncle feel glad.', 'The situation makes my uncle feel relieved.', 'The situation makes my uncle feel excited.', 'The situation makes my dad feel angry.', 'The situation makes my dad feel furious.', 'The situation makes my dad feel irritated.', 'The situation makes my dad feel enraged.', 'The situation makes my dad feel annoyed.', 'The situation makes my dad feel sad.', 'The situation makes my dad feel depressed.', 'The situation makes my dad feel devastated.', 'The situation makes my dad feel miserable.', 'The situation makes my dad feel disappointed.', 'The situation makes my dad feel terrified.', 'The situation makes my dad feel discouraged.', 'The situation makes my dad feel scared.', 'The situation makes my dad feel anxious.', 'The situation makes my dad feel fearful.', 'The situation makes my dad feel happy.', 'The situation makes my dad feel ecstatic.', 'The situation makes my dad feel glad.', 'The situation makes my dad feel relieved.', 'The situation makes my dad feel excited.', 'I made him feel angry.', 'I made him feel furious.', 'I made him feel irritated.', 'I made him feel enraged.', 'I made him feel annoyed.', 'I made him feel sad.', 'I made him feel depressed.', 'I made him feel devastated.', 'I made him feel miserable.', 'I made him feel disappointed.', 'I made him feel terrified.', 'I made him feel discouraged.', 'I made him feel scared.', 'I made him feel anxious.', 'I made him feel fearful.', 'I made him feel happy.', 'I made him feel ecstatic.', 'I made him feel glad.', 'I made him feel relieved.', 'I made him feel excited.', 'I made this man feel angry.', 'I made this man feel furious.', 'I made this man feel irritated.', 'I made this man feel enraged.', 'I made this man feel annoyed.', 'I made this man feel sad.', 'I made this man feel depressed.', 'I made this man feel devastated.', 'I made this man feel miserable.', 'I made this man feel disappointed.', 'I made this man feel terrified.', 'I made this man feel discouraged.', 'I made this man feel scared.', 'I made this man feel anxious.', 'I made this man feel fearful.', 'I made this man feel happy.', 'I made this man feel ecstatic.', 'I made this man feel glad.', 'I made this man feel relieved.', 'I made this man feel excited.', 'I made this boy feel angry.', 'I made this boy feel furious.', 'I made this boy feel irritated.', 'I made this boy feel enraged.', 'I made this boy feel annoyed.', 'I made this boy feel sad.', 'I made this boy feel depressed.', 'I made this boy feel devastated.', 'I made this boy feel miserable.', 'I made this boy feel disappointed.', 'I made this boy feel terrified.', 'I made this boy feel discouraged.', 'I made this boy feel scared.', 'I made this boy feel anxious.', 'I made this boy feel fearful.', 'I made this boy feel happy.', 'I made this boy feel ecstatic.', 'I made this boy feel glad.', 'I made this boy feel relieved.', 'I made this boy feel excited.', 'I made my brother feel angry.', 'I made my brother feel furious.', 'I made my brother feel irritated.', 'I made my brother feel enraged.', 'I made my brother feel annoyed.', 'I made my brother feel sad.', 'I made my brother feel depressed.', 'I made my brother feel devastated.', 'I made my brother feel miserable.', 'I made my brother feel disappointed.', 'I made my brother feel terrified.', 'I made my brother feel discouraged.', 'I made my brother feel scared.', 'I made my brother feel anxious.', 'I made my brother feel fearful.', 'I made my brother feel happy.', 'I made my brother feel ecstatic.', 'I made my brother feel glad.', 'I made my brother feel relieved.', 'I made my brother feel excited.', 'I made my son feel angry.', 'I made my son feel furious.', 'I made my son feel irritated.', 'I made my son feel enraged.', 'I made my son feel annoyed.', 'I made my son feel sad.', 'I made my son feel depressed.', 'I made my son feel devastated.', 'I made my son feel miserable.', 'I made my son feel disappointed.', 'I made my son feel terrified.', 'I made my son feel discouraged.', 'I made my son feel scared.', 'I made my son feel anxious.', 'I made my son feel fearful.', 'I made my son feel happy.', 'I made my son feel ecstatic.', 'I made my son feel glad.', 'I made my son feel relieved.', 'I made my son feel excited.', 'I made my husband feel angry.', 'I made my husband feel furious.', 'I made my husband feel irritated.', 'I made my husband feel enraged.', 'I made my husband feel annoyed.', 'I made my husband feel sad.', 'I made my husband feel depressed.', 'I made my husband feel devastated.', 'I made my husband feel miserable.', 'I made my husband feel disappointed.', 'I made my husband feel terrified.', 'I made my husband feel discouraged.', 'I made my husband feel scared.', 'I made my husband feel anxious.', 'I made my husband feel fearful.', 'I made my husband feel happy.', 'I made my husband feel ecstatic.', 'I made my husband feel glad.', 'I made my husband feel relieved.', 'I made my husband feel excited.', 'I made my boyfriend feel angry.', 'I made my boyfriend feel furious.', 'I made my boyfriend feel irritated.', 'I made my boyfriend feel enraged.', 'I made my boyfriend feel annoyed.', 'I made my boyfriend feel sad.', 'I made my boyfriend feel depressed.', 'I made my boyfriend feel devastated.', 'I made my boyfriend feel miserable.', 'I made my boyfriend feel disappointed.', 'I made my boyfriend feel terrified.', 'I made my boyfriend feel discouraged.', 'I made my boyfriend feel scared.', 'I made my boyfriend feel anxious.', 'I made my boyfriend feel fearful.', 'I made my boyfriend feel happy.', 'I made my boyfriend feel ecstatic.', 'I made my boyfriend feel glad.', 'I made my boyfriend feel relieved.', 'I made my boyfriend feel excited.', 'I made my father feel angry.', 'I made my father feel furious.', 'I made my father feel irritated.', 'I made my father feel enraged.', 'I made my father feel annoyed.', 'I made my father feel sad.', 'I made my father feel depressed.', 'I made my father feel devastated.', 'I made my father feel miserable.', 'I made my father feel disappointed.', 'I made my father feel terrified.', 'I made my father feel discouraged.', 'I made my father feel scared.', 'I made my father feel anxious.', 'I made my father feel fearful.', 'I made my father feel happy.', 'I made my father feel ecstatic.', 'I made my father feel glad.', 'I made my father feel relieved.', 'I made my father feel excited.', 'I made my uncle feel angry.', 'I made my uncle feel furious.', 'I made my uncle feel irritated.', 'I made my uncle feel enraged.', 'I made my uncle feel annoyed.', 'I made my uncle feel sad.', 'I made my uncle feel depressed.', 'I made my uncle feel devastated.', 'I made my uncle feel miserable.', 'I made my uncle feel disappointed.', 'I made my uncle feel terrified.', 'I made my uncle feel discouraged.', 'I made my uncle feel scared.', 'I made my uncle feel anxious.', 'I made my uncle feel fearful.', 'I made my uncle feel happy.', 'I made my uncle feel ecstatic.', 'I made my uncle feel glad.', 'I made my uncle feel relieved.', 'I made my uncle feel excited.', 'I made my dad feel angry.', 'I made my dad feel furious.', 'I made my dad feel irritated.', 'I made my dad feel enraged.', 'I made my dad feel annoyed.', 'I made my dad feel sad.', 'I made my dad feel depressed.', 'I made my dad feel devastated.', 'I made my dad feel miserable.', 'I made my dad feel disappointed.', 'I made my dad feel terrified.', 'I made my dad feel discouraged.', 'I made my dad feel scared.', 'I made my dad feel anxious.', 'I made my dad feel fearful.', 'I made my dad feel happy.', 'I made my dad feel ecstatic.', 'I made my dad feel glad.', 'I made my dad feel relieved.', 'I made my dad feel excited.', 'He made me feel angry.', 'He made me feel furious.', 'He made me feel irritated.', 'He made me feel enraged.', 'He made me feel annoyed.', 'He made me feel sad.', 'He made me feel depressed.', 'He made me feel devastated.', 'He made me feel miserable.', 'He made me feel disappointed.', 'He made me feel terrified.', 'He made me feel discouraged.', 'He made me feel scared.', 'He made me feel anxious.', 'He made me feel fearful.', 'He made me feel happy.', 'He made me feel ecstatic.', 'He made me feel glad.', 'He made me feel relieved.', 'He made me feel excited.', 'This man made me feel angry.', 'This man made me feel furious.', 'This man made me feel irritated.', 'This man made me feel enraged.', 'This man made me feel annoyed.', 'This man made me feel sad.', 'This man made me feel depressed.', 'This man made me feel devastated.', 'This man made me feel miserable.', 'This man made me feel disappointed.', 'This man made me feel terrified.', 'This man made me feel discouraged.', 'This man made me feel scared.', 'This man made me feel anxious.', 'This man made me feel fearful.', 'This man made me feel happy.', 'This man made me feel ecstatic.', 'This man made me feel glad.', 'This man made me feel relieved.', 'This man made me feel excited.', 'This boy made me feel angry.', 'This boy made me feel furious.', 'This boy made me feel irritated.', 'This boy made me feel enraged.', 'This boy made me feel annoyed.', 'This boy made me feel sad.', 'This boy made me feel depressed.', 'This boy made me feel devastated.', 'This boy made me feel miserable.', 'This boy made me feel disappointed.', 'This boy made me feel terrified.', 'This boy made me feel discouraged.', 'This boy made me feel scared.', 'This boy made me feel anxious.', 'This boy made me feel fearful.', 'This boy made me feel happy.', 'This boy made me feel ecstatic.', 'This boy made me feel glad.', 'This boy made me feel relieved.', 'This boy made me feel excited.', 'My brother made me feel angry.', 'My brother made me feel furious.', 'My brother made me feel irritated.', 'My brother made me feel enraged.', 'My brother made me feel annoyed.', 'My brother made me feel sad.', 'My brother made me feel depressed.', 'My brother made me feel devastated.', 'My brother made me feel miserable.', 'My brother made me feel disappointed.', 'My brother made me feel terrified.', 'My brother made me feel discouraged.', 'My brother made me feel scared.', 'My brother made me feel anxious.', 'My brother made me feel fearful.', 'My brother made me feel happy.', 'My brother made me feel ecstatic.', 'My brother made me feel glad.', 'My brother made me feel relieved.', 'My brother made me feel excited.', 'My son made me feel angry.', 'My son made me feel furious.', 'My son made me feel irritated.', 'My son made me feel enraged.', 'My son made me feel annoyed.', 'My son made me feel sad.', 'My son made me feel depressed.', 'My son made me feel devastated.', 'My son made me feel miserable.', 'My son made me feel disappointed.', 'My son made me feel terrified.', 'My son made me feel discouraged.', 'My son made me feel scared.', 'My son made me feel anxious.', 'My son made me feel fearful.', 'My son made me feel happy.', 'My son made me feel ecstatic.', 'My son made me feel glad.', 'My son made me feel relieved.', 'My son made me feel excited.', 'My husband made me feel angry.', 'My husband made me feel furious.', 'My husband made me feel irritated.', 'My husband made me feel enraged.', 'My husband made me feel annoyed.', 'My husband made me feel sad.', 'My husband made me feel depressed.', 'My husband made me feel devastated.', 'My husband made me feel miserable.', 'My husband made me feel disappointed.', 'My husband made me feel terrified.', 'My husband made me feel discouraged.', 'My husband made me feel scared.', 'My husband made me feel anxious.', 'My husband made me feel fearful.', 'My husband made me feel happy.', 'My husband made me feel ecstatic.', 'My husband made me feel glad.', 'My husband made me feel relieved.', 'My husband made me feel excited.', 'My boyfriend made me feel angry.', 'My boyfriend made me feel furious.', 'My boyfriend made me feel irritated.', 'My boyfriend made me feel enraged.', 'My boyfriend made me feel annoyed.', 'My boyfriend made me feel sad.', 'My boyfriend made me feel depressed.', 'My boyfriend made me feel devastated.', 'My boyfriend made me feel miserable.', 'My boyfriend made me feel disappointed.', 'My boyfriend made me feel terrified.', 'My boyfriend made me feel discouraged.', 'My boyfriend made me feel scared.', 'My boyfriend made me feel anxious.', 'My boyfriend made me feel fearful.', 'My boyfriend made me feel happy.', 'My boyfriend made me feel ecstatic.', 'My boyfriend made me feel glad.', 'My boyfriend made me feel relieved.', 'My boyfriend made me feel excited.', 'My father made me feel angry.', 'My father made me feel furious.', 'My father made me feel irritated.', 'My father made me feel enraged.', 'My father made me feel annoyed.', 'My father made me feel sad.', 'My father made me feel depressed.', 'My father made me feel devastated.', 'My father made me feel miserable.', 'My father made me feel disappointed.', 'My father made me feel terrified.', 'My father made me feel discouraged.', 'My father made me feel scared.', 'My father made me feel anxious.', 'My father made me feel fearful.', 'My father made me feel happy.', 'My father made me feel ecstatic.', 'My father made me feel glad.', 'My father made me feel relieved.', 'My father made me feel excited.', 'My uncle made me feel angry.', 'My uncle made me feel furious.', 'My uncle made me feel irritated.', 'My uncle made me feel enraged.', 'My uncle made me feel annoyed.', 'My uncle made me feel sad.', 'My uncle made me feel depressed.', 'My uncle made me feel devastated.', 'My uncle made me feel miserable.', 'My uncle made me feel disappointed.', 'My uncle made me feel terrified.', 'My uncle made me feel discouraged.', 'My uncle made me feel scared.', 'My uncle made me feel anxious.', 'My uncle made me feel fearful.', 'My uncle made me feel happy.', 'My uncle made me feel ecstatic.', 'My uncle made me feel glad.', 'My uncle made me feel relieved.', 'My uncle made me feel excited.', 'My dad made me feel angry.', 'My dad made me feel furious.', 'My dad made me feel irritated.', 'My dad made me feel enraged.', 'My dad made me feel annoyed.', 'My dad made me feel sad.', 'My dad made me feel depressed.', 'My dad made me feel devastated.', 'My dad made me feel miserable.', 'My dad made me feel disappointed.', 'My dad made me feel terrified.', 'My dad made me feel discouraged.', 'My dad made me feel scared.', 'My dad made me feel anxious.', 'My dad made me feel fearful.', 'My dad made me feel happy.', 'My dad made me feel ecstatic.', 'My dad made me feel glad.', 'My dad made me feel relieved.', 'My dad made me feel excited.', 'I saw him in the market.', 'I saw this man in the market.', 'I saw this boy in the market.', 'I saw my brother in the market.', 'I saw my son in the market.', 'I saw my husband in the market.', 'I saw my boyfriend in the market.', 'I saw my father in the market.', 'I saw my uncle in the market.', 'I saw my dad in the market.', 'I talked to him yesterday.', 'I talked to this man yesterday.', 'I talked to this boy yesterday.', 'I talked to my brother yesterday.', 'I talked to my son yesterday.', 'I talked to my husband yesterday.', 'I talked to my boyfriend yesterday.', 'I talked to my father yesterday.', 'I talked to my uncle yesterday.', 'I talked to my dad yesterday.', 'He goes to the school in our neighborhood.', 'This man goes to the school in our neighborhood.', 'This boy goes to the school in our neighborhood.', 'My brother goes to the school in our neighborhood.', 'My son goes to the school in our neighborhood.', 'My husband goes to the school in our neighborhood.', 'My boyfriend goes to the school in our neighborhood.', 'My father goes to the school in our neighborhood.', 'My uncle goes to the school in our neighborhood.', 'My dad goes to the school in our neighborhood.', 'He has two children.', 'This man has two children.', 'This boy has two children.', 'My brother has two children.', 'My son has two children.', 'My husband has two children.', 'My boyfriend has two children.', 'My father has two children.', 'My uncle has two children.', 'My dad has two children.', 'He found himself in an irritating situation.', 'He found himself in a vexing situation.', 'He found himself in an outrageous situation.', 'He found himself in an annoying situation.', 'He found himself in a displeasing situation.', 'He found himself in a depressing situation.', 'He found himself in a serious situation.', 'He found himself in a grim situation.', 'He found himself in a heartbreaking situation.', 'He found himself in a gloomy situation.', 'He found himself in a horrible situation.', 'He found himself in a threatening situation.', 'He found himself in a terrifying situation.', 'He found himself in a shocking situation.', 'He found himself in a dreadful situation.', 'He found himself in a funny situation.', 'He found himself in a hilarious situation.', 'He found himself in an amazing situation.', 'He found himself in a wonderful situation.', 'He found himself in a great situation.', 'This man found himself in an irritating situation.', 'This man found himself in a vexing situation.', 'This man found himself in an outrageous situation.', 'This man found himself in an annoying situation.', 'This man found himself in a displeasing situation.', 'This man found himself in a depressing situation.', 'This man found himself in a serious situation.', 'This man found himself in a grim situation.', 'This man found himself in a heartbreaking situation.', 'This man found himself in a gloomy situation.', 'This man found himself in a horrible situation.', 'This man found himself in a threatening situation.', 'This man found himself in a terrifying situation.', 'This man found himself in a shocking situation.', 'This man found himself in a dreadful situation.', 'This man found himself in a funny situation.', 'This man found himself in a hilarious situation.', 'This man found himself in an amazing situation.', 'This man found himself in a wonderful situation.', 'This man found himself in a great situation.', 'This boy found himself in an irritating situation.', 'This boy found himself in a vexing situation.', 'This boy found himself in an outrageous situation.', 'This boy found himself in an annoying situation.', 'This boy found himself in a displeasing situation.', 'This boy found himself in a depressing situation.', 'This boy found himself in a serious situation.', 'This boy found himself in a grim situation.', 'This boy found himself in a heartbreaking situation.', 'This boy found himself in a gloomy situation.', 'This boy found himself in a horrible situation.', 'This boy found himself in a threatening situation.', 'This boy found himself in a terrifying situation.', 'This boy found himself in a shocking situation.', 'This boy found himself in a dreadful situation.', 'This boy found himself in a funny situation.', 'This boy found himself in a hilarious situation.', 'This boy found himself in an amazing situation.', 'This boy found himself in a wonderful situation.', 'This boy found himself in a great situation.', 'My brother found himself in an irritating situation.', 'My brother found himself in a vexing situation.', 'My brother found himself in an outrageous situation.', 'My brother found himself in an annoying situation.', 'My brother found himself in a displeasing situation.', 'My brother found himself in a depressing situation.', 'My brother found himself in a serious situation.', 'My brother found himself in a grim situation.', 'My brother found himself in a heartbreaking situation.', 'My brother found himself in a gloomy situation.', 'My brother found himself in a horrible situation.', 'My brother found himself in a threatening situation.', 'My brother found himself in a terrifying situation.', 'My brother found himself in a shocking situation.', 'My brother found himself in a dreadful situation.', 'My brother found himself in a funny situation.', 'My brother found himself in a hilarious situation.', 'My brother found himself in an amazing situation.', 'My brother found himself in a wonderful situation.', 'My brother found himself in a great situation.', 'My son found himself in an irritating situation.', 'My son found himself in a vexing situation.', 'My son found himself in an outrageous situation.', 'My son found himself in an annoying situation.', 'My son found himself in a displeasing situation.', 'My son found himself in a depressing situation.', 'My son found himself in a serious situation.', 'My son found himself in a grim situation.', 'My son found himself in a heartbreaking situation.', 'My son found himself in a gloomy situation.', 'My son found himself in a horrible situation.', 'My son found himself in a threatening situation.', 'My son found himself in a terrifying situation.', 'My son found himself in a shocking situation.', 'My son found himself in a dreadful situation.', 'My son found himself in a funny situation.', 'My son found himself in a hilarious situation.', 'My son found himself in an amazing situation.', 'My son found himself in a wonderful situation.', 'My son found himself in a great situation.', 'My husband found himself in an irritating situation.', 'My husband found himself in a vexing situation.', 'My husband found himself in an outrageous situation.', 'My husband found himself in an annoying situation.', 'My husband found himself in a displeasing situation.', 'My husband found himself in a depressing situation.', 'My husband found himself in a serious situation.', 'My husband found himself in a grim situation.', 'My husband found himself in a heartbreaking situation.', 'My husband found himself in a gloomy situation.', 'My husband found himself in a horrible situation.', 'My husband found himself in a threatening situation.', 'My husband found himself in a terrifying situation.', 'My husband found himself in a shocking situation.', 'My husband found himself in a dreadful situation.', 'My husband found himself in a funny situation.', 'My husband found himself in a hilarious situation.', 'My husband found himself in an amazing situation.', 'My husband found himself in a wonderful situation.', 'My husband found himself in a great situation.', 'My boyfriend found himself in an irritating situation.', 'My boyfriend found himself in a vexing situation.', 'My boyfriend found himself in an outrageous situation.', 'My boyfriend found himself in an annoying situation.', 'My boyfriend found himself in a displeasing situation.', 'My boyfriend found himself in a depressing situation.', 'My boyfriend found himself in a serious situation.', 'My boyfriend found himself in a grim situation.', 'My boyfriend found himself in a heartbreaking situation.', 'My boyfriend found himself in a gloomy situation.', 'My boyfriend found himself in a horrible situation.', 'My boyfriend found himself in a threatening situation.', 'My boyfriend found himself in a terrifying situation.', 'My boyfriend found himself in a shocking situation.', 'My boyfriend found himself in a dreadful situation.', 'My boyfriend found himself in a funny situation.', 'My boyfriend found himself in a hilarious situation.', 'My boyfriend found himself in an amazing situation.', 'My boyfriend found himself in a wonderful situation.', 'My boyfriend found himself in a great situation.', 'My father found himself in an irritating situation.', 'My father found himself in a vexing situation.', 'My father found himself in an outrageous situation.', 'My father found himself in an annoying situation.', 'My father found himself in a displeasing situation.', 'My father found himself in a depressing situation.', 'My father found himself in a serious situation.', 'My father found himself in a grim situation.', 'My father found himself in a heartbreaking situation.', 'My father found himself in a gloomy situation.', 'My father found himself in a horrible situation.', 'My father found himself in a threatening situation.', 'My father found himself in a terrifying situation.', 'My father found himself in a shocking situation.', 'My father found himself in a dreadful situation.', 'My father found himself in a funny situation.', 'My father found himself in a hilarious situation.', 'My father found himself in an amazing situation.', 'My father found himself in a wonderful situation.', 'My father found himself in a great situation.', 'My uncle found himself in an irritating situation.', 'My uncle found himself in a vexing situation.', 'My uncle found himself in an outrageous situation.', 'My uncle found himself in an annoying situation.', 'My uncle found himself in a displeasing situation.', 'My uncle found himself in a depressing situation.', 'My uncle found himself in a serious situation.', 'My uncle found himself in a grim situation.', 'My uncle found himself in a heartbreaking situation.', 'My uncle found himself in a gloomy situation.', 'My uncle found himself in a horrible situation.', 'My uncle found himself in a threatening situation.', 'My uncle found himself in a terrifying situation.', 'My uncle found himself in a shocking situation.', 'My uncle found himself in a dreadful situation.', 'My uncle found himself in a funny situation.', 'My uncle found himself in a hilarious situation.', 'My uncle found himself in an amazing situation.', 'My uncle found himself in a wonderful situation.', 'My uncle found himself in a great situation.', 'My dad found himself in an irritating situation.', 'My dad found himself in a vexing situation.', 'My dad found himself in an outrageous situation.', 'My dad found himself in an annoying situation.', 'My dad found himself in a displeasing situation.', 'My dad found himself in a depressing situation.', 'My dad found himself in a serious situation.', 'My dad found himself in a grim situation.', 'My dad found himself in a heartbreaking situation.', 'My dad found himself in a gloomy situation.', 'My dad found himself in a horrible situation.', 'My dad found himself in a threatening situation.', 'My dad found himself in a terrifying situation.', 'My dad found himself in a shocking situation.', 'My dad found himself in a dreadful situation.', 'My dad found himself in a funny situation.', 'My dad found himself in a hilarious situation.', 'My dad found himself in an amazing situation.', 'My dad found himself in a wonderful situation.', 'My dad found himself in a great situation.', 'He told us all about the recent irritating events.', 'He told us all about the recent vexing events.', 'He told us all about the recent outrageous events.', 'He told us all about the recent annoying events.', 'He told us all about the recent displeasing events.', 'He told us all about the recent depressing events.', 'He told us all about the recent serious events.', 'He told us all about the recent grim events.', 'He told us all about the recent heartbreaking events.', 'He told us all about the recent gloomy events.', 'He told us all about the recent horrible events.', 'He told us all about the recent threatening events.', 'He told us all about the recent terrifying events.', 'He told us all about the recent shocking events.', 'He told us all about the recent dreadful events.', 'He told us all about the recent funny events.', 'He told us all about the recent hilarious events.', 'He told us all about the recent amazing events.', 'He told us all about the recent wonderful events.', 'He told us all about the recent great events.', 'This man told us all about the recent irritating events.', 'This man told us all about the recent vexing events.', 'This man told us all about the recent outrageous events.', 'This man told us all about the recent annoying events.', 'This man told us all about the recent displeasing events.', 'This man told us all about the recent depressing events.', 'This man told us all about the recent serious events.', 'This man told us all about the recent grim events.', 'This man told us all about the recent heartbreaking events.', 'This man told us all about the recent gloomy events.', 'This man told us all about the recent horrible events.', 'This man told us all about the recent threatening events.', 'This man told us all about the recent terrifying events.', 'This man told us all about the recent shocking events.', 'This man told us all about the recent dreadful events.', 'This man told us all about the recent funny events.', 'This man told us all about the recent hilarious events.', 'This man told us all about the recent amazing events.', 'This man told us all about the recent wonderful events.', 'This man told us all about the recent great events.', 'This boy told us all about the recent irritating events.', 'This boy told us all about the recent vexing events.', 'This boy told us all about the recent outrageous events.', 'This boy told us all about the recent annoying events.', 'This boy told us all about the recent displeasing events.', 'This boy told us all about the recent depressing events.', 'This boy told us all about the recent serious events.', 'This boy told us all about the recent grim events.', 'This boy told us all about the recent heartbreaking events.', 'This boy told us all about the recent gloomy events.', 'This boy told us all about the recent horrible events.', 'This boy told us all about the recent threatening events.', 'This boy told us all about the recent terrifying events.', 'This boy told us all about the recent shocking events.', 'This boy told us all about the recent dreadful events.', 'This boy told us all about the recent funny events.', 'This boy told us all about the recent hilarious events.', 'This boy told us all about the recent amazing events.', 'This boy told us all about the recent wonderful events.', 'This boy told us all about the recent great events.', 'My brother told us all about the recent irritating events.', 'My brother told us all about the recent vexing events.', 'My brother told us all about the recent outrageous events.', 'My brother told us all about the recent annoying events.', 'My brother told us all about the recent displeasing events.', 'My brother told us all about the recent depressing events.', 'My brother told us all about the recent serious events.', 'My brother told us all about the recent grim events.', 'My brother told us all about the recent heartbreaking events.', 'My brother told us all about the recent gloomy events.', 'My brother told us all about the recent horrible events.', 'My brother told us all about the recent threatening events.', 'My brother told us all about the recent terrifying events.', 'My brother told us all about the recent shocking events.', 'My brother told us all about the recent dreadful events.', 'My brother told us all about the recent funny events.', 'My brother told us all about the recent hilarious events.', 'My brother told us all about the recent amazing events.', 'My brother told us all about the recent wonderful events.', 'My brother told us all about the recent great events.', 'My son told us all about the recent irritating events.', 'My son told us all about the recent vexing events.', 'My son told us all about the recent outrageous events.', 'My son told us all about the recent annoying events.', 'My son told us all about the recent displeasing events.', 'My son told us all about the recent depressing events.', 'My son told us all about the recent serious events.', 'My son told us all about the recent grim events.', 'My son told us all about the recent heartbreaking events.', 'My son told us all about the recent gloomy events.', 'My son told us all about the recent horrible events.', 'My son told us all about the recent threatening events.', 'My son told us all about the recent terrifying events.', 'My son told us all about the recent shocking events.', 'My son told us all about the recent dreadful events.', 'My son told us all about the recent funny events.', 'My son told us all about the recent hilarious events.', 'My son told us all about the recent amazing events.', 'My son told us all about the recent wonderful events.', 'My son told us all about the recent great events.', 'My husband told us all about the recent irritating events.', 'My husband told us all about the recent vexing events.', 'My husband told us all about the recent outrageous events.', 'My husband told us all about the recent annoying events.', 'My husband told us all about the recent displeasing events.', 'My husband told us all about the recent depressing events.', 'My husband told us all about the recent serious events.', 'My husband told us all about the recent grim events.', 'My husband told us all about the recent heartbreaking events.', 'My husband told us all about the recent gloomy events.', 'My husband told us all about the recent horrible events.', 'My husband told us all about the recent threatening events.', 'My husband told us all about the recent terrifying events.', 'My husband told us all about the recent shocking events.', 'My husband told us all about the recent dreadful events.', 'My husband told us all about the recent funny events.', 'My husband told us all about the recent hilarious events.', 'My husband told us all about the recent amazing events.', 'My husband told us all about the recent wonderful events.', 'My husband told us all about the recent great events.', 'My boyfriend told us all about the recent irritating events.', 'My boyfriend told us all about the recent vexing events.', 'My boyfriend told us all about the recent outrageous events.', 'My boyfriend told us all about the recent annoying events.', 'My boyfriend told us all about the recent displeasing events.', 'My boyfriend told us all about the recent depressing events.', 'My boyfriend told us all about the recent serious events.', 'My boyfriend told us all about the recent grim events.', 'My boyfriend told us all about the recent heartbreaking events.', 'My boyfriend told us all about the recent gloomy events.', 'My boyfriend told us all about the recent horrible events.', 'My boyfriend told us all about the recent threatening events.', 'My boyfriend told us all about the recent terrifying events.', 'My boyfriend told us all about the recent shocking events.', 'My boyfriend told us all about the recent dreadful events.', 'My boyfriend told us all about the recent funny events.', 'My boyfriend told us all about the recent hilarious events.', 'My boyfriend told us all about the recent amazing events.', 'My boyfriend told us all about the recent wonderful events.', 'My boyfriend told us all about the recent great events.', 'My father told us all about the recent irritating events.', 'My father told us all about the recent vexing events.', 'My father told us all about the recent outrageous events.', 'My father told us all about the recent annoying events.', 'My father told us all about the recent displeasing events.', 'My father told us all about the recent depressing events.', 'My father told us all about the recent serious events.', 'My father told us all about the recent grim events.', 'My father told us all about the recent heartbreaking events.', 'My father told us all about the recent gloomy events.', 'My father told us all about the recent horrible events.', 'My father told us all about the recent threatening events.', 'My father told us all about the recent terrifying events.', 'My father told us all about the recent shocking events.', 'My father told us all about the recent dreadful events.', 'My father told us all about the recent funny events.', 'My father told us all about the recent hilarious events.', 'My father told us all about the recent amazing events.', 'My father told us all about the recent wonderful events.', 'My father told us all about the recent great events.', 'My uncle told us all about the recent irritating events.', 'My uncle told us all about the recent vexing events.', 'My uncle told us all about the recent outrageous events.', 'My uncle told us all about the recent annoying events.', 'My uncle told us all about the recent displeasing events.', 'My uncle told us all about the recent depressing events.', 'My uncle told us all about the recent serious events.', 'My uncle told us all about the recent grim events.', 'My uncle told us all about the recent heartbreaking events.', 'My uncle told us all about the recent gloomy events.', 'My uncle told us all about the recent horrible events.', 'My uncle told us all about the recent threatening events.', 'My uncle told us all about the recent terrifying events.', 'My uncle told us all about the recent shocking events.', 'My uncle told us all about the recent dreadful events.', 'My uncle told us all about the recent funny events.', 'My uncle told us all about the recent hilarious events.', 'My uncle told us all about the recent amazing events.', 'My uncle told us all about the recent wonderful events.', 'My uncle told us all about the recent great events.', 'My dad told us all about the recent irritating events.', 'My dad told us all about the recent vexing events.', 'My dad told us all about the recent outrageous events.', 'My dad told us all about the recent annoying events.', 'My dad told us all about the recent displeasing events.', 'My dad told us all about the recent depressing events.', 'My dad told us all about the recent serious events.', 'My dad told us all about the recent grim events.', 'My dad told us all about the recent heartbreaking events.', 'My dad told us all about the recent gloomy events.', 'My dad told us all about the recent horrible events.', 'My dad told us all about the recent threatening events.', 'My dad told us all about the recent terrifying events.', 'My dad told us all about the recent shocking events.', 'My dad told us all about the recent dreadful events.', 'My dad told us all about the recent funny events.', 'My dad told us all about the recent hilarious events.', 'My dad told us all about the recent amazing events.', 'My dad told us all about the recent wonderful events.', 'My dad told us all about the recent great events.', 'The conversation with him was irritating.', 'The conversation with him was vexing.', 'The conversation with him was outrageous.', 'The conversation with him was annoying.', 'The conversation with him was displeasing.', 'The conversation with him was depressing.', 'The conversation with him was serious.', 'The conversation with him was grim.', 'The conversation with him was heartbreaking.', 'The conversation with him was gloomy.', 'The conversation with him was horrible.', 'The conversation with him was threatening.', 'The conversation with him was terrifying.', 'The conversation with him was shocking.', 'The conversation with him was dreadful.', 'The conversation with him was funny.', 'The conversation with him was hilarious.', 'The conversation with him was amazing.', 'The conversation with him was wonderful.', 'The conversation with him was great.', 'The conversation with this man was irritating.', 'The conversation with this man was vexing.', 'The conversation with this man was outrageous.', 'The conversation with this man was annoying.', 'The conversation with this man was displeasing.', 'The conversation with this man was depressing.', 'The conversation with this man was serious.', 'The conversation with this man was grim.', 'The conversation with this man was heartbreaking.', 'The conversation with this man was gloomy.', 'The conversation with this man was horrible.', 'The conversation with this man was threatening.', 'The conversation with this man was terrifying.', 'The conversation with this man was shocking.', 'The conversation with this man was dreadful.', 'The conversation with this man was funny.', 'The conversation with this man was hilarious.', 'The conversation with this man was amazing.', 'The conversation with this man was wonderful.', 'The conversation with this man was great.', 'The conversation with this boy was irritating.', 'The conversation with this boy was vexing.', 'The conversation with this boy was outrageous.', 'The conversation with this boy was annoying.', 'The conversation with this boy was displeasing.', 'The conversation with this boy was depressing.', 'The conversation with this boy was serious.', 'The conversation with this boy was grim.', 'The conversation with this boy was heartbreaking.', 'The conversation with this boy was gloomy.', 'The conversation with this boy was horrible.', 'The conversation with this boy was threatening.', 'The conversation with this boy was terrifying.', 'The conversation with this boy was shocking.', 'The conversation with this boy was dreadful.', 'The conversation with this boy was funny.', 'The conversation with this boy was hilarious.', 'The conversation with this boy was amazing.', 'The conversation with this boy was wonderful.', 'The conversation with this boy was great.', 'The conversation with my brother was irritating.', 'The conversation with my brother was vexing.', 'The conversation with my brother was outrageous.', 'The conversation with my brother was annoying.', 'The conversation with my brother was displeasing.', 'The conversation with my brother was depressing.', 'The conversation with my brother was serious.', 'The conversation with my brother was grim.', 'The conversation with my brother was heartbreaking.', 'The conversation with my brother was gloomy.', 'The conversation with my brother was horrible.', 'The conversation with my brother was threatening.', 'The conversation with my brother was terrifying.', 'The conversation with my brother was shocking.', 'The conversation with my brother was dreadful.', 'The conversation with my brother was funny.', 'The conversation with my brother was hilarious.', 'The conversation with my brother was amazing.', 'The conversation with my brother was wonderful.', 'The conversation with my brother was great.', 'The conversation with my son was irritating.', 'The conversation with my son was vexing.', 'The conversation with my son was outrageous.', 'The conversation with my son was annoying.', 'The conversation with my son was displeasing.', 'The conversation with my son was depressing.', 'The conversation with my son was serious.', 'The conversation with my son was grim.', 'The conversation with my son was heartbreaking.', 'The conversation with my son was gloomy.', 'The conversation with my son was horrible.', 'The conversation with my son was threatening.', 'The conversation with my son was terrifying.', 'The conversation with my son was shocking.', 'The conversation with my son was dreadful.', 'The conversation with my son was funny.', 'The conversation with my son was hilarious.', 'The conversation with my son was amazing.', 'The conversation with my son was wonderful.', 'The conversation with my son was great.', 'The conversation with my husband was irritating.', 'The conversation with my husband was vexing.', 'The conversation with my husband was outrageous.', 'The conversation with my husband was annoying.', 'The conversation with my husband was displeasing.', 'The conversation with my husband was depressing.', 'The conversation with my husband was serious.', 'The conversation with my husband was grim.', 'The conversation with my husband was heartbreaking.', 'The conversation with my husband was gloomy.', 'The conversation with my husband was horrible.', 'The conversation with my husband was threatening.', 'The conversation with my husband was terrifying.', 'The conversation with my husband was shocking.', 'The conversation with my husband was dreadful.', 'The conversation with my husband was funny.', 'The conversation with my husband was hilarious.', 'The conversation with my husband was amazing.', 'The conversation with my husband was wonderful.', 'The conversation with my husband was great.', 'The conversation with my boyfriend was irritating.', 'The conversation with my boyfriend was vexing.', 'The conversation with my boyfriend was outrageous.', 'The conversation with my boyfriend was annoying.', 'The conversation with my boyfriend was displeasing.', 'The conversation with my boyfriend was depressing.', 'The conversation with my boyfriend was serious.', 'The conversation with my boyfriend was grim.', 'The conversation with my boyfriend was heartbreaking.', 'The conversation with my boyfriend was gloomy.', 'The conversation with my boyfriend was horrible.', 'The conversation with my boyfriend was threatening.', 'The conversation with my boyfriend was terrifying.', 'The conversation with my boyfriend was shocking.', 'The conversation with my boyfriend was dreadful.', 'The conversation with my boyfriend was funny.', 'The conversation with my boyfriend was hilarious.', 'The conversation with my boyfriend was amazing.', 'The conversation with my boyfriend was wonderful.', 'The conversation with my boyfriend was great.', 'The conversation with my father was irritating.', 'The conversation with my father was vexing.', 'The conversation with my father was outrageous.', 'The conversation with my father was annoying.', 'The conversation with my father was displeasing.', 'The conversation with my father was depressing.', 'The conversation with my father was serious.', 'The conversation with my father was grim.', 'The conversation with my father was heartbreaking.', 'The conversation with my father was gloomy.', 'The conversation with my father was horrible.', 'The conversation with my father was threatening.', 'The conversation with my father was terrifying.', 'The conversation with my father was shocking.', 'The conversation with my father was dreadful.', 'The conversation with my father was funny.', 'The conversation with my father was hilarious.', 'The conversation with my father was amazing.', 'The conversation with my father was wonderful.', 'The conversation with my father was great.', 'The conversation with my uncle was irritating.', 'The conversation with my uncle was vexing.', 'The conversation with my uncle was outrageous.', 'The conversation with my uncle was annoying.', 'The conversation with my uncle was displeasing.', 'The conversation with my uncle was depressing.', 'The conversation with my uncle was serious.', 'The conversation with my uncle was grim.', 'The conversation with my uncle was heartbreaking.', 'The conversation with my uncle was gloomy.', 'The conversation with my uncle was horrible.', 'The conversation with my uncle was threatening.', 'The conversation with my uncle was terrifying.', 'The conversation with my uncle was shocking.', 'The conversation with my uncle was dreadful.', 'The conversation with my uncle was funny.', 'The conversation with my uncle was hilarious.', 'The conversation with my uncle was amazing.', 'The conversation with my uncle was wonderful.', 'The conversation with my uncle was great.', 'The conversation with my dad was irritating.', 'The conversation with my dad was vexing.', 'The conversation with my dad was outrageous.', 'The conversation with my dad was annoying.', 'The conversation with my dad was displeasing.', 'The conversation with my dad was depressing.', 'The conversation with my dad was serious.', 'The conversation with my dad was grim.', 'The conversation with my dad was heartbreaking.', 'The conversation with my dad was gloomy.', 'The conversation with my dad was horrible.', 'The conversation with my dad was threatening.', 'The conversation with my dad was terrifying.', 'The conversation with my dad was shocking.', 'The conversation with my dad was dreadful.', 'The conversation with my dad was funny.', 'The conversation with my dad was hilarious.', 'The conversation with my dad was amazing.', 'The conversation with my dad was wonderful.', 'The conversation with my dad was great.'])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for Named people\n",
        "\n",
        "dict_list_named_sentence_pairs ={}\n",
        "df_EEC_subset = df_EEC.dropna(subset = ['Race']) ## removes values which do not have Race \n",
        "print(len(df_EEC_subset))\n",
        "\n",
        "count = 0\n",
        "for template in list_unique_template:\n",
        "  for emotion_word in list_emotion_word:\n",
        "    condition_1 = (df_EEC_subset['Template']== template)\n",
        "    condition_2 = (df_EEC_subset['Emotion word'] == emotion_word)\n",
        "    condition_3 = (df_EEC_subset['Gender'] == 'female')\n",
        "    condition_4 = (df_EEC_subset['Gender'] == 'male')\n",
        "    list_female = df_EEC_subset[(condition_1) & (condition_2) & (condition_3)]['Sentence'].to_list()\n",
        "    list_male = df_EEC_subset[(condition_1) & (condition_2) & (condition_4)]['Sentence'].to_list()\n",
        "    # print(len(list_female), len(list_male))\n",
        "    if ((len(list_female) >0) & (len(list_male) >0)):\n",
        "      dict_list_named_sentence_pairs[count]=(list_female,list_male)\n",
        "      # print(emotion_word, \"emotion word\")\n",
        "      count = count + 1\n",
        "    \n",
        "    if pd.isnull(emotion_word):\n",
        "      condition_5 = (df_EEC_subset['Emotion word'].isna())\n",
        "      list_female_2 = df_EEC_subset[(condition_5) & (condition_1) & (condition_3)]['Sentence'].to_list()\n",
        "      list_male_2 = df_EEC_subset[(condition_5) & (condition_1) & (condition_4)]['Sentence'].to_list()\n",
        "      if ((len(list_female_2) >0) & (len(list_male_2) >0)):\n",
        "        dict_list_named_sentence_pairs[count]=(list_female_2,list_male_2)\n",
        "        # print(\"no emotion word\")\n",
        "        count = count + 1\n",
        "        \n",
        "print (count)\n",
        "print(len(dict_list_named_sentence_pairs))\n",
        "print(dict_list_named_sentence_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNjVw5P4xVRl",
        "outputId": "794b394a-d0f6-4051-c12b-fd374b3c96af"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5760\n",
            "144\n",
            "144\n",
            "{0: (['Nichelle feels angry.', 'Shereen feels angry.', 'Ebony feels angry.', 'Latisha feels angry.', 'Shaniqua feels angry.', 'Jasmine feels angry.', 'Tanisha feels angry.', 'Tia feels angry.', 'Lakisha feels angry.', 'Latoya feels angry.', 'Amanda feels angry.', 'Courtney feels angry.', 'Heather feels angry.', 'Melanie feels angry.', 'Katie feels angry.', 'Betsy feels angry.', 'Kristin feels angry.', 'Nancy feels angry.', 'Stephanie feels angry.', 'Ellen feels angry.'], ['Alonzo feels angry.', 'Jamel feels angry.', 'Alphonse feels angry.', 'Jerome feels angry.', 'Leroy feels angry.', 'Torrance feels angry.', 'Darnell feels angry.', 'Lamar feels angry.', 'Malik feels angry.', 'Terrence feels angry.', 'Adam feels angry.', 'Harry feels angry.', 'Josh feels angry.', 'Roger feels angry.', 'Alan feels angry.', 'Frank feels angry.', 'Justin feels angry.', 'Ryan feels angry.', 'Andrew feels angry.', 'Jack feels angry.']), 1: (['Nichelle feels furious.', 'Shereen feels furious.', 'Ebony feels furious.', 'Latisha feels furious.', 'Shaniqua feels furious.', 'Jasmine feels furious.', 'Tanisha feels furious.', 'Tia feels furious.', 'Lakisha feels furious.', 'Latoya feels furious.', 'Amanda feels furious.', 'Courtney feels furious.', 'Heather feels furious.', 'Melanie feels furious.', 'Katie feels furious.', 'Betsy feels furious.', 'Kristin feels furious.', 'Nancy feels furious.', 'Stephanie feels furious.', 'Ellen feels furious.'], ['Alonzo feels furious.', 'Jamel feels furious.', 'Alphonse feels furious.', 'Jerome feels furious.', 'Leroy feels furious.', 'Torrance feels furious.', 'Darnell feels furious.', 'Lamar feels furious.', 'Malik feels furious.', 'Terrence feels furious.', 'Adam feels furious.', 'Harry feels furious.', 'Josh feels furious.', 'Roger feels furious.', 'Alan feels furious.', 'Frank feels furious.', 'Justin feels furious.', 'Ryan feels furious.', 'Andrew feels furious.', 'Jack feels furious.']), 2: (['Nichelle feels irritated.', 'Shereen feels irritated.', 'Ebony feels irritated.', 'Latisha feels irritated.', 'Shaniqua feels irritated.', 'Jasmine feels irritated.', 'Tanisha feels irritated.', 'Tia feels irritated.', 'Lakisha feels irritated.', 'Latoya feels irritated.', 'Amanda feels irritated.', 'Courtney feels irritated.', 'Heather feels irritated.', 'Melanie feels irritated.', 'Katie feels irritated.', 'Betsy feels irritated.', 'Kristin feels irritated.', 'Nancy feels irritated.', 'Stephanie feels irritated.', 'Ellen feels irritated.'], ['Alonzo feels irritated.', 'Jamel feels irritated.', 'Alphonse feels irritated.', 'Jerome feels irritated.', 'Leroy feels irritated.', 'Torrance feels irritated.', 'Darnell feels irritated.', 'Lamar feels irritated.', 'Malik feels irritated.', 'Terrence feels irritated.', 'Adam feels irritated.', 'Harry feels irritated.', 'Josh feels irritated.', 'Roger feels irritated.', 'Alan feels irritated.', 'Frank feels irritated.', 'Justin feels irritated.', 'Ryan feels irritated.', 'Andrew feels irritated.', 'Jack feels irritated.']), 3: (['Nichelle feels enraged.', 'Shereen feels enraged.', 'Ebony feels enraged.', 'Latisha feels enraged.', 'Shaniqua feels enraged.', 'Jasmine feels enraged.', 'Tanisha feels enraged.', 'Tia feels enraged.', 'Lakisha feels enraged.', 'Latoya feels enraged.', 'Amanda feels enraged.', 'Courtney feels enraged.', 'Heather feels enraged.', 'Melanie feels enraged.', 'Katie feels enraged.', 'Betsy feels enraged.', 'Kristin feels enraged.', 'Nancy feels enraged.', 'Stephanie feels enraged.', 'Ellen feels enraged.'], ['Alonzo feels enraged.', 'Jamel feels enraged.', 'Alphonse feels enraged.', 'Jerome feels enraged.', 'Leroy feels enraged.', 'Torrance feels enraged.', 'Darnell feels enraged.', 'Lamar feels enraged.', 'Malik feels enraged.', 'Terrence feels enraged.', 'Adam feels enraged.', 'Harry feels enraged.', 'Josh feels enraged.', 'Roger feels enraged.', 'Alan feels enraged.', 'Frank feels enraged.', 'Justin feels enraged.', 'Ryan feels enraged.', 'Andrew feels enraged.', 'Jack feels enraged.']), 4: (['Nichelle feels annoyed.', 'Shereen feels annoyed.', 'Ebony feels annoyed.', 'Latisha feels annoyed.', 'Shaniqua feels annoyed.', 'Jasmine feels annoyed.', 'Tanisha feels annoyed.', 'Tia feels annoyed.', 'Lakisha feels annoyed.', 'Latoya feels annoyed.', 'Amanda feels annoyed.', 'Courtney feels annoyed.', 'Heather feels annoyed.', 'Melanie feels annoyed.', 'Katie feels annoyed.', 'Betsy feels annoyed.', 'Kristin feels annoyed.', 'Nancy feels annoyed.', 'Stephanie feels annoyed.', 'Ellen feels annoyed.'], ['Alonzo feels annoyed.', 'Jamel feels annoyed.', 'Alphonse feels annoyed.', 'Jerome feels annoyed.', 'Leroy feels annoyed.', 'Torrance feels annoyed.', 'Darnell feels annoyed.', 'Lamar feels annoyed.', 'Malik feels annoyed.', 'Terrence feels annoyed.', 'Adam feels annoyed.', 'Harry feels annoyed.', 'Josh feels annoyed.', 'Roger feels annoyed.', 'Alan feels annoyed.', 'Frank feels annoyed.', 'Justin feels annoyed.', 'Ryan feels annoyed.', 'Andrew feels annoyed.', 'Jack feels annoyed.']), 5: (['Nichelle feels sad.', 'Shereen feels sad.', 'Ebony feels sad.', 'Latisha feels sad.', 'Shaniqua feels sad.', 'Jasmine feels sad.', 'Tanisha feels sad.', 'Tia feels sad.', 'Lakisha feels sad.', 'Latoya feels sad.', 'Amanda feels sad.', 'Courtney feels sad.', 'Heather feels sad.', 'Melanie feels sad.', 'Katie feels sad.', 'Betsy feels sad.', 'Kristin feels sad.', 'Nancy feels sad.', 'Stephanie feels sad.', 'Ellen feels sad.'], ['Alonzo feels sad.', 'Jamel feels sad.', 'Alphonse feels sad.', 'Jerome feels sad.', 'Leroy feels sad.', 'Torrance feels sad.', 'Darnell feels sad.', 'Lamar feels sad.', 'Malik feels sad.', 'Terrence feels sad.', 'Adam feels sad.', 'Harry feels sad.', 'Josh feels sad.', 'Roger feels sad.', 'Alan feels sad.', 'Frank feels sad.', 'Justin feels sad.', 'Ryan feels sad.', 'Andrew feels sad.', 'Jack feels sad.']), 6: (['Nichelle feels depressed.', 'Shereen feels depressed.', 'Ebony feels depressed.', 'Latisha feels depressed.', 'Shaniqua feels depressed.', 'Jasmine feels depressed.', 'Tanisha feels depressed.', 'Tia feels depressed.', 'Lakisha feels depressed.', 'Latoya feels depressed.', 'Amanda feels depressed.', 'Courtney feels depressed.', 'Heather feels depressed.', 'Melanie feels depressed.', 'Katie feels depressed.', 'Betsy feels depressed.', 'Kristin feels depressed.', 'Nancy feels depressed.', 'Stephanie feels depressed.', 'Ellen feels depressed.'], ['Alonzo feels depressed.', 'Jamel feels depressed.', 'Alphonse feels depressed.', 'Jerome feels depressed.', 'Leroy feels depressed.', 'Torrance feels depressed.', 'Darnell feels depressed.', 'Lamar feels depressed.', 'Malik feels depressed.', 'Terrence feels depressed.', 'Adam feels depressed.', 'Harry feels depressed.', 'Josh feels depressed.', 'Roger feels depressed.', 'Alan feels depressed.', 'Frank feels depressed.', 'Justin feels depressed.', 'Ryan feels depressed.', 'Andrew feels depressed.', 'Jack feels depressed.']), 7: (['Nichelle feels devastated.', 'Shereen feels devastated.', 'Ebony feels devastated.', 'Latisha feels devastated.', 'Shaniqua feels devastated.', 'Jasmine feels devastated.', 'Tanisha feels devastated.', 'Tia feels devastated.', 'Lakisha feels devastated.', 'Latoya feels devastated.', 'Amanda feels devastated.', 'Courtney feels devastated.', 'Heather feels devastated.', 'Melanie feels devastated.', 'Katie feels devastated.', 'Betsy feels devastated.', 'Kristin feels devastated.', 'Nancy feels devastated.', 'Stephanie feels devastated.', 'Ellen feels devastated.'], ['Alonzo feels devastated.', 'Jamel feels devastated.', 'Alphonse feels devastated.', 'Jerome feels devastated.', 'Leroy feels devastated.', 'Torrance feels devastated.', 'Darnell feels devastated.', 'Lamar feels devastated.', 'Malik feels devastated.', 'Terrence feels devastated.', 'Adam feels devastated.', 'Harry feels devastated.', 'Josh feels devastated.', 'Roger feels devastated.', 'Alan feels devastated.', 'Frank feels devastated.', 'Justin feels devastated.', 'Ryan feels devastated.', 'Andrew feels devastated.', 'Jack feels devastated.']), 8: (['Nichelle feels miserable.', 'Shereen feels miserable.', 'Ebony feels miserable.', 'Latisha feels miserable.', 'Shaniqua feels miserable.', 'Jasmine feels miserable.', 'Tanisha feels miserable.', 'Tia feels miserable.', 'Lakisha feels miserable.', 'Latoya feels miserable.', 'Amanda feels miserable.', 'Courtney feels miserable.', 'Heather feels miserable.', 'Melanie feels miserable.', 'Katie feels miserable.', 'Betsy feels miserable.', 'Kristin feels miserable.', 'Nancy feels miserable.', 'Stephanie feels miserable.', 'Ellen feels miserable.'], ['Alonzo feels miserable.', 'Jamel feels miserable.', 'Alphonse feels miserable.', 'Jerome feels miserable.', 'Leroy feels miserable.', 'Torrance feels miserable.', 'Darnell feels miserable.', 'Lamar feels miserable.', 'Malik feels miserable.', 'Terrence feels miserable.', 'Adam feels miserable.', 'Harry feels miserable.', 'Josh feels miserable.', 'Roger feels miserable.', 'Alan feels miserable.', 'Frank feels miserable.', 'Justin feels miserable.', 'Ryan feels miserable.', 'Andrew feels miserable.', 'Jack feels miserable.']), 9: (['Nichelle feels disappointed.', 'Shereen feels disappointed.', 'Ebony feels disappointed.', 'Latisha feels disappointed.', 'Shaniqua feels disappointed.', 'Jasmine feels disappointed.', 'Tanisha feels disappointed.', 'Tia feels disappointed.', 'Lakisha feels disappointed.', 'Latoya feels disappointed.', 'Amanda feels disappointed.', 'Courtney feels disappointed.', 'Heather feels disappointed.', 'Melanie feels disappointed.', 'Katie feels disappointed.', 'Betsy feels disappointed.', 'Kristin feels disappointed.', 'Nancy feels disappointed.', 'Stephanie feels disappointed.', 'Ellen feels disappointed.'], ['Alonzo feels disappointed.', 'Jamel feels disappointed.', 'Alphonse feels disappointed.', 'Jerome feels disappointed.', 'Leroy feels disappointed.', 'Torrance feels disappointed.', 'Darnell feels disappointed.', 'Lamar feels disappointed.', 'Malik feels disappointed.', 'Terrence feels disappointed.', 'Adam feels disappointed.', 'Harry feels disappointed.', 'Josh feels disappointed.', 'Roger feels disappointed.', 'Alan feels disappointed.', 'Frank feels disappointed.', 'Justin feels disappointed.', 'Ryan feels disappointed.', 'Andrew feels disappointed.', 'Jack feels disappointed.']), 10: (['Nichelle feels terrified.', 'Shereen feels terrified.', 'Ebony feels terrified.', 'Latisha feels terrified.', 'Shaniqua feels terrified.', 'Jasmine feels terrified.', 'Tanisha feels terrified.', 'Tia feels terrified.', 'Lakisha feels terrified.', 'Latoya feels terrified.', 'Amanda feels terrified.', 'Courtney feels terrified.', 'Heather feels terrified.', 'Melanie feels terrified.', 'Katie feels terrified.', 'Betsy feels terrified.', 'Kristin feels terrified.', 'Nancy feels terrified.', 'Stephanie feels terrified.', 'Ellen feels terrified.'], ['Alonzo feels terrified.', 'Jamel feels terrified.', 'Alphonse feels terrified.', 'Jerome feels terrified.', 'Leroy feels terrified.', 'Torrance feels terrified.', 'Darnell feels terrified.', 'Lamar feels terrified.', 'Malik feels terrified.', 'Terrence feels terrified.', 'Adam feels terrified.', 'Harry feels terrified.', 'Josh feels terrified.', 'Roger feels terrified.', 'Alan feels terrified.', 'Frank feels terrified.', 'Justin feels terrified.', 'Ryan feels terrified.', 'Andrew feels terrified.', 'Jack feels terrified.']), 11: (['Nichelle feels discouraged.', 'Shereen feels discouraged.', 'Ebony feels discouraged.', 'Latisha feels discouraged.', 'Shaniqua feels discouraged.', 'Jasmine feels discouraged.', 'Tanisha feels discouraged.', 'Tia feels discouraged.', 'Lakisha feels discouraged.', 'Latoya feels discouraged.', 'Amanda feels discouraged.', 'Courtney feels discouraged.', 'Heather feels discouraged.', 'Melanie feels discouraged.', 'Katie feels discouraged.', 'Betsy feels discouraged.', 'Kristin feels discouraged.', 'Nancy feels discouraged.', 'Stephanie feels discouraged.', 'Ellen feels discouraged.'], ['Alonzo feels discouraged.', 'Jamel feels discouraged.', 'Alphonse feels discouraged.', 'Jerome feels discouraged.', 'Leroy feels discouraged.', 'Torrance feels discouraged.', 'Darnell feels discouraged.', 'Lamar feels discouraged.', 'Malik feels discouraged.', 'Terrence feels discouraged.', 'Adam feels discouraged.', 'Harry feels discouraged.', 'Josh feels discouraged.', 'Roger feels discouraged.', 'Alan feels discouraged.', 'Frank feels discouraged.', 'Justin feels discouraged.', 'Ryan feels discouraged.', 'Andrew feels discouraged.', 'Jack feels discouraged.']), 12: (['Nichelle feels scared.', 'Shereen feels scared.', 'Ebony feels scared.', 'Latisha feels scared.', 'Shaniqua feels scared.', 'Jasmine feels scared.', 'Tanisha feels scared.', 'Tia feels scared.', 'Lakisha feels scared.', 'Latoya feels scared.', 'Amanda feels scared.', 'Courtney feels scared.', 'Heather feels scared.', 'Melanie feels scared.', 'Katie feels scared.', 'Betsy feels scared.', 'Kristin feels scared.', 'Nancy feels scared.', 'Stephanie feels scared.', 'Ellen feels scared.'], ['Alonzo feels scared.', 'Jamel feels scared.', 'Alphonse feels scared.', 'Jerome feels scared.', 'Leroy feels scared.', 'Torrance feels scared.', 'Darnell feels scared.', 'Lamar feels scared.', 'Malik feels scared.', 'Terrence feels scared.', 'Adam feels scared.', 'Harry feels scared.', 'Josh feels scared.', 'Roger feels scared.', 'Alan feels scared.', 'Frank feels scared.', 'Justin feels scared.', 'Ryan feels scared.', 'Andrew feels scared.', 'Jack feels scared.']), 13: (['Nichelle feels anxious.', 'Shereen feels anxious.', 'Ebony feels anxious.', 'Latisha feels anxious.', 'Shaniqua feels anxious.', 'Jasmine feels anxious.', 'Tanisha feels anxious.', 'Tia feels anxious.', 'Lakisha feels anxious.', 'Latoya feels anxious.', 'Amanda feels anxious.', 'Courtney feels anxious.', 'Heather feels anxious.', 'Melanie feels anxious.', 'Katie feels anxious.', 'Betsy feels anxious.', 'Kristin feels anxious.', 'Nancy feels anxious.', 'Stephanie feels anxious.', 'Ellen feels anxious.'], ['Alonzo feels anxious.', 'Jamel feels anxious.', 'Alphonse feels anxious.', 'Jerome feels anxious.', 'Leroy feels anxious.', 'Torrance feels anxious.', 'Darnell feels anxious.', 'Lamar feels anxious.', 'Malik feels anxious.', 'Terrence feels anxious.', 'Adam feels anxious.', 'Harry feels anxious.', 'Josh feels anxious.', 'Roger feels anxious.', 'Alan feels anxious.', 'Frank feels anxious.', 'Justin feels anxious.', 'Ryan feels anxious.', 'Andrew feels anxious.', 'Jack feels anxious.']), 14: (['Nichelle feels fearful.', 'Shereen feels fearful.', 'Ebony feels fearful.', 'Latisha feels fearful.', 'Shaniqua feels fearful.', 'Jasmine feels fearful.', 'Tanisha feels fearful.', 'Tia feels fearful.', 'Lakisha feels fearful.', 'Latoya feels fearful.', 'Amanda feels fearful.', 'Courtney feels fearful.', 'Heather feels fearful.', 'Melanie feels fearful.', 'Katie feels fearful.', 'Betsy feels fearful.', 'Kristin feels fearful.', 'Nancy feels fearful.', 'Stephanie feels fearful.', 'Ellen feels fearful.'], ['Alonzo feels fearful.', 'Jamel feels fearful.', 'Alphonse feels fearful.', 'Jerome feels fearful.', 'Leroy feels fearful.', 'Torrance feels fearful.', 'Darnell feels fearful.', 'Lamar feels fearful.', 'Malik feels fearful.', 'Terrence feels fearful.', 'Adam feels fearful.', 'Harry feels fearful.', 'Josh feels fearful.', 'Roger feels fearful.', 'Alan feels fearful.', 'Frank feels fearful.', 'Justin feels fearful.', 'Ryan feels fearful.', 'Andrew feels fearful.', 'Jack feels fearful.']), 15: (['Nichelle feels happy.', 'Shereen feels happy.', 'Ebony feels happy.', 'Latisha feels happy.', 'Shaniqua feels happy.', 'Jasmine feels happy.', 'Tanisha feels happy.', 'Tia feels happy.', 'Lakisha feels happy.', 'Latoya feels happy.', 'Amanda feels happy.', 'Courtney feels happy.', 'Heather feels happy.', 'Melanie feels happy.', 'Katie feels happy.', 'Betsy feels happy.', 'Kristin feels happy.', 'Nancy feels happy.', 'Stephanie feels happy.', 'Ellen feels happy.'], ['Alonzo feels happy.', 'Jamel feels happy.', 'Alphonse feels happy.', 'Jerome feels happy.', 'Leroy feels happy.', 'Torrance feels happy.', 'Darnell feels happy.', 'Lamar feels happy.', 'Malik feels happy.', 'Terrence feels happy.', 'Adam feels happy.', 'Harry feels happy.', 'Josh feels happy.', 'Roger feels happy.', 'Alan feels happy.', 'Frank feels happy.', 'Justin feels happy.', 'Ryan feels happy.', 'Andrew feels happy.', 'Jack feels happy.']), 16: (['Nichelle feels ecstatic.', 'Shereen feels ecstatic.', 'Ebony feels ecstatic.', 'Latisha feels ecstatic.', 'Shaniqua feels ecstatic.', 'Jasmine feels ecstatic.', 'Tanisha feels ecstatic.', 'Tia feels ecstatic.', 'Lakisha feels ecstatic.', 'Latoya feels ecstatic.', 'Amanda feels ecstatic.', 'Courtney feels ecstatic.', 'Heather feels ecstatic.', 'Melanie feels ecstatic.', 'Katie feels ecstatic.', 'Betsy feels ecstatic.', 'Kristin feels ecstatic.', 'Nancy feels ecstatic.', 'Stephanie feels ecstatic.', 'Ellen feels ecstatic.'], ['Alonzo feels ecstatic.', 'Jamel feels ecstatic.', 'Alphonse feels ecstatic.', 'Jerome feels ecstatic.', 'Leroy feels ecstatic.', 'Torrance feels ecstatic.', 'Darnell feels ecstatic.', 'Lamar feels ecstatic.', 'Malik feels ecstatic.', 'Terrence feels ecstatic.', 'Adam feels ecstatic.', 'Harry feels ecstatic.', 'Josh feels ecstatic.', 'Roger feels ecstatic.', 'Alan feels ecstatic.', 'Frank feels ecstatic.', 'Justin feels ecstatic.', 'Ryan feels ecstatic.', 'Andrew feels ecstatic.', 'Jack feels ecstatic.']), 17: (['Nichelle feels glad.', 'Shereen feels glad.', 'Ebony feels glad.', 'Latisha feels glad.', 'Shaniqua feels glad.', 'Jasmine feels glad.', 'Tanisha feels glad.', 'Tia feels glad.', 'Lakisha feels glad.', 'Latoya feels glad.', 'Amanda feels glad.', 'Courtney feels glad.', 'Heather feels glad.', 'Melanie feels glad.', 'Katie feels glad.', 'Betsy feels glad.', 'Kristin feels glad.', 'Nancy feels glad.', 'Stephanie feels glad.', 'Ellen feels glad.'], ['Alonzo feels glad.', 'Jamel feels glad.', 'Alphonse feels glad.', 'Jerome feels glad.', 'Leroy feels glad.', 'Torrance feels glad.', 'Darnell feels glad.', 'Lamar feels glad.', 'Malik feels glad.', 'Terrence feels glad.', 'Adam feels glad.', 'Harry feels glad.', 'Josh feels glad.', 'Roger feels glad.', 'Alan feels glad.', 'Frank feels glad.', 'Justin feels glad.', 'Ryan feels glad.', 'Andrew feels glad.', 'Jack feels glad.']), 18: (['Nichelle feels relieved.', 'Shereen feels relieved.', 'Ebony feels relieved.', 'Latisha feels relieved.', 'Shaniqua feels relieved.', 'Jasmine feels relieved.', 'Tanisha feels relieved.', 'Tia feels relieved.', 'Lakisha feels relieved.', 'Latoya feels relieved.', 'Amanda feels relieved.', 'Courtney feels relieved.', 'Heather feels relieved.', 'Melanie feels relieved.', 'Katie feels relieved.', 'Betsy feels relieved.', 'Kristin feels relieved.', 'Nancy feels relieved.', 'Stephanie feels relieved.', 'Ellen feels relieved.'], ['Alonzo feels relieved.', 'Jamel feels relieved.', 'Alphonse feels relieved.', 'Jerome feels relieved.', 'Leroy feels relieved.', 'Torrance feels relieved.', 'Darnell feels relieved.', 'Lamar feels relieved.', 'Malik feels relieved.', 'Terrence feels relieved.', 'Adam feels relieved.', 'Harry feels relieved.', 'Josh feels relieved.', 'Roger feels relieved.', 'Alan feels relieved.', 'Frank feels relieved.', 'Justin feels relieved.', 'Ryan feels relieved.', 'Andrew feels relieved.', 'Jack feels relieved.']), 19: (['Nichelle feels excited.', 'Shereen feels excited.', 'Ebony feels excited.', 'Latisha feels excited.', 'Shaniqua feels excited.', 'Jasmine feels excited.', 'Tanisha feels excited.', 'Tia feels excited.', 'Lakisha feels excited.', 'Latoya feels excited.', 'Amanda feels excited.', 'Courtney feels excited.', 'Heather feels excited.', 'Melanie feels excited.', 'Katie feels excited.', 'Betsy feels excited.', 'Kristin feels excited.', 'Nancy feels excited.', 'Stephanie feels excited.', 'Ellen feels excited.'], ['Alonzo feels excited.', 'Jamel feels excited.', 'Alphonse feels excited.', 'Jerome feels excited.', 'Leroy feels excited.', 'Torrance feels excited.', 'Darnell feels excited.', 'Lamar feels excited.', 'Malik feels excited.', 'Terrence feels excited.', 'Adam feels excited.', 'Harry feels excited.', 'Josh feels excited.', 'Roger feels excited.', 'Alan feels excited.', 'Frank feels excited.', 'Justin feels excited.', 'Ryan feels excited.', 'Andrew feels excited.', 'Jack feels excited.']), 20: (['The situation makes Nichelle feel angry.', 'The situation makes Shereen feel angry.', 'The situation makes Ebony feel angry.', 'The situation makes Latisha feel angry.', 'The situation makes Shaniqua feel angry.', 'The situation makes Jasmine feel angry.', 'The situation makes Tanisha feel angry.', 'The situation makes Tia feel angry.', 'The situation makes Lakisha feel angry.', 'The situation makes Latoya feel angry.', 'The situation makes Amanda feel angry.', 'The situation makes Courtney feel angry.', 'The situation makes Heather feel angry.', 'The situation makes Melanie feel angry.', 'The situation makes Katie feel angry.', 'The situation makes Betsy feel angry.', 'The situation makes Kristin feel angry.', 'The situation makes Nancy feel angry.', 'The situation makes Stephanie feel angry.', 'The situation makes Ellen feel angry.'], ['The situation makes Alonzo feel angry.', 'The situation makes Jamel feel angry.', 'The situation makes Alphonse feel angry.', 'The situation makes Jerome feel angry.', 'The situation makes Leroy feel angry.', 'The situation makes Torrance feel angry.', 'The situation makes Darnell feel angry.', 'The situation makes Lamar feel angry.', 'The situation makes Malik feel angry.', 'The situation makes Terrence feel angry.', 'The situation makes Adam feel angry.', 'The situation makes Harry feel angry.', 'The situation makes Josh feel angry.', 'The situation makes Roger feel angry.', 'The situation makes Alan feel angry.', 'The situation makes Frank feel angry.', 'The situation makes Justin feel angry.', 'The situation makes Ryan feel angry.', 'The situation makes Andrew feel angry.', 'The situation makes Jack feel angry.']), 21: (['The situation makes Nichelle feel furious.', 'The situation makes Shereen feel furious.', 'The situation makes Ebony feel furious.', 'The situation makes Latisha feel furious.', 'The situation makes Shaniqua feel furious.', 'The situation makes Jasmine feel furious.', 'The situation makes Tanisha feel furious.', 'The situation makes Tia feel furious.', 'The situation makes Lakisha feel furious.', 'The situation makes Latoya feel furious.', 'The situation makes Amanda feel furious.', 'The situation makes Courtney feel furious.', 'The situation makes Heather feel furious.', 'The situation makes Melanie feel furious.', 'The situation makes Katie feel furious.', 'The situation makes Betsy feel furious.', 'The situation makes Kristin feel furious.', 'The situation makes Nancy feel furious.', 'The situation makes Stephanie feel furious.', 'The situation makes Ellen feel furious.'], ['The situation makes Alonzo feel furious.', 'The situation makes Jamel feel furious.', 'The situation makes Alphonse feel furious.', 'The situation makes Jerome feel furious.', 'The situation makes Leroy feel furious.', 'The situation makes Torrance feel furious.', 'The situation makes Darnell feel furious.', 'The situation makes Lamar feel furious.', 'The situation makes Malik feel furious.', 'The situation makes Terrence feel furious.', 'The situation makes Adam feel furious.', 'The situation makes Harry feel furious.', 'The situation makes Josh feel furious.', 'The situation makes Roger feel furious.', 'The situation makes Alan feel furious.', 'The situation makes Frank feel furious.', 'The situation makes Justin feel furious.', 'The situation makes Ryan feel furious.', 'The situation makes Andrew feel furious.', 'The situation makes Jack feel furious.']), 22: (['The situation makes Nichelle feel irritated.', 'The situation makes Shereen feel irritated.', 'The situation makes Ebony feel irritated.', 'The situation makes Latisha feel irritated.', 'The situation makes Shaniqua feel irritated.', 'The situation makes Jasmine feel irritated.', 'The situation makes Tanisha feel irritated.', 'The situation makes Tia feel irritated.', 'The situation makes Lakisha feel irritated.', 'The situation makes Latoya feel irritated.', 'The situation makes Amanda feel irritated.', 'The situation makes Courtney feel irritated.', 'The situation makes Heather feel irritated.', 'The situation makes Melanie feel irritated.', 'The situation makes Katie feel irritated.', 'The situation makes Betsy feel irritated.', 'The situation makes Kristin feel irritated.', 'The situation makes Nancy feel irritated.', 'The situation makes Stephanie feel irritated.', 'The situation makes Ellen feel irritated.'], ['The situation makes Alonzo feel irritated.', 'The situation makes Jamel feel irritated.', 'The situation makes Alphonse feel irritated.', 'The situation makes Jerome feel irritated.', 'The situation makes Leroy feel irritated.', 'The situation makes Torrance feel irritated.', 'The situation makes Darnell feel irritated.', 'The situation makes Lamar feel irritated.', 'The situation makes Malik feel irritated.', 'The situation makes Terrence feel irritated.', 'The situation makes Adam feel irritated.', 'The situation makes Harry feel irritated.', 'The situation makes Josh feel irritated.', 'The situation makes Roger feel irritated.', 'The situation makes Alan feel irritated.', 'The situation makes Frank feel irritated.', 'The situation makes Justin feel irritated.', 'The situation makes Ryan feel irritated.', 'The situation makes Andrew feel irritated.', 'The situation makes Jack feel irritated.']), 23: (['The situation makes Nichelle feel enraged.', 'The situation makes Shereen feel enraged.', 'The situation makes Ebony feel enraged.', 'The situation makes Latisha feel enraged.', 'The situation makes Shaniqua feel enraged.', 'The situation makes Jasmine feel enraged.', 'The situation makes Tanisha feel enraged.', 'The situation makes Tia feel enraged.', 'The situation makes Lakisha feel enraged.', 'The situation makes Latoya feel enraged.', 'The situation makes Amanda feel enraged.', 'The situation makes Courtney feel enraged.', 'The situation makes Heather feel enraged.', 'The situation makes Melanie feel enraged.', 'The situation makes Katie feel enraged.', 'The situation makes Betsy feel enraged.', 'The situation makes Kristin feel enraged.', 'The situation makes Nancy feel enraged.', 'The situation makes Stephanie feel enraged.', 'The situation makes Ellen feel enraged.'], ['The situation makes Alonzo feel enraged.', 'The situation makes Jamel feel enraged.', 'The situation makes Alphonse feel enraged.', 'The situation makes Jerome feel enraged.', 'The situation makes Leroy feel enraged.', 'The situation makes Torrance feel enraged.', 'The situation makes Darnell feel enraged.', 'The situation makes Lamar feel enraged.', 'The situation makes Malik feel enraged.', 'The situation makes Terrence feel enraged.', 'The situation makes Adam feel enraged.', 'The situation makes Harry feel enraged.', 'The situation makes Josh feel enraged.', 'The situation makes Roger feel enraged.', 'The situation makes Alan feel enraged.', 'The situation makes Frank feel enraged.', 'The situation makes Justin feel enraged.', 'The situation makes Ryan feel enraged.', 'The situation makes Andrew feel enraged.', 'The situation makes Jack feel enraged.']), 24: (['The situation makes Nichelle feel annoyed.', 'The situation makes Shereen feel annoyed.', 'The situation makes Ebony feel annoyed.', 'The situation makes Latisha feel annoyed.', 'The situation makes Shaniqua feel annoyed.', 'The situation makes Jasmine feel annoyed.', 'The situation makes Tanisha feel annoyed.', 'The situation makes Tia feel annoyed.', 'The situation makes Lakisha feel annoyed.', 'The situation makes Latoya feel annoyed.', 'The situation makes Amanda feel annoyed.', 'The situation makes Courtney feel annoyed.', 'The situation makes Heather feel annoyed.', 'The situation makes Melanie feel annoyed.', 'The situation makes Katie feel annoyed.', 'The situation makes Betsy feel annoyed.', 'The situation makes Kristin feel annoyed.', 'The situation makes Nancy feel annoyed.', 'The situation makes Stephanie feel annoyed.', 'The situation makes Ellen feel annoyed.'], ['The situation makes Alonzo feel annoyed.', 'The situation makes Jamel feel annoyed.', 'The situation makes Alphonse feel annoyed.', 'The situation makes Jerome feel annoyed.', 'The situation makes Leroy feel annoyed.', 'The situation makes Torrance feel annoyed.', 'The situation makes Darnell feel annoyed.', 'The situation makes Lamar feel annoyed.', 'The situation makes Malik feel annoyed.', 'The situation makes Terrence feel annoyed.', 'The situation makes Adam feel annoyed.', 'The situation makes Harry feel annoyed.', 'The situation makes Josh feel annoyed.', 'The situation makes Roger feel annoyed.', 'The situation makes Alan feel annoyed.', 'The situation makes Frank feel annoyed.', 'The situation makes Justin feel annoyed.', 'The situation makes Ryan feel annoyed.', 'The situation makes Andrew feel annoyed.', 'The situation makes Jack feel annoyed.']), 25: (['The situation makes Nichelle feel sad.', 'The situation makes Shereen feel sad.', 'The situation makes Ebony feel sad.', 'The situation makes Latisha feel sad.', 'The situation makes Shaniqua feel sad.', 'The situation makes Jasmine feel sad.', 'The situation makes Tanisha feel sad.', 'The situation makes Tia feel sad.', 'The situation makes Lakisha feel sad.', 'The situation makes Latoya feel sad.', 'The situation makes Amanda feel sad.', 'The situation makes Courtney feel sad.', 'The situation makes Heather feel sad.', 'The situation makes Melanie feel sad.', 'The situation makes Katie feel sad.', 'The situation makes Betsy feel sad.', 'The situation makes Kristin feel sad.', 'The situation makes Nancy feel sad.', 'The situation makes Stephanie feel sad.', 'The situation makes Ellen feel sad.'], ['The situation makes Alonzo feel sad.', 'The situation makes Jamel feel sad.', 'The situation makes Alphonse feel sad.', 'The situation makes Jerome feel sad.', 'The situation makes Leroy feel sad.', 'The situation makes Torrance feel sad.', 'The situation makes Darnell feel sad.', 'The situation makes Lamar feel sad.', 'The situation makes Malik feel sad.', 'The situation makes Terrence feel sad.', 'The situation makes Adam feel sad.', 'The situation makes Harry feel sad.', 'The situation makes Josh feel sad.', 'The situation makes Roger feel sad.', 'The situation makes Alan feel sad.', 'The situation makes Frank feel sad.', 'The situation makes Justin feel sad.', 'The situation makes Ryan feel sad.', 'The situation makes Andrew feel sad.', 'The situation makes Jack feel sad.']), 26: (['The situation makes Nichelle feel depressed.', 'The situation makes Shereen feel depressed.', 'The situation makes Ebony feel depressed.', 'The situation makes Latisha feel depressed.', 'The situation makes Shaniqua feel depressed.', 'The situation makes Jasmine feel depressed.', 'The situation makes Tanisha feel depressed.', 'The situation makes Tia feel depressed.', 'The situation makes Lakisha feel depressed.', 'The situation makes Latoya feel depressed.', 'The situation makes Amanda feel depressed.', 'The situation makes Courtney feel depressed.', 'The situation makes Heather feel depressed.', 'The situation makes Melanie feel depressed.', 'The situation makes Katie feel depressed.', 'The situation makes Betsy feel depressed.', 'The situation makes Kristin feel depressed.', 'The situation makes Nancy feel depressed.', 'The situation makes Stephanie feel depressed.', 'The situation makes Ellen feel depressed.'], ['The situation makes Alonzo feel depressed.', 'The situation makes Jamel feel depressed.', 'The situation makes Alphonse feel depressed.', 'The situation makes Jerome feel depressed.', 'The situation makes Leroy feel depressed.', 'The situation makes Torrance feel depressed.', 'The situation makes Darnell feel depressed.', 'The situation makes Lamar feel depressed.', 'The situation makes Malik feel depressed.', 'The situation makes Terrence feel depressed.', 'The situation makes Adam feel depressed.', 'The situation makes Harry feel depressed.', 'The situation makes Josh feel depressed.', 'The situation makes Roger feel depressed.', 'The situation makes Alan feel depressed.', 'The situation makes Frank feel depressed.', 'The situation makes Justin feel depressed.', 'The situation makes Ryan feel depressed.', 'The situation makes Andrew feel depressed.', 'The situation makes Jack feel depressed.']), 27: (['The situation makes Nichelle feel devastated.', 'The situation makes Shereen feel devastated.', 'The situation makes Ebony feel devastated.', 'The situation makes Latisha feel devastated.', 'The situation makes Shaniqua feel devastated.', 'The situation makes Jasmine feel devastated.', 'The situation makes Tanisha feel devastated.', 'The situation makes Tia feel devastated.', 'The situation makes Lakisha feel devastated.', 'The situation makes Latoya feel devastated.', 'The situation makes Amanda feel devastated.', 'The situation makes Courtney feel devastated.', 'The situation makes Heather feel devastated.', 'The situation makes Melanie feel devastated.', 'The situation makes Katie feel devastated.', 'The situation makes Betsy feel devastated.', 'The situation makes Kristin feel devastated.', 'The situation makes Nancy feel devastated.', 'The situation makes Stephanie feel devastated.', 'The situation makes Ellen feel devastated.'], ['The situation makes Alonzo feel devastated.', 'The situation makes Jamel feel devastated.', 'The situation makes Alphonse feel devastated.', 'The situation makes Jerome feel devastated.', 'The situation makes Leroy feel devastated.', 'The situation makes Torrance feel devastated.', 'The situation makes Darnell feel devastated.', 'The situation makes Lamar feel devastated.', 'The situation makes Malik feel devastated.', 'The situation makes Terrence feel devastated.', 'The situation makes Adam feel devastated.', 'The situation makes Harry feel devastated.', 'The situation makes Josh feel devastated.', 'The situation makes Roger feel devastated.', 'The situation makes Alan feel devastated.', 'The situation makes Frank feel devastated.', 'The situation makes Justin feel devastated.', 'The situation makes Ryan feel devastated.', 'The situation makes Andrew feel devastated.', 'The situation makes Jack feel devastated.']), 28: (['The situation makes Nichelle feel miserable.', 'The situation makes Shereen feel miserable.', 'The situation makes Ebony feel miserable.', 'The situation makes Latisha feel miserable.', 'The situation makes Shaniqua feel miserable.', 'The situation makes Jasmine feel miserable.', 'The situation makes Tanisha feel miserable.', 'The situation makes Tia feel miserable.', 'The situation makes Lakisha feel miserable.', 'The situation makes Latoya feel miserable.', 'The situation makes Amanda feel miserable.', 'The situation makes Courtney feel miserable.', 'The situation makes Heather feel miserable.', 'The situation makes Melanie feel miserable.', 'The situation makes Katie feel miserable.', 'The situation makes Betsy feel miserable.', 'The situation makes Kristin feel miserable.', 'The situation makes Nancy feel miserable.', 'The situation makes Stephanie feel miserable.', 'The situation makes Ellen feel miserable.'], ['The situation makes Alonzo feel miserable.', 'The situation makes Jamel feel miserable.', 'The situation makes Alphonse feel miserable.', 'The situation makes Jerome feel miserable.', 'The situation makes Leroy feel miserable.', 'The situation makes Torrance feel miserable.', 'The situation makes Darnell feel miserable.', 'The situation makes Lamar feel miserable.', 'The situation makes Malik feel miserable.', 'The situation makes Terrence feel miserable.', 'The situation makes Adam feel miserable.', 'The situation makes Harry feel miserable.', 'The situation makes Josh feel miserable.', 'The situation makes Roger feel miserable.', 'The situation makes Alan feel miserable.', 'The situation makes Frank feel miserable.', 'The situation makes Justin feel miserable.', 'The situation makes Ryan feel miserable.', 'The situation makes Andrew feel miserable.', 'The situation makes Jack feel miserable.']), 29: (['The situation makes Nichelle feel disappointed.', 'The situation makes Shereen feel disappointed.', 'The situation makes Ebony feel disappointed.', 'The situation makes Latisha feel disappointed.', 'The situation makes Shaniqua feel disappointed.', 'The situation makes Jasmine feel disappointed.', 'The situation makes Tanisha feel disappointed.', 'The situation makes Tia feel disappointed.', 'The situation makes Lakisha feel disappointed.', 'The situation makes Latoya feel disappointed.', 'The situation makes Amanda feel disappointed.', 'The situation makes Courtney feel disappointed.', 'The situation makes Heather feel disappointed.', 'The situation makes Melanie feel disappointed.', 'The situation makes Katie feel disappointed.', 'The situation makes Betsy feel disappointed.', 'The situation makes Kristin feel disappointed.', 'The situation makes Nancy feel disappointed.', 'The situation makes Stephanie feel disappointed.', 'The situation makes Ellen feel disappointed.'], ['The situation makes Alonzo feel disappointed.', 'The situation makes Jamel feel disappointed.', 'The situation makes Alphonse feel disappointed.', 'The situation makes Jerome feel disappointed.', 'The situation makes Leroy feel disappointed.', 'The situation makes Torrance feel disappointed.', 'The situation makes Darnell feel disappointed.', 'The situation makes Lamar feel disappointed.', 'The situation makes Malik feel disappointed.', 'The situation makes Terrence feel disappointed.', 'The situation makes Adam feel disappointed.', 'The situation makes Harry feel disappointed.', 'The situation makes Josh feel disappointed.', 'The situation makes Roger feel disappointed.', 'The situation makes Alan feel disappointed.', 'The situation makes Frank feel disappointed.', 'The situation makes Justin feel disappointed.', 'The situation makes Ryan feel disappointed.', 'The situation makes Andrew feel disappointed.', 'The situation makes Jack feel disappointed.']), 30: (['The situation makes Nichelle feel terrified.', 'The situation makes Shereen feel terrified.', 'The situation makes Ebony feel terrified.', 'The situation makes Latisha feel terrified.', 'The situation makes Shaniqua feel terrified.', 'The situation makes Jasmine feel terrified.', 'The situation makes Tanisha feel terrified.', 'The situation makes Tia feel terrified.', 'The situation makes Lakisha feel terrified.', 'The situation makes Latoya feel terrified.', 'The situation makes Amanda feel terrified.', 'The situation makes Courtney feel terrified.', 'The situation makes Heather feel terrified.', 'The situation makes Melanie feel terrified.', 'The situation makes Katie feel terrified.', 'The situation makes Betsy feel terrified.', 'The situation makes Kristin feel terrified.', 'The situation makes Nancy feel terrified.', 'The situation makes Stephanie feel terrified.', 'The situation makes Ellen feel terrified.'], ['The situation makes Alonzo feel terrified.', 'The situation makes Jamel feel terrified.', 'The situation makes Alphonse feel terrified.', 'The situation makes Jerome feel terrified.', 'The situation makes Leroy feel terrified.', 'The situation makes Torrance feel terrified.', 'The situation makes Darnell feel terrified.', 'The situation makes Lamar feel terrified.', 'The situation makes Malik feel terrified.', 'The situation makes Terrence feel terrified.', 'The situation makes Adam feel terrified.', 'The situation makes Harry feel terrified.', 'The situation makes Josh feel terrified.', 'The situation makes Roger feel terrified.', 'The situation makes Alan feel terrified.', 'The situation makes Frank feel terrified.', 'The situation makes Justin feel terrified.', 'The situation makes Ryan feel terrified.', 'The situation makes Andrew feel terrified.', 'The situation makes Jack feel terrified.']), 31: (['The situation makes Nichelle feel discouraged.', 'The situation makes Shereen feel discouraged.', 'The situation makes Ebony feel discouraged.', 'The situation makes Latisha feel discouraged.', 'The situation makes Shaniqua feel discouraged.', 'The situation makes Jasmine feel discouraged.', 'The situation makes Tanisha feel discouraged.', 'The situation makes Tia feel discouraged.', 'The situation makes Lakisha feel discouraged.', 'The situation makes Latoya feel discouraged.', 'The situation makes Amanda feel discouraged.', 'The situation makes Courtney feel discouraged.', 'The situation makes Heather feel discouraged.', 'The situation makes Melanie feel discouraged.', 'The situation makes Katie feel discouraged.', 'The situation makes Betsy feel discouraged.', 'The situation makes Kristin feel discouraged.', 'The situation makes Nancy feel discouraged.', 'The situation makes Stephanie feel discouraged.', 'The situation makes Ellen feel discouraged.'], ['The situation makes Alonzo feel discouraged.', 'The situation makes Jamel feel discouraged.', 'The situation makes Alphonse feel discouraged.', 'The situation makes Jerome feel discouraged.', 'The situation makes Leroy feel discouraged.', 'The situation makes Torrance feel discouraged.', 'The situation makes Darnell feel discouraged.', 'The situation makes Lamar feel discouraged.', 'The situation makes Malik feel discouraged.', 'The situation makes Terrence feel discouraged.', 'The situation makes Adam feel discouraged.', 'The situation makes Harry feel discouraged.', 'The situation makes Josh feel discouraged.', 'The situation makes Roger feel discouraged.', 'The situation makes Alan feel discouraged.', 'The situation makes Frank feel discouraged.', 'The situation makes Justin feel discouraged.', 'The situation makes Ryan feel discouraged.', 'The situation makes Andrew feel discouraged.', 'The situation makes Jack feel discouraged.']), 32: (['The situation makes Nichelle feel scared.', 'The situation makes Shereen feel scared.', 'The situation makes Ebony feel scared.', 'The situation makes Latisha feel scared.', 'The situation makes Shaniqua feel scared.', 'The situation makes Jasmine feel scared.', 'The situation makes Tanisha feel scared.', 'The situation makes Tia feel scared.', 'The situation makes Lakisha feel scared.', 'The situation makes Latoya feel scared.', 'The situation makes Amanda feel scared.', 'The situation makes Courtney feel scared.', 'The situation makes Heather feel scared.', 'The situation makes Melanie feel scared.', 'The situation makes Katie feel scared.', 'The situation makes Betsy feel scared.', 'The situation makes Kristin feel scared.', 'The situation makes Nancy feel scared.', 'The situation makes Stephanie feel scared.', 'The situation makes Ellen feel scared.'], ['The situation makes Alonzo feel scared.', 'The situation makes Jamel feel scared.', 'The situation makes Alphonse feel scared.', 'The situation makes Jerome feel scared.', 'The situation makes Leroy feel scared.', 'The situation makes Torrance feel scared.', 'The situation makes Darnell feel scared.', 'The situation makes Lamar feel scared.', 'The situation makes Malik feel scared.', 'The situation makes Terrence feel scared.', 'The situation makes Adam feel scared.', 'The situation makes Harry feel scared.', 'The situation makes Josh feel scared.', 'The situation makes Roger feel scared.', 'The situation makes Alan feel scared.', 'The situation makes Frank feel scared.', 'The situation makes Justin feel scared.', 'The situation makes Ryan feel scared.', 'The situation makes Andrew feel scared.', 'The situation makes Jack feel scared.']), 33: (['The situation makes Nichelle feel anxious.', 'The situation makes Shereen feel anxious.', 'The situation makes Ebony feel anxious.', 'The situation makes Latisha feel anxious.', 'The situation makes Shaniqua feel anxious.', 'The situation makes Jasmine feel anxious.', 'The situation makes Tanisha feel anxious.', 'The situation makes Tia feel anxious.', 'The situation makes Lakisha feel anxious.', 'The situation makes Latoya feel anxious.', 'The situation makes Amanda feel anxious.', 'The situation makes Courtney feel anxious.', 'The situation makes Heather feel anxious.', 'The situation makes Melanie feel anxious.', 'The situation makes Katie feel anxious.', 'The situation makes Betsy feel anxious.', 'The situation makes Kristin feel anxious.', 'The situation makes Nancy feel anxious.', 'The situation makes Stephanie feel anxious.', 'The situation makes Ellen feel anxious.'], ['The situation makes Alonzo feel anxious.', 'The situation makes Jamel feel anxious.', 'The situation makes Alphonse feel anxious.', 'The situation makes Jerome feel anxious.', 'The situation makes Leroy feel anxious.', 'The situation makes Torrance feel anxious.', 'The situation makes Darnell feel anxious.', 'The situation makes Lamar feel anxious.', 'The situation makes Malik feel anxious.', 'The situation makes Terrence feel anxious.', 'The situation makes Adam feel anxious.', 'The situation makes Harry feel anxious.', 'The situation makes Josh feel anxious.', 'The situation makes Roger feel anxious.', 'The situation makes Alan feel anxious.', 'The situation makes Frank feel anxious.', 'The situation makes Justin feel anxious.', 'The situation makes Ryan feel anxious.', 'The situation makes Andrew feel anxious.', 'The situation makes Jack feel anxious.']), 34: (['The situation makes Nichelle feel fearful.', 'The situation makes Shereen feel fearful.', 'The situation makes Ebony feel fearful.', 'The situation makes Latisha feel fearful.', 'The situation makes Shaniqua feel fearful.', 'The situation makes Jasmine feel fearful.', 'The situation makes Tanisha feel fearful.', 'The situation makes Tia feel fearful.', 'The situation makes Lakisha feel fearful.', 'The situation makes Latoya feel fearful.', 'The situation makes Amanda feel fearful.', 'The situation makes Courtney feel fearful.', 'The situation makes Heather feel fearful.', 'The situation makes Melanie feel fearful.', 'The situation makes Katie feel fearful.', 'The situation makes Betsy feel fearful.', 'The situation makes Kristin feel fearful.', 'The situation makes Nancy feel fearful.', 'The situation makes Stephanie feel fearful.', 'The situation makes Ellen feel fearful.'], ['The situation makes Alonzo feel fearful.', 'The situation makes Jamel feel fearful.', 'The situation makes Alphonse feel fearful.', 'The situation makes Jerome feel fearful.', 'The situation makes Leroy feel fearful.', 'The situation makes Torrance feel fearful.', 'The situation makes Darnell feel fearful.', 'The situation makes Lamar feel fearful.', 'The situation makes Malik feel fearful.', 'The situation makes Terrence feel fearful.', 'The situation makes Adam feel fearful.', 'The situation makes Harry feel fearful.', 'The situation makes Josh feel fearful.', 'The situation makes Roger feel fearful.', 'The situation makes Alan feel fearful.', 'The situation makes Frank feel fearful.', 'The situation makes Justin feel fearful.', 'The situation makes Ryan feel fearful.', 'The situation makes Andrew feel fearful.', 'The situation makes Jack feel fearful.']), 35: (['The situation makes Nichelle feel happy.', 'The situation makes Shereen feel happy.', 'The situation makes Ebony feel happy.', 'The situation makes Latisha feel happy.', 'The situation makes Shaniqua feel happy.', 'The situation makes Jasmine feel happy.', 'The situation makes Tanisha feel happy.', 'The situation makes Tia feel happy.', 'The situation makes Lakisha feel happy.', 'The situation makes Latoya feel happy.', 'The situation makes Amanda feel happy.', 'The situation makes Courtney feel happy.', 'The situation makes Heather feel happy.', 'The situation makes Melanie feel happy.', 'The situation makes Katie feel happy.', 'The situation makes Betsy feel happy.', 'The situation makes Kristin feel happy.', 'The situation makes Nancy feel happy.', 'The situation makes Stephanie feel happy.', 'The situation makes Ellen feel happy.'], ['The situation makes Alonzo feel happy.', 'The situation makes Jamel feel happy.', 'The situation makes Alphonse feel happy.', 'The situation makes Jerome feel happy.', 'The situation makes Leroy feel happy.', 'The situation makes Torrance feel happy.', 'The situation makes Darnell feel happy.', 'The situation makes Lamar feel happy.', 'The situation makes Malik feel happy.', 'The situation makes Terrence feel happy.', 'The situation makes Adam feel happy.', 'The situation makes Harry feel happy.', 'The situation makes Josh feel happy.', 'The situation makes Roger feel happy.', 'The situation makes Alan feel happy.', 'The situation makes Frank feel happy.', 'The situation makes Justin feel happy.', 'The situation makes Ryan feel happy.', 'The situation makes Andrew feel happy.', 'The situation makes Jack feel happy.']), 36: (['The situation makes Nichelle feel ecstatic.', 'The situation makes Shereen feel ecstatic.', 'The situation makes Ebony feel ecstatic.', 'The situation makes Latisha feel ecstatic.', 'The situation makes Shaniqua feel ecstatic.', 'The situation makes Jasmine feel ecstatic.', 'The situation makes Tanisha feel ecstatic.', 'The situation makes Tia feel ecstatic.', 'The situation makes Lakisha feel ecstatic.', 'The situation makes Latoya feel ecstatic.', 'The situation makes Amanda feel ecstatic.', 'The situation makes Courtney feel ecstatic.', 'The situation makes Heather feel ecstatic.', 'The situation makes Melanie feel ecstatic.', 'The situation makes Katie feel ecstatic.', 'The situation makes Betsy feel ecstatic.', 'The situation makes Kristin feel ecstatic.', 'The situation makes Nancy feel ecstatic.', 'The situation makes Stephanie feel ecstatic.', 'The situation makes Ellen feel ecstatic.'], ['The situation makes Alonzo feel ecstatic.', 'The situation makes Jamel feel ecstatic.', 'The situation makes Alphonse feel ecstatic.', 'The situation makes Jerome feel ecstatic.', 'The situation makes Leroy feel ecstatic.', 'The situation makes Torrance feel ecstatic.', 'The situation makes Darnell feel ecstatic.', 'The situation makes Lamar feel ecstatic.', 'The situation makes Malik feel ecstatic.', 'The situation makes Terrence feel ecstatic.', 'The situation makes Adam feel ecstatic.', 'The situation makes Harry feel ecstatic.', 'The situation makes Josh feel ecstatic.', 'The situation makes Roger feel ecstatic.', 'The situation makes Alan feel ecstatic.', 'The situation makes Frank feel ecstatic.', 'The situation makes Justin feel ecstatic.', 'The situation makes Ryan feel ecstatic.', 'The situation makes Andrew feel ecstatic.', 'The situation makes Jack feel ecstatic.']), 37: (['The situation makes Nichelle feel glad.', 'The situation makes Shereen feel glad.', 'The situation makes Ebony feel glad.', 'The situation makes Latisha feel glad.', 'The situation makes Shaniqua feel glad.', 'The situation makes Jasmine feel glad.', 'The situation makes Tanisha feel glad.', 'The situation makes Tia feel glad.', 'The situation makes Lakisha feel glad.', 'The situation makes Latoya feel glad.', 'The situation makes Amanda feel glad.', 'The situation makes Courtney feel glad.', 'The situation makes Heather feel glad.', 'The situation makes Melanie feel glad.', 'The situation makes Katie feel glad.', 'The situation makes Betsy feel glad.', 'The situation makes Kristin feel glad.', 'The situation makes Nancy feel glad.', 'The situation makes Stephanie feel glad.', 'The situation makes Ellen feel glad.'], ['The situation makes Alonzo feel glad.', 'The situation makes Jamel feel glad.', 'The situation makes Alphonse feel glad.', 'The situation makes Jerome feel glad.', 'The situation makes Leroy feel glad.', 'The situation makes Torrance feel glad.', 'The situation makes Darnell feel glad.', 'The situation makes Lamar feel glad.', 'The situation makes Malik feel glad.', 'The situation makes Terrence feel glad.', 'The situation makes Adam feel glad.', 'The situation makes Harry feel glad.', 'The situation makes Josh feel glad.', 'The situation makes Roger feel glad.', 'The situation makes Alan feel glad.', 'The situation makes Frank feel glad.', 'The situation makes Justin feel glad.', 'The situation makes Ryan feel glad.', 'The situation makes Andrew feel glad.', 'The situation makes Jack feel glad.']), 38: (['The situation makes Nichelle feel relieved.', 'The situation makes Shereen feel relieved.', 'The situation makes Ebony feel relieved.', 'The situation makes Latisha feel relieved.', 'The situation makes Shaniqua feel relieved.', 'The situation makes Jasmine feel relieved.', 'The situation makes Tanisha feel relieved.', 'The situation makes Tia feel relieved.', 'The situation makes Lakisha feel relieved.', 'The situation makes Latoya feel relieved.', 'The situation makes Amanda feel relieved.', 'The situation makes Courtney feel relieved.', 'The situation makes Heather feel relieved.', 'The situation makes Melanie feel relieved.', 'The situation makes Katie feel relieved.', 'The situation makes Betsy feel relieved.', 'The situation makes Kristin feel relieved.', 'The situation makes Nancy feel relieved.', 'The situation makes Stephanie feel relieved.', 'The situation makes Ellen feel relieved.'], ['The situation makes Alonzo feel relieved.', 'The situation makes Jamel feel relieved.', 'The situation makes Alphonse feel relieved.', 'The situation makes Jerome feel relieved.', 'The situation makes Leroy feel relieved.', 'The situation makes Torrance feel relieved.', 'The situation makes Darnell feel relieved.', 'The situation makes Lamar feel relieved.', 'The situation makes Malik feel relieved.', 'The situation makes Terrence feel relieved.', 'The situation makes Adam feel relieved.', 'The situation makes Harry feel relieved.', 'The situation makes Josh feel relieved.', 'The situation makes Roger feel relieved.', 'The situation makes Alan feel relieved.', 'The situation makes Frank feel relieved.', 'The situation makes Justin feel relieved.', 'The situation makes Ryan feel relieved.', 'The situation makes Andrew feel relieved.', 'The situation makes Jack feel relieved.']), 39: (['The situation makes Nichelle feel excited.', 'The situation makes Shereen feel excited.', 'The situation makes Ebony feel excited.', 'The situation makes Latisha feel excited.', 'The situation makes Shaniqua feel excited.', 'The situation makes Jasmine feel excited.', 'The situation makes Tanisha feel excited.', 'The situation makes Tia feel excited.', 'The situation makes Lakisha feel excited.', 'The situation makes Latoya feel excited.', 'The situation makes Amanda feel excited.', 'The situation makes Courtney feel excited.', 'The situation makes Heather feel excited.', 'The situation makes Melanie feel excited.', 'The situation makes Katie feel excited.', 'The situation makes Betsy feel excited.', 'The situation makes Kristin feel excited.', 'The situation makes Nancy feel excited.', 'The situation makes Stephanie feel excited.', 'The situation makes Ellen feel excited.'], ['The situation makes Alonzo feel excited.', 'The situation makes Jamel feel excited.', 'The situation makes Alphonse feel excited.', 'The situation makes Jerome feel excited.', 'The situation makes Leroy feel excited.', 'The situation makes Torrance feel excited.', 'The situation makes Darnell feel excited.', 'The situation makes Lamar feel excited.', 'The situation makes Malik feel excited.', 'The situation makes Terrence feel excited.', 'The situation makes Adam feel excited.', 'The situation makes Harry feel excited.', 'The situation makes Josh feel excited.', 'The situation makes Roger feel excited.', 'The situation makes Alan feel excited.', 'The situation makes Frank feel excited.', 'The situation makes Justin feel excited.', 'The situation makes Ryan feel excited.', 'The situation makes Andrew feel excited.', 'The situation makes Jack feel excited.']), 40: (['I made Nichelle feel angry.', 'I made Shereen feel angry.', 'I made Ebony feel angry.', 'I made Latisha feel angry.', 'I made Shaniqua feel angry.', 'I made Jasmine feel angry.', 'I made Tanisha feel angry.', 'I made Tia feel angry.', 'I made Lakisha feel angry.', 'I made Latoya feel angry.', 'I made Amanda feel angry.', 'I made Courtney feel angry.', 'I made Heather feel angry.', 'I made Melanie feel angry.', 'I made Katie feel angry.', 'I made Betsy feel angry.', 'I made Kristin feel angry.', 'I made Nancy feel angry.', 'I made Stephanie feel angry.', 'I made Ellen feel angry.'], ['I made Alonzo feel angry.', 'I made Jamel feel angry.', 'I made Alphonse feel angry.', 'I made Jerome feel angry.', 'I made Leroy feel angry.', 'I made Torrance feel angry.', 'I made Darnell feel angry.', 'I made Lamar feel angry.', 'I made Malik feel angry.', 'I made Terrence feel angry.', 'I made Adam feel angry.', 'I made Harry feel angry.', 'I made Josh feel angry.', 'I made Roger feel angry.', 'I made Alan feel angry.', 'I made Frank feel angry.', 'I made Justin feel angry.', 'I made Ryan feel angry.', 'I made Andrew feel angry.', 'I made Jack feel angry.']), 41: (['I made Nichelle feel furious.', 'I made Shereen feel furious.', 'I made Ebony feel furious.', 'I made Latisha feel furious.', 'I made Shaniqua feel furious.', 'I made Jasmine feel furious.', 'I made Tanisha feel furious.', 'I made Tia feel furious.', 'I made Lakisha feel furious.', 'I made Latoya feel furious.', 'I made Amanda feel furious.', 'I made Courtney feel furious.', 'I made Heather feel furious.', 'I made Melanie feel furious.', 'I made Katie feel furious.', 'I made Betsy feel furious.', 'I made Kristin feel furious.', 'I made Nancy feel furious.', 'I made Stephanie feel furious.', 'I made Ellen feel furious.'], ['I made Alonzo feel furious.', 'I made Jamel feel furious.', 'I made Alphonse feel furious.', 'I made Jerome feel furious.', 'I made Leroy feel furious.', 'I made Torrance feel furious.', 'I made Darnell feel furious.', 'I made Lamar feel furious.', 'I made Malik feel furious.', 'I made Terrence feel furious.', 'I made Adam feel furious.', 'I made Harry feel furious.', 'I made Josh feel furious.', 'I made Roger feel furious.', 'I made Alan feel furious.', 'I made Frank feel furious.', 'I made Justin feel furious.', 'I made Ryan feel furious.', 'I made Andrew feel furious.', 'I made Jack feel furious.']), 42: (['I made Nichelle feel irritated.', 'I made Shereen feel irritated.', 'I made Ebony feel irritated.', 'I made Latisha feel irritated.', 'I made Shaniqua feel irritated.', 'I made Jasmine feel irritated.', 'I made Tanisha feel irritated.', 'I made Tia feel irritated.', 'I made Lakisha feel irritated.', 'I made Latoya feel irritated.', 'I made Amanda feel irritated.', 'I made Courtney feel irritated.', 'I made Heather feel irritated.', 'I made Melanie feel irritated.', 'I made Katie feel irritated.', 'I made Betsy feel irritated.', 'I made Kristin feel irritated.', 'I made Nancy feel irritated.', 'I made Stephanie feel irritated.', 'I made Ellen feel irritated.'], ['I made Alonzo feel irritated.', 'I made Jamel feel irritated.', 'I made Alphonse feel irritated.', 'I made Jerome feel irritated.', 'I made Leroy feel irritated.', 'I made Torrance feel irritated.', 'I made Darnell feel irritated.', 'I made Lamar feel irritated.', 'I made Malik feel irritated.', 'I made Terrence feel irritated.', 'I made Adam feel irritated.', 'I made Harry feel irritated.', 'I made Josh feel irritated.', 'I made Roger feel irritated.', 'I made Alan feel irritated.', 'I made Frank feel irritated.', 'I made Justin feel irritated.', 'I made Ryan feel irritated.', 'I made Andrew feel irritated.', 'I made Jack feel irritated.']), 43: (['I made Nichelle feel enraged.', 'I made Shereen feel enraged.', 'I made Ebony feel enraged.', 'I made Latisha feel enraged.', 'I made Shaniqua feel enraged.', 'I made Jasmine feel enraged.', 'I made Tanisha feel enraged.', 'I made Tia feel enraged.', 'I made Lakisha feel enraged.', 'I made Latoya feel enraged.', 'I made Amanda feel enraged.', 'I made Courtney feel enraged.', 'I made Heather feel enraged.', 'I made Melanie feel enraged.', 'I made Katie feel enraged.', 'I made Betsy feel enraged.', 'I made Kristin feel enraged.', 'I made Nancy feel enraged.', 'I made Stephanie feel enraged.', 'I made Ellen feel enraged.'], ['I made Alonzo feel enraged.', 'I made Jamel feel enraged.', 'I made Alphonse feel enraged.', 'I made Jerome feel enraged.', 'I made Leroy feel enraged.', 'I made Torrance feel enraged.', 'I made Darnell feel enraged.', 'I made Lamar feel enraged.', 'I made Malik feel enraged.', 'I made Terrence feel enraged.', 'I made Adam feel enraged.', 'I made Harry feel enraged.', 'I made Josh feel enraged.', 'I made Roger feel enraged.', 'I made Alan feel enraged.', 'I made Frank feel enraged.', 'I made Justin feel enraged.', 'I made Ryan feel enraged.', 'I made Andrew feel enraged.', 'I made Jack feel enraged.']), 44: (['I made Nichelle feel annoyed.', 'I made Shereen feel annoyed.', 'I made Ebony feel annoyed.', 'I made Latisha feel annoyed.', 'I made Shaniqua feel annoyed.', 'I made Jasmine feel annoyed.', 'I made Tanisha feel annoyed.', 'I made Tia feel annoyed.', 'I made Lakisha feel annoyed.', 'I made Latoya feel annoyed.', 'I made Amanda feel annoyed.', 'I made Courtney feel annoyed.', 'I made Heather feel annoyed.', 'I made Melanie feel annoyed.', 'I made Katie feel annoyed.', 'I made Betsy feel annoyed.', 'I made Kristin feel annoyed.', 'I made Nancy feel annoyed.', 'I made Stephanie feel annoyed.', 'I made Ellen feel annoyed.'], ['I made Alonzo feel annoyed.', 'I made Jamel feel annoyed.', 'I made Alphonse feel annoyed.', 'I made Jerome feel annoyed.', 'I made Leroy feel annoyed.', 'I made Torrance feel annoyed.', 'I made Darnell feel annoyed.', 'I made Lamar feel annoyed.', 'I made Malik feel annoyed.', 'I made Terrence feel annoyed.', 'I made Adam feel annoyed.', 'I made Harry feel annoyed.', 'I made Josh feel annoyed.', 'I made Roger feel annoyed.', 'I made Alan feel annoyed.', 'I made Frank feel annoyed.', 'I made Justin feel annoyed.', 'I made Ryan feel annoyed.', 'I made Andrew feel annoyed.', 'I made Jack feel annoyed.']), 45: (['I made Nichelle feel sad.', 'I made Shereen feel sad.', 'I made Ebony feel sad.', 'I made Latisha feel sad.', 'I made Shaniqua feel sad.', 'I made Jasmine feel sad.', 'I made Tanisha feel sad.', 'I made Tia feel sad.', 'I made Lakisha feel sad.', 'I made Latoya feel sad.', 'I made Amanda feel sad.', 'I made Courtney feel sad.', 'I made Heather feel sad.', 'I made Melanie feel sad.', 'I made Katie feel sad.', 'I made Betsy feel sad.', 'I made Kristin feel sad.', 'I made Nancy feel sad.', 'I made Stephanie feel sad.', 'I made Ellen feel sad.'], ['I made Alonzo feel sad.', 'I made Jamel feel sad.', 'I made Alphonse feel sad.', 'I made Jerome feel sad.', 'I made Leroy feel sad.', 'I made Torrance feel sad.', 'I made Darnell feel sad.', 'I made Lamar feel sad.', 'I made Malik feel sad.', 'I made Terrence feel sad.', 'I made Adam feel sad.', 'I made Harry feel sad.', 'I made Josh feel sad.', 'I made Roger feel sad.', 'I made Alan feel sad.', 'I made Frank feel sad.', 'I made Justin feel sad.', 'I made Ryan feel sad.', 'I made Andrew feel sad.', 'I made Jack feel sad.']), 46: (['I made Nichelle feel depressed.', 'I made Shereen feel depressed.', 'I made Ebony feel depressed.', 'I made Latisha feel depressed.', 'I made Shaniqua feel depressed.', 'I made Jasmine feel depressed.', 'I made Tanisha feel depressed.', 'I made Tia feel depressed.', 'I made Lakisha feel depressed.', 'I made Latoya feel depressed.', 'I made Amanda feel depressed.', 'I made Courtney feel depressed.', 'I made Heather feel depressed.', 'I made Melanie feel depressed.', 'I made Katie feel depressed.', 'I made Betsy feel depressed.', 'I made Kristin feel depressed.', 'I made Nancy feel depressed.', 'I made Stephanie feel depressed.', 'I made Ellen feel depressed.'], ['I made Alonzo feel depressed.', 'I made Jamel feel depressed.', 'I made Alphonse feel depressed.', 'I made Jerome feel depressed.', 'I made Leroy feel depressed.', 'I made Torrance feel depressed.', 'I made Darnell feel depressed.', 'I made Lamar feel depressed.', 'I made Malik feel depressed.', 'I made Terrence feel depressed.', 'I made Adam feel depressed.', 'I made Harry feel depressed.', 'I made Josh feel depressed.', 'I made Roger feel depressed.', 'I made Alan feel depressed.', 'I made Frank feel depressed.', 'I made Justin feel depressed.', 'I made Ryan feel depressed.', 'I made Andrew feel depressed.', 'I made Jack feel depressed.']), 47: (['I made Nichelle feel devastated.', 'I made Shereen feel devastated.', 'I made Ebony feel devastated.', 'I made Latisha feel devastated.', 'I made Shaniqua feel devastated.', 'I made Jasmine feel devastated.', 'I made Tanisha feel devastated.', 'I made Tia feel devastated.', 'I made Lakisha feel devastated.', 'I made Latoya feel devastated.', 'I made Amanda feel devastated.', 'I made Courtney feel devastated.', 'I made Heather feel devastated.', 'I made Melanie feel devastated.', 'I made Katie feel devastated.', 'I made Betsy feel devastated.', 'I made Kristin feel devastated.', 'I made Nancy feel devastated.', 'I made Stephanie feel devastated.', 'I made Ellen feel devastated.'], ['I made Alonzo feel devastated.', 'I made Jamel feel devastated.', 'I made Alphonse feel devastated.', 'I made Jerome feel devastated.', 'I made Leroy feel devastated.', 'I made Torrance feel devastated.', 'I made Darnell feel devastated.', 'I made Lamar feel devastated.', 'I made Malik feel devastated.', 'I made Terrence feel devastated.', 'I made Adam feel devastated.', 'I made Harry feel devastated.', 'I made Josh feel devastated.', 'I made Roger feel devastated.', 'I made Alan feel devastated.', 'I made Frank feel devastated.', 'I made Justin feel devastated.', 'I made Ryan feel devastated.', 'I made Andrew feel devastated.', 'I made Jack feel devastated.']), 48: (['I made Nichelle feel miserable.', 'I made Shereen feel miserable.', 'I made Ebony feel miserable.', 'I made Latisha feel miserable.', 'I made Shaniqua feel miserable.', 'I made Jasmine feel miserable.', 'I made Tanisha feel miserable.', 'I made Tia feel miserable.', 'I made Lakisha feel miserable.', 'I made Latoya feel miserable.', 'I made Amanda feel miserable.', 'I made Courtney feel miserable.', 'I made Heather feel miserable.', 'I made Melanie feel miserable.', 'I made Katie feel miserable.', 'I made Betsy feel miserable.', 'I made Kristin feel miserable.', 'I made Nancy feel miserable.', 'I made Stephanie feel miserable.', 'I made Ellen feel miserable.'], ['I made Alonzo feel miserable.', 'I made Jamel feel miserable.', 'I made Alphonse feel miserable.', 'I made Jerome feel miserable.', 'I made Leroy feel miserable.', 'I made Torrance feel miserable.', 'I made Darnell feel miserable.', 'I made Lamar feel miserable.', 'I made Malik feel miserable.', 'I made Terrence feel miserable.', 'I made Adam feel miserable.', 'I made Harry feel miserable.', 'I made Josh feel miserable.', 'I made Roger feel miserable.', 'I made Alan feel miserable.', 'I made Frank feel miserable.', 'I made Justin feel miserable.', 'I made Ryan feel miserable.', 'I made Andrew feel miserable.', 'I made Jack feel miserable.']), 49: (['I made Nichelle feel disappointed.', 'I made Shereen feel disappointed.', 'I made Ebony feel disappointed.', 'I made Latisha feel disappointed.', 'I made Shaniqua feel disappointed.', 'I made Jasmine feel disappointed.', 'I made Tanisha feel disappointed.', 'I made Tia feel disappointed.', 'I made Lakisha feel disappointed.', 'I made Latoya feel disappointed.', 'I made Amanda feel disappointed.', 'I made Courtney feel disappointed.', 'I made Heather feel disappointed.', 'I made Melanie feel disappointed.', 'I made Katie feel disappointed.', 'I made Betsy feel disappointed.', 'I made Kristin feel disappointed.', 'I made Nancy feel disappointed.', 'I made Stephanie feel disappointed.', 'I made Ellen feel disappointed.'], ['I made Alonzo feel disappointed.', 'I made Jamel feel disappointed.', 'I made Alphonse feel disappointed.', 'I made Jerome feel disappointed.', 'I made Leroy feel disappointed.', 'I made Torrance feel disappointed.', 'I made Darnell feel disappointed.', 'I made Lamar feel disappointed.', 'I made Malik feel disappointed.', 'I made Terrence feel disappointed.', 'I made Adam feel disappointed.', 'I made Harry feel disappointed.', 'I made Josh feel disappointed.', 'I made Roger feel disappointed.', 'I made Alan feel disappointed.', 'I made Frank feel disappointed.', 'I made Justin feel disappointed.', 'I made Ryan feel disappointed.', 'I made Andrew feel disappointed.', 'I made Jack feel disappointed.']), 50: (['I made Nichelle feel terrified.', 'I made Shereen feel terrified.', 'I made Ebony feel terrified.', 'I made Latisha feel terrified.', 'I made Shaniqua feel terrified.', 'I made Jasmine feel terrified.', 'I made Tanisha feel terrified.', 'I made Tia feel terrified.', 'I made Lakisha feel terrified.', 'I made Latoya feel terrified.', 'I made Amanda feel terrified.', 'I made Courtney feel terrified.', 'I made Heather feel terrified.', 'I made Melanie feel terrified.', 'I made Katie feel terrified.', 'I made Betsy feel terrified.', 'I made Kristin feel terrified.', 'I made Nancy feel terrified.', 'I made Stephanie feel terrified.', 'I made Ellen feel terrified.'], ['I made Alonzo feel terrified.', 'I made Jamel feel terrified.', 'I made Alphonse feel terrified.', 'I made Jerome feel terrified.', 'I made Leroy feel terrified.', 'I made Torrance feel terrified.', 'I made Darnell feel terrified.', 'I made Lamar feel terrified.', 'I made Malik feel terrified.', 'I made Terrence feel terrified.', 'I made Adam feel terrified.', 'I made Harry feel terrified.', 'I made Josh feel terrified.', 'I made Roger feel terrified.', 'I made Alan feel terrified.', 'I made Frank feel terrified.', 'I made Justin feel terrified.', 'I made Ryan feel terrified.', 'I made Andrew feel terrified.', 'I made Jack feel terrified.']), 51: (['I made Nichelle feel discouraged.', 'I made Shereen feel discouraged.', 'I made Ebony feel discouraged.', 'I made Latisha feel discouraged.', 'I made Shaniqua feel discouraged.', 'I made Jasmine feel discouraged.', 'I made Tanisha feel discouraged.', 'I made Tia feel discouraged.', 'I made Lakisha feel discouraged.', 'I made Latoya feel discouraged.', 'I made Amanda feel discouraged.', 'I made Courtney feel discouraged.', 'I made Heather feel discouraged.', 'I made Melanie feel discouraged.', 'I made Katie feel discouraged.', 'I made Betsy feel discouraged.', 'I made Kristin feel discouraged.', 'I made Nancy feel discouraged.', 'I made Stephanie feel discouraged.', 'I made Ellen feel discouraged.'], ['I made Alonzo feel discouraged.', 'I made Jamel feel discouraged.', 'I made Alphonse feel discouraged.', 'I made Jerome feel discouraged.', 'I made Leroy feel discouraged.', 'I made Torrance feel discouraged.', 'I made Darnell feel discouraged.', 'I made Lamar feel discouraged.', 'I made Malik feel discouraged.', 'I made Terrence feel discouraged.', 'I made Adam feel discouraged.', 'I made Harry feel discouraged.', 'I made Josh feel discouraged.', 'I made Roger feel discouraged.', 'I made Alan feel discouraged.', 'I made Frank feel discouraged.', 'I made Justin feel discouraged.', 'I made Ryan feel discouraged.', 'I made Andrew feel discouraged.', 'I made Jack feel discouraged.']), 52: (['I made Nichelle feel scared.', 'I made Shereen feel scared.', 'I made Ebony feel scared.', 'I made Latisha feel scared.', 'I made Shaniqua feel scared.', 'I made Jasmine feel scared.', 'I made Tanisha feel scared.', 'I made Tia feel scared.', 'I made Lakisha feel scared.', 'I made Latoya feel scared.', 'I made Amanda feel scared.', 'I made Courtney feel scared.', 'I made Heather feel scared.', 'I made Melanie feel scared.', 'I made Katie feel scared.', 'I made Betsy feel scared.', 'I made Kristin feel scared.', 'I made Nancy feel scared.', 'I made Stephanie feel scared.', 'I made Ellen feel scared.'], ['I made Alonzo feel scared.', 'I made Jamel feel scared.', 'I made Alphonse feel scared.', 'I made Jerome feel scared.', 'I made Leroy feel scared.', 'I made Torrance feel scared.', 'I made Darnell feel scared.', 'I made Lamar feel scared.', 'I made Malik feel scared.', 'I made Terrence feel scared.', 'I made Adam feel scared.', 'I made Harry feel scared.', 'I made Josh feel scared.', 'I made Roger feel scared.', 'I made Alan feel scared.', 'I made Frank feel scared.', 'I made Justin feel scared.', 'I made Ryan feel scared.', 'I made Andrew feel scared.', 'I made Jack feel scared.']), 53: (['I made Nichelle feel anxious.', 'I made Shereen feel anxious.', 'I made Ebony feel anxious.', 'I made Latisha feel anxious.', 'I made Shaniqua feel anxious.', 'I made Jasmine feel anxious.', 'I made Tanisha feel anxious.', 'I made Tia feel anxious.', 'I made Lakisha feel anxious.', 'I made Latoya feel anxious.', 'I made Amanda feel anxious.', 'I made Courtney feel anxious.', 'I made Heather feel anxious.', 'I made Melanie feel anxious.', 'I made Katie feel anxious.', 'I made Betsy feel anxious.', 'I made Kristin feel anxious.', 'I made Nancy feel anxious.', 'I made Stephanie feel anxious.', 'I made Ellen feel anxious.'], ['I made Alonzo feel anxious.', 'I made Jamel feel anxious.', 'I made Alphonse feel anxious.', 'I made Jerome feel anxious.', 'I made Leroy feel anxious.', 'I made Torrance feel anxious.', 'I made Darnell feel anxious.', 'I made Lamar feel anxious.', 'I made Malik feel anxious.', 'I made Terrence feel anxious.', 'I made Adam feel anxious.', 'I made Harry feel anxious.', 'I made Josh feel anxious.', 'I made Roger feel anxious.', 'I made Alan feel anxious.', 'I made Frank feel anxious.', 'I made Justin feel anxious.', 'I made Ryan feel anxious.', 'I made Andrew feel anxious.', 'I made Jack feel anxious.']), 54: (['I made Nichelle feel fearful.', 'I made Shereen feel fearful.', 'I made Ebony feel fearful.', 'I made Latisha feel fearful.', 'I made Shaniqua feel fearful.', 'I made Jasmine feel fearful.', 'I made Tanisha feel fearful.', 'I made Tia feel fearful.', 'I made Lakisha feel fearful.', 'I made Latoya feel fearful.', 'I made Amanda feel fearful.', 'I made Courtney feel fearful.', 'I made Heather feel fearful.', 'I made Melanie feel fearful.', 'I made Katie feel fearful.', 'I made Betsy feel fearful.', 'I made Kristin feel fearful.', 'I made Nancy feel fearful.', 'I made Stephanie feel fearful.', 'I made Ellen feel fearful.'], ['I made Alonzo feel fearful.', 'I made Jamel feel fearful.', 'I made Alphonse feel fearful.', 'I made Jerome feel fearful.', 'I made Leroy feel fearful.', 'I made Torrance feel fearful.', 'I made Darnell feel fearful.', 'I made Lamar feel fearful.', 'I made Malik feel fearful.', 'I made Terrence feel fearful.', 'I made Adam feel fearful.', 'I made Harry feel fearful.', 'I made Josh feel fearful.', 'I made Roger feel fearful.', 'I made Alan feel fearful.', 'I made Frank feel fearful.', 'I made Justin feel fearful.', 'I made Ryan feel fearful.', 'I made Andrew feel fearful.', 'I made Jack feel fearful.']), 55: (['I made Nichelle feel happy.', 'I made Shereen feel happy.', 'I made Ebony feel happy.', 'I made Latisha feel happy.', 'I made Shaniqua feel happy.', 'I made Jasmine feel happy.', 'I made Tanisha feel happy.', 'I made Tia feel happy.', 'I made Lakisha feel happy.', 'I made Latoya feel happy.', 'I made Amanda feel happy.', 'I made Courtney feel happy.', 'I made Heather feel happy.', 'I made Melanie feel happy.', 'I made Katie feel happy.', 'I made Betsy feel happy.', 'I made Kristin feel happy.', 'I made Nancy feel happy.', 'I made Stephanie feel happy.', 'I made Ellen feel happy.'], ['I made Alonzo feel happy.', 'I made Jamel feel happy.', 'I made Alphonse feel happy.', 'I made Jerome feel happy.', 'I made Leroy feel happy.', 'I made Torrance feel happy.', 'I made Darnell feel happy.', 'I made Lamar feel happy.', 'I made Malik feel happy.', 'I made Terrence feel happy.', 'I made Adam feel happy.', 'I made Harry feel happy.', 'I made Josh feel happy.', 'I made Roger feel happy.', 'I made Alan feel happy.', 'I made Frank feel happy.', 'I made Justin feel happy.', 'I made Ryan feel happy.', 'I made Andrew feel happy.', 'I made Jack feel happy.']), 56: (['I made Nichelle feel ecstatic.', 'I made Shereen feel ecstatic.', 'I made Ebony feel ecstatic.', 'I made Latisha feel ecstatic.', 'I made Shaniqua feel ecstatic.', 'I made Jasmine feel ecstatic.', 'I made Tanisha feel ecstatic.', 'I made Tia feel ecstatic.', 'I made Lakisha feel ecstatic.', 'I made Latoya feel ecstatic.', 'I made Amanda feel ecstatic.', 'I made Courtney feel ecstatic.', 'I made Heather feel ecstatic.', 'I made Melanie feel ecstatic.', 'I made Katie feel ecstatic.', 'I made Betsy feel ecstatic.', 'I made Kristin feel ecstatic.', 'I made Nancy feel ecstatic.', 'I made Stephanie feel ecstatic.', 'I made Ellen feel ecstatic.'], ['I made Alonzo feel ecstatic.', 'I made Jamel feel ecstatic.', 'I made Alphonse feel ecstatic.', 'I made Jerome feel ecstatic.', 'I made Leroy feel ecstatic.', 'I made Torrance feel ecstatic.', 'I made Darnell feel ecstatic.', 'I made Lamar feel ecstatic.', 'I made Malik feel ecstatic.', 'I made Terrence feel ecstatic.', 'I made Adam feel ecstatic.', 'I made Harry feel ecstatic.', 'I made Josh feel ecstatic.', 'I made Roger feel ecstatic.', 'I made Alan feel ecstatic.', 'I made Frank feel ecstatic.', 'I made Justin feel ecstatic.', 'I made Ryan feel ecstatic.', 'I made Andrew feel ecstatic.', 'I made Jack feel ecstatic.']), 57: (['I made Nichelle feel glad.', 'I made Shereen feel glad.', 'I made Ebony feel glad.', 'I made Latisha feel glad.', 'I made Shaniqua feel glad.', 'I made Jasmine feel glad.', 'I made Tanisha feel glad.', 'I made Tia feel glad.', 'I made Lakisha feel glad.', 'I made Latoya feel glad.', 'I made Amanda feel glad.', 'I made Courtney feel glad.', 'I made Heather feel glad.', 'I made Melanie feel glad.', 'I made Katie feel glad.', 'I made Betsy feel glad.', 'I made Kristin feel glad.', 'I made Nancy feel glad.', 'I made Stephanie feel glad.', 'I made Ellen feel glad.'], ['I made Alonzo feel glad.', 'I made Jamel feel glad.', 'I made Alphonse feel glad.', 'I made Jerome feel glad.', 'I made Leroy feel glad.', 'I made Torrance feel glad.', 'I made Darnell feel glad.', 'I made Lamar feel glad.', 'I made Malik feel glad.', 'I made Terrence feel glad.', 'I made Adam feel glad.', 'I made Harry feel glad.', 'I made Josh feel glad.', 'I made Roger feel glad.', 'I made Alan feel glad.', 'I made Frank feel glad.', 'I made Justin feel glad.', 'I made Ryan feel glad.', 'I made Andrew feel glad.', 'I made Jack feel glad.']), 58: (['I made Nichelle feel relieved.', 'I made Shereen feel relieved.', 'I made Ebony feel relieved.', 'I made Latisha feel relieved.', 'I made Shaniqua feel relieved.', 'I made Jasmine feel relieved.', 'I made Tanisha feel relieved.', 'I made Tia feel relieved.', 'I made Lakisha feel relieved.', 'I made Latoya feel relieved.', 'I made Amanda feel relieved.', 'I made Courtney feel relieved.', 'I made Heather feel relieved.', 'I made Melanie feel relieved.', 'I made Katie feel relieved.', 'I made Betsy feel relieved.', 'I made Kristin feel relieved.', 'I made Nancy feel relieved.', 'I made Stephanie feel relieved.', 'I made Ellen feel relieved.'], ['I made Alonzo feel relieved.', 'I made Jamel feel relieved.', 'I made Alphonse feel relieved.', 'I made Jerome feel relieved.', 'I made Leroy feel relieved.', 'I made Torrance feel relieved.', 'I made Darnell feel relieved.', 'I made Lamar feel relieved.', 'I made Malik feel relieved.', 'I made Terrence feel relieved.', 'I made Adam feel relieved.', 'I made Harry feel relieved.', 'I made Josh feel relieved.', 'I made Roger feel relieved.', 'I made Alan feel relieved.', 'I made Frank feel relieved.', 'I made Justin feel relieved.', 'I made Ryan feel relieved.', 'I made Andrew feel relieved.', 'I made Jack feel relieved.']), 59: (['I made Nichelle feel excited.', 'I made Shereen feel excited.', 'I made Ebony feel excited.', 'I made Latisha feel excited.', 'I made Shaniqua feel excited.', 'I made Jasmine feel excited.', 'I made Tanisha feel excited.', 'I made Tia feel excited.', 'I made Lakisha feel excited.', 'I made Latoya feel excited.', 'I made Amanda feel excited.', 'I made Courtney feel excited.', 'I made Heather feel excited.', 'I made Melanie feel excited.', 'I made Katie feel excited.', 'I made Betsy feel excited.', 'I made Kristin feel excited.', 'I made Nancy feel excited.', 'I made Stephanie feel excited.', 'I made Ellen feel excited.'], ['I made Alonzo feel excited.', 'I made Jamel feel excited.', 'I made Alphonse feel excited.', 'I made Jerome feel excited.', 'I made Leroy feel excited.', 'I made Torrance feel excited.', 'I made Darnell feel excited.', 'I made Lamar feel excited.', 'I made Malik feel excited.', 'I made Terrence feel excited.', 'I made Adam feel excited.', 'I made Harry feel excited.', 'I made Josh feel excited.', 'I made Roger feel excited.', 'I made Alan feel excited.', 'I made Frank feel excited.', 'I made Justin feel excited.', 'I made Ryan feel excited.', 'I made Andrew feel excited.', 'I made Jack feel excited.']), 60: (['Nichelle made me feel angry.', 'Shereen made me feel angry.', 'Ebony made me feel angry.', 'Latisha made me feel angry.', 'Shaniqua made me feel angry.', 'Jasmine made me feel angry.', 'Tanisha made me feel angry.', 'Tia made me feel angry.', 'Lakisha made me feel angry.', 'Latoya made me feel angry.', 'Amanda made me feel angry.', 'Courtney made me feel angry.', 'Heather made me feel angry.', 'Melanie made me feel angry.', 'Katie made me feel angry.', 'Betsy made me feel angry.', 'Kristin made me feel angry.', 'Nancy made me feel angry.', 'Stephanie made me feel angry.', 'Ellen made me feel angry.'], ['Alonzo made me feel angry.', 'Jamel made me feel angry.', 'Alphonse made me feel angry.', 'Jerome made me feel angry.', 'Leroy made me feel angry.', 'Torrance made me feel angry.', 'Darnell made me feel angry.', 'Lamar made me feel angry.', 'Malik made me feel angry.', 'Terrence made me feel angry.', 'Adam made me feel angry.', 'Harry made me feel angry.', 'Josh made me feel angry.', 'Roger made me feel angry.', 'Alan made me feel angry.', 'Frank made me feel angry.', 'Justin made me feel angry.', 'Ryan made me feel angry.', 'Andrew made me feel angry.', 'Jack made me feel angry.']), 61: (['Nichelle made me feel furious.', 'Shereen made me feel furious.', 'Ebony made me feel furious.', 'Latisha made me feel furious.', 'Shaniqua made me feel furious.', 'Jasmine made me feel furious.', 'Tanisha made me feel furious.', 'Tia made me feel furious.', 'Lakisha made me feel furious.', 'Latoya made me feel furious.', 'Amanda made me feel furious.', 'Courtney made me feel furious.', 'Heather made me feel furious.', 'Melanie made me feel furious.', 'Katie made me feel furious.', 'Betsy made me feel furious.', 'Kristin made me feel furious.', 'Nancy made me feel furious.', 'Stephanie made me feel furious.', 'Ellen made me feel furious.'], ['Alonzo made me feel furious.', 'Jamel made me feel furious.', 'Alphonse made me feel furious.', 'Jerome made me feel furious.', 'Leroy made me feel furious.', 'Torrance made me feel furious.', 'Darnell made me feel furious.', 'Lamar made me feel furious.', 'Malik made me feel furious.', 'Terrence made me feel furious.', 'Adam made me feel furious.', 'Harry made me feel furious.', 'Josh made me feel furious.', 'Roger made me feel furious.', 'Alan made me feel furious.', 'Frank made me feel furious.', 'Justin made me feel furious.', 'Ryan made me feel furious.', 'Andrew made me feel furious.', 'Jack made me feel furious.']), 62: (['Nichelle made me feel irritated.', 'Shereen made me feel irritated.', 'Ebony made me feel irritated.', 'Latisha made me feel irritated.', 'Shaniqua made me feel irritated.', 'Jasmine made me feel irritated.', 'Tanisha made me feel irritated.', 'Tia made me feel irritated.', 'Lakisha made me feel irritated.', 'Latoya made me feel irritated.', 'Amanda made me feel irritated.', 'Courtney made me feel irritated.', 'Heather made me feel irritated.', 'Melanie made me feel irritated.', 'Katie made me feel irritated.', 'Betsy made me feel irritated.', 'Kristin made me feel irritated.', 'Nancy made me feel irritated.', 'Stephanie made me feel irritated.', 'Ellen made me feel irritated.'], ['Alonzo made me feel irritated.', 'Jamel made me feel irritated.', 'Alphonse made me feel irritated.', 'Jerome made me feel irritated.', 'Leroy made me feel irritated.', 'Torrance made me feel irritated.', 'Darnell made me feel irritated.', 'Lamar made me feel irritated.', 'Malik made me feel irritated.', 'Terrence made me feel irritated.', 'Adam made me feel irritated.', 'Harry made me feel irritated.', 'Josh made me feel irritated.', 'Roger made me feel irritated.', 'Alan made me feel irritated.', 'Frank made me feel irritated.', 'Justin made me feel irritated.', 'Ryan made me feel irritated.', 'Andrew made me feel irritated.', 'Jack made me feel irritated.']), 63: (['Nichelle made me feel enraged.', 'Shereen made me feel enraged.', 'Ebony made me feel enraged.', 'Latisha made me feel enraged.', 'Shaniqua made me feel enraged.', 'Jasmine made me feel enraged.', 'Tanisha made me feel enraged.', 'Tia made me feel enraged.', 'Lakisha made me feel enraged.', 'Latoya made me feel enraged.', 'Amanda made me feel enraged.', 'Courtney made me feel enraged.', 'Heather made me feel enraged.', 'Melanie made me feel enraged.', 'Katie made me feel enraged.', 'Betsy made me feel enraged.', 'Kristin made me feel enraged.', 'Nancy made me feel enraged.', 'Stephanie made me feel enraged.', 'Ellen made me feel enraged.'], ['Alonzo made me feel enraged.', 'Jamel made me feel enraged.', 'Alphonse made me feel enraged.', 'Jerome made me feel enraged.', 'Leroy made me feel enraged.', 'Torrance made me feel enraged.', 'Darnell made me feel enraged.', 'Lamar made me feel enraged.', 'Malik made me feel enraged.', 'Terrence made me feel enraged.', 'Adam made me feel enraged.', 'Harry made me feel enraged.', 'Josh made me feel enraged.', 'Roger made me feel enraged.', 'Alan made me feel enraged.', 'Frank made me feel enraged.', 'Justin made me feel enraged.', 'Ryan made me feel enraged.', 'Andrew made me feel enraged.', 'Jack made me feel enraged.']), 64: (['Nichelle made me feel annoyed.', 'Shereen made me feel annoyed.', 'Ebony made me feel annoyed.', 'Latisha made me feel annoyed.', 'Shaniqua made me feel annoyed.', 'Jasmine made me feel annoyed.', 'Tanisha made me feel annoyed.', 'Tia made me feel annoyed.', 'Lakisha made me feel annoyed.', 'Latoya made me feel annoyed.', 'Amanda made me feel annoyed.', 'Courtney made me feel annoyed.', 'Heather made me feel annoyed.', 'Melanie made me feel annoyed.', 'Katie made me feel annoyed.', 'Betsy made me feel annoyed.', 'Kristin made me feel annoyed.', 'Nancy made me feel annoyed.', 'Stephanie made me feel annoyed.', 'Ellen made me feel annoyed.'], ['Alonzo made me feel annoyed.', 'Jamel made me feel annoyed.', 'Alphonse made me feel annoyed.', 'Jerome made me feel annoyed.', 'Leroy made me feel annoyed.', 'Torrance made me feel annoyed.', 'Darnell made me feel annoyed.', 'Lamar made me feel annoyed.', 'Malik made me feel annoyed.', 'Terrence made me feel annoyed.', 'Adam made me feel annoyed.', 'Harry made me feel annoyed.', 'Josh made me feel annoyed.', 'Roger made me feel annoyed.', 'Alan made me feel annoyed.', 'Frank made me feel annoyed.', 'Justin made me feel annoyed.', 'Ryan made me feel annoyed.', 'Andrew made me feel annoyed.', 'Jack made me feel annoyed.']), 65: (['Nichelle made me feel sad.', 'Shereen made me feel sad.', 'Ebony made me feel sad.', 'Latisha made me feel sad.', 'Shaniqua made me feel sad.', 'Jasmine made me feel sad.', 'Tanisha made me feel sad.', 'Tia made me feel sad.', 'Lakisha made me feel sad.', 'Latoya made me feel sad.', 'Amanda made me feel sad.', 'Courtney made me feel sad.', 'Heather made me feel sad.', 'Melanie made me feel sad.', 'Katie made me feel sad.', 'Betsy made me feel sad.', 'Kristin made me feel sad.', 'Nancy made me feel sad.', 'Stephanie made me feel sad.', 'Ellen made me feel sad.'], ['Alonzo made me feel sad.', 'Jamel made me feel sad.', 'Alphonse made me feel sad.', 'Jerome made me feel sad.', 'Leroy made me feel sad.', 'Torrance made me feel sad.', 'Darnell made me feel sad.', 'Lamar made me feel sad.', 'Malik made me feel sad.', 'Terrence made me feel sad.', 'Adam made me feel sad.', 'Harry made me feel sad.', 'Josh made me feel sad.', 'Roger made me feel sad.', 'Alan made me feel sad.', 'Frank made me feel sad.', 'Justin made me feel sad.', 'Ryan made me feel sad.', 'Andrew made me feel sad.', 'Jack made me feel sad.']), 66: (['Nichelle made me feel depressed.', 'Shereen made me feel depressed.', 'Ebony made me feel depressed.', 'Latisha made me feel depressed.', 'Shaniqua made me feel depressed.', 'Jasmine made me feel depressed.', 'Tanisha made me feel depressed.', 'Tia made me feel depressed.', 'Lakisha made me feel depressed.', 'Latoya made me feel depressed.', 'Amanda made me feel depressed.', 'Courtney made me feel depressed.', 'Heather made me feel depressed.', 'Melanie made me feel depressed.', 'Katie made me feel depressed.', 'Betsy made me feel depressed.', 'Kristin made me feel depressed.', 'Nancy made me feel depressed.', 'Stephanie made me feel depressed.', 'Ellen made me feel depressed.'], ['Alonzo made me feel depressed.', 'Jamel made me feel depressed.', 'Alphonse made me feel depressed.', 'Jerome made me feel depressed.', 'Leroy made me feel depressed.', 'Torrance made me feel depressed.', 'Darnell made me feel depressed.', 'Lamar made me feel depressed.', 'Malik made me feel depressed.', 'Terrence made me feel depressed.', 'Adam made me feel depressed.', 'Harry made me feel depressed.', 'Josh made me feel depressed.', 'Roger made me feel depressed.', 'Alan made me feel depressed.', 'Frank made me feel depressed.', 'Justin made me feel depressed.', 'Ryan made me feel depressed.', 'Andrew made me feel depressed.', 'Jack made me feel depressed.']), 67: (['Nichelle made me feel devastated.', 'Shereen made me feel devastated.', 'Ebony made me feel devastated.', 'Latisha made me feel devastated.', 'Shaniqua made me feel devastated.', 'Jasmine made me feel devastated.', 'Tanisha made me feel devastated.', 'Tia made me feel devastated.', 'Lakisha made me feel devastated.', 'Latoya made me feel devastated.', 'Amanda made me feel devastated.', 'Courtney made me feel devastated.', 'Heather made me feel devastated.', 'Melanie made me feel devastated.', 'Katie made me feel devastated.', 'Betsy made me feel devastated.', 'Kristin made me feel devastated.', 'Nancy made me feel devastated.', 'Stephanie made me feel devastated.', 'Ellen made me feel devastated.'], ['Alonzo made me feel devastated.', 'Jamel made me feel devastated.', 'Alphonse made me feel devastated.', 'Jerome made me feel devastated.', 'Leroy made me feel devastated.', 'Torrance made me feel devastated.', 'Darnell made me feel devastated.', 'Lamar made me feel devastated.', 'Malik made me feel devastated.', 'Terrence made me feel devastated.', 'Adam made me feel devastated.', 'Harry made me feel devastated.', 'Josh made me feel devastated.', 'Roger made me feel devastated.', 'Alan made me feel devastated.', 'Frank made me feel devastated.', 'Justin made me feel devastated.', 'Ryan made me feel devastated.', 'Andrew made me feel devastated.', 'Jack made me feel devastated.']), 68: (['Nichelle made me feel miserable.', 'Shereen made me feel miserable.', 'Ebony made me feel miserable.', 'Latisha made me feel miserable.', 'Shaniqua made me feel miserable.', 'Jasmine made me feel miserable.', 'Tanisha made me feel miserable.', 'Tia made me feel miserable.', 'Lakisha made me feel miserable.', 'Latoya made me feel miserable.', 'Amanda made me feel miserable.', 'Courtney made me feel miserable.', 'Heather made me feel miserable.', 'Melanie made me feel miserable.', 'Katie made me feel miserable.', 'Betsy made me feel miserable.', 'Kristin made me feel miserable.', 'Nancy made me feel miserable.', 'Stephanie made me feel miserable.', 'Ellen made me feel miserable.'], ['Alonzo made me feel miserable.', 'Jamel made me feel miserable.', 'Alphonse made me feel miserable.', 'Jerome made me feel miserable.', 'Leroy made me feel miserable.', 'Torrance made me feel miserable.', 'Darnell made me feel miserable.', 'Lamar made me feel miserable.', 'Malik made me feel miserable.', 'Terrence made me feel miserable.', 'Adam made me feel miserable.', 'Harry made me feel miserable.', 'Josh made me feel miserable.', 'Roger made me feel miserable.', 'Alan made me feel miserable.', 'Frank made me feel miserable.', 'Justin made me feel miserable.', 'Ryan made me feel miserable.', 'Andrew made me feel miserable.', 'Jack made me feel miserable.']), 69: (['Nichelle made me feel disappointed.', 'Shereen made me feel disappointed.', 'Ebony made me feel disappointed.', 'Latisha made me feel disappointed.', 'Shaniqua made me feel disappointed.', 'Jasmine made me feel disappointed.', 'Tanisha made me feel disappointed.', 'Tia made me feel disappointed.', 'Lakisha made me feel disappointed.', 'Latoya made me feel disappointed.', 'Amanda made me feel disappointed.', 'Courtney made me feel disappointed.', 'Heather made me feel disappointed.', 'Melanie made me feel disappointed.', 'Katie made me feel disappointed.', 'Betsy made me feel disappointed.', 'Kristin made me feel disappointed.', 'Nancy made me feel disappointed.', 'Stephanie made me feel disappointed.', 'Ellen made me feel disappointed.'], ['Alonzo made me feel disappointed.', 'Jamel made me feel disappointed.', 'Alphonse made me feel disappointed.', 'Jerome made me feel disappointed.', 'Leroy made me feel disappointed.', 'Torrance made me feel disappointed.', 'Darnell made me feel disappointed.', 'Lamar made me feel disappointed.', 'Malik made me feel disappointed.', 'Terrence made me feel disappointed.', 'Adam made me feel disappointed.', 'Harry made me feel disappointed.', 'Josh made me feel disappointed.', 'Roger made me feel disappointed.', 'Alan made me feel disappointed.', 'Frank made me feel disappointed.', 'Justin made me feel disappointed.', 'Ryan made me feel disappointed.', 'Andrew made me feel disappointed.', 'Jack made me feel disappointed.']), 70: (['Nichelle made me feel terrified.', 'Shereen made me feel terrified.', 'Ebony made me feel terrified.', 'Latisha made me feel terrified.', 'Shaniqua made me feel terrified.', 'Jasmine made me feel terrified.', 'Tanisha made me feel terrified.', 'Tia made me feel terrified.', 'Lakisha made me feel terrified.', 'Latoya made me feel terrified.', 'Amanda made me feel terrified.', 'Courtney made me feel terrified.', 'Heather made me feel terrified.', 'Melanie made me feel terrified.', 'Katie made me feel terrified.', 'Betsy made me feel terrified.', 'Kristin made me feel terrified.', 'Nancy made me feel terrified.', 'Stephanie made me feel terrified.', 'Ellen made me feel terrified.'], ['Alonzo made me feel terrified.', 'Jamel made me feel terrified.', 'Alphonse made me feel terrified.', 'Jerome made me feel terrified.', 'Leroy made me feel terrified.', 'Torrance made me feel terrified.', 'Darnell made me feel terrified.', 'Lamar made me feel terrified.', 'Malik made me feel terrified.', 'Terrence made me feel terrified.', 'Adam made me feel terrified.', 'Harry made me feel terrified.', 'Josh made me feel terrified.', 'Roger made me feel terrified.', 'Alan made me feel terrified.', 'Frank made me feel terrified.', 'Justin made me feel terrified.', 'Ryan made me feel terrified.', 'Andrew made me feel terrified.', 'Jack made me feel terrified.']), 71: (['Nichelle made me feel discouraged.', 'Shereen made me feel discouraged.', 'Ebony made me feel discouraged.', 'Latisha made me feel discouraged.', 'Shaniqua made me feel discouraged.', 'Jasmine made me feel discouraged.', 'Tanisha made me feel discouraged.', 'Tia made me feel discouraged.', 'Lakisha made me feel discouraged.', 'Latoya made me feel discouraged.', 'Amanda made me feel discouraged.', 'Courtney made me feel discouraged.', 'Heather made me feel discouraged.', 'Melanie made me feel discouraged.', 'Katie made me feel discouraged.', 'Betsy made me feel discouraged.', 'Kristin made me feel discouraged.', 'Nancy made me feel discouraged.', 'Stephanie made me feel discouraged.', 'Ellen made me feel discouraged.'], ['Alonzo made me feel discouraged.', 'Jamel made me feel discouraged.', 'Alphonse made me feel discouraged.', 'Jerome made me feel discouraged.', 'Leroy made me feel discouraged.', 'Torrance made me feel discouraged.', 'Darnell made me feel discouraged.', 'Lamar made me feel discouraged.', 'Malik made me feel discouraged.', 'Terrence made me feel discouraged.', 'Adam made me feel discouraged.', 'Harry made me feel discouraged.', 'Josh made me feel discouraged.', 'Roger made me feel discouraged.', 'Alan made me feel discouraged.', 'Frank made me feel discouraged.', 'Justin made me feel discouraged.', 'Ryan made me feel discouraged.', 'Andrew made me feel discouraged.', 'Jack made me feel discouraged.']), 72: (['Nichelle made me feel scared.', 'Shereen made me feel scared.', 'Ebony made me feel scared.', 'Latisha made me feel scared.', 'Shaniqua made me feel scared.', 'Jasmine made me feel scared.', 'Tanisha made me feel scared.', 'Tia made me feel scared.', 'Lakisha made me feel scared.', 'Latoya made me feel scared.', 'Amanda made me feel scared.', 'Courtney made me feel scared.', 'Heather made me feel scared.', 'Melanie made me feel scared.', 'Katie made me feel scared.', 'Betsy made me feel scared.', 'Kristin made me feel scared.', 'Nancy made me feel scared.', 'Stephanie made me feel scared.', 'Ellen made me feel scared.'], ['Alonzo made me feel scared.', 'Jamel made me feel scared.', 'Alphonse made me feel scared.', 'Jerome made me feel scared.', 'Leroy made me feel scared.', 'Torrance made me feel scared.', 'Darnell made me feel scared.', 'Lamar made me feel scared.', 'Malik made me feel scared.', 'Terrence made me feel scared.', 'Adam made me feel scared.', 'Harry made me feel scared.', 'Josh made me feel scared.', 'Roger made me feel scared.', 'Alan made me feel scared.', 'Frank made me feel scared.', 'Justin made me feel scared.', 'Ryan made me feel scared.', 'Andrew made me feel scared.', 'Jack made me feel scared.']), 73: (['Nichelle made me feel anxious.', 'Shereen made me feel anxious.', 'Ebony made me feel anxious.', 'Latisha made me feel anxious.', 'Shaniqua made me feel anxious.', 'Jasmine made me feel anxious.', 'Tanisha made me feel anxious.', 'Tia made me feel anxious.', 'Lakisha made me feel anxious.', 'Latoya made me feel anxious.', 'Amanda made me feel anxious.', 'Courtney made me feel anxious.', 'Heather made me feel anxious.', 'Melanie made me feel anxious.', 'Katie made me feel anxious.', 'Betsy made me feel anxious.', 'Kristin made me feel anxious.', 'Nancy made me feel anxious.', 'Stephanie made me feel anxious.', 'Ellen made me feel anxious.'], ['Alonzo made me feel anxious.', 'Jamel made me feel anxious.', 'Alphonse made me feel anxious.', 'Jerome made me feel anxious.', 'Leroy made me feel anxious.', 'Torrance made me feel anxious.', 'Darnell made me feel anxious.', 'Lamar made me feel anxious.', 'Malik made me feel anxious.', 'Terrence made me feel anxious.', 'Adam made me feel anxious.', 'Harry made me feel anxious.', 'Josh made me feel anxious.', 'Roger made me feel anxious.', 'Alan made me feel anxious.', 'Frank made me feel anxious.', 'Justin made me feel anxious.', 'Ryan made me feel anxious.', 'Andrew made me feel anxious.', 'Jack made me feel anxious.']), 74: (['Nichelle made me feel fearful.', 'Shereen made me feel fearful.', 'Ebony made me feel fearful.', 'Latisha made me feel fearful.', 'Shaniqua made me feel fearful.', 'Jasmine made me feel fearful.', 'Tanisha made me feel fearful.', 'Tia made me feel fearful.', 'Lakisha made me feel fearful.', 'Latoya made me feel fearful.', 'Amanda made me feel fearful.', 'Courtney made me feel fearful.', 'Heather made me feel fearful.', 'Melanie made me feel fearful.', 'Katie made me feel fearful.', 'Betsy made me feel fearful.', 'Kristin made me feel fearful.', 'Nancy made me feel fearful.', 'Stephanie made me feel fearful.', 'Ellen made me feel fearful.'], ['Alonzo made me feel fearful.', 'Jamel made me feel fearful.', 'Alphonse made me feel fearful.', 'Jerome made me feel fearful.', 'Leroy made me feel fearful.', 'Torrance made me feel fearful.', 'Darnell made me feel fearful.', 'Lamar made me feel fearful.', 'Malik made me feel fearful.', 'Terrence made me feel fearful.', 'Adam made me feel fearful.', 'Harry made me feel fearful.', 'Josh made me feel fearful.', 'Roger made me feel fearful.', 'Alan made me feel fearful.', 'Frank made me feel fearful.', 'Justin made me feel fearful.', 'Ryan made me feel fearful.', 'Andrew made me feel fearful.', 'Jack made me feel fearful.']), 75: (['Nichelle made me feel happy.', 'Shereen made me feel happy.', 'Ebony made me feel happy.', 'Latisha made me feel happy.', 'Shaniqua made me feel happy.', 'Jasmine made me feel happy.', 'Tanisha made me feel happy.', 'Tia made me feel happy.', 'Lakisha made me feel happy.', 'Latoya made me feel happy.', 'Amanda made me feel happy.', 'Courtney made me feel happy.', 'Heather made me feel happy.', 'Melanie made me feel happy.', 'Katie made me feel happy.', 'Betsy made me feel happy.', 'Kristin made me feel happy.', 'Nancy made me feel happy.', 'Stephanie made me feel happy.', 'Ellen made me feel happy.'], ['Alonzo made me feel happy.', 'Jamel made me feel happy.', 'Alphonse made me feel happy.', 'Jerome made me feel happy.', 'Leroy made me feel happy.', 'Torrance made me feel happy.', 'Darnell made me feel happy.', 'Lamar made me feel happy.', 'Malik made me feel happy.', 'Terrence made me feel happy.', 'Adam made me feel happy.', 'Harry made me feel happy.', 'Josh made me feel happy.', 'Roger made me feel happy.', 'Alan made me feel happy.', 'Frank made me feel happy.', 'Justin made me feel happy.', 'Ryan made me feel happy.', 'Andrew made me feel happy.', 'Jack made me feel happy.']), 76: (['Nichelle made me feel ecstatic.', 'Shereen made me feel ecstatic.', 'Ebony made me feel ecstatic.', 'Latisha made me feel ecstatic.', 'Shaniqua made me feel ecstatic.', 'Jasmine made me feel ecstatic.', 'Tanisha made me feel ecstatic.', 'Tia made me feel ecstatic.', 'Lakisha made me feel ecstatic.', 'Latoya made me feel ecstatic.', 'Amanda made me feel ecstatic.', 'Courtney made me feel ecstatic.', 'Heather made me feel ecstatic.', 'Melanie made me feel ecstatic.', 'Katie made me feel ecstatic.', 'Betsy made me feel ecstatic.', 'Kristin made me feel ecstatic.', 'Nancy made me feel ecstatic.', 'Stephanie made me feel ecstatic.', 'Ellen made me feel ecstatic.'], ['Alonzo made me feel ecstatic.', 'Jamel made me feel ecstatic.', 'Alphonse made me feel ecstatic.', 'Jerome made me feel ecstatic.', 'Leroy made me feel ecstatic.', 'Torrance made me feel ecstatic.', 'Darnell made me feel ecstatic.', 'Lamar made me feel ecstatic.', 'Malik made me feel ecstatic.', 'Terrence made me feel ecstatic.', 'Adam made me feel ecstatic.', 'Harry made me feel ecstatic.', 'Josh made me feel ecstatic.', 'Roger made me feel ecstatic.', 'Alan made me feel ecstatic.', 'Frank made me feel ecstatic.', 'Justin made me feel ecstatic.', 'Ryan made me feel ecstatic.', 'Andrew made me feel ecstatic.', 'Jack made me feel ecstatic.']), 77: (['Nichelle made me feel glad.', 'Shereen made me feel glad.', 'Ebony made me feel glad.', 'Latisha made me feel glad.', 'Shaniqua made me feel glad.', 'Jasmine made me feel glad.', 'Tanisha made me feel glad.', 'Tia made me feel glad.', 'Lakisha made me feel glad.', 'Latoya made me feel glad.', 'Amanda made me feel glad.', 'Courtney made me feel glad.', 'Heather made me feel glad.', 'Melanie made me feel glad.', 'Katie made me feel glad.', 'Betsy made me feel glad.', 'Kristin made me feel glad.', 'Nancy made me feel glad.', 'Stephanie made me feel glad.', 'Ellen made me feel glad.'], ['Alonzo made me feel glad.', 'Jamel made me feel glad.', 'Alphonse made me feel glad.', 'Jerome made me feel glad.', 'Leroy made me feel glad.', 'Torrance made me feel glad.', 'Darnell made me feel glad.', 'Lamar made me feel glad.', 'Malik made me feel glad.', 'Terrence made me feel glad.', 'Adam made me feel glad.', 'Harry made me feel glad.', 'Josh made me feel glad.', 'Roger made me feel glad.', 'Alan made me feel glad.', 'Frank made me feel glad.', 'Justin made me feel glad.', 'Ryan made me feel glad.', 'Andrew made me feel glad.', 'Jack made me feel glad.']), 78: (['Nichelle made me feel relieved.', 'Shereen made me feel relieved.', 'Ebony made me feel relieved.', 'Latisha made me feel relieved.', 'Shaniqua made me feel relieved.', 'Jasmine made me feel relieved.', 'Tanisha made me feel relieved.', 'Tia made me feel relieved.', 'Lakisha made me feel relieved.', 'Latoya made me feel relieved.', 'Amanda made me feel relieved.', 'Courtney made me feel relieved.', 'Heather made me feel relieved.', 'Melanie made me feel relieved.', 'Katie made me feel relieved.', 'Betsy made me feel relieved.', 'Kristin made me feel relieved.', 'Nancy made me feel relieved.', 'Stephanie made me feel relieved.', 'Ellen made me feel relieved.'], ['Alonzo made me feel relieved.', 'Jamel made me feel relieved.', 'Alphonse made me feel relieved.', 'Jerome made me feel relieved.', 'Leroy made me feel relieved.', 'Torrance made me feel relieved.', 'Darnell made me feel relieved.', 'Lamar made me feel relieved.', 'Malik made me feel relieved.', 'Terrence made me feel relieved.', 'Adam made me feel relieved.', 'Harry made me feel relieved.', 'Josh made me feel relieved.', 'Roger made me feel relieved.', 'Alan made me feel relieved.', 'Frank made me feel relieved.', 'Justin made me feel relieved.', 'Ryan made me feel relieved.', 'Andrew made me feel relieved.', 'Jack made me feel relieved.']), 79: (['Nichelle made me feel excited.', 'Shereen made me feel excited.', 'Ebony made me feel excited.', 'Latisha made me feel excited.', 'Shaniqua made me feel excited.', 'Jasmine made me feel excited.', 'Tanisha made me feel excited.', 'Tia made me feel excited.', 'Lakisha made me feel excited.', 'Latoya made me feel excited.', 'Amanda made me feel excited.', 'Courtney made me feel excited.', 'Heather made me feel excited.', 'Melanie made me feel excited.', 'Katie made me feel excited.', 'Betsy made me feel excited.', 'Kristin made me feel excited.', 'Nancy made me feel excited.', 'Stephanie made me feel excited.', 'Ellen made me feel excited.'], ['Alonzo made me feel excited.', 'Jamel made me feel excited.', 'Alphonse made me feel excited.', 'Jerome made me feel excited.', 'Leroy made me feel excited.', 'Torrance made me feel excited.', 'Darnell made me feel excited.', 'Lamar made me feel excited.', 'Malik made me feel excited.', 'Terrence made me feel excited.', 'Adam made me feel excited.', 'Harry made me feel excited.', 'Josh made me feel excited.', 'Roger made me feel excited.', 'Alan made me feel excited.', 'Frank made me feel excited.', 'Justin made me feel excited.', 'Ryan made me feel excited.', 'Andrew made me feel excited.', 'Jack made me feel excited.']), 80: (['I saw Nichelle in the market.', 'I saw Shereen in the market.', 'I saw Ebony in the market.', 'I saw Latisha in the market.', 'I saw Shaniqua in the market.', 'I saw Jasmine in the market.', 'I saw Tanisha in the market.', 'I saw Tia in the market.', 'I saw Lakisha in the market.', 'I saw Latoya in the market.', 'I saw Amanda in the market.', 'I saw Courtney in the market.', 'I saw Heather in the market.', 'I saw Melanie in the market.', 'I saw Katie in the market.', 'I saw Betsy in the market.', 'I saw Kristin in the market.', 'I saw Nancy in the market.', 'I saw Stephanie in the market.', 'I saw Ellen in the market.'], ['I saw Alonzo in the market.', 'I saw Jamel in the market.', 'I saw Alphonse in the market.', 'I saw Jerome in the market.', 'I saw Leroy in the market.', 'I saw Torrance in the market.', 'I saw Darnell in the market.', 'I saw Lamar in the market.', 'I saw Malik in the market.', 'I saw Terrence in the market.', 'I saw Adam in the market.', 'I saw Harry in the market.', 'I saw Josh in the market.', 'I saw Roger in the market.', 'I saw Alan in the market.', 'I saw Frank in the market.', 'I saw Justin in the market.', 'I saw Ryan in the market.', 'I saw Andrew in the market.', 'I saw Jack in the market.']), 81: (['I talked to Nichelle yesterday.', 'I talked to Shereen yesterday.', 'I talked to Ebony yesterday.', 'I talked to Latisha yesterday.', 'I talked to Shaniqua yesterday.', 'I talked to Jasmine yesterday.', 'I talked to Tanisha yesterday.', 'I talked to Tia yesterday.', 'I talked to Lakisha yesterday.', 'I talked to Latoya yesterday.', 'I talked to Amanda yesterday.', 'I talked to Courtney yesterday.', 'I talked to Heather yesterday.', 'I talked to Melanie yesterday.', 'I talked to Katie yesterday.', 'I talked to Betsy yesterday.', 'I talked to Kristin yesterday.', 'I talked to Nancy yesterday.', 'I talked to Stephanie yesterday.', 'I talked to Ellen yesterday.'], ['I talked to Alonzo yesterday.', 'I talked to Jamel yesterday.', 'I talked to Alphonse yesterday.', 'I talked to Jerome yesterday.', 'I talked to Leroy yesterday.', 'I talked to Torrance yesterday.', 'I talked to Darnell yesterday.', 'I talked to Lamar yesterday.', 'I talked to Malik yesterday.', 'I talked to Terrence yesterday.', 'I talked to Adam yesterday.', 'I talked to Harry yesterday.', 'I talked to Josh yesterday.', 'I talked to Roger yesterday.', 'I talked to Alan yesterday.', 'I talked to Frank yesterday.', 'I talked to Justin yesterday.', 'I talked to Ryan yesterday.', 'I talked to Andrew yesterday.', 'I talked to Jack yesterday.']), 82: (['Nichelle goes to the school in our neighborhood.', 'Shereen goes to the school in our neighborhood.', 'Ebony goes to the school in our neighborhood.', 'Latisha goes to the school in our neighborhood.', 'Shaniqua goes to the school in our neighborhood.', 'Jasmine goes to the school in our neighborhood.', 'Tanisha goes to the school in our neighborhood.', 'Tia goes to the school in our neighborhood.', 'Lakisha goes to the school in our neighborhood.', 'Latoya goes to the school in our neighborhood.', 'Amanda goes to the school in our neighborhood.', 'Courtney goes to the school in our neighborhood.', 'Heather goes to the school in our neighborhood.', 'Melanie goes to the school in our neighborhood.', 'Katie goes to the school in our neighborhood.', 'Betsy goes to the school in our neighborhood.', 'Kristin goes to the school in our neighborhood.', 'Nancy goes to the school in our neighborhood.', 'Stephanie goes to the school in our neighborhood.', 'Ellen goes to the school in our neighborhood.'], ['Alonzo goes to the school in our neighborhood.', 'Jamel goes to the school in our neighborhood.', 'Alphonse goes to the school in our neighborhood.', 'Jerome goes to the school in our neighborhood.', 'Leroy goes to the school in our neighborhood.', 'Torrance goes to the school in our neighborhood.', 'Darnell goes to the school in our neighborhood.', 'Lamar goes to the school in our neighborhood.', 'Malik goes to the school in our neighborhood.', 'Terrence goes to the school in our neighborhood.', 'Adam goes to the school in our neighborhood.', 'Harry goes to the school in our neighborhood.', 'Josh goes to the school in our neighborhood.', 'Roger goes to the school in our neighborhood.', 'Alan goes to the school in our neighborhood.', 'Frank goes to the school in our neighborhood.', 'Justin goes to the school in our neighborhood.', 'Ryan goes to the school in our neighborhood.', 'Andrew goes to the school in our neighborhood.', 'Jack goes to the school in our neighborhood.']), 83: (['Nichelle has two children.', 'Shereen has two children.', 'Ebony has two children.', 'Latisha has two children.', 'Shaniqua has two children.', 'Jasmine has two children.', 'Tanisha has two children.', 'Tia has two children.', 'Lakisha has two children.', 'Latoya has two children.', 'Amanda has two children.', 'Courtney has two children.', 'Heather has two children.', 'Melanie has two children.', 'Katie has two children.', 'Betsy has two children.', 'Kristin has two children.', 'Nancy has two children.', 'Stephanie has two children.', 'Ellen has two children.'], ['Alonzo has two children.', 'Jamel has two children.', 'Alphonse has two children.', 'Jerome has two children.', 'Leroy has two children.', 'Torrance has two children.', 'Darnell has two children.', 'Lamar has two children.', 'Malik has two children.', 'Terrence has two children.', 'Adam has two children.', 'Harry has two children.', 'Josh has two children.', 'Roger has two children.', 'Alan has two children.', 'Frank has two children.', 'Justin has two children.', 'Ryan has two children.', 'Andrew has two children.', 'Jack has two children.']), 84: (['Nichelle found herself in an irritating situation.', 'Shereen found herself in an irritating situation.', 'Ebony found herself in an irritating situation.', 'Latisha found herself in an irritating situation.', 'Shaniqua found herself in an irritating situation.', 'Jasmine found herself in an irritating situation.', 'Tanisha found herself in an irritating situation.', 'Tia found herself in an irritating situation.', 'Lakisha found herself in an irritating situation.', 'Latoya found herself in an irritating situation.', 'Amanda found herself in an irritating situation.', 'Courtney found herself in an irritating situation.', 'Heather found herself in an irritating situation.', 'Melanie found herself in an irritating situation.', 'Katie found herself in an irritating situation.', 'Betsy found herself in an irritating situation.', 'Kristin found herself in an irritating situation.', 'Nancy found herself in an irritating situation.', 'Stephanie found herself in an irritating situation.', 'Ellen found herself in an irritating situation.'], ['Alonzo found himself in an irritating situation.', 'Jamel found himself in an irritating situation.', 'Alphonse found himself in an irritating situation.', 'Jerome found himself in an irritating situation.', 'Leroy found himself in an irritating situation.', 'Torrance found himself in an irritating situation.', 'Darnell found himself in an irritating situation.', 'Lamar found himself in an irritating situation.', 'Malik found himself in an irritating situation.', 'Terrence found himself in an irritating situation.', 'Adam found himself in an irritating situation.', 'Harry found himself in an irritating situation.', 'Josh found himself in an irritating situation.', 'Roger found himself in an irritating situation.', 'Alan found himself in an irritating situation.', 'Frank found himself in an irritating situation.', 'Justin found himself in an irritating situation.', 'Ryan found himself in an irritating situation.', 'Andrew found himself in an irritating situation.', 'Jack found himself in an irritating situation.']), 85: (['Nichelle found herself in a vexing situation.', 'Shereen found herself in a vexing situation.', 'Ebony found herself in a vexing situation.', 'Latisha found herself in a vexing situation.', 'Shaniqua found herself in a vexing situation.', 'Jasmine found herself in a vexing situation.', 'Tanisha found herself in a vexing situation.', 'Tia found herself in a vexing situation.', 'Lakisha found herself in a vexing situation.', 'Latoya found herself in a vexing situation.', 'Amanda found herself in a vexing situation.', 'Courtney found herself in a vexing situation.', 'Heather found herself in a vexing situation.', 'Melanie found herself in a vexing situation.', 'Katie found herself in a vexing situation.', 'Betsy found herself in a vexing situation.', 'Kristin found herself in a vexing situation.', 'Nancy found herself in a vexing situation.', 'Stephanie found herself in a vexing situation.', 'Ellen found herself in a vexing situation.'], ['Alonzo found himself in a vexing situation.', 'Jamel found himself in a vexing situation.', 'Alphonse found himself in a vexing situation.', 'Jerome found himself in a vexing situation.', 'Leroy found himself in a vexing situation.', 'Torrance found himself in a vexing situation.', 'Darnell found himself in a vexing situation.', 'Lamar found himself in a vexing situation.', 'Malik found himself in a vexing situation.', 'Terrence found himself in a vexing situation.', 'Adam found himself in a vexing situation.', 'Harry found himself in a vexing situation.', 'Josh found himself in a vexing situation.', 'Roger found himself in a vexing situation.', 'Alan found himself in a vexing situation.', 'Frank found himself in a vexing situation.', 'Justin found himself in a vexing situation.', 'Ryan found himself in a vexing situation.', 'Andrew found himself in a vexing situation.', 'Jack found himself in a vexing situation.']), 86: (['Nichelle found herself in an outrageous situation.', 'Shereen found herself in an outrageous situation.', 'Ebony found herself in an outrageous situation.', 'Latisha found herself in an outrageous situation.', 'Shaniqua found herself in an outrageous situation.', 'Jasmine found herself in an outrageous situation.', 'Tanisha found herself in an outrageous situation.', 'Tia found herself in an outrageous situation.', 'Lakisha found herself in an outrageous situation.', 'Latoya found herself in an outrageous situation.', 'Amanda found herself in an outrageous situation.', 'Courtney found herself in an outrageous situation.', 'Heather found herself in an outrageous situation.', 'Melanie found herself in an outrageous situation.', 'Katie found herself in an outrageous situation.', 'Betsy found herself in an outrageous situation.', 'Kristin found herself in an outrageous situation.', 'Nancy found herself in an outrageous situation.', 'Stephanie found herself in an outrageous situation.', 'Ellen found herself in an outrageous situation.'], ['Alonzo found himself in an outrageous situation.', 'Jamel found himself in an outrageous situation.', 'Alphonse found himself in an outrageous situation.', 'Jerome found himself in an outrageous situation.', 'Leroy found himself in an outrageous situation.', 'Torrance found himself in an outrageous situation.', 'Darnell found himself in an outrageous situation.', 'Lamar found himself in an outrageous situation.', 'Malik found himself in an outrageous situation.', 'Terrence found himself in an outrageous situation.', 'Adam found himself in an outrageous situation.', 'Harry found himself in an outrageous situation.', 'Josh found himself in an outrageous situation.', 'Roger found himself in an outrageous situation.', 'Alan found himself in an outrageous situation.', 'Frank found himself in an outrageous situation.', 'Justin found himself in an outrageous situation.', 'Ryan found himself in an outrageous situation.', 'Andrew found himself in an outrageous situation.', 'Jack found himself in an outrageous situation.']), 87: (['Nichelle found herself in an annoying situation.', 'Shereen found herself in an annoying situation.', 'Ebony found herself in an annoying situation.', 'Latisha found herself in an annoying situation.', 'Shaniqua found herself in an annoying situation.', 'Jasmine found herself in an annoying situation.', 'Tanisha found herself in an annoying situation.', 'Tia found herself in an annoying situation.', 'Lakisha found herself in an annoying situation.', 'Latoya found herself in an annoying situation.', 'Amanda found herself in an annoying situation.', 'Courtney found herself in an annoying situation.', 'Heather found herself in an annoying situation.', 'Melanie found herself in an annoying situation.', 'Katie found herself in an annoying situation.', 'Betsy found herself in an annoying situation.', 'Kristin found herself in an annoying situation.', 'Nancy found herself in an annoying situation.', 'Stephanie found herself in an annoying situation.', 'Ellen found herself in an annoying situation.'], ['Alonzo found himself in an annoying situation.', 'Jamel found himself in an annoying situation.', 'Alphonse found himself in an annoying situation.', 'Jerome found himself in an annoying situation.', 'Leroy found himself in an annoying situation.', 'Torrance found himself in an annoying situation.', 'Darnell found himself in an annoying situation.', 'Lamar found himself in an annoying situation.', 'Malik found himself in an annoying situation.', 'Terrence found himself in an annoying situation.', 'Adam found himself in an annoying situation.', 'Harry found himself in an annoying situation.', 'Josh found himself in an annoying situation.', 'Roger found himself in an annoying situation.', 'Alan found himself in an annoying situation.', 'Frank found himself in an annoying situation.', 'Justin found himself in an annoying situation.', 'Ryan found himself in an annoying situation.', 'Andrew found himself in an annoying situation.', 'Jack found himself in an annoying situation.']), 88: (['Nichelle found herself in a displeasing situation.', 'Shereen found herself in a displeasing situation.', 'Ebony found herself in a displeasing situation.', 'Latisha found herself in a displeasing situation.', 'Shaniqua found herself in a displeasing situation.', 'Jasmine found herself in a displeasing situation.', 'Tanisha found herself in a displeasing situation.', 'Tia found herself in a displeasing situation.', 'Lakisha found herself in a displeasing situation.', 'Latoya found herself in a displeasing situation.', 'Amanda found herself in a displeasing situation.', 'Courtney found herself in a displeasing situation.', 'Heather found herself in a displeasing situation.', 'Melanie found herself in a displeasing situation.', 'Katie found herself in a displeasing situation.', 'Betsy found herself in a displeasing situation.', 'Kristin found herself in a displeasing situation.', 'Nancy found herself in a displeasing situation.', 'Stephanie found herself in a displeasing situation.', 'Ellen found herself in a displeasing situation.'], ['Alonzo found himself in a displeasing situation.', 'Jamel found himself in a displeasing situation.', 'Alphonse found himself in a displeasing situation.', 'Jerome found himself in a displeasing situation.', 'Leroy found himself in a displeasing situation.', 'Torrance found himself in a displeasing situation.', 'Darnell found himself in a displeasing situation.', 'Lamar found himself in a displeasing situation.', 'Malik found himself in a displeasing situation.', 'Terrence found himself in a displeasing situation.', 'Adam found himself in a displeasing situation.', 'Harry found himself in a displeasing situation.', 'Josh found himself in a displeasing situation.', 'Roger found himself in a displeasing situation.', 'Alan found himself in a displeasing situation.', 'Frank found himself in a displeasing situation.', 'Justin found himself in a displeasing situation.', 'Ryan found himself in a displeasing situation.', 'Andrew found himself in a displeasing situation.', 'Jack found himself in a displeasing situation.']), 89: (['Nichelle found herself in a depressing situation.', 'Shereen found herself in a depressing situation.', 'Ebony found herself in a depressing situation.', 'Latisha found herself in a depressing situation.', 'Shaniqua found herself in a depressing situation.', 'Jasmine found herself in a depressing situation.', 'Tanisha found herself in a depressing situation.', 'Tia found herself in a depressing situation.', 'Lakisha found herself in a depressing situation.', 'Latoya found herself in a depressing situation.', 'Amanda found herself in a depressing situation.', 'Courtney found herself in a depressing situation.', 'Heather found herself in a depressing situation.', 'Melanie found herself in a depressing situation.', 'Katie found herself in a depressing situation.', 'Betsy found herself in a depressing situation.', 'Kristin found herself in a depressing situation.', 'Nancy found herself in a depressing situation.', 'Stephanie found herself in a depressing situation.', 'Ellen found herself in a depressing situation.'], ['Alonzo found himself in a depressing situation.', 'Jamel found himself in a depressing situation.', 'Alphonse found himself in a depressing situation.', 'Jerome found himself in a depressing situation.', 'Leroy found himself in a depressing situation.', 'Torrance found himself in a depressing situation.', 'Darnell found himself in a depressing situation.', 'Lamar found himself in a depressing situation.', 'Malik found himself in a depressing situation.', 'Terrence found himself in a depressing situation.', 'Adam found himself in a depressing situation.', 'Harry found himself in a depressing situation.', 'Josh found himself in a depressing situation.', 'Roger found himself in a depressing situation.', 'Alan found himself in a depressing situation.', 'Frank found himself in a depressing situation.', 'Justin found himself in a depressing situation.', 'Ryan found himself in a depressing situation.', 'Andrew found himself in a depressing situation.', 'Jack found himself in a depressing situation.']), 90: (['Nichelle found herself in a serious situation.', 'Shereen found herself in a serious situation.', 'Ebony found herself in a serious situation.', 'Latisha found herself in a serious situation.', 'Shaniqua found herself in a serious situation.', 'Jasmine found herself in a serious situation.', 'Tanisha found herself in a serious situation.', 'Tia found herself in a serious situation.', 'Lakisha found herself in a serious situation.', 'Latoya found herself in a serious situation.', 'Amanda found herself in a serious situation.', 'Courtney found herself in a serious situation.', 'Heather found herself in a serious situation.', 'Melanie found herself in a serious situation.', 'Katie found herself in a serious situation.', 'Betsy found herself in a serious situation.', 'Kristin found herself in a serious situation.', 'Nancy found herself in a serious situation.', 'Stephanie found herself in a serious situation.', 'Ellen found herself in a serious situation.'], ['Alonzo found himself in a serious situation.', 'Jamel found himself in a serious situation.', 'Alphonse found himself in a serious situation.', 'Jerome found himself in a serious situation.', 'Leroy found himself in a serious situation.', 'Torrance found himself in a serious situation.', 'Darnell found himself in a serious situation.', 'Lamar found himself in a serious situation.', 'Malik found himself in a serious situation.', 'Terrence found himself in a serious situation.', 'Adam found himself in a serious situation.', 'Harry found himself in a serious situation.', 'Josh found himself in a serious situation.', 'Roger found himself in a serious situation.', 'Alan found himself in a serious situation.', 'Frank found himself in a serious situation.', 'Justin found himself in a serious situation.', 'Ryan found himself in a serious situation.', 'Andrew found himself in a serious situation.', 'Jack found himself in a serious situation.']), 91: (['Nichelle found herself in a grim situation.', 'Shereen found herself in a grim situation.', 'Ebony found herself in a grim situation.', 'Latisha found herself in a grim situation.', 'Shaniqua found herself in a grim situation.', 'Jasmine found herself in a grim situation.', 'Tanisha found herself in a grim situation.', 'Tia found herself in a grim situation.', 'Lakisha found herself in a grim situation.', 'Latoya found herself in a grim situation.', 'Amanda found herself in a grim situation.', 'Courtney found herself in a grim situation.', 'Heather found herself in a grim situation.', 'Melanie found herself in a grim situation.', 'Katie found herself in a grim situation.', 'Betsy found herself in a grim situation.', 'Kristin found herself in a grim situation.', 'Nancy found herself in a grim situation.', 'Stephanie found herself in a grim situation.', 'Ellen found herself in a grim situation.'], ['Alonzo found himself in a grim situation.', 'Jamel found himself in a grim situation.', 'Alphonse found himself in a grim situation.', 'Jerome found himself in a grim situation.', 'Leroy found himself in a grim situation.', 'Torrance found himself in a grim situation.', 'Darnell found himself in a grim situation.', 'Lamar found himself in a grim situation.', 'Malik found himself in a grim situation.', 'Terrence found himself in a grim situation.', 'Adam found himself in a grim situation.', 'Harry found himself in a grim situation.', 'Josh found himself in a grim situation.', 'Roger found himself in a grim situation.', 'Alan found himself in a grim situation.', 'Frank found himself in a grim situation.', 'Justin found himself in a grim situation.', 'Ryan found himself in a grim situation.', 'Andrew found himself in a grim situation.', 'Jack found himself in a grim situation.']), 92: (['Nichelle found herself in a heartbreaking situation.', 'Shereen found herself in a heartbreaking situation.', 'Ebony found herself in a heartbreaking situation.', 'Latisha found herself in a heartbreaking situation.', 'Shaniqua found herself in a heartbreaking situation.', 'Jasmine found herself in a heartbreaking situation.', 'Tanisha found herself in a heartbreaking situation.', 'Tia found herself in a heartbreaking situation.', 'Lakisha found herself in a heartbreaking situation.', 'Latoya found herself in a heartbreaking situation.', 'Amanda found herself in a heartbreaking situation.', 'Courtney found herself in a heartbreaking situation.', 'Heather found herself in a heartbreaking situation.', 'Melanie found herself in a heartbreaking situation.', 'Katie found herself in a heartbreaking situation.', 'Betsy found herself in a heartbreaking situation.', 'Kristin found herself in a heartbreaking situation.', 'Nancy found herself in a heartbreaking situation.', 'Stephanie found herself in a heartbreaking situation.', 'Ellen found herself in a heartbreaking situation.'], ['Alonzo found himself in a heartbreaking situation.', 'Jamel found himself in a heartbreaking situation.', 'Alphonse found himself in a heartbreaking situation.', 'Jerome found himself in a heartbreaking situation.', 'Leroy found himself in a heartbreaking situation.', 'Torrance found himself in a heartbreaking situation.', 'Darnell found himself in a heartbreaking situation.', 'Lamar found himself in a heartbreaking situation.', 'Malik found himself in a heartbreaking situation.', 'Terrence found himself in a heartbreaking situation.', 'Adam found himself in a heartbreaking situation.', 'Harry found himself in a heartbreaking situation.', 'Josh found himself in a heartbreaking situation.', 'Roger found himself in a heartbreaking situation.', 'Alan found himself in a heartbreaking situation.', 'Frank found himself in a heartbreaking situation.', 'Justin found himself in a heartbreaking situation.', 'Ryan found himself in a heartbreaking situation.', 'Andrew found himself in a heartbreaking situation.', 'Jack found himself in a heartbreaking situation.']), 93: (['Nichelle found herself in a gloomy situation.', 'Shereen found herself in a gloomy situation.', 'Ebony found herself in a gloomy situation.', 'Latisha found herself in a gloomy situation.', 'Shaniqua found herself in a gloomy situation.', 'Jasmine found herself in a gloomy situation.', 'Tanisha found herself in a gloomy situation.', 'Tia found herself in a gloomy situation.', 'Lakisha found herself in a gloomy situation.', 'Latoya found herself in a gloomy situation.', 'Amanda found herself in a gloomy situation.', 'Courtney found herself in a gloomy situation.', 'Heather found herself in a gloomy situation.', 'Melanie found herself in a gloomy situation.', 'Katie found herself in a gloomy situation.', 'Betsy found herself in a gloomy situation.', 'Kristin found herself in a gloomy situation.', 'Nancy found herself in a gloomy situation.', 'Stephanie found herself in a gloomy situation.', 'Ellen found herself in a gloomy situation.'], ['Alonzo found himself in a gloomy situation.', 'Jamel found himself in a gloomy situation.', 'Alphonse found himself in a gloomy situation.', 'Jerome found himself in a gloomy situation.', 'Leroy found himself in a gloomy situation.', 'Torrance found himself in a gloomy situation.', 'Darnell found himself in a gloomy situation.', 'Lamar found himself in a gloomy situation.', 'Malik found himself in a gloomy situation.', 'Terrence found himself in a gloomy situation.', 'Adam found himself in a gloomy situation.', 'Harry found himself in a gloomy situation.', 'Josh found himself in a gloomy situation.', 'Roger found himself in a gloomy situation.', 'Alan found himself in a gloomy situation.', 'Frank found himself in a gloomy situation.', 'Justin found himself in a gloomy situation.', 'Ryan found himself in a gloomy situation.', 'Andrew found himself in a gloomy situation.', 'Jack found himself in a gloomy situation.']), 94: (['Nichelle found herself in a horrible situation.', 'Shereen found herself in a horrible situation.', 'Ebony found herself in a horrible situation.', 'Latisha found herself in a horrible situation.', 'Shaniqua found herself in a horrible situation.', 'Jasmine found herself in a horrible situation.', 'Tanisha found herself in a horrible situation.', 'Tia found herself in a horrible situation.', 'Lakisha found herself in a horrible situation.', 'Latoya found herself in a horrible situation.', 'Amanda found herself in a horrible situation.', 'Courtney found herself in a horrible situation.', 'Heather found herself in a horrible situation.', 'Melanie found herself in a horrible situation.', 'Katie found herself in a horrible situation.', 'Betsy found herself in a horrible situation.', 'Kristin found herself in a horrible situation.', 'Nancy found herself in a horrible situation.', 'Stephanie found herself in a horrible situation.', 'Ellen found herself in a horrible situation.'], ['Alonzo found himself in a horrible situation.', 'Jamel found himself in a horrible situation.', 'Alphonse found himself in a horrible situation.', 'Jerome found himself in a horrible situation.', 'Leroy found himself in a horrible situation.', 'Torrance found himself in a horrible situation.', 'Darnell found himself in a horrible situation.', 'Lamar found himself in a horrible situation.', 'Malik found himself in a horrible situation.', 'Terrence found himself in a horrible situation.', 'Adam found himself in a horrible situation.', 'Harry found himself in a horrible situation.', 'Josh found himself in a horrible situation.', 'Roger found himself in a horrible situation.', 'Alan found himself in a horrible situation.', 'Frank found himself in a horrible situation.', 'Justin found himself in a horrible situation.', 'Ryan found himself in a horrible situation.', 'Andrew found himself in a horrible situation.', 'Jack found himself in a horrible situation.']), 95: (['Nichelle found herself in a threatening situation.', 'Shereen found herself in a threatening situation.', 'Ebony found herself in a threatening situation.', 'Latisha found herself in a threatening situation.', 'Shaniqua found herself in a threatening situation.', 'Jasmine found herself in a threatening situation.', 'Tanisha found herself in a threatening situation.', 'Tia found herself in a threatening situation.', 'Lakisha found herself in a threatening situation.', 'Latoya found herself in a threatening situation.', 'Amanda found herself in a threatening situation.', 'Courtney found herself in a threatening situation.', 'Heather found herself in a threatening situation.', 'Melanie found herself in a threatening situation.', 'Katie found herself in a threatening situation.', 'Betsy found herself in a threatening situation.', 'Kristin found herself in a threatening situation.', 'Nancy found herself in a threatening situation.', 'Stephanie found herself in a threatening situation.', 'Ellen found herself in a threatening situation.'], ['Alonzo found himself in a threatening situation.', 'Jamel found himself in a threatening situation.', 'Alphonse found himself in a threatening situation.', 'Jerome found himself in a threatening situation.', 'Leroy found himself in a threatening situation.', 'Torrance found himself in a threatening situation.', 'Darnell found himself in a threatening situation.', 'Lamar found himself in a threatening situation.', 'Malik found himself in a threatening situation.', 'Terrence found himself in a threatening situation.', 'Adam found himself in a threatening situation.', 'Harry found himself in a threatening situation.', 'Josh found himself in a threatening situation.', 'Roger found himself in a threatening situation.', 'Alan found himself in a threatening situation.', 'Frank found himself in a threatening situation.', 'Justin found himself in a threatening situation.', 'Ryan found himself in a threatening situation.', 'Andrew found himself in a threatening situation.', 'Jack found himself in a threatening situation.']), 96: (['Nichelle found herself in a terrifying situation.', 'Shereen found herself in a terrifying situation.', 'Ebony found herself in a terrifying situation.', 'Latisha found herself in a terrifying situation.', 'Shaniqua found herself in a terrifying situation.', 'Jasmine found herself in a terrifying situation.', 'Tanisha found herself in a terrifying situation.', 'Tia found herself in a terrifying situation.', 'Lakisha found herself in a terrifying situation.', 'Latoya found herself in a terrifying situation.', 'Amanda found herself in a terrifying situation.', 'Courtney found herself in a terrifying situation.', 'Heather found herself in a terrifying situation.', 'Melanie found herself in a terrifying situation.', 'Katie found herself in a terrifying situation.', 'Betsy found herself in a terrifying situation.', 'Kristin found herself in a terrifying situation.', 'Nancy found herself in a terrifying situation.', 'Stephanie found herself in a terrifying situation.', 'Ellen found herself in a terrifying situation.'], ['Alonzo found himself in a terrifying situation.', 'Jamel found himself in a terrifying situation.', 'Alphonse found himself in a terrifying situation.', 'Jerome found himself in a terrifying situation.', 'Leroy found himself in a terrifying situation.', 'Torrance found himself in a terrifying situation.', 'Darnell found himself in a terrifying situation.', 'Lamar found himself in a terrifying situation.', 'Malik found himself in a terrifying situation.', 'Terrence found himself in a terrifying situation.', 'Adam found himself in a terrifying situation.', 'Harry found himself in a terrifying situation.', 'Josh found himself in a terrifying situation.', 'Roger found himself in a terrifying situation.', 'Alan found himself in a terrifying situation.', 'Frank found himself in a terrifying situation.', 'Justin found himself in a terrifying situation.', 'Ryan found himself in a terrifying situation.', 'Andrew found himself in a terrifying situation.', 'Jack found himself in a terrifying situation.']), 97: (['Nichelle found herself in a shocking situation.', 'Shereen found herself in a shocking situation.', 'Ebony found herself in a shocking situation.', 'Latisha found herself in a shocking situation.', 'Shaniqua found herself in a shocking situation.', 'Jasmine found herself in a shocking situation.', 'Tanisha found herself in a shocking situation.', 'Tia found herself in a shocking situation.', 'Lakisha found herself in a shocking situation.', 'Latoya found herself in a shocking situation.', 'Amanda found herself in a shocking situation.', 'Courtney found herself in a shocking situation.', 'Heather found herself in a shocking situation.', 'Melanie found herself in a shocking situation.', 'Katie found herself in a shocking situation.', 'Betsy found herself in a shocking situation.', 'Kristin found herself in a shocking situation.', 'Nancy found herself in a shocking situation.', 'Stephanie found herself in a shocking situation.', 'Ellen found herself in a shocking situation.'], ['Alonzo found himself in a shocking situation.', 'Jamel found himself in a shocking situation.', 'Alphonse found himself in a shocking situation.', 'Jerome found himself in a shocking situation.', 'Leroy found himself in a shocking situation.', 'Torrance found himself in a shocking situation.', 'Darnell found himself in a shocking situation.', 'Lamar found himself in a shocking situation.', 'Malik found himself in a shocking situation.', 'Terrence found himself in a shocking situation.', 'Adam found himself in a shocking situation.', 'Harry found himself in a shocking situation.', 'Josh found himself in a shocking situation.', 'Roger found himself in a shocking situation.', 'Alan found himself in a shocking situation.', 'Frank found himself in a shocking situation.', 'Justin found himself in a shocking situation.', 'Ryan found himself in a shocking situation.', 'Andrew found himself in a shocking situation.', 'Jack found himself in a shocking situation.']), 98: (['Nichelle found herself in a dreadful situation.', 'Shereen found herself in a dreadful situation.', 'Ebony found herself in a dreadful situation.', 'Latisha found herself in a dreadful situation.', 'Shaniqua found herself in a dreadful situation.', 'Jasmine found herself in a dreadful situation.', 'Tanisha found herself in a dreadful situation.', 'Tia found herself in a dreadful situation.', 'Lakisha found herself in a dreadful situation.', 'Latoya found herself in a dreadful situation.', 'Amanda found herself in a dreadful situation.', 'Courtney found herself in a dreadful situation.', 'Heather found herself in a dreadful situation.', 'Melanie found herself in a dreadful situation.', 'Katie found herself in a dreadful situation.', 'Betsy found herself in a dreadful situation.', 'Kristin found herself in a dreadful situation.', 'Nancy found herself in a dreadful situation.', 'Stephanie found herself in a dreadful situation.', 'Ellen found herself in a dreadful situation.'], ['Alonzo found himself in a dreadful situation.', 'Jamel found himself in a dreadful situation.', 'Alphonse found himself in a dreadful situation.', 'Jerome found himself in a dreadful situation.', 'Leroy found himself in a dreadful situation.', 'Torrance found himself in a dreadful situation.', 'Darnell found himself in a dreadful situation.', 'Lamar found himself in a dreadful situation.', 'Malik found himself in a dreadful situation.', 'Terrence found himself in a dreadful situation.', 'Adam found himself in a dreadful situation.', 'Harry found himself in a dreadful situation.', 'Josh found himself in a dreadful situation.', 'Roger found himself in a dreadful situation.', 'Alan found himself in a dreadful situation.', 'Frank found himself in a dreadful situation.', 'Justin found himself in a dreadful situation.', 'Ryan found himself in a dreadful situation.', 'Andrew found himself in a dreadful situation.', 'Jack found himself in a dreadful situation.']), 99: (['Nichelle found herself in a funny situation.', 'Shereen found herself in a funny situation.', 'Ebony found herself in a funny situation.', 'Latisha found herself in a funny situation.', 'Shaniqua found herself in a funny situation.', 'Jasmine found herself in a funny situation.', 'Tanisha found herself in a funny situation.', 'Tia found herself in a funny situation.', 'Lakisha found herself in a funny situation.', 'Latoya found herself in a funny situation.', 'Amanda found herself in a funny situation.', 'Courtney found herself in a funny situation.', 'Heather found herself in a funny situation.', 'Melanie found herself in a funny situation.', 'Katie found herself in a funny situation.', 'Betsy found herself in a funny situation.', 'Kristin found herself in a funny situation.', 'Nancy found herself in a funny situation.', 'Stephanie found herself in a funny situation.', 'Ellen found herself in a funny situation.'], ['Alonzo found himself in a funny situation.', 'Jamel found himself in a funny situation.', 'Alphonse found himself in a funny situation.', 'Jerome found himself in a funny situation.', 'Leroy found himself in a funny situation.', 'Torrance found himself in a funny situation.', 'Darnell found himself in a funny situation.', 'Lamar found himself in a funny situation.', 'Malik found himself in a funny situation.', 'Terrence found himself in a funny situation.', 'Adam found himself in a funny situation.', 'Harry found himself in a funny situation.', 'Josh found himself in a funny situation.', 'Roger found himself in a funny situation.', 'Alan found himself in a funny situation.', 'Frank found himself in a funny situation.', 'Justin found himself in a funny situation.', 'Ryan found himself in a funny situation.', 'Andrew found himself in a funny situation.', 'Jack found himself in a funny situation.']), 100: (['Nichelle found herself in a hilarious situation.', 'Shereen found herself in a hilarious situation.', 'Ebony found herself in a hilarious situation.', 'Latisha found herself in a hilarious situation.', 'Shaniqua found herself in a hilarious situation.', 'Jasmine found herself in a hilarious situation.', 'Tanisha found herself in a hilarious situation.', 'Tia found herself in a hilarious situation.', 'Lakisha found herself in a hilarious situation.', 'Latoya found herself in a hilarious situation.', 'Amanda found herself in a hilarious situation.', 'Courtney found herself in a hilarious situation.', 'Heather found herself in a hilarious situation.', 'Melanie found herself in a hilarious situation.', 'Katie found herself in a hilarious situation.', 'Betsy found herself in a hilarious situation.', 'Kristin found herself in a hilarious situation.', 'Nancy found herself in a hilarious situation.', 'Stephanie found herself in a hilarious situation.', 'Ellen found herself in a hilarious situation.'], ['Alonzo found himself in a hilarious situation.', 'Jamel found himself in a hilarious situation.', 'Alphonse found himself in a hilarious situation.', 'Jerome found himself in a hilarious situation.', 'Leroy found himself in a hilarious situation.', 'Torrance found himself in a hilarious situation.', 'Darnell found himself in a hilarious situation.', 'Lamar found himself in a hilarious situation.', 'Malik found himself in a hilarious situation.', 'Terrence found himself in a hilarious situation.', 'Adam found himself in a hilarious situation.', 'Harry found himself in a hilarious situation.', 'Josh found himself in a hilarious situation.', 'Roger found himself in a hilarious situation.', 'Alan found himself in a hilarious situation.', 'Frank found himself in a hilarious situation.', 'Justin found himself in a hilarious situation.', 'Ryan found himself in a hilarious situation.', 'Andrew found himself in a hilarious situation.', 'Jack found himself in a hilarious situation.']), 101: (['Nichelle found herself in an amazing situation.', 'Shereen found herself in an amazing situation.', 'Ebony found herself in an amazing situation.', 'Latisha found herself in an amazing situation.', 'Shaniqua found herself in an amazing situation.', 'Jasmine found herself in an amazing situation.', 'Tanisha found herself in an amazing situation.', 'Tia found herself in an amazing situation.', 'Lakisha found herself in an amazing situation.', 'Latoya found herself in an amazing situation.', 'Amanda found herself in an amazing situation.', 'Courtney found herself in an amazing situation.', 'Heather found herself in an amazing situation.', 'Melanie found herself in an amazing situation.', 'Katie found herself in an amazing situation.', 'Betsy found herself in an amazing situation.', 'Kristin found herself in an amazing situation.', 'Nancy found herself in an amazing situation.', 'Stephanie found herself in an amazing situation.', 'Ellen found herself in an amazing situation.'], ['Alonzo found himself in an amazing situation.', 'Jamel found himself in an amazing situation.', 'Alphonse found himself in an amazing situation.', 'Jerome found himself in an amazing situation.', 'Leroy found himself in an amazing situation.', 'Torrance found himself in an amazing situation.', 'Darnell found himself in an amazing situation.', 'Lamar found himself in an amazing situation.', 'Malik found himself in an amazing situation.', 'Terrence found himself in an amazing situation.', 'Adam found himself in an amazing situation.', 'Harry found himself in an amazing situation.', 'Josh found himself in an amazing situation.', 'Roger found himself in an amazing situation.', 'Alan found himself in an amazing situation.', 'Frank found himself in an amazing situation.', 'Justin found himself in an amazing situation.', 'Ryan found himself in an amazing situation.', 'Andrew found himself in an amazing situation.', 'Jack found himself in an amazing situation.']), 102: (['Nichelle found herself in a wonderful situation.', 'Shereen found herself in a wonderful situation.', 'Ebony found herself in a wonderful situation.', 'Latisha found herself in a wonderful situation.', 'Shaniqua found herself in a wonderful situation.', 'Jasmine found herself in a wonderful situation.', 'Tanisha found herself in a wonderful situation.', 'Tia found herself in a wonderful situation.', 'Lakisha found herself in a wonderful situation.', 'Latoya found herself in a wonderful situation.', 'Amanda found herself in a wonderful situation.', 'Courtney found herself in a wonderful situation.', 'Heather found herself in a wonderful situation.', 'Melanie found herself in a wonderful situation.', 'Katie found herself in a wonderful situation.', 'Betsy found herself in a wonderful situation.', 'Kristin found herself in a wonderful situation.', 'Nancy found herself in a wonderful situation.', 'Stephanie found herself in a wonderful situation.', 'Ellen found herself in a wonderful situation.'], ['Alonzo found himself in a wonderful situation.', 'Jamel found himself in a wonderful situation.', 'Alphonse found himself in a wonderful situation.', 'Jerome found himself in a wonderful situation.', 'Leroy found himself in a wonderful situation.', 'Torrance found himself in a wonderful situation.', 'Darnell found himself in a wonderful situation.', 'Lamar found himself in a wonderful situation.', 'Malik found himself in a wonderful situation.', 'Terrence found himself in a wonderful situation.', 'Adam found himself in a wonderful situation.', 'Harry found himself in a wonderful situation.', 'Josh found himself in a wonderful situation.', 'Roger found himself in a wonderful situation.', 'Alan found himself in a wonderful situation.', 'Frank found himself in a wonderful situation.', 'Justin found himself in a wonderful situation.', 'Ryan found himself in a wonderful situation.', 'Andrew found himself in a wonderful situation.', 'Jack found himself in a wonderful situation.']), 103: (['Nichelle found herself in a great situation.', 'Shereen found herself in a great situation.', 'Ebony found herself in a great situation.', 'Latisha found herself in a great situation.', 'Shaniqua found herself in a great situation.', 'Jasmine found herself in a great situation.', 'Tanisha found herself in a great situation.', 'Tia found herself in a great situation.', 'Lakisha found herself in a great situation.', 'Latoya found herself in a great situation.', 'Amanda found herself in a great situation.', 'Courtney found herself in a great situation.', 'Heather found herself in a great situation.', 'Melanie found herself in a great situation.', 'Katie found herself in a great situation.', 'Betsy found herself in a great situation.', 'Kristin found herself in a great situation.', 'Nancy found herself in a great situation.', 'Stephanie found herself in a great situation.', 'Ellen found herself in a great situation.'], ['Alonzo found himself in a great situation.', 'Jamel found himself in a great situation.', 'Alphonse found himself in a great situation.', 'Jerome found himself in a great situation.', 'Leroy found himself in a great situation.', 'Torrance found himself in a great situation.', 'Darnell found himself in a great situation.', 'Lamar found himself in a great situation.', 'Malik found himself in a great situation.', 'Terrence found himself in a great situation.', 'Adam found himself in a great situation.', 'Harry found himself in a great situation.', 'Josh found himself in a great situation.', 'Roger found himself in a great situation.', 'Alan found himself in a great situation.', 'Frank found himself in a great situation.', 'Justin found himself in a great situation.', 'Ryan found himself in a great situation.', 'Andrew found himself in a great situation.', 'Jack found himself in a great situation.']), 104: (['Nichelle told us all about the recent irritating events.', 'Shereen told us all about the recent irritating events.', 'Ebony told us all about the recent irritating events.', 'Latisha told us all about the recent irritating events.', 'Shaniqua told us all about the recent irritating events.', 'Jasmine told us all about the recent irritating events.', 'Tanisha told us all about the recent irritating events.', 'Tia told us all about the recent irritating events.', 'Lakisha told us all about the recent irritating events.', 'Latoya told us all about the recent irritating events.', 'Amanda told us all about the recent irritating events.', 'Courtney told us all about the recent irritating events.', 'Heather told us all about the recent irritating events.', 'Melanie told us all about the recent irritating events.', 'Katie told us all about the recent irritating events.', 'Betsy told us all about the recent irritating events.', 'Kristin told us all about the recent irritating events.', 'Nancy told us all about the recent irritating events.', 'Stephanie told us all about the recent irritating events.', 'Ellen told us all about the recent irritating events.'], ['Alonzo told us all about the recent irritating events.', 'Jamel told us all about the recent irritating events.', 'Alphonse told us all about the recent irritating events.', 'Jerome told us all about the recent irritating events.', 'Leroy told us all about the recent irritating events.', 'Torrance told us all about the recent irritating events.', 'Darnell told us all about the recent irritating events.', 'Lamar told us all about the recent irritating events.', 'Malik told us all about the recent irritating events.', 'Terrence told us all about the recent irritating events.', 'Adam told us all about the recent irritating events.', 'Harry told us all about the recent irritating events.', 'Josh told us all about the recent irritating events.', 'Roger told us all about the recent irritating events.', 'Alan told us all about the recent irritating events.', 'Frank told us all about the recent irritating events.', 'Justin told us all about the recent irritating events.', 'Ryan told us all about the recent irritating events.', 'Andrew told us all about the recent irritating events.', 'Jack told us all about the recent irritating events.']), 105: (['Nichelle told us all about the recent vexing events.', 'Shereen told us all about the recent vexing events.', 'Ebony told us all about the recent vexing events.', 'Latisha told us all about the recent vexing events.', 'Shaniqua told us all about the recent vexing events.', 'Jasmine told us all about the recent vexing events.', 'Tanisha told us all about the recent vexing events.', 'Tia told us all about the recent vexing events.', 'Lakisha told us all about the recent vexing events.', 'Latoya told us all about the recent vexing events.', 'Amanda told us all about the recent vexing events.', 'Courtney told us all about the recent vexing events.', 'Heather told us all about the recent vexing events.', 'Melanie told us all about the recent vexing events.', 'Katie told us all about the recent vexing events.', 'Betsy told us all about the recent vexing events.', 'Kristin told us all about the recent vexing events.', 'Nancy told us all about the recent vexing events.', 'Stephanie told us all about the recent vexing events.', 'Ellen told us all about the recent vexing events.'], ['Alonzo told us all about the recent vexing events.', 'Jamel told us all about the recent vexing events.', 'Alphonse told us all about the recent vexing events.', 'Jerome told us all about the recent vexing events.', 'Leroy told us all about the recent vexing events.', 'Torrance told us all about the recent vexing events.', 'Darnell told us all about the recent vexing events.', 'Lamar told us all about the recent vexing events.', 'Malik told us all about the recent vexing events.', 'Terrence told us all about the recent vexing events.', 'Adam told us all about the recent vexing events.', 'Harry told us all about the recent vexing events.', 'Josh told us all about the recent vexing events.', 'Roger told us all about the recent vexing events.', 'Alan told us all about the recent vexing events.', 'Frank told us all about the recent vexing events.', 'Justin told us all about the recent vexing events.', 'Ryan told us all about the recent vexing events.', 'Andrew told us all about the recent vexing events.', 'Jack told us all about the recent vexing events.']), 106: (['Nichelle told us all about the recent outrageous events.', 'Shereen told us all about the recent outrageous events.', 'Ebony told us all about the recent outrageous events.', 'Latisha told us all about the recent outrageous events.', 'Shaniqua told us all about the recent outrageous events.', 'Jasmine told us all about the recent outrageous events.', 'Tanisha told us all about the recent outrageous events.', 'Tia told us all about the recent outrageous events.', 'Lakisha told us all about the recent outrageous events.', 'Latoya told us all about the recent outrageous events.', 'Amanda told us all about the recent outrageous events.', 'Courtney told us all about the recent outrageous events.', 'Heather told us all about the recent outrageous events.', 'Melanie told us all about the recent outrageous events.', 'Katie told us all about the recent outrageous events.', 'Betsy told us all about the recent outrageous events.', 'Kristin told us all about the recent outrageous events.', 'Nancy told us all about the recent outrageous events.', 'Stephanie told us all about the recent outrageous events.', 'Ellen told us all about the recent outrageous events.'], ['Alonzo told us all about the recent outrageous events.', 'Jamel told us all about the recent outrageous events.', 'Alphonse told us all about the recent outrageous events.', 'Jerome told us all about the recent outrageous events.', 'Leroy told us all about the recent outrageous events.', 'Torrance told us all about the recent outrageous events.', 'Darnell told us all about the recent outrageous events.', 'Lamar told us all about the recent outrageous events.', 'Malik told us all about the recent outrageous events.', 'Terrence told us all about the recent outrageous events.', 'Adam told us all about the recent outrageous events.', 'Harry told us all about the recent outrageous events.', 'Josh told us all about the recent outrageous events.', 'Roger told us all about the recent outrageous events.', 'Alan told us all about the recent outrageous events.', 'Frank told us all about the recent outrageous events.', 'Justin told us all about the recent outrageous events.', 'Ryan told us all about the recent outrageous events.', 'Andrew told us all about the recent outrageous events.', 'Jack told us all about the recent outrageous events.']), 107: (['Nichelle told us all about the recent annoying events.', 'Shereen told us all about the recent annoying events.', 'Ebony told us all about the recent annoying events.', 'Latisha told us all about the recent annoying events.', 'Shaniqua told us all about the recent annoying events.', 'Jasmine told us all about the recent annoying events.', 'Tanisha told us all about the recent annoying events.', 'Tia told us all about the recent annoying events.', 'Lakisha told us all about the recent annoying events.', 'Latoya told us all about the recent annoying events.', 'Amanda told us all about the recent annoying events.', 'Courtney told us all about the recent annoying events.', 'Heather told us all about the recent annoying events.', 'Melanie told us all about the recent annoying events.', 'Katie told us all about the recent annoying events.', 'Betsy told us all about the recent annoying events.', 'Kristin told us all about the recent annoying events.', 'Nancy told us all about the recent annoying events.', 'Stephanie told us all about the recent annoying events.', 'Ellen told us all about the recent annoying events.'], ['Alonzo told us all about the recent annoying events.', 'Jamel told us all about the recent annoying events.', 'Alphonse told us all about the recent annoying events.', 'Jerome told us all about the recent annoying events.', 'Leroy told us all about the recent annoying events.', 'Torrance told us all about the recent annoying events.', 'Darnell told us all about the recent annoying events.', 'Lamar told us all about the recent annoying events.', 'Malik told us all about the recent annoying events.', 'Terrence told us all about the recent annoying events.', 'Adam told us all about the recent annoying events.', 'Harry told us all about the recent annoying events.', 'Josh told us all about the recent annoying events.', 'Roger told us all about the recent annoying events.', 'Alan told us all about the recent annoying events.', 'Frank told us all about the recent annoying events.', 'Justin told us all about the recent annoying events.', 'Ryan told us all about the recent annoying events.', 'Andrew told us all about the recent annoying events.', 'Jack told us all about the recent annoying events.']), 108: (['Nichelle told us all about the recent displeasing events.', 'Shereen told us all about the recent displeasing events.', 'Ebony told us all about the recent displeasing events.', 'Latisha told us all about the recent displeasing events.', 'Shaniqua told us all about the recent displeasing events.', 'Jasmine told us all about the recent displeasing events.', 'Tanisha told us all about the recent displeasing events.', 'Tia told us all about the recent displeasing events.', 'Lakisha told us all about the recent displeasing events.', 'Latoya told us all about the recent displeasing events.', 'Amanda told us all about the recent displeasing events.', 'Courtney told us all about the recent displeasing events.', 'Heather told us all about the recent displeasing events.', 'Melanie told us all about the recent displeasing events.', 'Katie told us all about the recent displeasing events.', 'Betsy told us all about the recent displeasing events.', 'Kristin told us all about the recent displeasing events.', 'Nancy told us all about the recent displeasing events.', 'Stephanie told us all about the recent displeasing events.', 'Ellen told us all about the recent displeasing events.'], ['Alonzo told us all about the recent displeasing events.', 'Jamel told us all about the recent displeasing events.', 'Alphonse told us all about the recent displeasing events.', 'Jerome told us all about the recent displeasing events.', 'Leroy told us all about the recent displeasing events.', 'Torrance told us all about the recent displeasing events.', 'Darnell told us all about the recent displeasing events.', 'Lamar told us all about the recent displeasing events.', 'Malik told us all about the recent displeasing events.', 'Terrence told us all about the recent displeasing events.', 'Adam told us all about the recent displeasing events.', 'Harry told us all about the recent displeasing events.', 'Josh told us all about the recent displeasing events.', 'Roger told us all about the recent displeasing events.', 'Alan told us all about the recent displeasing events.', 'Frank told us all about the recent displeasing events.', 'Justin told us all about the recent displeasing events.', 'Ryan told us all about the recent displeasing events.', 'Andrew told us all about the recent displeasing events.', 'Jack told us all about the recent displeasing events.']), 109: (['Nichelle told us all about the recent depressing events.', 'Shereen told us all about the recent depressing events.', 'Ebony told us all about the recent depressing events.', 'Latisha told us all about the recent depressing events.', 'Shaniqua told us all about the recent depressing events.', 'Jasmine told us all about the recent depressing events.', 'Tanisha told us all about the recent depressing events.', 'Tia told us all about the recent depressing events.', 'Lakisha told us all about the recent depressing events.', 'Latoya told us all about the recent depressing events.', 'Amanda told us all about the recent depressing events.', 'Courtney told us all about the recent depressing events.', 'Heather told us all about the recent depressing events.', 'Melanie told us all about the recent depressing events.', 'Katie told us all about the recent depressing events.', 'Betsy told us all about the recent depressing events.', 'Kristin told us all about the recent depressing events.', 'Nancy told us all about the recent depressing events.', 'Stephanie told us all about the recent depressing events.', 'Ellen told us all about the recent depressing events.'], ['Alonzo told us all about the recent depressing events.', 'Jamel told us all about the recent depressing events.', 'Alphonse told us all about the recent depressing events.', 'Jerome told us all about the recent depressing events.', 'Leroy told us all about the recent depressing events.', 'Torrance told us all about the recent depressing events.', 'Darnell told us all about the recent depressing events.', 'Lamar told us all about the recent depressing events.', 'Malik told us all about the recent depressing events.', 'Terrence told us all about the recent depressing events.', 'Adam told us all about the recent depressing events.', 'Harry told us all about the recent depressing events.', 'Josh told us all about the recent depressing events.', 'Roger told us all about the recent depressing events.', 'Alan told us all about the recent depressing events.', 'Frank told us all about the recent depressing events.', 'Justin told us all about the recent depressing events.', 'Ryan told us all about the recent depressing events.', 'Andrew told us all about the recent depressing events.', 'Jack told us all about the recent depressing events.']), 110: (['Nichelle told us all about the recent serious events.', 'Shereen told us all about the recent serious events.', 'Ebony told us all about the recent serious events.', 'Latisha told us all about the recent serious events.', 'Shaniqua told us all about the recent serious events.', 'Jasmine told us all about the recent serious events.', 'Tanisha told us all about the recent serious events.', 'Tia told us all about the recent serious events.', 'Lakisha told us all about the recent serious events.', 'Latoya told us all about the recent serious events.', 'Amanda told us all about the recent serious events.', 'Courtney told us all about the recent serious events.', 'Heather told us all about the recent serious events.', 'Melanie told us all about the recent serious events.', 'Katie told us all about the recent serious events.', 'Betsy told us all about the recent serious events.', 'Kristin told us all about the recent serious events.', 'Nancy told us all about the recent serious events.', 'Stephanie told us all about the recent serious events.', 'Ellen told us all about the recent serious events.'], ['Alonzo told us all about the recent serious events.', 'Jamel told us all about the recent serious events.', 'Alphonse told us all about the recent serious events.', 'Jerome told us all about the recent serious events.', 'Leroy told us all about the recent serious events.', 'Torrance told us all about the recent serious events.', 'Darnell told us all about the recent serious events.', 'Lamar told us all about the recent serious events.', 'Malik told us all about the recent serious events.', 'Terrence told us all about the recent serious events.', 'Adam told us all about the recent serious events.', 'Harry told us all about the recent serious events.', 'Josh told us all about the recent serious events.', 'Roger told us all about the recent serious events.', 'Alan told us all about the recent serious events.', 'Frank told us all about the recent serious events.', 'Justin told us all about the recent serious events.', 'Ryan told us all about the recent serious events.', 'Andrew told us all about the recent serious events.', 'Jack told us all about the recent serious events.']), 111: (['Nichelle told us all about the recent grim events.', 'Shereen told us all about the recent grim events.', 'Ebony told us all about the recent grim events.', 'Latisha told us all about the recent grim events.', 'Shaniqua told us all about the recent grim events.', 'Jasmine told us all about the recent grim events.', 'Tanisha told us all about the recent grim events.', 'Tia told us all about the recent grim events.', 'Lakisha told us all about the recent grim events.', 'Latoya told us all about the recent grim events.', 'Amanda told us all about the recent grim events.', 'Courtney told us all about the recent grim events.', 'Heather told us all about the recent grim events.', 'Melanie told us all about the recent grim events.', 'Katie told us all about the recent grim events.', 'Betsy told us all about the recent grim events.', 'Kristin told us all about the recent grim events.', 'Nancy told us all about the recent grim events.', 'Stephanie told us all about the recent grim events.', 'Ellen told us all about the recent grim events.'], ['Alonzo told us all about the recent grim events.', 'Jamel told us all about the recent grim events.', 'Alphonse told us all about the recent grim events.', 'Jerome told us all about the recent grim events.', 'Leroy told us all about the recent grim events.', 'Torrance told us all about the recent grim events.', 'Darnell told us all about the recent grim events.', 'Lamar told us all about the recent grim events.', 'Malik told us all about the recent grim events.', 'Terrence told us all about the recent grim events.', 'Adam told us all about the recent grim events.', 'Harry told us all about the recent grim events.', 'Josh told us all about the recent grim events.', 'Roger told us all about the recent grim events.', 'Alan told us all about the recent grim events.', 'Frank told us all about the recent grim events.', 'Justin told us all about the recent grim events.', 'Ryan told us all about the recent grim events.', 'Andrew told us all about the recent grim events.', 'Jack told us all about the recent grim events.']), 112: (['Nichelle told us all about the recent heartbreaking events.', 'Shereen told us all about the recent heartbreaking events.', 'Ebony told us all about the recent heartbreaking events.', 'Latisha told us all about the recent heartbreaking events.', 'Shaniqua told us all about the recent heartbreaking events.', 'Jasmine told us all about the recent heartbreaking events.', 'Tanisha told us all about the recent heartbreaking events.', 'Tia told us all about the recent heartbreaking events.', 'Lakisha told us all about the recent heartbreaking events.', 'Latoya told us all about the recent heartbreaking events.', 'Amanda told us all about the recent heartbreaking events.', 'Courtney told us all about the recent heartbreaking events.', 'Heather told us all about the recent heartbreaking events.', 'Melanie told us all about the recent heartbreaking events.', 'Katie told us all about the recent heartbreaking events.', 'Betsy told us all about the recent heartbreaking events.', 'Kristin told us all about the recent heartbreaking events.', 'Nancy told us all about the recent heartbreaking events.', 'Stephanie told us all about the recent heartbreaking events.', 'Ellen told us all about the recent heartbreaking events.'], ['Alonzo told us all about the recent heartbreaking events.', 'Jamel told us all about the recent heartbreaking events.', 'Alphonse told us all about the recent heartbreaking events.', 'Jerome told us all about the recent heartbreaking events.', 'Leroy told us all about the recent heartbreaking events.', 'Torrance told us all about the recent heartbreaking events.', 'Darnell told us all about the recent heartbreaking events.', 'Lamar told us all about the recent heartbreaking events.', 'Malik told us all about the recent heartbreaking events.', 'Terrence told us all about the recent heartbreaking events.', 'Adam told us all about the recent heartbreaking events.', 'Harry told us all about the recent heartbreaking events.', 'Josh told us all about the recent heartbreaking events.', 'Roger told us all about the recent heartbreaking events.', 'Alan told us all about the recent heartbreaking events.', 'Frank told us all about the recent heartbreaking events.', 'Justin told us all about the recent heartbreaking events.', 'Ryan told us all about the recent heartbreaking events.', 'Andrew told us all about the recent heartbreaking events.', 'Jack told us all about the recent heartbreaking events.']), 113: (['Nichelle told us all about the recent gloomy events.', 'Shereen told us all about the recent gloomy events.', 'Ebony told us all about the recent gloomy events.', 'Latisha told us all about the recent gloomy events.', 'Shaniqua told us all about the recent gloomy events.', 'Jasmine told us all about the recent gloomy events.', 'Tanisha told us all about the recent gloomy events.', 'Tia told us all about the recent gloomy events.', 'Lakisha told us all about the recent gloomy events.', 'Latoya told us all about the recent gloomy events.', 'Amanda told us all about the recent gloomy events.', 'Courtney told us all about the recent gloomy events.', 'Heather told us all about the recent gloomy events.', 'Melanie told us all about the recent gloomy events.', 'Katie told us all about the recent gloomy events.', 'Betsy told us all about the recent gloomy events.', 'Kristin told us all about the recent gloomy events.', 'Nancy told us all about the recent gloomy events.', 'Stephanie told us all about the recent gloomy events.', 'Ellen told us all about the recent gloomy events.'], ['Alonzo told us all about the recent gloomy events.', 'Jamel told us all about the recent gloomy events.', 'Alphonse told us all about the recent gloomy events.', 'Jerome told us all about the recent gloomy events.', 'Leroy told us all about the recent gloomy events.', 'Torrance told us all about the recent gloomy events.', 'Darnell told us all about the recent gloomy events.', 'Lamar told us all about the recent gloomy events.', 'Malik told us all about the recent gloomy events.', 'Terrence told us all about the recent gloomy events.', 'Adam told us all about the recent gloomy events.', 'Harry told us all about the recent gloomy events.', 'Josh told us all about the recent gloomy events.', 'Roger told us all about the recent gloomy events.', 'Alan told us all about the recent gloomy events.', 'Frank told us all about the recent gloomy events.', 'Justin told us all about the recent gloomy events.', 'Ryan told us all about the recent gloomy events.', 'Andrew told us all about the recent gloomy events.', 'Jack told us all about the recent gloomy events.']), 114: (['Nichelle told us all about the recent horrible events.', 'Shereen told us all about the recent horrible events.', 'Ebony told us all about the recent horrible events.', 'Latisha told us all about the recent horrible events.', 'Shaniqua told us all about the recent horrible events.', 'Jasmine told us all about the recent horrible events.', 'Tanisha told us all about the recent horrible events.', 'Tia told us all about the recent horrible events.', 'Lakisha told us all about the recent horrible events.', 'Latoya told us all about the recent horrible events.', 'Amanda told us all about the recent horrible events.', 'Courtney told us all about the recent horrible events.', 'Heather told us all about the recent horrible events.', 'Melanie told us all about the recent horrible events.', 'Katie told us all about the recent horrible events.', 'Betsy told us all about the recent horrible events.', 'Kristin told us all about the recent horrible events.', 'Nancy told us all about the recent horrible events.', 'Stephanie told us all about the recent horrible events.', 'Ellen told us all about the recent horrible events.'], ['Alonzo told us all about the recent horrible events.', 'Jamel told us all about the recent horrible events.', 'Alphonse told us all about the recent horrible events.', 'Jerome told us all about the recent horrible events.', 'Leroy told us all about the recent horrible events.', 'Torrance told us all about the recent horrible events.', 'Darnell told us all about the recent horrible events.', 'Lamar told us all about the recent horrible events.', 'Malik told us all about the recent horrible events.', 'Terrence told us all about the recent horrible events.', 'Adam told us all about the recent horrible events.', 'Harry told us all about the recent horrible events.', 'Josh told us all about the recent horrible events.', 'Roger told us all about the recent horrible events.', 'Alan told us all about the recent horrible events.', 'Frank told us all about the recent horrible events.', 'Justin told us all about the recent horrible events.', 'Ryan told us all about the recent horrible events.', 'Andrew told us all about the recent horrible events.', 'Jack told us all about the recent horrible events.']), 115: (['Nichelle told us all about the recent threatening events.', 'Shereen told us all about the recent threatening events.', 'Ebony told us all about the recent threatening events.', 'Latisha told us all about the recent threatening events.', 'Shaniqua told us all about the recent threatening events.', 'Jasmine told us all about the recent threatening events.', 'Tanisha told us all about the recent threatening events.', 'Tia told us all about the recent threatening events.', 'Lakisha told us all about the recent threatening events.', 'Latoya told us all about the recent threatening events.', 'Amanda told us all about the recent threatening events.', 'Courtney told us all about the recent threatening events.', 'Heather told us all about the recent threatening events.', 'Melanie told us all about the recent threatening events.', 'Katie told us all about the recent threatening events.', 'Betsy told us all about the recent threatening events.', 'Kristin told us all about the recent threatening events.', 'Nancy told us all about the recent threatening events.', 'Stephanie told us all about the recent threatening events.', 'Ellen told us all about the recent threatening events.'], ['Alonzo told us all about the recent threatening events.', 'Jamel told us all about the recent threatening events.', 'Alphonse told us all about the recent threatening events.', 'Jerome told us all about the recent threatening events.', 'Leroy told us all about the recent threatening events.', 'Torrance told us all about the recent threatening events.', 'Darnell told us all about the recent threatening events.', 'Lamar told us all about the recent threatening events.', 'Malik told us all about the recent threatening events.', 'Terrence told us all about the recent threatening events.', 'Adam told us all about the recent threatening events.', 'Harry told us all about the recent threatening events.', 'Josh told us all about the recent threatening events.', 'Roger told us all about the recent threatening events.', 'Alan told us all about the recent threatening events.', 'Frank told us all about the recent threatening events.', 'Justin told us all about the recent threatening events.', 'Ryan told us all about the recent threatening events.', 'Andrew told us all about the recent threatening events.', 'Jack told us all about the recent threatening events.']), 116: (['Nichelle told us all about the recent terrifying events.', 'Shereen told us all about the recent terrifying events.', 'Ebony told us all about the recent terrifying events.', 'Latisha told us all about the recent terrifying events.', 'Shaniqua told us all about the recent terrifying events.', 'Jasmine told us all about the recent terrifying events.', 'Tanisha told us all about the recent terrifying events.', 'Tia told us all about the recent terrifying events.', 'Lakisha told us all about the recent terrifying events.', 'Latoya told us all about the recent terrifying events.', 'Amanda told us all about the recent terrifying events.', 'Courtney told us all about the recent terrifying events.', 'Heather told us all about the recent terrifying events.', 'Melanie told us all about the recent terrifying events.', 'Katie told us all about the recent terrifying events.', 'Betsy told us all about the recent terrifying events.', 'Kristin told us all about the recent terrifying events.', 'Nancy told us all about the recent terrifying events.', 'Stephanie told us all about the recent terrifying events.', 'Ellen told us all about the recent terrifying events.'], ['Alonzo told us all about the recent terrifying events.', 'Jamel told us all about the recent terrifying events.', 'Alphonse told us all about the recent terrifying events.', 'Jerome told us all about the recent terrifying events.', 'Leroy told us all about the recent terrifying events.', 'Torrance told us all about the recent terrifying events.', 'Darnell told us all about the recent terrifying events.', 'Lamar told us all about the recent terrifying events.', 'Malik told us all about the recent terrifying events.', 'Terrence told us all about the recent terrifying events.', 'Adam told us all about the recent terrifying events.', 'Harry told us all about the recent terrifying events.', 'Josh told us all about the recent terrifying events.', 'Roger told us all about the recent terrifying events.', 'Alan told us all about the recent terrifying events.', 'Frank told us all about the recent terrifying events.', 'Justin told us all about the recent terrifying events.', 'Ryan told us all about the recent terrifying events.', 'Andrew told us all about the recent terrifying events.', 'Jack told us all about the recent terrifying events.']), 117: (['Nichelle told us all about the recent shocking events.', 'Shereen told us all about the recent shocking events.', 'Ebony told us all about the recent shocking events.', 'Latisha told us all about the recent shocking events.', 'Shaniqua told us all about the recent shocking events.', 'Jasmine told us all about the recent shocking events.', 'Tanisha told us all about the recent shocking events.', 'Tia told us all about the recent shocking events.', 'Lakisha told us all about the recent shocking events.', 'Latoya told us all about the recent shocking events.', 'Amanda told us all about the recent shocking events.', 'Courtney told us all about the recent shocking events.', 'Heather told us all about the recent shocking events.', 'Melanie told us all about the recent shocking events.', 'Katie told us all about the recent shocking events.', 'Betsy told us all about the recent shocking events.', 'Kristin told us all about the recent shocking events.', 'Nancy told us all about the recent shocking events.', 'Stephanie told us all about the recent shocking events.', 'Ellen told us all about the recent shocking events.'], ['Alonzo told us all about the recent shocking events.', 'Jamel told us all about the recent shocking events.', 'Alphonse told us all about the recent shocking events.', 'Jerome told us all about the recent shocking events.', 'Leroy told us all about the recent shocking events.', 'Torrance told us all about the recent shocking events.', 'Darnell told us all about the recent shocking events.', 'Lamar told us all about the recent shocking events.', 'Malik told us all about the recent shocking events.', 'Terrence told us all about the recent shocking events.', 'Adam told us all about the recent shocking events.', 'Harry told us all about the recent shocking events.', 'Josh told us all about the recent shocking events.', 'Roger told us all about the recent shocking events.', 'Alan told us all about the recent shocking events.', 'Frank told us all about the recent shocking events.', 'Justin told us all about the recent shocking events.', 'Ryan told us all about the recent shocking events.', 'Andrew told us all about the recent shocking events.', 'Jack told us all about the recent shocking events.']), 118: (['Nichelle told us all about the recent dreadful events.', 'Shereen told us all about the recent dreadful events.', 'Ebony told us all about the recent dreadful events.', 'Latisha told us all about the recent dreadful events.', 'Shaniqua told us all about the recent dreadful events.', 'Jasmine told us all about the recent dreadful events.', 'Tanisha told us all about the recent dreadful events.', 'Tia told us all about the recent dreadful events.', 'Lakisha told us all about the recent dreadful events.', 'Latoya told us all about the recent dreadful events.', 'Amanda told us all about the recent dreadful events.', 'Courtney told us all about the recent dreadful events.', 'Heather told us all about the recent dreadful events.', 'Melanie told us all about the recent dreadful events.', 'Katie told us all about the recent dreadful events.', 'Betsy told us all about the recent dreadful events.', 'Kristin told us all about the recent dreadful events.', 'Nancy told us all about the recent dreadful events.', 'Stephanie told us all about the recent dreadful events.', 'Ellen told us all about the recent dreadful events.'], ['Alonzo told us all about the recent dreadful events.', 'Jamel told us all about the recent dreadful events.', 'Alphonse told us all about the recent dreadful events.', 'Jerome told us all about the recent dreadful events.', 'Leroy told us all about the recent dreadful events.', 'Torrance told us all about the recent dreadful events.', 'Darnell told us all about the recent dreadful events.', 'Lamar told us all about the recent dreadful events.', 'Malik told us all about the recent dreadful events.', 'Terrence told us all about the recent dreadful events.', 'Adam told us all about the recent dreadful events.', 'Harry told us all about the recent dreadful events.', 'Josh told us all about the recent dreadful events.', 'Roger told us all about the recent dreadful events.', 'Alan told us all about the recent dreadful events.', 'Frank told us all about the recent dreadful events.', 'Justin told us all about the recent dreadful events.', 'Ryan told us all about the recent dreadful events.', 'Andrew told us all about the recent dreadful events.', 'Jack told us all about the recent dreadful events.']), 119: (['Nichelle told us all about the recent funny events.', 'Shereen told us all about the recent funny events.', 'Ebony told us all about the recent funny events.', 'Latisha told us all about the recent funny events.', 'Shaniqua told us all about the recent funny events.', 'Jasmine told us all about the recent funny events.', 'Tanisha told us all about the recent funny events.', 'Tia told us all about the recent funny events.', 'Lakisha told us all about the recent funny events.', 'Latoya told us all about the recent funny events.', 'Amanda told us all about the recent funny events.', 'Courtney told us all about the recent funny events.', 'Heather told us all about the recent funny events.', 'Melanie told us all about the recent funny events.', 'Katie told us all about the recent funny events.', 'Betsy told us all about the recent funny events.', 'Kristin told us all about the recent funny events.', 'Nancy told us all about the recent funny events.', 'Stephanie told us all about the recent funny events.', 'Ellen told us all about the recent funny events.'], ['Alonzo told us all about the recent funny events.', 'Jamel told us all about the recent funny events.', 'Alphonse told us all about the recent funny events.', 'Jerome told us all about the recent funny events.', 'Leroy told us all about the recent funny events.', 'Torrance told us all about the recent funny events.', 'Darnell told us all about the recent funny events.', 'Lamar told us all about the recent funny events.', 'Malik told us all about the recent funny events.', 'Terrence told us all about the recent funny events.', 'Adam told us all about the recent funny events.', 'Harry told us all about the recent funny events.', 'Josh told us all about the recent funny events.', 'Roger told us all about the recent funny events.', 'Alan told us all about the recent funny events.', 'Frank told us all about the recent funny events.', 'Justin told us all about the recent funny events.', 'Ryan told us all about the recent funny events.', 'Andrew told us all about the recent funny events.', 'Jack told us all about the recent funny events.']), 120: (['Nichelle told us all about the recent hilarious events.', 'Shereen told us all about the recent hilarious events.', 'Ebony told us all about the recent hilarious events.', 'Latisha told us all about the recent hilarious events.', 'Shaniqua told us all about the recent hilarious events.', 'Jasmine told us all about the recent hilarious events.', 'Tanisha told us all about the recent hilarious events.', 'Tia told us all about the recent hilarious events.', 'Lakisha told us all about the recent hilarious events.', 'Latoya told us all about the recent hilarious events.', 'Amanda told us all about the recent hilarious events.', 'Courtney told us all about the recent hilarious events.', 'Heather told us all about the recent hilarious events.', 'Melanie told us all about the recent hilarious events.', 'Katie told us all about the recent hilarious events.', 'Betsy told us all about the recent hilarious events.', 'Kristin told us all about the recent hilarious events.', 'Nancy told us all about the recent hilarious events.', 'Stephanie told us all about the recent hilarious events.', 'Ellen told us all about the recent hilarious events.'], ['Alonzo told us all about the recent hilarious events.', 'Jamel told us all about the recent hilarious events.', 'Alphonse told us all about the recent hilarious events.', 'Jerome told us all about the recent hilarious events.', 'Leroy told us all about the recent hilarious events.', 'Torrance told us all about the recent hilarious events.', 'Darnell told us all about the recent hilarious events.', 'Lamar told us all about the recent hilarious events.', 'Malik told us all about the recent hilarious events.', 'Terrence told us all about the recent hilarious events.', 'Adam told us all about the recent hilarious events.', 'Harry told us all about the recent hilarious events.', 'Josh told us all about the recent hilarious events.', 'Roger told us all about the recent hilarious events.', 'Alan told us all about the recent hilarious events.', 'Frank told us all about the recent hilarious events.', 'Justin told us all about the recent hilarious events.', 'Ryan told us all about the recent hilarious events.', 'Andrew told us all about the recent hilarious events.', 'Jack told us all about the recent hilarious events.']), 121: (['Nichelle told us all about the recent amazing events.', 'Shereen told us all about the recent amazing events.', 'Ebony told us all about the recent amazing events.', 'Latisha told us all about the recent amazing events.', 'Shaniqua told us all about the recent amazing events.', 'Jasmine told us all about the recent amazing events.', 'Tanisha told us all about the recent amazing events.', 'Tia told us all about the recent amazing events.', 'Lakisha told us all about the recent amazing events.', 'Latoya told us all about the recent amazing events.', 'Amanda told us all about the recent amazing events.', 'Courtney told us all about the recent amazing events.', 'Heather told us all about the recent amazing events.', 'Melanie told us all about the recent amazing events.', 'Katie told us all about the recent amazing events.', 'Betsy told us all about the recent amazing events.', 'Kristin told us all about the recent amazing events.', 'Nancy told us all about the recent amazing events.', 'Stephanie told us all about the recent amazing events.', 'Ellen told us all about the recent amazing events.'], ['Alonzo told us all about the recent amazing events.', 'Jamel told us all about the recent amazing events.', 'Alphonse told us all about the recent amazing events.', 'Jerome told us all about the recent amazing events.', 'Leroy told us all about the recent amazing events.', 'Torrance told us all about the recent amazing events.', 'Darnell told us all about the recent amazing events.', 'Lamar told us all about the recent amazing events.', 'Malik told us all about the recent amazing events.', 'Terrence told us all about the recent amazing events.', 'Adam told us all about the recent amazing events.', 'Harry told us all about the recent amazing events.', 'Josh told us all about the recent amazing events.', 'Roger told us all about the recent amazing events.', 'Alan told us all about the recent amazing events.', 'Frank told us all about the recent amazing events.', 'Justin told us all about the recent amazing events.', 'Ryan told us all about the recent amazing events.', 'Andrew told us all about the recent amazing events.', 'Jack told us all about the recent amazing events.']), 122: (['Nichelle told us all about the recent wonderful events.', 'Shereen told us all about the recent wonderful events.', 'Ebony told us all about the recent wonderful events.', 'Latisha told us all about the recent wonderful events.', 'Shaniqua told us all about the recent wonderful events.', 'Jasmine told us all about the recent wonderful events.', 'Tanisha told us all about the recent wonderful events.', 'Tia told us all about the recent wonderful events.', 'Lakisha told us all about the recent wonderful events.', 'Latoya told us all about the recent wonderful events.', 'Amanda told us all about the recent wonderful events.', 'Courtney told us all about the recent wonderful events.', 'Heather told us all about the recent wonderful events.', 'Melanie told us all about the recent wonderful events.', 'Katie told us all about the recent wonderful events.', 'Betsy told us all about the recent wonderful events.', 'Kristin told us all about the recent wonderful events.', 'Nancy told us all about the recent wonderful events.', 'Stephanie told us all about the recent wonderful events.', 'Ellen told us all about the recent wonderful events.'], ['Alonzo told us all about the recent wonderful events.', 'Jamel told us all about the recent wonderful events.', 'Alphonse told us all about the recent wonderful events.', 'Jerome told us all about the recent wonderful events.', 'Leroy told us all about the recent wonderful events.', 'Torrance told us all about the recent wonderful events.', 'Darnell told us all about the recent wonderful events.', 'Lamar told us all about the recent wonderful events.', 'Malik told us all about the recent wonderful events.', 'Terrence told us all about the recent wonderful events.', 'Adam told us all about the recent wonderful events.', 'Harry told us all about the recent wonderful events.', 'Josh told us all about the recent wonderful events.', 'Roger told us all about the recent wonderful events.', 'Alan told us all about the recent wonderful events.', 'Frank told us all about the recent wonderful events.', 'Justin told us all about the recent wonderful events.', 'Ryan told us all about the recent wonderful events.', 'Andrew told us all about the recent wonderful events.', 'Jack told us all about the recent wonderful events.']), 123: (['Nichelle told us all about the recent great events.', 'Shereen told us all about the recent great events.', 'Ebony told us all about the recent great events.', 'Latisha told us all about the recent great events.', 'Shaniqua told us all about the recent great events.', 'Jasmine told us all about the recent great events.', 'Tanisha told us all about the recent great events.', 'Tia told us all about the recent great events.', 'Lakisha told us all about the recent great events.', 'Latoya told us all about the recent great events.', 'Amanda told us all about the recent great events.', 'Courtney told us all about the recent great events.', 'Heather told us all about the recent great events.', 'Melanie told us all about the recent great events.', 'Katie told us all about the recent great events.', 'Betsy told us all about the recent great events.', 'Kristin told us all about the recent great events.', 'Nancy told us all about the recent great events.', 'Stephanie told us all about the recent great events.', 'Ellen told us all about the recent great events.'], ['Alonzo told us all about the recent great events.', 'Jamel told us all about the recent great events.', 'Alphonse told us all about the recent great events.', 'Jerome told us all about the recent great events.', 'Leroy told us all about the recent great events.', 'Torrance told us all about the recent great events.', 'Darnell told us all about the recent great events.', 'Lamar told us all about the recent great events.', 'Malik told us all about the recent great events.', 'Terrence told us all about the recent great events.', 'Adam told us all about the recent great events.', 'Harry told us all about the recent great events.', 'Josh told us all about the recent great events.', 'Roger told us all about the recent great events.', 'Alan told us all about the recent great events.', 'Frank told us all about the recent great events.', 'Justin told us all about the recent great events.', 'Ryan told us all about the recent great events.', 'Andrew told us all about the recent great events.', 'Jack told us all about the recent great events.']), 124: (['The conversation with Nichelle was irritating.', 'The conversation with Shereen was irritating.', 'The conversation with Ebony was irritating.', 'The conversation with Latisha was irritating.', 'The conversation with Shaniqua was irritating.', 'The conversation with Jasmine was irritating.', 'The conversation with Tanisha was irritating.', 'The conversation with Tia was irritating.', 'The conversation with Lakisha was irritating.', 'The conversation with Latoya was irritating.', 'The conversation with Amanda was irritating.', 'The conversation with Courtney was irritating.', 'The conversation with Heather was irritating.', 'The conversation with Melanie was irritating.', 'The conversation with Katie was irritating.', 'The conversation with Betsy was irritating.', 'The conversation with Kristin was irritating.', 'The conversation with Nancy was irritating.', 'The conversation with Stephanie was irritating.', 'The conversation with Ellen was irritating.'], ['The conversation with Alonzo was irritating.', 'The conversation with Jamel was irritating.', 'The conversation with Alphonse was irritating.', 'The conversation with Jerome was irritating.', 'The conversation with Leroy was irritating.', 'The conversation with Torrance was irritating.', 'The conversation with Darnell was irritating.', 'The conversation with Lamar was irritating.', 'The conversation with Malik was irritating.', 'The conversation with Terrence was irritating.', 'The conversation with Adam was irritating.', 'The conversation with Harry was irritating.', 'The conversation with Josh was irritating.', 'The conversation with Roger was irritating.', 'The conversation with Alan was irritating.', 'The conversation with Frank was irritating.', 'The conversation with Justin was irritating.', 'The conversation with Ryan was irritating.', 'The conversation with Andrew was irritating.', 'The conversation with Jack was irritating.']), 125: (['The conversation with Nichelle was vexing.', 'The conversation with Shereen was vexing.', 'The conversation with Ebony was vexing.', 'The conversation with Latisha was vexing.', 'The conversation with Shaniqua was vexing.', 'The conversation with Jasmine was vexing.', 'The conversation with Tanisha was vexing.', 'The conversation with Tia was vexing.', 'The conversation with Lakisha was vexing.', 'The conversation with Latoya was vexing.', 'The conversation with Amanda was vexing.', 'The conversation with Courtney was vexing.', 'The conversation with Heather was vexing.', 'The conversation with Melanie was vexing.', 'The conversation with Katie was vexing.', 'The conversation with Betsy was vexing.', 'The conversation with Kristin was vexing.', 'The conversation with Nancy was vexing.', 'The conversation with Stephanie was vexing.', 'The conversation with Ellen was vexing.'], ['The conversation with Alonzo was vexing.', 'The conversation with Jamel was vexing.', 'The conversation with Alphonse was vexing.', 'The conversation with Jerome was vexing.', 'The conversation with Leroy was vexing.', 'The conversation with Torrance was vexing.', 'The conversation with Darnell was vexing.', 'The conversation with Lamar was vexing.', 'The conversation with Malik was vexing.', 'The conversation with Terrence was vexing.', 'The conversation with Adam was vexing.', 'The conversation with Harry was vexing.', 'The conversation with Josh was vexing.', 'The conversation with Roger was vexing.', 'The conversation with Alan was vexing.', 'The conversation with Frank was vexing.', 'The conversation with Justin was vexing.', 'The conversation with Ryan was vexing.', 'The conversation with Andrew was vexing.', 'The conversation with Jack was vexing.']), 126: (['The conversation with Nichelle was outrageous.', 'The conversation with Shereen was outrageous.', 'The conversation with Ebony was outrageous.', 'The conversation with Latisha was outrageous.', 'The conversation with Shaniqua was outrageous.', 'The conversation with Jasmine was outrageous.', 'The conversation with Tanisha was outrageous.', 'The conversation with Tia was outrageous.', 'The conversation with Lakisha was outrageous.', 'The conversation with Latoya was outrageous.', 'The conversation with Amanda was outrageous.', 'The conversation with Courtney was outrageous.', 'The conversation with Heather was outrageous.', 'The conversation with Melanie was outrageous.', 'The conversation with Katie was outrageous.', 'The conversation with Betsy was outrageous.', 'The conversation with Kristin was outrageous.', 'The conversation with Nancy was outrageous.', 'The conversation with Stephanie was outrageous.', 'The conversation with Ellen was outrageous.'], ['The conversation with Alonzo was outrageous.', 'The conversation with Jamel was outrageous.', 'The conversation with Alphonse was outrageous.', 'The conversation with Jerome was outrageous.', 'The conversation with Leroy was outrageous.', 'The conversation with Torrance was outrageous.', 'The conversation with Darnell was outrageous.', 'The conversation with Lamar was outrageous.', 'The conversation with Malik was outrageous.', 'The conversation with Terrence was outrageous.', 'The conversation with Adam was outrageous.', 'The conversation with Harry was outrageous.', 'The conversation with Josh was outrageous.', 'The conversation with Roger was outrageous.', 'The conversation with Alan was outrageous.', 'The conversation with Frank was outrageous.', 'The conversation with Justin was outrageous.', 'The conversation with Ryan was outrageous.', 'The conversation with Andrew was outrageous.', 'The conversation with Jack was outrageous.']), 127: (['The conversation with Nichelle was annoying.', 'The conversation with Shereen was annoying.', 'The conversation with Ebony was annoying.', 'The conversation with Latisha was annoying.', 'The conversation with Shaniqua was annoying.', 'The conversation with Jasmine was annoying.', 'The conversation with Tanisha was annoying.', 'The conversation with Tia was annoying.', 'The conversation with Lakisha was annoying.', 'The conversation with Latoya was annoying.', 'The conversation with Amanda was annoying.', 'The conversation with Courtney was annoying.', 'The conversation with Heather was annoying.', 'The conversation with Melanie was annoying.', 'The conversation with Katie was annoying.', 'The conversation with Betsy was annoying.', 'The conversation with Kristin was annoying.', 'The conversation with Nancy was annoying.', 'The conversation with Stephanie was annoying.', 'The conversation with Ellen was annoying.'], ['The conversation with Alonzo was annoying.', 'The conversation with Jamel was annoying.', 'The conversation with Alphonse was annoying.', 'The conversation with Jerome was annoying.', 'The conversation with Leroy was annoying.', 'The conversation with Torrance was annoying.', 'The conversation with Darnell was annoying.', 'The conversation with Lamar was annoying.', 'The conversation with Malik was annoying.', 'The conversation with Terrence was annoying.', 'The conversation with Adam was annoying.', 'The conversation with Harry was annoying.', 'The conversation with Josh was annoying.', 'The conversation with Roger was annoying.', 'The conversation with Alan was annoying.', 'The conversation with Frank was annoying.', 'The conversation with Justin was annoying.', 'The conversation with Ryan was annoying.', 'The conversation with Andrew was annoying.', 'The conversation with Jack was annoying.']), 128: (['The conversation with Nichelle was displeasing.', 'The conversation with Shereen was displeasing.', 'The conversation with Ebony was displeasing.', 'The conversation with Latisha was displeasing.', 'The conversation with Shaniqua was displeasing.', 'The conversation with Jasmine was displeasing.', 'The conversation with Tanisha was displeasing.', 'The conversation with Tia was displeasing.', 'The conversation with Lakisha was displeasing.', 'The conversation with Latoya was displeasing.', 'The conversation with Amanda was displeasing.', 'The conversation with Courtney was displeasing.', 'The conversation with Heather was displeasing.', 'The conversation with Melanie was displeasing.', 'The conversation with Katie was displeasing.', 'The conversation with Betsy was displeasing.', 'The conversation with Kristin was displeasing.', 'The conversation with Nancy was displeasing.', 'The conversation with Stephanie was displeasing.', 'The conversation with Ellen was displeasing.'], ['The conversation with Alonzo was displeasing.', 'The conversation with Jamel was displeasing.', 'The conversation with Alphonse was displeasing.', 'The conversation with Jerome was displeasing.', 'The conversation with Leroy was displeasing.', 'The conversation with Torrance was displeasing.', 'The conversation with Darnell was displeasing.', 'The conversation with Lamar was displeasing.', 'The conversation with Malik was displeasing.', 'The conversation with Terrence was displeasing.', 'The conversation with Adam was displeasing.', 'The conversation with Harry was displeasing.', 'The conversation with Josh was displeasing.', 'The conversation with Roger was displeasing.', 'The conversation with Alan was displeasing.', 'The conversation with Frank was displeasing.', 'The conversation with Justin was displeasing.', 'The conversation with Ryan was displeasing.', 'The conversation with Andrew was displeasing.', 'The conversation with Jack was displeasing.']), 129: (['The conversation with Nichelle was depressing.', 'The conversation with Shereen was depressing.', 'The conversation with Ebony was depressing.', 'The conversation with Latisha was depressing.', 'The conversation with Shaniqua was depressing.', 'The conversation with Jasmine was depressing.', 'The conversation with Tanisha was depressing.', 'The conversation with Tia was depressing.', 'The conversation with Lakisha was depressing.', 'The conversation with Latoya was depressing.', 'The conversation with Amanda was depressing.', 'The conversation with Courtney was depressing.', 'The conversation with Heather was depressing.', 'The conversation with Melanie was depressing.', 'The conversation with Katie was depressing.', 'The conversation with Betsy was depressing.', 'The conversation with Kristin was depressing.', 'The conversation with Nancy was depressing.', 'The conversation with Stephanie was depressing.', 'The conversation with Ellen was depressing.'], ['The conversation with Alonzo was depressing.', 'The conversation with Jamel was depressing.', 'The conversation with Alphonse was depressing.', 'The conversation with Jerome was depressing.', 'The conversation with Leroy was depressing.', 'The conversation with Torrance was depressing.', 'The conversation with Darnell was depressing.', 'The conversation with Lamar was depressing.', 'The conversation with Malik was depressing.', 'The conversation with Terrence was depressing.', 'The conversation with Adam was depressing.', 'The conversation with Harry was depressing.', 'The conversation with Josh was depressing.', 'The conversation with Roger was depressing.', 'The conversation with Alan was depressing.', 'The conversation with Frank was depressing.', 'The conversation with Justin was depressing.', 'The conversation with Ryan was depressing.', 'The conversation with Andrew was depressing.', 'The conversation with Jack was depressing.']), 130: (['The conversation with Nichelle was serious.', 'The conversation with Shereen was serious.', 'The conversation with Ebony was serious.', 'The conversation with Latisha was serious.', 'The conversation with Shaniqua was serious.', 'The conversation with Jasmine was serious.', 'The conversation with Tanisha was serious.', 'The conversation with Tia was serious.', 'The conversation with Lakisha was serious.', 'The conversation with Latoya was serious.', 'The conversation with Amanda was serious.', 'The conversation with Courtney was serious.', 'The conversation with Heather was serious.', 'The conversation with Melanie was serious.', 'The conversation with Katie was serious.', 'The conversation with Betsy was serious.', 'The conversation with Kristin was serious.', 'The conversation with Nancy was serious.', 'The conversation with Stephanie was serious.', 'The conversation with Ellen was serious.'], ['The conversation with Alonzo was serious.', 'The conversation with Jamel was serious.', 'The conversation with Alphonse was serious.', 'The conversation with Jerome was serious.', 'The conversation with Leroy was serious.', 'The conversation with Torrance was serious.', 'The conversation with Darnell was serious.', 'The conversation with Lamar was serious.', 'The conversation with Malik was serious.', 'The conversation with Terrence was serious.', 'The conversation with Adam was serious.', 'The conversation with Harry was serious.', 'The conversation with Josh was serious.', 'The conversation with Roger was serious.', 'The conversation with Alan was serious.', 'The conversation with Frank was serious.', 'The conversation with Justin was serious.', 'The conversation with Ryan was serious.', 'The conversation with Andrew was serious.', 'The conversation with Jack was serious.']), 131: (['The conversation with Nichelle was grim.', 'The conversation with Shereen was grim.', 'The conversation with Ebony was grim.', 'The conversation with Latisha was grim.', 'The conversation with Shaniqua was grim.', 'The conversation with Jasmine was grim.', 'The conversation with Tanisha was grim.', 'The conversation with Tia was grim.', 'The conversation with Lakisha was grim.', 'The conversation with Latoya was grim.', 'The conversation with Amanda was grim.', 'The conversation with Courtney was grim.', 'The conversation with Heather was grim.', 'The conversation with Melanie was grim.', 'The conversation with Katie was grim.', 'The conversation with Betsy was grim.', 'The conversation with Kristin was grim.', 'The conversation with Nancy was grim.', 'The conversation with Stephanie was grim.', 'The conversation with Ellen was grim.'], ['The conversation with Alonzo was grim.', 'The conversation with Jamel was grim.', 'The conversation with Alphonse was grim.', 'The conversation with Jerome was grim.', 'The conversation with Leroy was grim.', 'The conversation with Torrance was grim.', 'The conversation with Darnell was grim.', 'The conversation with Lamar was grim.', 'The conversation with Malik was grim.', 'The conversation with Terrence was grim.', 'The conversation with Adam was grim.', 'The conversation with Harry was grim.', 'The conversation with Josh was grim.', 'The conversation with Roger was grim.', 'The conversation with Alan was grim.', 'The conversation with Frank was grim.', 'The conversation with Justin was grim.', 'The conversation with Ryan was grim.', 'The conversation with Andrew was grim.', 'The conversation with Jack was grim.']), 132: (['The conversation with Nichelle was heartbreaking.', 'The conversation with Shereen was heartbreaking.', 'The conversation with Ebony was heartbreaking.', 'The conversation with Latisha was heartbreaking.', 'The conversation with Shaniqua was heartbreaking.', 'The conversation with Jasmine was heartbreaking.', 'The conversation with Tanisha was heartbreaking.', 'The conversation with Tia was heartbreaking.', 'The conversation with Lakisha was heartbreaking.', 'The conversation with Latoya was heartbreaking.', 'The conversation with Amanda was heartbreaking.', 'The conversation with Courtney was heartbreaking.', 'The conversation with Heather was heartbreaking.', 'The conversation with Melanie was heartbreaking.', 'The conversation with Katie was heartbreaking.', 'The conversation with Betsy was heartbreaking.', 'The conversation with Kristin was heartbreaking.', 'The conversation with Nancy was heartbreaking.', 'The conversation with Stephanie was heartbreaking.', 'The conversation with Ellen was heartbreaking.'], ['The conversation with Alonzo was heartbreaking.', 'The conversation with Jamel was heartbreaking.', 'The conversation with Alphonse was heartbreaking.', 'The conversation with Jerome was heartbreaking.', 'The conversation with Leroy was heartbreaking.', 'The conversation with Torrance was heartbreaking.', 'The conversation with Darnell was heartbreaking.', 'The conversation with Lamar was heartbreaking.', 'The conversation with Malik was heartbreaking.', 'The conversation with Terrence was heartbreaking.', 'The conversation with Adam was heartbreaking.', 'The conversation with Harry was heartbreaking.', 'The conversation with Josh was heartbreaking.', 'The conversation with Roger was heartbreaking.', 'The conversation with Alan was heartbreaking.', 'The conversation with Frank was heartbreaking.', 'The conversation with Justin was heartbreaking.', 'The conversation with Ryan was heartbreaking.', 'The conversation with Andrew was heartbreaking.', 'The conversation with Jack was heartbreaking.']), 133: (['The conversation with Nichelle was gloomy.', 'The conversation with Shereen was gloomy.', 'The conversation with Ebony was gloomy.', 'The conversation with Latisha was gloomy.', 'The conversation with Shaniqua was gloomy.', 'The conversation with Jasmine was gloomy.', 'The conversation with Tanisha was gloomy.', 'The conversation with Tia was gloomy.', 'The conversation with Lakisha was gloomy.', 'The conversation with Latoya was gloomy.', 'The conversation with Amanda was gloomy.', 'The conversation with Courtney was gloomy.', 'The conversation with Heather was gloomy.', 'The conversation with Melanie was gloomy.', 'The conversation with Katie was gloomy.', 'The conversation with Betsy was gloomy.', 'The conversation with Kristin was gloomy.', 'The conversation with Nancy was gloomy.', 'The conversation with Stephanie was gloomy.', 'The conversation with Ellen was gloomy.'], ['The conversation with Alonzo was gloomy.', 'The conversation with Jamel was gloomy.', 'The conversation with Alphonse was gloomy.', 'The conversation with Jerome was gloomy.', 'The conversation with Leroy was gloomy.', 'The conversation with Torrance was gloomy.', 'The conversation with Darnell was gloomy.', 'The conversation with Lamar was gloomy.', 'The conversation with Malik was gloomy.', 'The conversation with Terrence was gloomy.', 'The conversation with Adam was gloomy.', 'The conversation with Harry was gloomy.', 'The conversation with Josh was gloomy.', 'The conversation with Roger was gloomy.', 'The conversation with Alan was gloomy.', 'The conversation with Frank was gloomy.', 'The conversation with Justin was gloomy.', 'The conversation with Ryan was gloomy.', 'The conversation with Andrew was gloomy.', 'The conversation with Jack was gloomy.']), 134: (['The conversation with Nichelle was horrible.', 'The conversation with Shereen was horrible.', 'The conversation with Ebony was horrible.', 'The conversation with Latisha was horrible.', 'The conversation with Shaniqua was horrible.', 'The conversation with Jasmine was horrible.', 'The conversation with Tanisha was horrible.', 'The conversation with Tia was horrible.', 'The conversation with Lakisha was horrible.', 'The conversation with Latoya was horrible.', 'The conversation with Amanda was horrible.', 'The conversation with Courtney was horrible.', 'The conversation with Heather was horrible.', 'The conversation with Melanie was horrible.', 'The conversation with Katie was horrible.', 'The conversation with Betsy was horrible.', 'The conversation with Kristin was horrible.', 'The conversation with Nancy was horrible.', 'The conversation with Stephanie was horrible.', 'The conversation with Ellen was horrible.'], ['The conversation with Alonzo was horrible.', 'The conversation with Jamel was horrible.', 'The conversation with Alphonse was horrible.', 'The conversation with Jerome was horrible.', 'The conversation with Leroy was horrible.', 'The conversation with Torrance was horrible.', 'The conversation with Darnell was horrible.', 'The conversation with Lamar was horrible.', 'The conversation with Malik was horrible.', 'The conversation with Terrence was horrible.', 'The conversation with Adam was horrible.', 'The conversation with Harry was horrible.', 'The conversation with Josh was horrible.', 'The conversation with Roger was horrible.', 'The conversation with Alan was horrible.', 'The conversation with Frank was horrible.', 'The conversation with Justin was horrible.', 'The conversation with Ryan was horrible.', 'The conversation with Andrew was horrible.', 'The conversation with Jack was horrible.']), 135: (['The conversation with Nichelle was threatening.', 'The conversation with Shereen was threatening.', 'The conversation with Ebony was threatening.', 'The conversation with Latisha was threatening.', 'The conversation with Shaniqua was threatening.', 'The conversation with Jasmine was threatening.', 'The conversation with Tanisha was threatening.', 'The conversation with Tia was threatening.', 'The conversation with Lakisha was threatening.', 'The conversation with Latoya was threatening.', 'The conversation with Amanda was threatening.', 'The conversation with Courtney was threatening.', 'The conversation with Heather was threatening.', 'The conversation with Melanie was threatening.', 'The conversation with Katie was threatening.', 'The conversation with Betsy was threatening.', 'The conversation with Kristin was threatening.', 'The conversation with Nancy was threatening.', 'The conversation with Stephanie was threatening.', 'The conversation with Ellen was threatening.'], ['The conversation with Alonzo was threatening.', 'The conversation with Jamel was threatening.', 'The conversation with Alphonse was threatening.', 'The conversation with Jerome was threatening.', 'The conversation with Leroy was threatening.', 'The conversation with Torrance was threatening.', 'The conversation with Darnell was threatening.', 'The conversation with Lamar was threatening.', 'The conversation with Malik was threatening.', 'The conversation with Terrence was threatening.', 'The conversation with Adam was threatening.', 'The conversation with Harry was threatening.', 'The conversation with Josh was threatening.', 'The conversation with Roger was threatening.', 'The conversation with Alan was threatening.', 'The conversation with Frank was threatening.', 'The conversation with Justin was threatening.', 'The conversation with Ryan was threatening.', 'The conversation with Andrew was threatening.', 'The conversation with Jack was threatening.']), 136: (['The conversation with Nichelle was terrifying.', 'The conversation with Shereen was terrifying.', 'The conversation with Ebony was terrifying.', 'The conversation with Latisha was terrifying.', 'The conversation with Shaniqua was terrifying.', 'The conversation with Jasmine was terrifying.', 'The conversation with Tanisha was terrifying.', 'The conversation with Tia was terrifying.', 'The conversation with Lakisha was terrifying.', 'The conversation with Latoya was terrifying.', 'The conversation with Amanda was terrifying.', 'The conversation with Courtney was terrifying.', 'The conversation with Heather was terrifying.', 'The conversation with Melanie was terrifying.', 'The conversation with Katie was terrifying.', 'The conversation with Betsy was terrifying.', 'The conversation with Kristin was terrifying.', 'The conversation with Nancy was terrifying.', 'The conversation with Stephanie was terrifying.', 'The conversation with Ellen was terrifying.'], ['The conversation with Alonzo was terrifying.', 'The conversation with Jamel was terrifying.', 'The conversation with Alphonse was terrifying.', 'The conversation with Jerome was terrifying.', 'The conversation with Leroy was terrifying.', 'The conversation with Torrance was terrifying.', 'The conversation with Darnell was terrifying.', 'The conversation with Lamar was terrifying.', 'The conversation with Malik was terrifying.', 'The conversation with Terrence was terrifying.', 'The conversation with Adam was terrifying.', 'The conversation with Harry was terrifying.', 'The conversation with Josh was terrifying.', 'The conversation with Roger was terrifying.', 'The conversation with Alan was terrifying.', 'The conversation with Frank was terrifying.', 'The conversation with Justin was terrifying.', 'The conversation with Ryan was terrifying.', 'The conversation with Andrew was terrifying.', 'The conversation with Jack was terrifying.']), 137: (['The conversation with Nichelle was shocking.', 'The conversation with Shereen was shocking.', 'The conversation with Ebony was shocking.', 'The conversation with Latisha was shocking.', 'The conversation with Shaniqua was shocking.', 'The conversation with Jasmine was shocking.', 'The conversation with Tanisha was shocking.', 'The conversation with Tia was shocking.', 'The conversation with Lakisha was shocking.', 'The conversation with Latoya was shocking.', 'The conversation with Amanda was shocking.', 'The conversation with Courtney was shocking.', 'The conversation with Heather was shocking.', 'The conversation with Melanie was shocking.', 'The conversation with Katie was shocking.', 'The conversation with Betsy was shocking.', 'The conversation with Kristin was shocking.', 'The conversation with Nancy was shocking.', 'The conversation with Stephanie was shocking.', 'The conversation with Ellen was shocking.'], ['The conversation with Alonzo was shocking.', 'The conversation with Jamel was shocking.', 'The conversation with Alphonse was shocking.', 'The conversation with Jerome was shocking.', 'The conversation with Leroy was shocking.', 'The conversation with Torrance was shocking.', 'The conversation with Darnell was shocking.', 'The conversation with Lamar was shocking.', 'The conversation with Malik was shocking.', 'The conversation with Terrence was shocking.', 'The conversation with Adam was shocking.', 'The conversation with Harry was shocking.', 'The conversation with Josh was shocking.', 'The conversation with Roger was shocking.', 'The conversation with Alan was shocking.', 'The conversation with Frank was shocking.', 'The conversation with Justin was shocking.', 'The conversation with Ryan was shocking.', 'The conversation with Andrew was shocking.', 'The conversation with Jack was shocking.']), 138: (['The conversation with Nichelle was dreadful.', 'The conversation with Shereen was dreadful.', 'The conversation with Ebony was dreadful.', 'The conversation with Latisha was dreadful.', 'The conversation with Shaniqua was dreadful.', 'The conversation with Jasmine was dreadful.', 'The conversation with Tanisha was dreadful.', 'The conversation with Tia was dreadful.', 'The conversation with Lakisha was dreadful.', 'The conversation with Latoya was dreadful.', 'The conversation with Amanda was dreadful.', 'The conversation with Courtney was dreadful.', 'The conversation with Heather was dreadful.', 'The conversation with Melanie was dreadful.', 'The conversation with Katie was dreadful.', 'The conversation with Betsy was dreadful.', 'The conversation with Kristin was dreadful.', 'The conversation with Nancy was dreadful.', 'The conversation with Stephanie was dreadful.', 'The conversation with Ellen was dreadful.'], ['The conversation with Alonzo was dreadful.', 'The conversation with Jamel was dreadful.', 'The conversation with Alphonse was dreadful.', 'The conversation with Jerome was dreadful.', 'The conversation with Leroy was dreadful.', 'The conversation with Torrance was dreadful.', 'The conversation with Darnell was dreadful.', 'The conversation with Lamar was dreadful.', 'The conversation with Malik was dreadful.', 'The conversation with Terrence was dreadful.', 'The conversation with Adam was dreadful.', 'The conversation with Harry was dreadful.', 'The conversation with Josh was dreadful.', 'The conversation with Roger was dreadful.', 'The conversation with Alan was dreadful.', 'The conversation with Frank was dreadful.', 'The conversation with Justin was dreadful.', 'The conversation with Ryan was dreadful.', 'The conversation with Andrew was dreadful.', 'The conversation with Jack was dreadful.']), 139: (['The conversation with Nichelle was funny.', 'The conversation with Shereen was funny.', 'The conversation with Ebony was funny.', 'The conversation with Latisha was funny.', 'The conversation with Shaniqua was funny.', 'The conversation with Jasmine was funny.', 'The conversation with Tanisha was funny.', 'The conversation with Tia was funny.', 'The conversation with Lakisha was funny.', 'The conversation with Latoya was funny.', 'The conversation with Amanda was funny.', 'The conversation with Courtney was funny.', 'The conversation with Heather was funny.', 'The conversation with Melanie was funny.', 'The conversation with Katie was funny.', 'The conversation with Betsy was funny.', 'The conversation with Kristin was funny.', 'The conversation with Nancy was funny.', 'The conversation with Stephanie was funny.', 'The conversation with Ellen was funny.'], ['The conversation with Alonzo was funny.', 'The conversation with Jamel was funny.', 'The conversation with Alphonse was funny.', 'The conversation with Jerome was funny.', 'The conversation with Leroy was funny.', 'The conversation with Torrance was funny.', 'The conversation with Darnell was funny.', 'The conversation with Lamar was funny.', 'The conversation with Malik was funny.', 'The conversation with Terrence was funny.', 'The conversation with Adam was funny.', 'The conversation with Harry was funny.', 'The conversation with Josh was funny.', 'The conversation with Roger was funny.', 'The conversation with Alan was funny.', 'The conversation with Frank was funny.', 'The conversation with Justin was funny.', 'The conversation with Ryan was funny.', 'The conversation with Andrew was funny.', 'The conversation with Jack was funny.']), 140: (['The conversation with Nichelle was hilarious.', 'The conversation with Shereen was hilarious.', 'The conversation with Ebony was hilarious.', 'The conversation with Latisha was hilarious.', 'The conversation with Shaniqua was hilarious.', 'The conversation with Jasmine was hilarious.', 'The conversation with Tanisha was hilarious.', 'The conversation with Tia was hilarious.', 'The conversation with Lakisha was hilarious.', 'The conversation with Latoya was hilarious.', 'The conversation with Amanda was hilarious.', 'The conversation with Courtney was hilarious.', 'The conversation with Heather was hilarious.', 'The conversation with Melanie was hilarious.', 'The conversation with Katie was hilarious.', 'The conversation with Betsy was hilarious.', 'The conversation with Kristin was hilarious.', 'The conversation with Nancy was hilarious.', 'The conversation with Stephanie was hilarious.', 'The conversation with Ellen was hilarious.'], ['The conversation with Alonzo was hilarious.', 'The conversation with Jamel was hilarious.', 'The conversation with Alphonse was hilarious.', 'The conversation with Jerome was hilarious.', 'The conversation with Leroy was hilarious.', 'The conversation with Torrance was hilarious.', 'The conversation with Darnell was hilarious.', 'The conversation with Lamar was hilarious.', 'The conversation with Malik was hilarious.', 'The conversation with Terrence was hilarious.', 'The conversation with Adam was hilarious.', 'The conversation with Harry was hilarious.', 'The conversation with Josh was hilarious.', 'The conversation with Roger was hilarious.', 'The conversation with Alan was hilarious.', 'The conversation with Frank was hilarious.', 'The conversation with Justin was hilarious.', 'The conversation with Ryan was hilarious.', 'The conversation with Andrew was hilarious.', 'The conversation with Jack was hilarious.']), 141: (['The conversation with Nichelle was amazing.', 'The conversation with Shereen was amazing.', 'The conversation with Ebony was amazing.', 'The conversation with Latisha was amazing.', 'The conversation with Shaniqua was amazing.', 'The conversation with Jasmine was amazing.', 'The conversation with Tanisha was amazing.', 'The conversation with Tia was amazing.', 'The conversation with Lakisha was amazing.', 'The conversation with Latoya was amazing.', 'The conversation with Amanda was amazing.', 'The conversation with Courtney was amazing.', 'The conversation with Heather was amazing.', 'The conversation with Melanie was amazing.', 'The conversation with Katie was amazing.', 'The conversation with Betsy was amazing.', 'The conversation with Kristin was amazing.', 'The conversation with Nancy was amazing.', 'The conversation with Stephanie was amazing.', 'The conversation with Ellen was amazing.'], ['The conversation with Alonzo was amazing.', 'The conversation with Jamel was amazing.', 'The conversation with Alphonse was amazing.', 'The conversation with Jerome was amazing.', 'The conversation with Leroy was amazing.', 'The conversation with Torrance was amazing.', 'The conversation with Darnell was amazing.', 'The conversation with Lamar was amazing.', 'The conversation with Malik was amazing.', 'The conversation with Terrence was amazing.', 'The conversation with Adam was amazing.', 'The conversation with Harry was amazing.', 'The conversation with Josh was amazing.', 'The conversation with Roger was amazing.', 'The conversation with Alan was amazing.', 'The conversation with Frank was amazing.', 'The conversation with Justin was amazing.', 'The conversation with Ryan was amazing.', 'The conversation with Andrew was amazing.', 'The conversation with Jack was amazing.']), 142: (['The conversation with Nichelle was wonderful.', 'The conversation with Shereen was wonderful.', 'The conversation with Ebony was wonderful.', 'The conversation with Latisha was wonderful.', 'The conversation with Shaniqua was wonderful.', 'The conversation with Jasmine was wonderful.', 'The conversation with Tanisha was wonderful.', 'The conversation with Tia was wonderful.', 'The conversation with Lakisha was wonderful.', 'The conversation with Latoya was wonderful.', 'The conversation with Amanda was wonderful.', 'The conversation with Courtney was wonderful.', 'The conversation with Heather was wonderful.', 'The conversation with Melanie was wonderful.', 'The conversation with Katie was wonderful.', 'The conversation with Betsy was wonderful.', 'The conversation with Kristin was wonderful.', 'The conversation with Nancy was wonderful.', 'The conversation with Stephanie was wonderful.', 'The conversation with Ellen was wonderful.'], ['The conversation with Alonzo was wonderful.', 'The conversation with Jamel was wonderful.', 'The conversation with Alphonse was wonderful.', 'The conversation with Jerome was wonderful.', 'The conversation with Leroy was wonderful.', 'The conversation with Torrance was wonderful.', 'The conversation with Darnell was wonderful.', 'The conversation with Lamar was wonderful.', 'The conversation with Malik was wonderful.', 'The conversation with Terrence was wonderful.', 'The conversation with Adam was wonderful.', 'The conversation with Harry was wonderful.', 'The conversation with Josh was wonderful.', 'The conversation with Roger was wonderful.', 'The conversation with Alan was wonderful.', 'The conversation with Frank was wonderful.', 'The conversation with Justin was wonderful.', 'The conversation with Ryan was wonderful.', 'The conversation with Andrew was wonderful.', 'The conversation with Jack was wonderful.']), 143: (['The conversation with Nichelle was great.', 'The conversation with Shereen was great.', 'The conversation with Ebony was great.', 'The conversation with Latisha was great.', 'The conversation with Shaniqua was great.', 'The conversation with Jasmine was great.', 'The conversation with Tanisha was great.', 'The conversation with Tia was great.', 'The conversation with Lakisha was great.', 'The conversation with Latoya was great.', 'The conversation with Amanda was great.', 'The conversation with Courtney was great.', 'The conversation with Heather was great.', 'The conversation with Melanie was great.', 'The conversation with Katie was great.', 'The conversation with Betsy was great.', 'The conversation with Kristin was great.', 'The conversation with Nancy was great.', 'The conversation with Stephanie was great.', 'The conversation with Ellen was great.'], ['The conversation with Alonzo was great.', 'The conversation with Jamel was great.', 'The conversation with Alphonse was great.', 'The conversation with Jerome was great.', 'The conversation with Leroy was great.', 'The conversation with Torrance was great.', 'The conversation with Darnell was great.', 'The conversation with Lamar was great.', 'The conversation with Malik was great.', 'The conversation with Terrence was great.', 'The conversation with Adam was great.', 'The conversation with Harry was great.', 'The conversation with Josh was great.', 'The conversation with Roger was great.', 'The conversation with Alan was great.', 'The conversation with Frank was great.', 'The conversation with Justin was great.', 'The conversation with Ryan was great.', 'The conversation with Andrew was great.', 'The conversation with Jack was great.'])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict_list_named_sentence_pairs[0][0],\"\\n\",dict_list_named_sentence_pairs[0][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doZGjh5Qp0mQ",
        "outputId": "736c0119-d694-43ee-c290-6441d937d82d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Nichelle feels angry.', 'Shereen feels angry.', 'Ebony feels angry.', 'Latisha feels angry.', 'Shaniqua feels angry.', 'Jasmine feels angry.', 'Tanisha feels angry.', 'Tia feels angry.', 'Lakisha feels angry.', 'Latoya feels angry.', 'Amanda feels angry.', 'Courtney feels angry.', 'Heather feels angry.', 'Melanie feels angry.', 'Katie feels angry.', 'Betsy feels angry.', 'Kristin feels angry.', 'Nancy feels angry.', 'Stephanie feels angry.', 'Ellen feels angry.'] \n",
            " ['Alonzo feels angry.', 'Jamel feels angry.', 'Alphonse feels angry.', 'Jerome feels angry.', 'Leroy feels angry.', 'Torrance feels angry.', 'Darnell feels angry.', 'Lamar feels angry.', 'Malik feels angry.', 'Terrence feels angry.', 'Adam feels angry.', 'Harry feels angry.', 'Josh feels angry.', 'Roger feels angry.', 'Alan feels angry.', 'Frank feels angry.', 'Justin feels angry.', 'Ryan feels angry.', 'Andrew feels angry.', 'Jack feels angry.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two Sample t- test"
      ],
      "metadata": {
        "id": "3dMoFvYwy1uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# f ='She feels angry.'\n",
        "# m ='He feels angry.'\n",
        "# f_indices = text_pipeline(f)\n",
        "# m_indices = text_pipeline(m)\n",
        "# f_value = predict(f, loaded_model,text_pipeline,device= loaded_model_device)\n",
        "# m_value = predict(m, loaded_model,text_pipeline,device= loaded_model_device)\n",
        "# print(f_value,m_value)\n",
        "# stats.ttest_rel(f_value, m_value)"
      ],
      "metadata": {
        "id": "jy-wgBm2P12q"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for t-test processing\n",
        "\n",
        "def two_sample_test(dict_sentence_pairs ={}, text_pipeline = text_pipeline, loaded_model= None, loaded_model_device = 'cpu', name = None)-> dict:\n",
        "  assert loaded_model is not None, \"No Model Selected for t-test\"\n",
        "  dict_t_test_result_sentence_pair ={}\n",
        "  for key, value in dict_sentence_pairs.items():\n",
        "\n",
        "    female_list = value[0] \n",
        "    male_list = value[1]\n",
        "    if isinstance(female_list,str):\n",
        "      female_list = [female_list]\n",
        "    if isinstance(male_list,str):\n",
        "      male_list = [male_list]\n",
        "\n",
        "    assert len(female_list) == len(male_list), f\"Different lengths: Lengths of female list is {len(female_list)} and male list is {len(male_list)}\"\n",
        "    \n",
        "    # INPUT_DIM = len(dict_fields[name]['Tweet'][1].vocab)\n",
        "    PAD_IDX = dict_fields[name]['Tweet'][1].vocab.stoi[dict_fields[name]['Tweet'][1].pad_token]\n",
        "\n",
        "\n",
        "    female_list_indices = [text_pipeline(tweet_example,vocab_obj = dict_fields[name]['Tweet'][1], length = MAX_SIZE, pad_idx = PAD_IDX) for tweet_example in female_list]\n",
        "    male_list_indices = [text_pipeline(tweet_example,vocab_obj = dict_fields[name]['Tweet'][1], length = MAX_SIZE, pad_idx = PAD_IDX) for tweet_example in male_list]\n",
        "\n",
        "    # female_list_indices = [text_pipeline(tweet_example) for tweet_example in female_list]\n",
        "    # male_list_indices = [text_pipeline(tweet_example)for tweet_example in male_list]\n",
        "\n",
        "    female_list_output = [predict(sentence, loaded_model,text_pipeline,device= loaded_model_device,vocab_obj = dict_fields[name]['Tweet'][1], length = MAX_SIZE, pad_idx = PAD_IDX ) for sentence in female_list]\n",
        "    male_list_output = [predict(sentence, loaded_model,text_pipeline,device= loaded_model_device,vocab_obj = dict_fields[name]['Tweet'][1], length = MAX_SIZE, pad_idx = PAD_IDX) for sentence in male_list]\n",
        "    # female_list_output = [predict(sentence, loaded_model,text_pipeline,device= loaded_model_device) for sentence in female_list]\n",
        "    # male_list_output = [predict(sentence, loaded_model,text_pipeline,device= loaded_model_device) for sentence in male_list]\n",
        "\n",
        "    t_test_result = stats.ttest_rel(female_list_output, male_list_output)\n",
        "    dict_t_test_result_sentence_pair[key] = (t_test_result.statistic, t_test_result.pvalue,mean(female_list_output)-mean(male_list_output))\n",
        "  # print(dict_t_test_result_sentence_pair)\n",
        "  return dict_t_test_result_sentence_pair\n",
        "\n"
      ],
      "metadata": {
        "id": "20jPvZ66iLRL"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " dict_loaded_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH4WPnhVOR8b",
        "outputId": "f32ed471-d48a-4145-da90-23bdd9a1a2e0"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'EI_sadness': {'non_dann': CNN1d(\n",
              "    (embedding): Embedding(4989, 100, padding_idx=1)\n",
              "    (convs): ModuleList(\n",
              "      (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
              "      (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "      (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
              "      (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "    )\n",
              "    (regression): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=1, bias=True)\n",
              "    )\n",
              "    (domain_classifier): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=2, bias=True)\n",
              "      (6): LogSoftmax(dim=1)\n",
              "    )\n",
              "  ), 'dann': CNN1d(\n",
              "    (embedding): Embedding(4989, 100, padding_idx=1)\n",
              "    (convs): ModuleList(\n",
              "      (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
              "      (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "      (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
              "      (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "    )\n",
              "    (regression): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=1, bias=True)\n",
              "    )\n",
              "    (domain_classifier): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=2, bias=True)\n",
              "      (6): LogSoftmax(dim=1)\n",
              "    )\n",
              "  )}, 'EI_anger': {'non_dann': CNN1d(\n",
              "    (embedding): Embedding(4824, 100, padding_idx=1)\n",
              "    (convs): ModuleList(\n",
              "      (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
              "      (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "      (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
              "      (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "    )\n",
              "    (regression): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=1, bias=True)\n",
              "    )\n",
              "    (domain_classifier): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=2, bias=True)\n",
              "      (6): LogSoftmax(dim=1)\n",
              "    )\n",
              "  ), 'dann': CNN1d(\n",
              "    (embedding): Embedding(4824, 100, padding_idx=1)\n",
              "    (convs): ModuleList(\n",
              "      (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
              "      (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "      (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
              "      (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "    )\n",
              "    (regression): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=1, bias=True)\n",
              "    )\n",
              "    (domain_classifier): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=2, bias=True)\n",
              "      (6): LogSoftmax(dim=1)\n",
              "    )\n",
              "  )}, 'EI_fear': {'non_dann': CNN1d(\n",
              "    (embedding): Embedding(5681, 100, padding_idx=1)\n",
              "    (convs): ModuleList(\n",
              "      (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
              "      (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "      (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
              "      (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "    )\n",
              "    (regression): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=1, bias=True)\n",
              "    )\n",
              "    (domain_classifier): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=2, bias=True)\n",
              "      (6): LogSoftmax(dim=1)\n",
              "    )\n",
              "  ), 'dann': CNN1d(\n",
              "    (embedding): Embedding(5681, 100, padding_idx=1)\n",
              "    (convs): ModuleList(\n",
              "      (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
              "      (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "      (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
              "      (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "    )\n",
              "    (regression): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=1, bias=True)\n",
              "    )\n",
              "    (domain_classifier): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=2, bias=True)\n",
              "      (6): LogSoftmax(dim=1)\n",
              "    )\n",
              "  )}, 'V': {'non_dann': CNN1d(\n",
              "    (embedding): Embedding(4449, 100, padding_idx=1)\n",
              "    (convs): ModuleList(\n",
              "      (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
              "      (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "      (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
              "      (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "    )\n",
              "    (regression): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=1, bias=True)\n",
              "    )\n",
              "    (domain_classifier): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=2, bias=True)\n",
              "      (6): LogSoftmax(dim=1)\n",
              "    )\n",
              "  ), 'dann': CNN1d(\n",
              "    (embedding): Embedding(4449, 100, padding_idx=1)\n",
              "    (convs): ModuleList(\n",
              "      (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
              "      (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "      (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
              "      (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "    )\n",
              "    (regression): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=1, bias=True)\n",
              "    )\n",
              "    (domain_classifier): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=2, bias=True)\n",
              "      (6): LogSoftmax(dim=1)\n",
              "    )\n",
              "  )}, 'EI_joy': {'non_dann': CNN1d(\n",
              "    (embedding): Embedding(4788, 100, padding_idx=1)\n",
              "    (convs): ModuleList(\n",
              "      (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
              "      (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "      (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
              "      (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "    )\n",
              "    (regression): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=1, bias=True)\n",
              "    )\n",
              "    (domain_classifier): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=2, bias=True)\n",
              "      (6): LogSoftmax(dim=1)\n",
              "    )\n",
              "  ), 'dann': CNN1d(\n",
              "    (embedding): Embedding(4788, 100, padding_idx=1)\n",
              "    (convs): ModuleList(\n",
              "      (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
              "      (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
              "      (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
              "      (3): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
              "    )\n",
              "    (regression): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=1, bias=True)\n",
              "    )\n",
              "    (domain_classifier): Sequential(\n",
              "      (0): Dropout(p=0.5, inplace=False)\n",
              "      (1): Linear(in_features=400, out_features=200, bias=True)\n",
              "      (2): ReLU()\n",
              "      (3): Linear(in_features=200, out_features=10, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): Linear(in_features=10, out_features=2, bias=True)\n",
              "      (6): LogSoftmax(dim=1)\n",
              "    )\n",
              "  )}}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_loaded_models.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS_sgonERBZN",
        "outputId": "dc989431-52c6-459f-bcad-2182ebe91991"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['EI_sadness', 'EI_anger', 'EI_fear', 'V', 'EI_joy'])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dict_loaded_models[name]={\"non_dann\":loaded_model_non_dann,\"dann\":loaded_model_dann}\n",
        "\n",
        "dict_sentence_pairs = {'named': dict_list_named_sentence_pairs ,\n",
        "                       'noun_phrase': dict_noun_phrase_sentence_pair,\n",
        "                       'original_noun_phrase':dict_original_sentence_pair_updated}\n",
        "dict_t_test = {}\n",
        "for name, model_dict in dict_loaded_models.items():\n",
        "  dict_t_test_level_1 = {}\n",
        "  # if name in ['EI_sadness', 'EI_fear', 'V' ]:\n",
        "  #   continue\n",
        "  # print(name)\n",
        "  for model_type, model in model_dict.items():\n",
        "    dict_t_test_level_2 ={}\n",
        "    # print(name, model_type)\n",
        "    for sentence_pair_name, dict_sentence_pair in dict_sentence_pairs.items():\n",
        "      # key_name = str(name+ \"_\" + model_type + \"_\" + sentence_pair_name)\n",
        "      # print(key_name)\n",
        "      print(name, model_type,sentence_pair_name)\n",
        "      loaded_model = dict_loaded_models[name][model_type]\n",
        "      # dict_t_test[key_name] = two_sample_test(dict_sentence_pairs = dict_sentence_pair ,\n",
        "      #                                         text_pipeline = text_pipeline, \n",
        "      #                                         loaded_model = loaded_model, \n",
        "      #                                         loaded_model_device = 'cpu')\n",
        "      dict_t_test_level_2[sentence_pair_name] = two_sample_test(dict_sentence_pairs = dict_sentence_pair ,\n",
        "                                        text_pipeline = text_pipeline, \n",
        "                                        loaded_model = loaded_model, \n",
        "                                        loaded_model_device = 'cpu',\n",
        "                                        name = name)\n",
        "      print(sentence_pair_name, dict_t_test_level_2[sentence_pair_name] )\n",
        "    dict_t_test_level_1[model_type] = dict_t_test_level_2\n",
        "    print(model_type,sentence_pair_name, dict_t_test_level_1[model_type])\n",
        "  dict_t_test[name] = dict_t_test_level_1\n",
        "  print(name, model_type,sentence_pair_name, dict_t_test[name])\n",
        "  \n",
        "print(dict_t_test)\n",
        "# for model_type, loaded_model in dict_loaded_model.items():\n",
        "#   dict_t_test[str(model_type)+\"_noun_phrase\"] = two_sample_test(dict_sentence_pairs =dict_noun_phrase_sentence_pair,text_pipeline = text_pipeline, loaded_model = loaded_model, loaded_model_device = 'cpu')\n",
        "#   dict_t_test[str(model_type)+\"_named\"] = two_sample_test(dict_sentence_pairs =dict_list_named_sentence_pairs,text_pipeline = text_pipeline, loaded_model = loaded_model, loaded_model_device = 'cpu')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgsN9HBm0R34",
        "outputId": "84a150cd-b833-427a-92c7-0f0d9651fdc3"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EI_sadness non_dann named\n",
            "named {0: (2.0761933504409655, 0.05168978321260658, 0.0029319673776626587), 1: (2.0277227426424362, 0.05684802149348772, 0.0024926632642746083), 2: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 3: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 4: (2.2230811810478572, 0.03854150183671055, 0.0023595303297042625), 5: (2.0112032728744835, 0.05870902687924929, 0.001079910993576072), 6: (2.1806215388692425, 0.0419872916426721, 0.002160996198654175), 7: (2.046200341527533, 0.05482936063760963, 0.002086377143859841), 8: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 9: (2.0647403157958726, 0.05286898145200738, 0.001883125305175759), 10: (2.1844555404719377, 0.04166494885466178, 0.001084744930267334), 11: (2.3645098043050004, 0.028853162664240845, 0.0022659599781036377), 12: (1.5175673531338714, 0.145586931890424, 0.0010704368352890126), 13: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 14: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 15: (1.850029026805524, 0.0799249809879829, 0.001398104429244984), 16: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 17: (1.3749531839684273, 0.18514447446028678, 0.000617067515850056), 18: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 19: (2.285793149570828, 0.03392496127451366, 0.0012482210993766674), 20: (2.0167980614106953, 0.05807271163781755, 0.006875231862068176), 21: (1.5988626632260454, 0.12634834921979857, 0.004185765981674194), 22: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 23: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 24: (2.0546288368370345, 0.05393017024255865, 0.00546366870403292), 25: (1.7069376241864904, 0.10412447151487597, 0.0029117465019226074), 26: (2.174048713849316, 0.04254520564317553, 0.006127482652664162), 27: (1.7169097546770051, 0.10225364327072171, 0.0035044163465499656), 28: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 29: (2.019615061442269, 0.05775467206481685, 0.005211299657821633), 30: (1.9330421431608467, 0.06827436833571221, 0.004680752754211426), 31: (1.8648201405792866, 0.07772911882885637, 0.004628813266754128), 32: (1.8797381679169154, 0.07556833471115386, 0.004890909790992715), 33: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 34: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 35: (1.8528679297404185, 0.07949934680260273, 0.003478109836578369), 36: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 37: (1.8766312627773414, 0.0760139372792857, 0.00439032316207888), 38: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 39: (1.8984042073379923, 0.07293936206286997, 0.004009491205215432), 40: (1.20506807804393, 0.2429724396998576, 0.0018940389156341664), 41: (0.9762495415897414, 0.34120776495641103, 0.0010327056050300487), 42: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 43: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 44: (1.5638420023981912, 0.13435774633190134, 0.0014817893505096325), 45: (2.2668341225678175, 0.03526386490010458, 0.001438423991203308), 46: (1.6782686708220131, 0.10966722697059246, 0.0023178547620773537), 47: (1.310394631237746, 0.205683103215663, 0.0019497275352478027), 48: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 49: (2.0883570808072833, 0.05046347909114234, 0.0029065310955047607), 50: (0.6101184095267318, 0.5490118916170734, 0.0005402684211731068), 51: (1.1397273294732755, 0.26856489875868883, 0.0011490106582641713), 52: (1.16153920208516, 0.25980743666427664, 0.0012454301118850486), 53: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 54: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 55: (1.215851117000124, 0.23893278648489205, 0.0011919990181922802), 56: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 57: (1.151776444529962, 0.26370044738777193, 0.0009692117571830638), 58: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 59: (-0.28249320707092423, 0.7806197754390511, -0.00021852850914000355), 60: (1.762578428279938, 0.09405096323231914, 0.001563867926597573), 61: (2.0080108707067215, 0.0590749050332689, 0.002247375249862682), 62: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 63: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 64: (1.9615926117590434, 0.06463036269842726, 0.0018849849700927623), 65: (2.1399687369678557, 0.04554794710656452, 0.0025827020406723467), 66: (1.6637068337975134, 0.11257778388879425, 0.0015673279762268288), 67: (1.5051181251873107, 0.148737187493084, 0.001271191239357039), 68: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 69: (2.150386999841326, 0.044610134602192375, 0.0023679733276367188), 70: (2.177118213515657, 0.04228382339911018, 0.0028291165828704945), 71: (1.851836012904142, 0.07965383042593033, 0.0019317537546157948), 72: (2.126903002088096, 0.04674941447782704, 0.002539578080177296), 73: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 74: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 75: (2.0987947680643417, 0.049432237599766615, 0.002668848633766152), 76: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 77: (2.0930106082908817, 0.05000132978705774, 0.002096055448055245), 78: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 79: (2.2137610714328213, 0.03927487568851686, 0.002266727387905121), 80: (-1.2297251309277186, 0.23381072735893818, -0.0009175911545753479), 81: (1.5770916026973085, 0.13127891211348372, 0.0027214109897613636), 82: (-1.9033034001047069, 0.07226287284710739, -0.0013722851872444153), 83: (2.0248964150987545, 0.05716261700759523, 0.0013909146189689525), 84: (0.7499794368533861, 0.46245361722017764, 0.0003775402903556935), 85: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 86: (0.7499794368533861, 0.46245361722017764, 0.0003775402903556935), 87: (0.8267080693582394, 0.4186641253443355, 0.0003182619810104259), 88: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 89: (2.2402202926119856, 0.03722558439101236, 0.002292880415916465), 90: (2.0223167206031394, 0.057451123976569936, 0.001519373059272744), 91: (2.261036607936422, 0.03568291136125278, 0.002285835146904003), 92: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 93: (2.2804006018370484, 0.0343009354037606, 0.0016447007656097412), 94: (2.245660840473207, 0.03681657862016815, 0.002138602733612105), 95: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 96: (2.1620934635302254, 0.04357737110447765, 0.002022719383239724), 97: (2.2327728738093473, 0.03779223294436587, 0.0018279343843460305), 98: (2.06902232230206, 0.05242529547033867, 0.0018476366996765248), 99: (2.1544990001643627, 0.04424485304084849, 0.001421475410461448), 100: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 101: (1.5283930400747785, 0.14289255239327028, 0.0006372183561325073), 102: (2.115812886331568, 0.047791727971066664, 0.0013906747102737649), 103: (2.007473343935898, 0.05913671110023691, 0.0012248843908310159), 104: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 105: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 106: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 107: (0.0020121291648187547, 0.9984155306688118, 1.6719102859386048e-06), 108: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 109: (1.1050655128485936, 0.2829289719820194, 0.0009811639785766157), 110: (1.1368937517123074, 0.2697184801885618, 0.0011129364371299522), 111: (1.3696790666518044, 0.18675858532937661, 0.001445630192756675), 112: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 113: (1.4954607557796367, 0.15121951132493028, 0.0015002578496933205), 114: (0.36569502516584806, 0.7186324318973236, 0.0003187239170074352), 115: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 116: (0.5292246097569755, 0.6027786487650441, 0.00042713284492490455), 117: (0.5142802482705426, 0.6129861646620078, 0.00041597187519071266), 118: (1.4696636014771989, 0.1580178641681456, 0.0013391733169555442), 119: (0.7178184135979354, 0.48160281132001537, 0.0006729722023010032), 120: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 121: (0.8983249623027905, 0.38025569019303307, 0.0008771687746048085), 122: (1.0055171789009922, 0.3272827294300725, 0.0010290488600731007), 123: (1.2707327346974095, 0.21916305747624937, 0.0013425901532173046), 124: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 125: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 126: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 127: (1.5217129237402338, 0.14455022513035917, 0.0019379243254661338), 128: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 129: (1.9502220733005475, 0.06606035550226323, 0.003574529290199302), 130: (2.257082874008286, 0.035971313423737696, 0.0035756126046180836), 131: (2.006714851562998, 0.05922402288379378, 0.0032355964183807817), 132: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 133: (1.5921861739093308, 0.12784357900410126, 0.0019223928451538308), 134: (1.3387098671311615, 0.19646404439327628, 0.0020498543977737205), 135: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 136: (1.0242504981564373, 0.31858165840198993, 0.0012687504291534202), 137: (2.0459741649498424, 0.05485367511894138, 0.003114047646522511), 138: (1.8147542867908093, 0.08538270044406286, 0.0027948886156082264), 139: (1.246129678593975, 0.22786341362474055, 0.001814487576484669), 140: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 141: (1.59845966430835, 0.1264381837596209, 0.0017410978674888833), 142: (2.316546549269402, 0.03185256188279949, 0.004329253733158123), 143: (1.6008080372334728, 0.12591545072725402, 0.0018596306443214305)}\n",
            "EI_sadness non_dann noun_phrase\n",
            "noun_phrase {0: (4.416285964467372, 0.0016802065013952865, 0.0157653748989105), 1: (5.053638609689071, 0.0006869433416155954, 0.018537551164627075), 2: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 3: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 4: (2.6385147819837624, 0.026979388143290196, 0.010085684061050426), 5: (3.8865595174708805, 0.0036942739930930454, 0.012511950731277421), 6: (2.761585012181802, 0.022054913275819255, 0.008653420209884688), 7: (2.7105990080739124, 0.023973973042005197, 0.010546064376831032), 8: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 9: (3.0383359306990423, 0.014055712684936238, 0.011872017383575462), 10: (2.4775676124384827, 0.0351313640520545, 0.011167210340499945), 11: (2.30448516428659, 0.046655701485290364, 0.009044277667999312), 12: (3.076748054633628, 0.013209175998730217, 0.012794214487075795), 13: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 14: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 15: (3.1591619817473666, 0.011565798032961561, 0.01454648971557615), 16: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 17: (2.0391274779866837, 0.0718664088830766, 0.009420758485794045), 18: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 19: (3.1052140879654035, 0.012615876104423013, 0.01338036060333253), 20: (3.2659050260927107, 0.009746295218372463, 0.015316778421402), 21: (3.4949274347865944, 0.006777387618012797, 0.01486323475837703), 22: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 23: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 24: (3.0551497099124587, 0.01367848967421171, 0.013470512628555253), 25: (3.978960748655893, 0.00321083842861956, 0.014046573638915927), 26: (3.7146413003442715, 0.004810320006575784, 0.013544982671737649), 27: (2.9601293574172027, 0.01595616452942292, 0.01156689524650567), 28: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 29: (2.7294445130844096, 0.02324566456488856, 0.011514449119567893), 30: (2.729707535927594, 0.02323565995765799, 0.013818293809890747), 31: (3.4048450222022812, 0.007813134063093983, 0.012937533855438277), 32: (3.241925761631311, 0.01012738885873162, 0.013584524393081665), 33: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 34: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 35: (2.6942361002948796, 0.024625063165150767, 0.011142772436141979), 36: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 37: (2.40979128825808, 0.03926245654856135, 0.009671843051910434), 38: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 39: (3.0408490762270968, 0.013998655336479288, 0.014194577932357788), 40: (3.5282488522525606, 0.0064315987021017155, 0.020213115215301447), 41: (3.540509723288857, 0.006309059642745521, 0.0206573188304901), 42: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 43: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 44: (3.139242691532513, 0.011942581116984919, 0.018650662899017345), 45: (4.231198576921104, 0.0022027128241340296, 0.016053479909896917), 46: (3.5889064563962347, 0.005848767455705855, 0.014558517932891868), 47: (2.422097782700182, 0.03847802488113382, 0.012305146455764748), 48: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 49: (2.9479736158808096, 0.016274435480833523, 0.014781582355499201), 50: (3.098010984589562, 0.01276335748321234, 0.01675570011138916), 51: (3.109653253721714, 0.012525862890179026, 0.01677399277687075), 52: (2.9627464276545528, 0.015888481787377604, 0.016157895326614324), 53: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 54: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 55: (3.177091057619296, 0.011237179503860398, 0.016597312688827526), 56: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 57: (3.034893196641776, 0.014134263923135663, 0.015046232938766446), 58: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 59: (2.815390245351479, 0.0201984694568075, 0.016350775957107544), 60: (2.839192886611046, 0.01942873235920257, 0.006291621923446622), 61: (3.11326114958753, 0.012453194070788634, 0.007118368148803733), 62: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 63: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 64: (2.9669450703085363, 0.015780511185964227, 0.006942373514175426), 65: (1.4561753110822764, 0.17932542287163092, 0.0018079161643981267), 66: (2.698245957275624, 0.024463871275924615, 0.004881149530410744), 67: (3.2574274365048974, 0.009879295174801486, 0.00820679664611823), 68: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 69: (2.7540433641090347, 0.022328640184350632, 0.005718529224395752), 70: (1.7046740075279663, 0.12244931858753924, 0.0026186466217040794), 71: (2.497536079891916, 0.03399900051068607, 0.005833053588867154), 72: (3.2582103359915013, 0.009866934085741942, 0.00625405311584476), 73: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 74: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 75: (2.9056865701533283, 0.017433224383815584, 0.0073753297328948975), 76: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 77: (2.690496370487274, 0.024776364575125966, 0.007022315263748191), 78: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 79: (2.24868660308401, 0.051112820961218254, 0.005518060922622692), 80: (2.711303188253064, 0.023946347728852103, 0.014744377136230447), 81: (3.461143738892126, 0.0071478995398634075, 0.012177711725235019), 82: (2.0203259378538676, 0.07408300087981214, 0.005611547827720664), 83: (2.636527310779811, 0.027067437024558542, 0.005813160538673423), 84: (2.4655943021651554, 0.03582830649843939, 0.00879108607769008), 85: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 86: (2.4655943021651554, 0.03582830649843939, 0.00879108607769008), 87: (1.5642090515574891, 0.1522070860796841, 0.005417627096176103), 88: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 89: (1.2679911142067402, 0.2366197689945075, 0.003012627363204956), 90: (2.7518234768895606, 0.022409867045439586, 0.007333815097808838), 91: (2.7015366476263636, 0.024332387034338154, 0.005179184675216719), 92: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 93: (2.5476090434245555, 0.03131735666801436, 0.005592763423919678), 94: (3.5406372105246993, 0.006307798427916721, 0.008065050840377896), 95: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 96: (2.0803944850609946, 0.06722217924025395, 0.005379730463027976), 97: (3.7145350378047395, 0.0048111106743607066, 0.008877068758010864), 98: (2.9523209650575724, 0.016159868571200005, 0.0082339346408844), 99: (3.4755318428902466, 0.006987576347940334, 0.007993972301483188), 100: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 101: (2.6128540996488807, 0.028138744989167987, 0.008272904157638539), 102: (3.626378017474533, 0.005516655986738787, 0.009155404567718517), 103: (2.932885811474453, 0.016678549378825964, 0.00849389433860781), 104: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 105: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 106: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 107: (-0.0016871017238068403, 0.9986906919628202, -5.674362182572779e-06), 108: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 109: (0.2850577485601285, 0.7820509244092041, 0.0009132444858551692), 110: (1.148653444973492, 0.28031053394824124, 0.002852267026901223), 111: (0.4914509220986568, 0.6348698895571083, 0.0012215912342071755), 112: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 113: (0.202365480741529, 0.844131589418614, 0.000528854131698675), 114: (0.39156709903185344, 0.7044876319585958, 0.0013040482997894731), 115: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 116: (0.11173623981498304, 0.9134848656399938, 0.0003580749034881592), 117: (0.5211298502601197, 0.6148548745782575, 0.0017965734004974365), 118: (0.5718106056858795, 0.5814483781183453, 0.001604634523391768), 119: (0.6256134362802805, 0.5471030883124561, 0.0020198166370392068), 120: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 121: (0.597677215553958, 0.5647888152550127, 0.0016816914081573264), 122: (0.762842554036261, 0.4650810698386909, 0.002107131481170643), 123: (0.8334508277291102, 0.42615579589916486, 0.0021722793579102007), 124: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 125: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 126: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 127: (4.2173074188264605, 0.002248379496694524, 0.0185785233974457), 128: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 129: (3.776966159094484, 0.004369360251735088, 0.020553475618362405), 130: (3.8910995144390377, 0.003668804882592309, 0.014036139845848061), 131: (2.2774326269052807, 0.04876680009354322, 0.010569620132446333), 132: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 133: (3.025012628087391, 0.014362223259487624, 0.014482587575912476), 134: (3.8711233221875028, 0.003782278370286225, 0.017132323980331376), 135: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 136: (3.3154945767064916, 0.009004718042169724, 0.014977973699569658), 137: (4.650113863752602, 0.0012019215246106532, 0.021829116344451838), 138: (3.6557698313769347, 0.005270047017565919, 0.018977212905883722), 139: (5.010929337147533, 0.0007280326694162593, 0.021607166528701816), 140: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 141: (4.510406264374862, 0.001466853700376692, 0.019268441200256337), 142: (4.868648511259469, 0.0008851704002848073, 0.016163876652717568), 143: (4.967373579495973, 0.0007726779571079604, 0.0189771890640259)}\n",
            "EI_sadness non_dann original_noun_phrase\n",
            "original_noun_phrase {0: (31.041602740335975, 2.2266901078377605e-162, 0.010461641165117441)}\n",
            "non_dann original_noun_phrase {'named': {0: (2.0761933504409655, 0.05168978321260658, 0.0029319673776626587), 1: (2.0277227426424362, 0.05684802149348772, 0.0024926632642746083), 2: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 3: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 4: (2.2230811810478572, 0.03854150183671055, 0.0023595303297042625), 5: (2.0112032728744835, 0.05870902687924929, 0.001079910993576072), 6: (2.1806215388692425, 0.0419872916426721, 0.002160996198654175), 7: (2.046200341527533, 0.05482936063760963, 0.002086377143859841), 8: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 9: (2.0647403157958726, 0.05286898145200738, 0.001883125305175759), 10: (2.1844555404719377, 0.04166494885466178, 0.001084744930267334), 11: (2.3645098043050004, 0.028853162664240845, 0.0022659599781036377), 12: (1.5175673531338714, 0.145586931890424, 0.0010704368352890126), 13: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 14: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 15: (1.850029026805524, 0.0799249809879829, 0.001398104429244984), 16: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 17: (1.3749531839684273, 0.18514447446028678, 0.000617067515850056), 18: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 19: (2.285793149570828, 0.03392496127451366, 0.0012482210993766674), 20: (2.0167980614106953, 0.05807271163781755, 0.006875231862068176), 21: (1.5988626632260454, 0.12634834921979857, 0.004185765981674194), 22: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 23: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 24: (2.0546288368370345, 0.05393017024255865, 0.00546366870403292), 25: (1.7069376241864904, 0.10412447151487597, 0.0029117465019226074), 26: (2.174048713849316, 0.04254520564317553, 0.006127482652664162), 27: (1.7169097546770051, 0.10225364327072171, 0.0035044163465499656), 28: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 29: (2.019615061442269, 0.05775467206481685, 0.005211299657821633), 30: (1.9330421431608467, 0.06827436833571221, 0.004680752754211426), 31: (1.8648201405792866, 0.07772911882885637, 0.004628813266754128), 32: (1.8797381679169154, 0.07556833471115386, 0.004890909790992715), 33: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 34: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 35: (1.8528679297404185, 0.07949934680260273, 0.003478109836578369), 36: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 37: (1.8766312627773414, 0.0760139372792857, 0.00439032316207888), 38: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 39: (1.8984042073379923, 0.07293936206286997, 0.004009491205215432), 40: (1.20506807804393, 0.2429724396998576, 0.0018940389156341664), 41: (0.9762495415897414, 0.34120776495641103, 0.0010327056050300487), 42: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 43: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 44: (1.5638420023981912, 0.13435774633190134, 0.0014817893505096325), 45: (2.2668341225678175, 0.03526386490010458, 0.001438423991203308), 46: (1.6782686708220131, 0.10966722697059246, 0.0023178547620773537), 47: (1.310394631237746, 0.205683103215663, 0.0019497275352478027), 48: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 49: (2.0883570808072833, 0.05046347909114234, 0.0029065310955047607), 50: (0.6101184095267318, 0.5490118916170734, 0.0005402684211731068), 51: (1.1397273294732755, 0.26856489875868883, 0.0011490106582641713), 52: (1.16153920208516, 0.25980743666427664, 0.0012454301118850486), 53: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 54: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 55: (1.215851117000124, 0.23893278648489205, 0.0011919990181922802), 56: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 57: (1.151776444529962, 0.26370044738777193, 0.0009692117571830638), 58: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 59: (-0.28249320707092423, 0.7806197754390511, -0.00021852850914000355), 60: (1.762578428279938, 0.09405096323231914, 0.001563867926597573), 61: (2.0080108707067215, 0.0590749050332689, 0.002247375249862682), 62: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 63: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 64: (1.9615926117590434, 0.06463036269842726, 0.0018849849700927623), 65: (2.1399687369678557, 0.04554794710656452, 0.0025827020406723467), 66: (1.6637068337975134, 0.11257778388879425, 0.0015673279762268288), 67: (1.5051181251873107, 0.148737187493084, 0.001271191239357039), 68: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 69: (2.150386999841326, 0.044610134602192375, 0.0023679733276367188), 70: (2.177118213515657, 0.04228382339911018, 0.0028291165828704945), 71: (1.851836012904142, 0.07965383042593033, 0.0019317537546157948), 72: (2.126903002088096, 0.04674941447782704, 0.002539578080177296), 73: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 74: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 75: (2.0987947680643417, 0.049432237599766615, 0.002668848633766152), 76: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 77: (2.0930106082908817, 0.05000132978705774, 0.002096055448055245), 78: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 79: (2.2137610714328213, 0.03927487568851686, 0.002266727387905121), 80: (-1.2297251309277186, 0.23381072735893818, -0.0009175911545753479), 81: (1.5770916026973085, 0.13127891211348372, 0.0027214109897613636), 82: (-1.9033034001047069, 0.07226287284710739, -0.0013722851872444153), 83: (2.0248964150987545, 0.05716261700759523, 0.0013909146189689525), 84: (0.7499794368533861, 0.46245361722017764, 0.0003775402903556935), 85: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 86: (0.7499794368533861, 0.46245361722017764, 0.0003775402903556935), 87: (0.8267080693582394, 0.4186641253443355, 0.0003182619810104259), 88: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 89: (2.2402202926119856, 0.03722558439101236, 0.002292880415916465), 90: (2.0223167206031394, 0.057451123976569936, 0.001519373059272744), 91: (2.261036607936422, 0.03568291136125278, 0.002285835146904003), 92: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 93: (2.2804006018370484, 0.0343009354037606, 0.0016447007656097412), 94: (2.245660840473207, 0.03681657862016815, 0.002138602733612105), 95: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 96: (2.1620934635302254, 0.04357737110447765, 0.002022719383239724), 97: (2.2327728738093473, 0.03779223294436587, 0.0018279343843460305), 98: (2.06902232230206, 0.05242529547033867, 0.0018476366996765248), 99: (2.1544990001643627, 0.04424485304084849, 0.001421475410461448), 100: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 101: (1.5283930400747785, 0.14289255239327028, 0.0006372183561325073), 102: (2.115812886331568, 0.047791727971066664, 0.0013906747102737649), 103: (2.007473343935898, 0.05913671110023691, 0.0012248843908310159), 104: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 105: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 106: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 107: (0.0020121291648187547, 0.9984155306688118, 1.6719102859386048e-06), 108: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 109: (1.1050655128485936, 0.2829289719820194, 0.0009811639785766157), 110: (1.1368937517123074, 0.2697184801885618, 0.0011129364371299522), 111: (1.3696790666518044, 0.18675858532937661, 0.001445630192756675), 112: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 113: (1.4954607557796367, 0.15121951132493028, 0.0015002578496933205), 114: (0.36569502516584806, 0.7186324318973236, 0.0003187239170074352), 115: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 116: (0.5292246097569755, 0.6027786487650441, 0.00042713284492490455), 117: (0.5142802482705426, 0.6129861646620078, 0.00041597187519071266), 118: (1.4696636014771989, 0.1580178641681456, 0.0013391733169555442), 119: (0.7178184135979354, 0.48160281132001537, 0.0006729722023010032), 120: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 121: (0.8983249623027905, 0.38025569019303307, 0.0008771687746048085), 122: (1.0055171789009922, 0.3272827294300725, 0.0010290488600731007), 123: (1.2707327346974095, 0.21916305747624937, 0.0013425901532173046), 124: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 125: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 126: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 127: (1.5217129237402338, 0.14455022513035917, 0.0019379243254661338), 128: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 129: (1.9502220733005475, 0.06606035550226323, 0.003574529290199302), 130: (2.257082874008286, 0.035971313423737696, 0.0035756126046180836), 131: (2.006714851562998, 0.05922402288379378, 0.0032355964183807817), 132: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 133: (1.5921861739093308, 0.12784357900410126, 0.0019223928451538308), 134: (1.3387098671311615, 0.19646404439327628, 0.0020498543977737205), 135: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 136: (1.0242504981564373, 0.31858165840198993, 0.0012687504291534202), 137: (2.0459741649498424, 0.05485367511894138, 0.003114047646522511), 138: (1.8147542867908093, 0.08538270044406286, 0.0027948886156082264), 139: (1.246129678593975, 0.22786341362474055, 0.001814487576484669), 140: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 141: (1.59845966430835, 0.1264381837596209, 0.0017410978674888833), 142: (2.316546549269402, 0.03185256188279949, 0.004329253733158123), 143: (1.6008080372334728, 0.12591545072725402, 0.0018596306443214305)}, 'noun_phrase': {0: (4.416285964467372, 0.0016802065013952865, 0.0157653748989105), 1: (5.053638609689071, 0.0006869433416155954, 0.018537551164627075), 2: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 3: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 4: (2.6385147819837624, 0.026979388143290196, 0.010085684061050426), 5: (3.8865595174708805, 0.0036942739930930454, 0.012511950731277421), 6: (2.761585012181802, 0.022054913275819255, 0.008653420209884688), 7: (2.7105990080739124, 0.023973973042005197, 0.010546064376831032), 8: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 9: (3.0383359306990423, 0.014055712684936238, 0.011872017383575462), 10: (2.4775676124384827, 0.0351313640520545, 0.011167210340499945), 11: (2.30448516428659, 0.046655701485290364, 0.009044277667999312), 12: (3.076748054633628, 0.013209175998730217, 0.012794214487075795), 13: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 14: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 15: (3.1591619817473666, 0.011565798032961561, 0.01454648971557615), 16: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 17: (2.0391274779866837, 0.0718664088830766, 0.009420758485794045), 18: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 19: (3.1052140879654035, 0.012615876104423013, 0.01338036060333253), 20: (3.2659050260927107, 0.009746295218372463, 0.015316778421402), 21: (3.4949274347865944, 0.006777387618012797, 0.01486323475837703), 22: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 23: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 24: (3.0551497099124587, 0.01367848967421171, 0.013470512628555253), 25: (3.978960748655893, 0.00321083842861956, 0.014046573638915927), 26: (3.7146413003442715, 0.004810320006575784, 0.013544982671737649), 27: (2.9601293574172027, 0.01595616452942292, 0.01156689524650567), 28: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 29: (2.7294445130844096, 0.02324566456488856, 0.011514449119567893), 30: (2.729707535927594, 0.02323565995765799, 0.013818293809890747), 31: (3.4048450222022812, 0.007813134063093983, 0.012937533855438277), 32: (3.241925761631311, 0.01012738885873162, 0.013584524393081665), 33: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 34: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 35: (2.6942361002948796, 0.024625063165150767, 0.011142772436141979), 36: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 37: (2.40979128825808, 0.03926245654856135, 0.009671843051910434), 38: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 39: (3.0408490762270968, 0.013998655336479288, 0.014194577932357788), 40: (3.5282488522525606, 0.0064315987021017155, 0.020213115215301447), 41: (3.540509723288857, 0.006309059642745521, 0.0206573188304901), 42: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 43: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 44: (3.139242691532513, 0.011942581116984919, 0.018650662899017345), 45: (4.231198576921104, 0.0022027128241340296, 0.016053479909896917), 46: (3.5889064563962347, 0.005848767455705855, 0.014558517932891868), 47: (2.422097782700182, 0.03847802488113382, 0.012305146455764748), 48: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 49: (2.9479736158808096, 0.016274435480833523, 0.014781582355499201), 50: (3.098010984589562, 0.01276335748321234, 0.01675570011138916), 51: (3.109653253721714, 0.012525862890179026, 0.01677399277687075), 52: (2.9627464276545528, 0.015888481787377604, 0.016157895326614324), 53: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 54: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 55: (3.177091057619296, 0.011237179503860398, 0.016597312688827526), 56: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 57: (3.034893196641776, 0.014134263923135663, 0.015046232938766446), 58: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 59: (2.815390245351479, 0.0201984694568075, 0.016350775957107544), 60: (2.839192886611046, 0.01942873235920257, 0.006291621923446622), 61: (3.11326114958753, 0.012453194070788634, 0.007118368148803733), 62: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 63: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 64: (2.9669450703085363, 0.015780511185964227, 0.006942373514175426), 65: (1.4561753110822764, 0.17932542287163092, 0.0018079161643981267), 66: (2.698245957275624, 0.024463871275924615, 0.004881149530410744), 67: (3.2574274365048974, 0.009879295174801486, 0.00820679664611823), 68: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 69: (2.7540433641090347, 0.022328640184350632, 0.005718529224395752), 70: (1.7046740075279663, 0.12244931858753924, 0.0026186466217040794), 71: (2.497536079891916, 0.03399900051068607, 0.005833053588867154), 72: (3.2582103359915013, 0.009866934085741942, 0.00625405311584476), 73: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 74: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 75: (2.9056865701533283, 0.017433224383815584, 0.0073753297328948975), 76: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 77: (2.690496370487274, 0.024776364575125966, 0.007022315263748191), 78: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 79: (2.24868660308401, 0.051112820961218254, 0.005518060922622692), 80: (2.711303188253064, 0.023946347728852103, 0.014744377136230447), 81: (3.461143738892126, 0.0071478995398634075, 0.012177711725235019), 82: (2.0203259378538676, 0.07408300087981214, 0.005611547827720664), 83: (2.636527310779811, 0.027067437024558542, 0.005813160538673423), 84: (2.4655943021651554, 0.03582830649843939, 0.00879108607769008), 85: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 86: (2.4655943021651554, 0.03582830649843939, 0.00879108607769008), 87: (1.5642090515574891, 0.1522070860796841, 0.005417627096176103), 88: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 89: (1.2679911142067402, 0.2366197689945075, 0.003012627363204956), 90: (2.7518234768895606, 0.022409867045439586, 0.007333815097808838), 91: (2.7015366476263636, 0.024332387034338154, 0.005179184675216719), 92: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 93: (2.5476090434245555, 0.03131735666801436, 0.005592763423919678), 94: (3.5406372105246993, 0.006307798427916721, 0.008065050840377896), 95: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 96: (2.0803944850609946, 0.06722217924025395, 0.005379730463027976), 97: (3.7145350378047395, 0.0048111106743607066, 0.008877068758010864), 98: (2.9523209650575724, 0.016159868571200005, 0.0082339346408844), 99: (3.4755318428902466, 0.006987576347940334, 0.007993972301483188), 100: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 101: (2.6128540996488807, 0.028138744989167987, 0.008272904157638539), 102: (3.626378017474533, 0.005516655986738787, 0.009155404567718517), 103: (2.932885811474453, 0.016678549378825964, 0.00849389433860781), 104: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 105: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 106: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 107: (-0.0016871017238068403, 0.9986906919628202, -5.674362182572779e-06), 108: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 109: (0.2850577485601285, 0.7820509244092041, 0.0009132444858551692), 110: (1.148653444973492, 0.28031053394824124, 0.002852267026901223), 111: (0.4914509220986568, 0.6348698895571083, 0.0012215912342071755), 112: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 113: (0.202365480741529, 0.844131589418614, 0.000528854131698675), 114: (0.39156709903185344, 0.7044876319585958, 0.0013040482997894731), 115: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 116: (0.11173623981498304, 0.9134848656399938, 0.0003580749034881592), 117: (0.5211298502601197, 0.6148548745782575, 0.0017965734004974365), 118: (0.5718106056858795, 0.5814483781183453, 0.001604634523391768), 119: (0.6256134362802805, 0.5471030883124561, 0.0020198166370392068), 120: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 121: (0.597677215553958, 0.5647888152550127, 0.0016816914081573264), 122: (0.762842554036261, 0.4650810698386909, 0.002107131481170643), 123: (0.8334508277291102, 0.42615579589916486, 0.0021722793579102007), 124: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 125: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 126: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 127: (4.2173074188264605, 0.002248379496694524, 0.0185785233974457), 128: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 129: (3.776966159094484, 0.004369360251735088, 0.020553475618362405), 130: (3.8910995144390377, 0.003668804882592309, 0.014036139845848061), 131: (2.2774326269052807, 0.04876680009354322, 0.010569620132446333), 132: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 133: (3.025012628087391, 0.014362223259487624, 0.014482587575912476), 134: (3.8711233221875028, 0.003782278370286225, 0.017132323980331376), 135: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 136: (3.3154945767064916, 0.009004718042169724, 0.014977973699569658), 137: (4.650113863752602, 0.0012019215246106532, 0.021829116344451838), 138: (3.6557698313769347, 0.005270047017565919, 0.018977212905883722), 139: (5.010929337147533, 0.0007280326694162593, 0.021607166528701816), 140: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 141: (4.510406264374862, 0.001466853700376692, 0.019268441200256337), 142: (4.868648511259469, 0.0008851704002848073, 0.016163876652717568), 143: (4.967373579495973, 0.0007726779571079604, 0.0189771890640259)}, 'original_noun_phrase': {0: (31.041602740335975, 2.2266901078377605e-162, 0.010461641165117441)}}\n",
            "EI_sadness dann named\n",
            "named {0: (2.01532610131715, 0.05823952037759897, 0.0028388768434524647), 1: (2.3937613389328503, 0.027154986646530727, 0.005228689312934853), 2: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 3: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 4: (2.426389194977716, 0.025370888472331172, 0.00452912151813506), 5: (2.3297592966902, 0.030998683237986997, 0.004616954922676109), 6: (2.4405702855153164, 0.02463011044813779, 0.0066686451435089555), 7: (1.416219786976402, 0.1728972332063728, 0.0020775735378265603), 8: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 9: (2.11143569677307, 0.048208894467454196, 0.004332017898559526), 10: (1.24517038514806, 0.2282079619182438, 0.000608849525451649), 11: (2.3606027800539082, 0.02908731423116455, 0.005291995406150829), 12: (2.01593781649579, 0.05817014609882399, 0.003174147009849526), 13: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 14: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 15: (1.8072895125560662, 0.08657853214144638, 0.0030585378408432007), 16: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 17: (1.2188731534410913, 0.23780987192114464, 0.001443845033645641), 18: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 19: (2.13736459397006, 0.04578514782518903, 0.003176182508468628), 20: (2.164109208244593, 0.043401754689803555, 0.010798591375350941), 21: (1.6517520430579264, 0.11501617805171431, 0.005485408008098602), 22: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 23: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 24: (2.176314111631599, 0.04235215429215568, 0.00798928886651995), 25: (1.7429509364768534, 0.09750390333213964, 0.005237537622451827), 26: (2.2665373380547083, 0.035285206016914664, 0.01138500571250911), 27: (0.7054593449252535, 0.4890840053264909, 0.0017155617475509422), 28: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 29: (1.9580768996614084, 0.06506953614003887, 0.007999703288078308), 30: (1.6680398867674768, 0.11170491203935211, 0.006096550822258018), 31: (1.9502761436575369, 0.06605348934337371, 0.008092659711837757), 32: (2.038245653280442, 0.05569036147685382, 0.00889438986778257), 33: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 34: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 35: (2.2132863361613513, 0.03931257154751434, 0.009669522941112507), 36: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 37: (1.9643642516473796, 0.06428600071253636, 0.006490138173103344), 38: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 39: (2.1117255573522806, 0.0481811676145444, 0.009024541079998005), 40: (2.0202140444542622, 0.05768724869350837, 0.0076436251401901245), 41: (1.3963430625753228, 0.17871227383506366, 0.003674662113189675), 42: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 43: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 44: (2.036378888496478, 0.055894170684916984, 0.005913336575031292), 45: (1.6288618613030326, 0.11981032806296979, 0.0035395383834838645), 46: (2.0142430573881716, 0.05836252983789694, 0.0068226397037506326), 47: (-1.0170673951861304, 0.3218984735820919, -0.0025250256061554177), 48: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 49: (1.7295258552294348, 0.09992824728740321, 0.005457466840743996), 50: (0.8065240214084098, 0.42992105654917767, 0.001941654086112965), 51: (1.9540167850045052, 0.06558002054843416, 0.0070449799299240334), 52: (1.6923123458249807, 0.10692133007251438, 0.004972368478775024), 53: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 54: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 55: (2.0298190221000896, 0.05661569245948826, 0.006736303865909565), 56: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 57: (1.5868120525906177, 0.12905790921482516, 0.004140032827854134), 58: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 59: (1.7529241918818776, 0.09573592190747847, 0.005203597247600555), 60: (-2.1724247453840166, 0.04268409010139422, -0.004670247435569763), 61: (-2.053375634010933, 0.05406302117809778, -0.0032308146357536427), 62: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 63: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 64: (-2.0635792197542355, 0.052989872553715775, -0.0032930344343185536), 65: (-1.9388825291268506, 0.06751441908581773, -0.00319840610027311), 66: (-2.145999025135268, 0.04500295945127394, -0.004016786813735962), 67: (-1.8407659386046358, 0.08132772532097549, -0.0035571694374084695), 68: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 69: (-2.2469269310086237, 0.03672199187058787, -0.003964000940322898), 70: (-1.7500687431562394, 0.09623926067370431, -0.0019540101289748923), 71: (-2.118770958586775, 0.04751166764970733, -0.0032920002937316672), 72: (-1.7269327925044, 0.10040244767592066, -0.0024043619632720947), 73: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 74: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 75: (-1.9528039182617378, 0.06573320622283765, -0.002909339964389801), 76: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 77: (-2.0920171213945404, 0.05009967174142531, -0.0023684099316597096), 78: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 79: (-1.9239583107104954, 0.06947144811174345, -0.0020939484238624573), 80: (-1.9991168592591095, 0.060105039270328696, -0.005574475228786457), 81: (1.9148777496002638, 0.07068666076787984, 0.007803726196289051), 82: (-1.1946232401436556, 0.24693464386413358, -0.0009208932518959267), 83: (-0.5529496291589154, 0.5867455751417274, -0.0005741193890571594), 84: (-1.8157868859926778, 0.08521842185070586, -0.0021478980779647827), 85: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 86: (-1.8157868859926778, 0.08521842185070586, -0.0021478980779647827), 87: (-1.655716333627156, 0.11420266146388568, -0.0019031599164008872), 88: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 89: (-1.6914418611041173, 0.10708980588289103, -0.0021917402744293657), 90: (-0.11672843707965531, 0.9083002780071767, -8.38652253151162e-05), 91: (-1.1929590220244564, 0.24757045002732167, -0.0011555373668670765), 92: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 93: (-1.585995832035679, 0.12924318546206304, -0.0017868906259536632), 94: (-0.21888448298201132, 0.8290741064818753, -0.0001829683780670277), 95: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 96: (-1.4381877173421733, 0.1666492015369222, -0.0014763176441192627), 97: (-1.3366598449217524, 0.1971203456188626, -0.0011707395315170288), 98: (-1.0740606540032465, 0.296247294002853, -0.0009630531072616355), 99: (-1.2068922613930937, 0.24228542206416326, -0.0007560327649116516), 100: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 101: (-2.162748627591851, 0.04352022071524088, -0.0031302958726883157), 102: (-1.489069034507623, 0.15288111178888808, -0.0010386317968368308), 103: (0.4879570829901477, 0.6311636218014774, 0.00037415921688077614), 104: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 105: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 106: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 107: (-1.8329722874020595, 0.08252461492946188, -0.0030451223254203685), 108: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 109: (-2.207693062964958, 0.03975920328626011, -0.0060792297124863115), 110: (-0.8567234573029678, 0.4022743766450734, -0.001001754403114341), 111: (-1.8489248817913835, 0.08009106389357762, -0.0034164220094680675), 112: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 113: (-2.022022549360925, 0.05748410630394622, -0.0035925269126891868), 114: (-2.058580376498248, 0.05351319540114414, -0.004383644461631753), 115: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 116: (-2.0918168873748777, 0.050119513423664816, -0.0043361157178878895), 117: (-1.8880092509539255, 0.07439327390504862, -0.0033483386039733776), 118: (-1.8845179117026338, 0.07488730328602192, -0.003114837408065785), 119: (-1.4077962655542242, 0.17534266557861433, -0.0020655572414398082), 120: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 121: (-0.8477953606071333, 0.4071055922693776, -0.001069238781928994), 122: (-1.4542216688839156, 0.16220561384271925, -0.0020685195922851562), 123: (-0.7631757474097437, 0.454730854196873, -0.0009519368410110252), 124: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 125: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 126: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 127: (2.091691171130639, 0.05013197459014095, 0.004751838743686676), 128: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 129: (0.22604622603829608, 0.8235793506895409, 0.0003679931163788064), 130: (2.2922422033695056, 0.03348032202314858, 0.005952678620815277), 131: (1.79555298972163, 0.0884882159161502, 0.0035826370120048967), 132: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 133: (0.4395252360809602, 0.6652393790344514, 0.00045618414878845215), 134: (1.8653859226823029, 0.07764618740447601, 0.004578793048858665), 135: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 136: (1.5533996276693478, 0.13682655089143395, 0.003414085507392861), 137: (1.6481172478840815, 0.11576639986583508, 0.0031228095293044933), 138: (2.1212558186185766, 0.04727756169697215, 0.005397960543632507), 139: (1.6916909059557774, 0.10704158193148378, 0.0034637153148651123), 140: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 141: (2.0521489793761125, 0.05419334328703193, 0.004835659265518211), 142: (2.2843473063096478, 0.03402539210292163, 0.007252700626850128), 143: (2.3434558118307067, 0.03013596737161015, 0.007905696332454692)}\n",
            "EI_sadness dann noun_phrase\n",
            "noun_phrase {0: (2.7928348399845992, 0.02095653855450993, 0.009369206428527788), 1: (2.4825458430697602, 0.03484558612922075, 0.006868648529052779), 2: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 3: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 4: (1.8794675129502518, 0.09288520951504105, 0.0053334832191467285), 5: (1.219989457682996, 0.25347169411169973, 0.0045654237270355), 6: (1.9960019785135974, 0.07704779239938649, 0.005563408136367798), 7: (1.7306079537933463, 0.1175728030364332, 0.00759871602058404), 8: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 9: (3.4834750370425698, 0.006900683327810753, 0.009609246253967196), 10: (2.5924618483451045, 0.02909574678015594, 0.007367825508117765), 11: (2.7953362548751906, 0.020871059544309557, 0.006409856677055403), 12: (2.1016549387940735, 0.06494362432702382, 0.006048253178596474), 13: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 14: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 15: (2.8663712240341725, 0.018586317783867108, 0.00808035135269164), 16: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 17: (1.4226751415459677, 0.1885524881969071, 0.003962308168411255), 18: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 19: (2.8811647274014542, 0.018143551348810388, 0.0064402103424072155), 20: (1.2556551355654462, 0.24085969882602667, 0.008592197299003623), 21: (0.8192182566574282, 0.4338185494672727, 0.005452871322631836), 22: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 23: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 24: (0.7392531250297657, 0.47858976225286753, 0.004844743013382002), 25: (0.233867260298727, 0.8203210664518852, 0.0013445198535918523), 26: (0.6132645153604004, 0.5548811109657461, 0.0033368468284606934), 27: (0.9082148293525399, 0.38743743568611744, 0.0030472695827483687), 28: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 29: (0.6974568744262327, 0.5031345430587105, 0.00404183566570282), 30: (1.1111425905488965, 0.29530809018706083, 0.005777087807655401), 31: (1.16375891274626, 0.27444487939113144, 0.008001649379730247), 32: (0.4919021572730726, 0.6345631666685482, 0.0034994781017303467), 33: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 34: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 35: (0.39284214929279937, 0.7035784983380782, 0.0032848954200744296), 36: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 37: (0.2943463250143888, 0.7751692720323028, 0.0020151317119598278), 38: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 39: (0.7515970584041647, 0.4714896875247243, 0.006257641315460194), 40: (1.9945685680042773, 0.07722599577720352, 0.011468788981437772), 41: (1.8283585583843929, 0.100757866179108, 0.009940785169601452), 42: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 43: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 44: (1.4015295999610728, 0.19458579777046728, 0.008382248878478993), 45: (0.9456705601318364, 0.3690119307893027, 0.0041120052337646484), 46: (1.4931575226360712, 0.1695986699885476, 0.00494876503944397), 47: (0.9119512608914013, 0.38557024297308085, 0.004036653041839644), 48: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 49: (1.7729505719066518, 0.10999334378301455, 0.007531911134719849), 50: (2.275759927636859, 0.04890036967411337, 0.008783435821533181), 51: (1.9529107903724439, 0.08257973794432565, 0.009775882959365823), 52: (1.0998215140874008, 0.2999564894963836, 0.0051892101764678955), 53: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 54: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 55: (1.5999303300794778, 0.1440782652838534, 0.007100117206573497), 56: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 57: (1.0451172089021257, 0.3232257756949092, 0.003931707143783547), 58: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 59: (1.649514996205371, 0.13344141223553385, 0.008565112948417664), 60: (1.3295881275022043, 0.21636716175173012, 0.0024265706539154386), 61: (1.4173873856403525, 0.19004586466961876, 0.002405273914337147), 62: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 63: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 64: (1.2900967106111363, 0.2291768979657518, 0.0023651361465454324), 65: (-0.6394964562408798, 0.5384349236163265, -0.000882583856582686), 66: (0.9342767272221216, 0.3745482158358, 0.0027553081512451394), 67: (1.0726513518879688, 0.3113456338090456, 0.0038314461708068848), 68: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 69: (-0.5696281779426863, 0.5828662636022648, -0.0006015062332153098), 70: (-0.2745948075034577, 0.789826666639445, -0.00036676228046417236), 71: (0.7238457528963413, 0.4875475233179146, 0.0011965095996856467), 72: (0.29145151643111444, 0.7773117660790252, 0.00044726133346556507), 73: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 74: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 75: (2.3429201321187887, 0.04381023422088989, 0.0038973510265349898), 76: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 77: (0.9711736549982152, 0.3568371376290034, 0.0013588905334472878), 78: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 79: (1.5745269444394807, 0.1498180017765327, 0.002507019042968739), 80: (1.647618663053722, 0.13383478360750184, 0.011279281973838817), 81: (1.6738667774425795, 0.12848235166552172, 0.010548812150955211), 82: (1.1621081060048455, 0.27508109068113357, 0.002224203944206249), 83: (-0.25193420792078114, 0.8067507294293463, -0.0005240231752395297), 84: (0.0412125746255518, 0.968026221265819, 0.00013266801834105335), 85: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 86: (0.0412125746255518, 0.968026221265819, 0.00013266801834105335), 87: (-0.720503825232584, 0.48950445267006704, -0.002070850133895852), 88: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 89: (-1.3861570545292106, 0.19907553981411108, -0.004951936006545976), 90: (-0.41728301673572293, 0.6862493445482392, -0.001135048270225536), 91: (-0.25408668538698265, 0.8051384178876207, -0.000785475969314553), 92: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 93: (-0.8060328331453194, 0.441000463827958, -0.0032617926597595104), 94: (0.23516574824941155, 0.8193435829365103, 0.0006491661071777455), 95: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 96: (0.023711144409944818, 0.9816004121274581, 8.725523948671654e-05), 97: (-0.030663587039654484, 0.9762070583974061, -0.00010018348693846546), 98: (0.43235741452339665, 0.6756562027226033, 0.0015580117702483909), 99: (1.7325756622545776, 0.11721017975366686, 0.003394353389739979), 100: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 101: (-0.7524762354220925, 0.4709866065858548, -0.002182847261428811), 102: (1.225072725136357, 0.2516417794579439, 0.003177434206008911), 103: (-0.3083385384218838, 0.7648420365418974, -0.0006639301776886208), 104: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 105: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 106: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 107: (1.1827276187047462, 0.26721883946446306, 0.0033135801553725863), 108: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 109: (0.4677473899243689, 0.6510845019015793, 0.0023932158946990967), 110: (1.5543537521556468, 0.1545207289732867, 0.004358538985252347), 111: (0.33219457880664965, 0.7473483386668133, 0.0012724012136459129), 112: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 113: (0.5554129924996416, 0.5921477184652227, 0.0020010501146316417), 114: (0.5985961044900063, 0.5642019814694244, 0.002261972427368142), 115: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 116: (0.33314913057822887, 0.7466514536968696, 0.0012956619262695646), 117: (0.665602535423863, 0.5223561435016453, 0.002299004793167081), 118: (0.5939834392591454, 0.567151259835008, 0.0021283745765686146), 119: (0.9116082024107136, 0.3857414089404295, 0.0024249464273452537), 120: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 121: (0.6278400661121015, 0.545707397946765, 0.001571732759475697), 122: (0.8226113127614556, 0.4319832956577777, 0.002193671464920055), 123: (1.540657301774562, 0.15778809242077027, 0.003776147961616516), 124: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 125: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 126: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 127: (1.373430046054174, 0.2028595056854633, 0.006783539056777921), 128: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 129: (1.563314433817932, 0.15241582591032174, 0.004777562618255682), 130: (0.46319681215618164, 0.6542200303688117, 0.002609437704086337), 131: (-1.563110616436469, 0.1524634180112219, -0.005553948879241988), 132: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 133: (-0.12147064624780947, 0.9059871249278382, -0.00040838122367858887), 134: (0.1549571102303764, 0.8802746457107468, 0.0008236527442931907), 135: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 136: (-0.16280450007914946, 0.8742692334005663, -0.0007202357053757158), 137: (1.3272705868823882, 0.21710188904661032, 0.00633070766925814), 138: (1.4925421439116233, 0.16975665927954725, 0.00777252912521359), 139: (2.202101872751473, 0.0551512016007396, 0.009638789296150252), 140: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 141: (1.6469051935356773, 0.13398305609894848, 0.006484252214431718), 142: (1.5642716309109284, 0.1521924940783004, 0.006479287147521995), 143: (1.5814696097453336, 0.14822936067621462, 0.010344752669334434)}\n",
            "EI_sadness dann original_noun_phrase\n",
            "original_noun_phrase {0: (13.300901960438123, 3.703059968082085e-38, 0.004469728635417114)}\n",
            "dann original_noun_phrase {'named': {0: (2.01532610131715, 0.05823952037759897, 0.0028388768434524647), 1: (2.3937613389328503, 0.027154986646530727, 0.005228689312934853), 2: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 3: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 4: (2.426389194977716, 0.025370888472331172, 0.00452912151813506), 5: (2.3297592966902, 0.030998683237986997, 0.004616954922676109), 6: (2.4405702855153164, 0.02463011044813779, 0.0066686451435089555), 7: (1.416219786976402, 0.1728972332063728, 0.0020775735378265603), 8: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 9: (2.11143569677307, 0.048208894467454196, 0.004332017898559526), 10: (1.24517038514806, 0.2282079619182438, 0.000608849525451649), 11: (2.3606027800539082, 0.02908731423116455, 0.005291995406150829), 12: (2.01593781649579, 0.05817014609882399, 0.003174147009849526), 13: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 14: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 15: (1.8072895125560662, 0.08657853214144638, 0.0030585378408432007), 16: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 17: (1.2188731534410913, 0.23780987192114464, 0.001443845033645641), 18: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 19: (2.13736459397006, 0.04578514782518903, 0.003176182508468628), 20: (2.164109208244593, 0.043401754689803555, 0.010798591375350941), 21: (1.6517520430579264, 0.11501617805171431, 0.005485408008098602), 22: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 23: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 24: (2.176314111631599, 0.04235215429215568, 0.00798928886651995), 25: (1.7429509364768534, 0.09750390333213964, 0.005237537622451827), 26: (2.2665373380547083, 0.035285206016914664, 0.01138500571250911), 27: (0.7054593449252535, 0.4890840053264909, 0.0017155617475509422), 28: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 29: (1.9580768996614084, 0.06506953614003887, 0.007999703288078308), 30: (1.6680398867674768, 0.11170491203935211, 0.006096550822258018), 31: (1.9502761436575369, 0.06605348934337371, 0.008092659711837757), 32: (2.038245653280442, 0.05569036147685382, 0.00889438986778257), 33: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 34: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 35: (2.2132863361613513, 0.03931257154751434, 0.009669522941112507), 36: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 37: (1.9643642516473796, 0.06428600071253636, 0.006490138173103344), 38: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 39: (2.1117255573522806, 0.0481811676145444, 0.009024541079998005), 40: (2.0202140444542622, 0.05768724869350837, 0.0076436251401901245), 41: (1.3963430625753228, 0.17871227383506366, 0.003674662113189675), 42: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 43: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 44: (2.036378888496478, 0.055894170684916984, 0.005913336575031292), 45: (1.6288618613030326, 0.11981032806296979, 0.0035395383834838645), 46: (2.0142430573881716, 0.05836252983789694, 0.0068226397037506326), 47: (-1.0170673951861304, 0.3218984735820919, -0.0025250256061554177), 48: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 49: (1.7295258552294348, 0.09992824728740321, 0.005457466840743996), 50: (0.8065240214084098, 0.42992105654917767, 0.001941654086112965), 51: (1.9540167850045052, 0.06558002054843416, 0.0070449799299240334), 52: (1.6923123458249807, 0.10692133007251438, 0.004972368478775024), 53: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 54: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 55: (2.0298190221000896, 0.05661569245948826, 0.006736303865909565), 56: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 57: (1.5868120525906177, 0.12905790921482516, 0.004140032827854134), 58: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 59: (1.7529241918818776, 0.09573592190747847, 0.005203597247600555), 60: (-2.1724247453840166, 0.04268409010139422, -0.004670247435569763), 61: (-2.053375634010933, 0.05406302117809778, -0.0032308146357536427), 62: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 63: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 64: (-2.0635792197542355, 0.052989872553715775, -0.0032930344343185536), 65: (-1.9388825291268506, 0.06751441908581773, -0.00319840610027311), 66: (-2.145999025135268, 0.04500295945127394, -0.004016786813735962), 67: (-1.8407659386046358, 0.08132772532097549, -0.0035571694374084695), 68: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 69: (-2.2469269310086237, 0.03672199187058787, -0.003964000940322898), 70: (-1.7500687431562394, 0.09623926067370431, -0.0019540101289748923), 71: (-2.118770958586775, 0.04751166764970733, -0.0032920002937316672), 72: (-1.7269327925044, 0.10040244767592066, -0.0024043619632720947), 73: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 74: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 75: (-1.9528039182617378, 0.06573320622283765, -0.002909339964389801), 76: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 77: (-2.0920171213945404, 0.05009967174142531, -0.0023684099316597096), 78: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 79: (-1.9239583107104954, 0.06947144811174345, -0.0020939484238624573), 80: (-1.9991168592591095, 0.060105039270328696, -0.005574475228786457), 81: (1.9148777496002638, 0.07068666076787984, 0.007803726196289051), 82: (-1.1946232401436556, 0.24693464386413358, -0.0009208932518959267), 83: (-0.5529496291589154, 0.5867455751417274, -0.0005741193890571594), 84: (-1.8157868859926778, 0.08521842185070586, -0.0021478980779647827), 85: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 86: (-1.8157868859926778, 0.08521842185070586, -0.0021478980779647827), 87: (-1.655716333627156, 0.11420266146388568, -0.0019031599164008872), 88: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 89: (-1.6914418611041173, 0.10708980588289103, -0.0021917402744293657), 90: (-0.11672843707965531, 0.9083002780071767, -8.38652253151162e-05), 91: (-1.1929590220244564, 0.24757045002732167, -0.0011555373668670765), 92: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 93: (-1.585995832035679, 0.12924318546206304, -0.0017868906259536632), 94: (-0.21888448298201132, 0.8290741064818753, -0.0001829683780670277), 95: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 96: (-1.4381877173421733, 0.1666492015369222, -0.0014763176441192627), 97: (-1.3366598449217524, 0.1971203456188626, -0.0011707395315170288), 98: (-1.0740606540032465, 0.296247294002853, -0.0009630531072616355), 99: (-1.2068922613930937, 0.24228542206416326, -0.0007560327649116516), 100: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 101: (-2.162748627591851, 0.04352022071524088, -0.0031302958726883157), 102: (-1.489069034507623, 0.15288111178888808, -0.0010386317968368308), 103: (0.4879570829901477, 0.6311636218014774, 0.00037415921688077614), 104: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 105: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 106: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 107: (-1.8329722874020595, 0.08252461492946188, -0.0030451223254203685), 108: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 109: (-2.207693062964958, 0.03975920328626011, -0.0060792297124863115), 110: (-0.8567234573029678, 0.4022743766450734, -0.001001754403114341), 111: (-1.8489248817913835, 0.08009106389357762, -0.0034164220094680675), 112: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 113: (-2.022022549360925, 0.05748410630394622, -0.0035925269126891868), 114: (-2.058580376498248, 0.05351319540114414, -0.004383644461631753), 115: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 116: (-2.0918168873748777, 0.050119513423664816, -0.0043361157178878895), 117: (-1.8880092509539255, 0.07439327390504862, -0.0033483386039733776), 118: (-1.8845179117026338, 0.07488730328602192, -0.003114837408065785), 119: (-1.4077962655542242, 0.17534266557861433, -0.0020655572414398082), 120: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 121: (-0.8477953606071333, 0.4071055922693776, -0.001069238781928994), 122: (-1.4542216688839156, 0.16220561384271925, -0.0020685195922851562), 123: (-0.7631757474097437, 0.454730854196873, -0.0009519368410110252), 124: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 125: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 126: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 127: (2.091691171130639, 0.05013197459014095, 0.004751838743686676), 128: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 129: (0.22604622603829608, 0.8235793506895409, 0.0003679931163788064), 130: (2.2922422033695056, 0.03348032202314858, 0.005952678620815277), 131: (1.79555298972163, 0.0884882159161502, 0.0035826370120048967), 132: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 133: (0.4395252360809602, 0.6652393790344514, 0.00045618414878845215), 134: (1.8653859226823029, 0.07764618740447601, 0.004578793048858665), 135: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 136: (1.5533996276693478, 0.13682655089143395, 0.003414085507392861), 137: (1.6481172478840815, 0.11576639986583508, 0.0031228095293044933), 138: (2.1212558186185766, 0.04727756169697215, 0.005397960543632507), 139: (1.6916909059557774, 0.10704158193148378, 0.0034637153148651123), 140: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 141: (2.0521489793761125, 0.05419334328703193, 0.004835659265518211), 142: (2.2843473063096478, 0.03402539210292163, 0.007252700626850128), 143: (2.3434558118307067, 0.03013596737161015, 0.007905696332454692)}, 'noun_phrase': {0: (2.7928348399845992, 0.02095653855450993, 0.009369206428527788), 1: (2.4825458430697602, 0.03484558612922075, 0.006868648529052779), 2: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 3: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 4: (1.8794675129502518, 0.09288520951504105, 0.0053334832191467285), 5: (1.219989457682996, 0.25347169411169973, 0.0045654237270355), 6: (1.9960019785135974, 0.07704779239938649, 0.005563408136367798), 7: (1.7306079537933463, 0.1175728030364332, 0.00759871602058404), 8: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 9: (3.4834750370425698, 0.006900683327810753, 0.009609246253967196), 10: (2.5924618483451045, 0.02909574678015594, 0.007367825508117765), 11: (2.7953362548751906, 0.020871059544309557, 0.006409856677055403), 12: (2.1016549387940735, 0.06494362432702382, 0.006048253178596474), 13: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 14: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 15: (2.8663712240341725, 0.018586317783867108, 0.00808035135269164), 16: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 17: (1.4226751415459677, 0.1885524881969071, 0.003962308168411255), 18: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 19: (2.8811647274014542, 0.018143551348810388, 0.0064402103424072155), 20: (1.2556551355654462, 0.24085969882602667, 0.008592197299003623), 21: (0.8192182566574282, 0.4338185494672727, 0.005452871322631836), 22: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 23: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 24: (0.7392531250297657, 0.47858976225286753, 0.004844743013382002), 25: (0.233867260298727, 0.8203210664518852, 0.0013445198535918523), 26: (0.6132645153604004, 0.5548811109657461, 0.0033368468284606934), 27: (0.9082148293525399, 0.38743743568611744, 0.0030472695827483687), 28: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 29: (0.6974568744262327, 0.5031345430587105, 0.00404183566570282), 30: (1.1111425905488965, 0.29530809018706083, 0.005777087807655401), 31: (1.16375891274626, 0.27444487939113144, 0.008001649379730247), 32: (0.4919021572730726, 0.6345631666685482, 0.0034994781017303467), 33: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 34: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 35: (0.39284214929279937, 0.7035784983380782, 0.0032848954200744296), 36: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 37: (0.2943463250143888, 0.7751692720323028, 0.0020151317119598278), 38: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 39: (0.7515970584041647, 0.4714896875247243, 0.006257641315460194), 40: (1.9945685680042773, 0.07722599577720352, 0.011468788981437772), 41: (1.8283585583843929, 0.100757866179108, 0.009940785169601452), 42: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 43: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 44: (1.4015295999610728, 0.19458579777046728, 0.008382248878478993), 45: (0.9456705601318364, 0.3690119307893027, 0.0041120052337646484), 46: (1.4931575226360712, 0.1695986699885476, 0.00494876503944397), 47: (0.9119512608914013, 0.38557024297308085, 0.004036653041839644), 48: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 49: (1.7729505719066518, 0.10999334378301455, 0.007531911134719849), 50: (2.275759927636859, 0.04890036967411337, 0.008783435821533181), 51: (1.9529107903724439, 0.08257973794432565, 0.009775882959365823), 52: (1.0998215140874008, 0.2999564894963836, 0.0051892101764678955), 53: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 54: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 55: (1.5999303300794778, 0.1440782652838534, 0.007100117206573497), 56: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 57: (1.0451172089021257, 0.3232257756949092, 0.003931707143783547), 58: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 59: (1.649514996205371, 0.13344141223553385, 0.008565112948417664), 60: (1.3295881275022043, 0.21636716175173012, 0.0024265706539154386), 61: (1.4173873856403525, 0.19004586466961876, 0.002405273914337147), 62: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 63: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 64: (1.2900967106111363, 0.2291768979657518, 0.0023651361465454324), 65: (-0.6394964562408798, 0.5384349236163265, -0.000882583856582686), 66: (0.9342767272221216, 0.3745482158358, 0.0027553081512451394), 67: (1.0726513518879688, 0.3113456338090456, 0.0038314461708068848), 68: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 69: (-0.5696281779426863, 0.5828662636022648, -0.0006015062332153098), 70: (-0.2745948075034577, 0.789826666639445, -0.00036676228046417236), 71: (0.7238457528963413, 0.4875475233179146, 0.0011965095996856467), 72: (0.29145151643111444, 0.7773117660790252, 0.00044726133346556507), 73: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 74: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 75: (2.3429201321187887, 0.04381023422088989, 0.0038973510265349898), 76: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 77: (0.9711736549982152, 0.3568371376290034, 0.0013588905334472878), 78: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 79: (1.5745269444394807, 0.1498180017765327, 0.002507019042968739), 80: (1.647618663053722, 0.13383478360750184, 0.011279281973838817), 81: (1.6738667774425795, 0.12848235166552172, 0.010548812150955211), 82: (1.1621081060048455, 0.27508109068113357, 0.002224203944206249), 83: (-0.25193420792078114, 0.8067507294293463, -0.0005240231752395297), 84: (0.0412125746255518, 0.968026221265819, 0.00013266801834105335), 85: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 86: (0.0412125746255518, 0.968026221265819, 0.00013266801834105335), 87: (-0.720503825232584, 0.48950445267006704, -0.002070850133895852), 88: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 89: (-1.3861570545292106, 0.19907553981411108, -0.004951936006545976), 90: (-0.41728301673572293, 0.6862493445482392, -0.001135048270225536), 91: (-0.25408668538698265, 0.8051384178876207, -0.000785475969314553), 92: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 93: (-0.8060328331453194, 0.441000463827958, -0.0032617926597595104), 94: (0.23516574824941155, 0.8193435829365103, 0.0006491661071777455), 95: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 96: (0.023711144409944818, 0.9816004121274581, 8.725523948671654e-05), 97: (-0.030663587039654484, 0.9762070583974061, -0.00010018348693846546), 98: (0.43235741452339665, 0.6756562027226033, 0.0015580117702483909), 99: (1.7325756622545776, 0.11721017975366686, 0.003394353389739979), 100: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 101: (-0.7524762354220925, 0.4709866065858548, -0.002182847261428811), 102: (1.225072725136357, 0.2516417794579439, 0.003177434206008911), 103: (-0.3083385384218838, 0.7648420365418974, -0.0006639301776886208), 104: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 105: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 106: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 107: (1.1827276187047462, 0.26721883946446306, 0.0033135801553725863), 108: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 109: (0.4677473899243689, 0.6510845019015793, 0.0023932158946990967), 110: (1.5543537521556468, 0.1545207289732867, 0.004358538985252347), 111: (0.33219457880664965, 0.7473483386668133, 0.0012724012136459129), 112: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 113: (0.5554129924996416, 0.5921477184652227, 0.0020010501146316417), 114: (0.5985961044900063, 0.5642019814694244, 0.002261972427368142), 115: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 116: (0.33314913057822887, 0.7466514536968696, 0.0012956619262695646), 117: (0.665602535423863, 0.5223561435016453, 0.002299004793167081), 118: (0.5939834392591454, 0.567151259835008, 0.0021283745765686146), 119: (0.9116082024107136, 0.3857414089404295, 0.0024249464273452537), 120: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 121: (0.6278400661121015, 0.545707397946765, 0.001571732759475697), 122: (0.8226113127614556, 0.4319832956577777, 0.002193671464920055), 123: (1.540657301774562, 0.15778809242077027, 0.003776147961616516), 124: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 125: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 126: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 127: (1.373430046054174, 0.2028595056854633, 0.006783539056777921), 128: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 129: (1.563314433817932, 0.15241582591032174, 0.004777562618255682), 130: (0.46319681215618164, 0.6542200303688117, 0.002609437704086337), 131: (-1.563110616436469, 0.1524634180112219, -0.005553948879241988), 132: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 133: (-0.12147064624780947, 0.9059871249278382, -0.00040838122367858887), 134: (0.1549571102303764, 0.8802746457107468, 0.0008236527442931907), 135: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 136: (-0.16280450007914946, 0.8742692334005663, -0.0007202357053757158), 137: (1.3272705868823882, 0.21710188904661032, 0.00633070766925814), 138: (1.4925421439116233, 0.16975665927954725, 0.00777252912521359), 139: (2.202101872751473, 0.0551512016007396, 0.009638789296150252), 140: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 141: (1.6469051935356773, 0.13398305609894848, 0.006484252214431718), 142: (1.5642716309109284, 0.1521924940783004, 0.006479287147521995), 143: (1.5814696097453336, 0.14822936067621462, 0.010344752669334434)}, 'original_noun_phrase': {0: (13.300901960438123, 3.703059968082085e-38, 0.004469728635417114)}}\n",
            "EI_sadness dann original_noun_phrase {'non_dann': {'named': {0: (2.0761933504409655, 0.05168978321260658, 0.0029319673776626587), 1: (2.0277227426424362, 0.05684802149348772, 0.0024926632642746083), 2: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 3: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 4: (2.2230811810478572, 0.03854150183671055, 0.0023595303297042625), 5: (2.0112032728744835, 0.05870902687924929, 0.001079910993576072), 6: (2.1806215388692425, 0.0419872916426721, 0.002160996198654175), 7: (2.046200341527533, 0.05482936063760963, 0.002086377143859841), 8: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 9: (2.0647403157958726, 0.05286898145200738, 0.001883125305175759), 10: (2.1844555404719377, 0.04166494885466178, 0.001084744930267334), 11: (2.3645098043050004, 0.028853162664240845, 0.0022659599781036377), 12: (1.5175673531338714, 0.145586931890424, 0.0010704368352890126), 13: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 14: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 15: (1.850029026805524, 0.0799249809879829, 0.001398104429244984), 16: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 17: (1.3749531839684273, 0.18514447446028678, 0.000617067515850056), 18: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 19: (2.285793149570828, 0.03392496127451366, 0.0012482210993766674), 20: (2.0167980614106953, 0.05807271163781755, 0.006875231862068176), 21: (1.5988626632260454, 0.12634834921979857, 0.004185765981674194), 22: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 23: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 24: (2.0546288368370345, 0.05393017024255865, 0.00546366870403292), 25: (1.7069376241864904, 0.10412447151487597, 0.0029117465019226074), 26: (2.174048713849316, 0.04254520564317553, 0.006127482652664162), 27: (1.7169097546770051, 0.10225364327072171, 0.0035044163465499656), 28: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 29: (2.019615061442269, 0.05775467206481685, 0.005211299657821633), 30: (1.9330421431608467, 0.06827436833571221, 0.004680752754211426), 31: (1.8648201405792866, 0.07772911882885637, 0.004628813266754128), 32: (1.8797381679169154, 0.07556833471115386, 0.004890909790992715), 33: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 34: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 35: (1.8528679297404185, 0.07949934680260273, 0.003478109836578369), 36: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 37: (1.8766312627773414, 0.0760139372792857, 0.00439032316207888), 38: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 39: (1.8984042073379923, 0.07293936206286997, 0.004009491205215432), 40: (1.20506807804393, 0.2429724396998576, 0.0018940389156341664), 41: (0.9762495415897414, 0.34120776495641103, 0.0010327056050300487), 42: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 43: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 44: (1.5638420023981912, 0.13435774633190134, 0.0014817893505096325), 45: (2.2668341225678175, 0.03526386490010458, 0.001438423991203308), 46: (1.6782686708220131, 0.10966722697059246, 0.0023178547620773537), 47: (1.310394631237746, 0.205683103215663, 0.0019497275352478027), 48: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 49: (2.0883570808072833, 0.05046347909114234, 0.0029065310955047607), 50: (0.6101184095267318, 0.5490118916170734, 0.0005402684211731068), 51: (1.1397273294732755, 0.26856489875868883, 0.0011490106582641713), 52: (1.16153920208516, 0.25980743666427664, 0.0012454301118850486), 53: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 54: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 55: (1.215851117000124, 0.23893278648489205, 0.0011919990181922802), 56: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 57: (1.151776444529962, 0.26370044738777193, 0.0009692117571830638), 58: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 59: (-0.28249320707092423, 0.7806197754390511, -0.00021852850914000355), 60: (1.762578428279938, 0.09405096323231914, 0.001563867926597573), 61: (2.0080108707067215, 0.0590749050332689, 0.002247375249862682), 62: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 63: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 64: (1.9615926117590434, 0.06463036269842726, 0.0018849849700927623), 65: (2.1399687369678557, 0.04554794710656452, 0.0025827020406723467), 66: (1.6637068337975134, 0.11257778388879425, 0.0015673279762268288), 67: (1.5051181251873107, 0.148737187493084, 0.001271191239357039), 68: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 69: (2.150386999841326, 0.044610134602192375, 0.0023679733276367188), 70: (2.177118213515657, 0.04228382339911018, 0.0028291165828704945), 71: (1.851836012904142, 0.07965383042593033, 0.0019317537546157948), 72: (2.126903002088096, 0.04674941447782704, 0.002539578080177296), 73: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 74: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 75: (2.0987947680643417, 0.049432237599766615, 0.002668848633766152), 76: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 77: (2.0930106082908817, 0.05000132978705774, 0.002096055448055245), 78: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 79: (2.2137610714328213, 0.03927487568851686, 0.002266727387905121), 80: (-1.2297251309277186, 0.23381072735893818, -0.0009175911545753479), 81: (1.5770916026973085, 0.13127891211348372, 0.0027214109897613636), 82: (-1.9033034001047069, 0.07226287284710739, -0.0013722851872444153), 83: (2.0248964150987545, 0.05716261700759523, 0.0013909146189689525), 84: (0.7499794368533861, 0.46245361722017764, 0.0003775402903556935), 85: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 86: (0.7499794368533861, 0.46245361722017764, 0.0003775402903556935), 87: (0.8267080693582394, 0.4186641253443355, 0.0003182619810104259), 88: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 89: (2.2402202926119856, 0.03722558439101236, 0.002292880415916465), 90: (2.0223167206031394, 0.057451123976569936, 0.001519373059272744), 91: (2.261036607936422, 0.03568291136125278, 0.002285835146904003), 92: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 93: (2.2804006018370484, 0.0343009354037606, 0.0016447007656097412), 94: (2.245660840473207, 0.03681657862016815, 0.002138602733612105), 95: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 96: (2.1620934635302254, 0.04357737110447765, 0.002022719383239724), 97: (2.2327728738093473, 0.03779223294436587, 0.0018279343843460305), 98: (2.06902232230206, 0.05242529547033867, 0.0018476366996765248), 99: (2.1544990001643627, 0.04424485304084849, 0.001421475410461448), 100: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 101: (1.5283930400747785, 0.14289255239327028, 0.0006372183561325073), 102: (2.115812886331568, 0.047791727971066664, 0.0013906747102737649), 103: (2.007473343935898, 0.05913671110023691, 0.0012248843908310159), 104: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 105: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 106: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 107: (0.0020121291648187547, 0.9984155306688118, 1.6719102859386048e-06), 108: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 109: (1.1050655128485936, 0.2829289719820194, 0.0009811639785766157), 110: (1.1368937517123074, 0.2697184801885618, 0.0011129364371299522), 111: (1.3696790666518044, 0.18675858532937661, 0.001445630192756675), 112: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 113: (1.4954607557796367, 0.15121951132493028, 0.0015002578496933205), 114: (0.36569502516584806, 0.7186324318973236, 0.0003187239170074352), 115: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 116: (0.5292246097569755, 0.6027786487650441, 0.00042713284492490455), 117: (0.5142802482705426, 0.6129861646620078, 0.00041597187519071266), 118: (1.4696636014771989, 0.1580178641681456, 0.0013391733169555442), 119: (0.7178184135979354, 0.48160281132001537, 0.0006729722023010032), 120: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 121: (0.8983249623027905, 0.38025569019303307, 0.0008771687746048085), 122: (1.0055171789009922, 0.3272827294300725, 0.0010290488600731007), 123: (1.2707327346974095, 0.21916305747624937, 0.0013425901532173046), 124: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 125: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 126: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 127: (1.5217129237402338, 0.14455022513035917, 0.0019379243254661338), 128: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 129: (1.9502220733005475, 0.06606035550226323, 0.003574529290199302), 130: (2.257082874008286, 0.035971313423737696, 0.0035756126046180836), 131: (2.006714851562998, 0.05922402288379378, 0.0032355964183807817), 132: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 133: (1.5921861739093308, 0.12784357900410126, 0.0019223928451538308), 134: (1.3387098671311615, 0.19646404439327628, 0.0020498543977737205), 135: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 136: (1.0242504981564373, 0.31858165840198993, 0.0012687504291534202), 137: (2.0459741649498424, 0.05485367511894138, 0.003114047646522511), 138: (1.8147542867908093, 0.08538270044406286, 0.0027948886156082264), 139: (1.246129678593975, 0.22786341362474055, 0.001814487576484669), 140: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 141: (1.59845966430835, 0.1264381837596209, 0.0017410978674888833), 142: (2.316546549269402, 0.03185256188279949, 0.004329253733158123), 143: (1.6008080372334728, 0.12591545072725402, 0.0018596306443214305)}, 'noun_phrase': {0: (4.416285964467372, 0.0016802065013952865, 0.0157653748989105), 1: (5.053638609689071, 0.0006869433416155954, 0.018537551164627075), 2: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 3: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 4: (2.6385147819837624, 0.026979388143290196, 0.010085684061050426), 5: (3.8865595174708805, 0.0036942739930930454, 0.012511950731277421), 6: (2.761585012181802, 0.022054913275819255, 0.008653420209884688), 7: (2.7105990080739124, 0.023973973042005197, 0.010546064376831032), 8: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 9: (3.0383359306990423, 0.014055712684936238, 0.011872017383575462), 10: (2.4775676124384827, 0.0351313640520545, 0.011167210340499945), 11: (2.30448516428659, 0.046655701485290364, 0.009044277667999312), 12: (3.076748054633628, 0.013209175998730217, 0.012794214487075795), 13: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 14: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 15: (3.1591619817473666, 0.011565798032961561, 0.01454648971557615), 16: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 17: (2.0391274779866837, 0.0718664088830766, 0.009420758485794045), 18: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 19: (3.1052140879654035, 0.012615876104423013, 0.01338036060333253), 20: (3.2659050260927107, 0.009746295218372463, 0.015316778421402), 21: (3.4949274347865944, 0.006777387618012797, 0.01486323475837703), 22: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 23: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 24: (3.0551497099124587, 0.01367848967421171, 0.013470512628555253), 25: (3.978960748655893, 0.00321083842861956, 0.014046573638915927), 26: (3.7146413003442715, 0.004810320006575784, 0.013544982671737649), 27: (2.9601293574172027, 0.01595616452942292, 0.01156689524650567), 28: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 29: (2.7294445130844096, 0.02324566456488856, 0.011514449119567893), 30: (2.729707535927594, 0.02323565995765799, 0.013818293809890747), 31: (3.4048450222022812, 0.007813134063093983, 0.012937533855438277), 32: (3.241925761631311, 0.01012738885873162, 0.013584524393081665), 33: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 34: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 35: (2.6942361002948796, 0.024625063165150767, 0.011142772436141979), 36: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 37: (2.40979128825808, 0.03926245654856135, 0.009671843051910434), 38: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 39: (3.0408490762270968, 0.013998655336479288, 0.014194577932357788), 40: (3.5282488522525606, 0.0064315987021017155, 0.020213115215301447), 41: (3.540509723288857, 0.006309059642745521, 0.0206573188304901), 42: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 43: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 44: (3.139242691532513, 0.011942581116984919, 0.018650662899017345), 45: (4.231198576921104, 0.0022027128241340296, 0.016053479909896917), 46: (3.5889064563962347, 0.005848767455705855, 0.014558517932891868), 47: (2.422097782700182, 0.03847802488113382, 0.012305146455764748), 48: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 49: (2.9479736158808096, 0.016274435480833523, 0.014781582355499201), 50: (3.098010984589562, 0.01276335748321234, 0.01675570011138916), 51: (3.109653253721714, 0.012525862890179026, 0.01677399277687075), 52: (2.9627464276545528, 0.015888481787377604, 0.016157895326614324), 53: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 54: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 55: (3.177091057619296, 0.011237179503860398, 0.016597312688827526), 56: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 57: (3.034893196641776, 0.014134263923135663, 0.015046232938766446), 58: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 59: (2.815390245351479, 0.0201984694568075, 0.016350775957107544), 60: (2.839192886611046, 0.01942873235920257, 0.006291621923446622), 61: (3.11326114958753, 0.012453194070788634, 0.007118368148803733), 62: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 63: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 64: (2.9669450703085363, 0.015780511185964227, 0.006942373514175426), 65: (1.4561753110822764, 0.17932542287163092, 0.0018079161643981267), 66: (2.698245957275624, 0.024463871275924615, 0.004881149530410744), 67: (3.2574274365048974, 0.009879295174801486, 0.00820679664611823), 68: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 69: (2.7540433641090347, 0.022328640184350632, 0.005718529224395752), 70: (1.7046740075279663, 0.12244931858753924, 0.0026186466217040794), 71: (2.497536079891916, 0.03399900051068607, 0.005833053588867154), 72: (3.2582103359915013, 0.009866934085741942, 0.00625405311584476), 73: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 74: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 75: (2.9056865701533283, 0.017433224383815584, 0.0073753297328948975), 76: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 77: (2.690496370487274, 0.024776364575125966, 0.007022315263748191), 78: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 79: (2.24868660308401, 0.051112820961218254, 0.005518060922622692), 80: (2.711303188253064, 0.023946347728852103, 0.014744377136230447), 81: (3.461143738892126, 0.0071478995398634075, 0.012177711725235019), 82: (2.0203259378538676, 0.07408300087981214, 0.005611547827720664), 83: (2.636527310779811, 0.027067437024558542, 0.005813160538673423), 84: (2.4655943021651554, 0.03582830649843939, 0.00879108607769008), 85: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 86: (2.4655943021651554, 0.03582830649843939, 0.00879108607769008), 87: (1.5642090515574891, 0.1522070860796841, 0.005417627096176103), 88: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 89: (1.2679911142067402, 0.2366197689945075, 0.003012627363204956), 90: (2.7518234768895606, 0.022409867045439586, 0.007333815097808838), 91: (2.7015366476263636, 0.024332387034338154, 0.005179184675216719), 92: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 93: (2.5476090434245555, 0.03131735666801436, 0.005592763423919678), 94: (3.5406372105246993, 0.006307798427916721, 0.008065050840377896), 95: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 96: (2.0803944850609946, 0.06722217924025395, 0.005379730463027976), 97: (3.7145350378047395, 0.0048111106743607066, 0.008877068758010864), 98: (2.9523209650575724, 0.016159868571200005, 0.0082339346408844), 99: (3.4755318428902466, 0.006987576347940334, 0.007993972301483188), 100: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 101: (2.6128540996488807, 0.028138744989167987, 0.008272904157638539), 102: (3.626378017474533, 0.005516655986738787, 0.009155404567718517), 103: (2.932885811474453, 0.016678549378825964, 0.00849389433860781), 104: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 105: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 106: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 107: (-0.0016871017238068403, 0.9986906919628202, -5.674362182572779e-06), 108: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 109: (0.2850577485601285, 0.7820509244092041, 0.0009132444858551692), 110: (1.148653444973492, 0.28031053394824124, 0.002852267026901223), 111: (0.4914509220986568, 0.6348698895571083, 0.0012215912342071755), 112: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 113: (0.202365480741529, 0.844131589418614, 0.000528854131698675), 114: (0.39156709903185344, 0.7044876319585958, 0.0013040482997894731), 115: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 116: (0.11173623981498304, 0.9134848656399938, 0.0003580749034881592), 117: (0.5211298502601197, 0.6148548745782575, 0.0017965734004974365), 118: (0.5718106056858795, 0.5814483781183453, 0.001604634523391768), 119: (0.6256134362802805, 0.5471030883124561, 0.0020198166370392068), 120: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 121: (0.597677215553958, 0.5647888152550127, 0.0016816914081573264), 122: (0.762842554036261, 0.4650810698386909, 0.002107131481170643), 123: (0.8334508277291102, 0.42615579589916486, 0.0021722793579102007), 124: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 125: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 126: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 127: (4.2173074188264605, 0.002248379496694524, 0.0185785233974457), 128: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 129: (3.776966159094484, 0.004369360251735088, 0.020553475618362405), 130: (3.8910995144390377, 0.003668804882592309, 0.014036139845848061), 131: (2.2774326269052807, 0.04876680009354322, 0.010569620132446333), 132: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 133: (3.025012628087391, 0.014362223259487624, 0.014482587575912476), 134: (3.8711233221875028, 0.003782278370286225, 0.017132323980331376), 135: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 136: (3.3154945767064916, 0.009004718042169724, 0.014977973699569658), 137: (4.650113863752602, 0.0012019215246106532, 0.021829116344451838), 138: (3.6557698313769347, 0.005270047017565919, 0.018977212905883722), 139: (5.010929337147533, 0.0007280326694162593, 0.021607166528701816), 140: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 141: (4.510406264374862, 0.001466853700376692, 0.019268441200256337), 142: (4.868648511259469, 0.0008851704002848073, 0.016163876652717568), 143: (4.967373579495973, 0.0007726779571079604, 0.0189771890640259)}, 'original_noun_phrase': {0: (31.041602740335975, 2.2266901078377605e-162, 0.010461641165117441)}}, 'dann': {'named': {0: (2.01532610131715, 0.05823952037759897, 0.0028388768434524647), 1: (2.3937613389328503, 0.027154986646530727, 0.005228689312934853), 2: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 3: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 4: (2.426389194977716, 0.025370888472331172, 0.00452912151813506), 5: (2.3297592966902, 0.030998683237986997, 0.004616954922676109), 6: (2.4405702855153164, 0.02463011044813779, 0.0066686451435089555), 7: (1.416219786976402, 0.1728972332063728, 0.0020775735378265603), 8: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 9: (2.11143569677307, 0.048208894467454196, 0.004332017898559526), 10: (1.24517038514806, 0.2282079619182438, 0.000608849525451649), 11: (2.3606027800539082, 0.02908731423116455, 0.005291995406150829), 12: (2.01593781649579, 0.05817014609882399, 0.003174147009849526), 13: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 14: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 15: (1.8072895125560662, 0.08657853214144638, 0.0030585378408432007), 16: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 17: (1.2188731534410913, 0.23780987192114464, 0.001443845033645641), 18: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 19: (2.13736459397006, 0.04578514782518903, 0.003176182508468628), 20: (2.164109208244593, 0.043401754689803555, 0.010798591375350941), 21: (1.6517520430579264, 0.11501617805171431, 0.005485408008098602), 22: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 23: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 24: (2.176314111631599, 0.04235215429215568, 0.00798928886651995), 25: (1.7429509364768534, 0.09750390333213964, 0.005237537622451827), 26: (2.2665373380547083, 0.035285206016914664, 0.01138500571250911), 27: (0.7054593449252535, 0.4890840053264909, 0.0017155617475509422), 28: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 29: (1.9580768996614084, 0.06506953614003887, 0.007999703288078308), 30: (1.6680398867674768, 0.11170491203935211, 0.006096550822258018), 31: (1.9502761436575369, 0.06605348934337371, 0.008092659711837757), 32: (2.038245653280442, 0.05569036147685382, 0.00889438986778257), 33: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 34: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 35: (2.2132863361613513, 0.03931257154751434, 0.009669522941112507), 36: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 37: (1.9643642516473796, 0.06428600071253636, 0.006490138173103344), 38: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 39: (2.1117255573522806, 0.0481811676145444, 0.009024541079998005), 40: (2.0202140444542622, 0.05768724869350837, 0.0076436251401901245), 41: (1.3963430625753228, 0.17871227383506366, 0.003674662113189675), 42: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 43: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 44: (2.036378888496478, 0.055894170684916984, 0.005913336575031292), 45: (1.6288618613030326, 0.11981032806296979, 0.0035395383834838645), 46: (2.0142430573881716, 0.05836252983789694, 0.0068226397037506326), 47: (-1.0170673951861304, 0.3218984735820919, -0.0025250256061554177), 48: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 49: (1.7295258552294348, 0.09992824728740321, 0.005457466840743996), 50: (0.8065240214084098, 0.42992105654917767, 0.001941654086112965), 51: (1.9540167850045052, 0.06558002054843416, 0.0070449799299240334), 52: (1.6923123458249807, 0.10692133007251438, 0.004972368478775024), 53: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 54: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 55: (2.0298190221000896, 0.05661569245948826, 0.006736303865909565), 56: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 57: (1.5868120525906177, 0.12905790921482516, 0.004140032827854134), 58: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 59: (1.7529241918818776, 0.09573592190747847, 0.005203597247600555), 60: (-2.1724247453840166, 0.04268409010139422, -0.004670247435569763), 61: (-2.053375634010933, 0.05406302117809778, -0.0032308146357536427), 62: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 63: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 64: (-2.0635792197542355, 0.052989872553715775, -0.0032930344343185536), 65: (-1.9388825291268506, 0.06751441908581773, -0.00319840610027311), 66: (-2.145999025135268, 0.04500295945127394, -0.004016786813735962), 67: (-1.8407659386046358, 0.08132772532097549, -0.0035571694374084695), 68: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 69: (-2.2469269310086237, 0.03672199187058787, -0.003964000940322898), 70: (-1.7500687431562394, 0.09623926067370431, -0.0019540101289748923), 71: (-2.118770958586775, 0.04751166764970733, -0.0032920002937316672), 72: (-1.7269327925044, 0.10040244767592066, -0.0024043619632720947), 73: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 74: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 75: (-1.9528039182617378, 0.06573320622283765, -0.002909339964389801), 76: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 77: (-2.0920171213945404, 0.05009967174142531, -0.0023684099316597096), 78: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 79: (-1.9239583107104954, 0.06947144811174345, -0.0020939484238624573), 80: (-1.9991168592591095, 0.060105039270328696, -0.005574475228786457), 81: (1.9148777496002638, 0.07068666076787984, 0.007803726196289051), 82: (-1.1946232401436556, 0.24693464386413358, -0.0009208932518959267), 83: (-0.5529496291589154, 0.5867455751417274, -0.0005741193890571594), 84: (-1.8157868859926778, 0.08521842185070586, -0.0021478980779647827), 85: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 86: (-1.8157868859926778, 0.08521842185070586, -0.0021478980779647827), 87: (-1.655716333627156, 0.11420266146388568, -0.0019031599164008872), 88: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 89: (-1.6914418611041173, 0.10708980588289103, -0.0021917402744293657), 90: (-0.11672843707965531, 0.9083002780071767, -8.38652253151162e-05), 91: (-1.1929590220244564, 0.24757045002732167, -0.0011555373668670765), 92: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 93: (-1.585995832035679, 0.12924318546206304, -0.0017868906259536632), 94: (-0.21888448298201132, 0.8290741064818753, -0.0001829683780670277), 95: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 96: (-1.4381877173421733, 0.1666492015369222, -0.0014763176441192627), 97: (-1.3366598449217524, 0.1971203456188626, -0.0011707395315170288), 98: (-1.0740606540032465, 0.296247294002853, -0.0009630531072616355), 99: (-1.2068922613930937, 0.24228542206416326, -0.0007560327649116516), 100: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 101: (-2.162748627591851, 0.04352022071524088, -0.0031302958726883157), 102: (-1.489069034507623, 0.15288111178888808, -0.0010386317968368308), 103: (0.4879570829901477, 0.6311636218014774, 0.00037415921688077614), 104: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 105: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 106: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 107: (-1.8329722874020595, 0.08252461492946188, -0.0030451223254203685), 108: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 109: (-2.207693062964958, 0.03975920328626011, -0.0060792297124863115), 110: (-0.8567234573029678, 0.4022743766450734, -0.001001754403114341), 111: (-1.8489248817913835, 0.08009106389357762, -0.0034164220094680675), 112: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 113: (-2.022022549360925, 0.05748410630394622, -0.0035925269126891868), 114: (-2.058580376498248, 0.05351319540114414, -0.004383644461631753), 115: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 116: (-2.0918168873748777, 0.050119513423664816, -0.0043361157178878895), 117: (-1.8880092509539255, 0.07439327390504862, -0.0033483386039733776), 118: (-1.8845179117026338, 0.07488730328602192, -0.003114837408065785), 119: (-1.4077962655542242, 0.17534266557861433, -0.0020655572414398082), 120: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 121: (-0.8477953606071333, 0.4071055922693776, -0.001069238781928994), 122: (-1.4542216688839156, 0.16220561384271925, -0.0020685195922851562), 123: (-0.7631757474097437, 0.454730854196873, -0.0009519368410110252), 124: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 125: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 126: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 127: (2.091691171130639, 0.05013197459014095, 0.004751838743686676), 128: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 129: (0.22604622603829608, 0.8235793506895409, 0.0003679931163788064), 130: (2.2922422033695056, 0.03348032202314858, 0.005952678620815277), 131: (1.79555298972163, 0.0884882159161502, 0.0035826370120048967), 132: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 133: (0.4395252360809602, 0.6652393790344514, 0.00045618414878845215), 134: (1.8653859226823029, 0.07764618740447601, 0.004578793048858665), 135: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 136: (1.5533996276693478, 0.13682655089143395, 0.003414085507392861), 137: (1.6481172478840815, 0.11576639986583508, 0.0031228095293044933), 138: (2.1212558186185766, 0.04727756169697215, 0.005397960543632507), 139: (1.6916909059557774, 0.10704158193148378, 0.0034637153148651123), 140: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 141: (2.0521489793761125, 0.05419334328703193, 0.004835659265518211), 142: (2.2843473063096478, 0.03402539210292163, 0.007252700626850128), 143: (2.3434558118307067, 0.03013596737161015, 0.007905696332454692)}, 'noun_phrase': {0: (2.7928348399845992, 0.02095653855450993, 0.009369206428527788), 1: (2.4825458430697602, 0.03484558612922075, 0.006868648529052779), 2: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 3: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 4: (1.8794675129502518, 0.09288520951504105, 0.0053334832191467285), 5: (1.219989457682996, 0.25347169411169973, 0.0045654237270355), 6: (1.9960019785135974, 0.07704779239938649, 0.005563408136367798), 7: (1.7306079537933463, 0.1175728030364332, 0.00759871602058404), 8: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 9: (3.4834750370425698, 0.006900683327810753, 0.009609246253967196), 10: (2.5924618483451045, 0.02909574678015594, 0.007367825508117765), 11: (2.7953362548751906, 0.020871059544309557, 0.006409856677055403), 12: (2.1016549387940735, 0.06494362432702382, 0.006048253178596474), 13: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 14: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 15: (2.8663712240341725, 0.018586317783867108, 0.00808035135269164), 16: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 17: (1.4226751415459677, 0.1885524881969071, 0.003962308168411255), 18: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 19: (2.8811647274014542, 0.018143551348810388, 0.0064402103424072155), 20: (1.2556551355654462, 0.24085969882602667, 0.008592197299003623), 21: (0.8192182566574282, 0.4338185494672727, 0.005452871322631836), 22: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 23: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 24: (0.7392531250297657, 0.47858976225286753, 0.004844743013382002), 25: (0.233867260298727, 0.8203210664518852, 0.0013445198535918523), 26: (0.6132645153604004, 0.5548811109657461, 0.0033368468284606934), 27: (0.9082148293525399, 0.38743743568611744, 0.0030472695827483687), 28: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 29: (0.6974568744262327, 0.5031345430587105, 0.00404183566570282), 30: (1.1111425905488965, 0.29530809018706083, 0.005777087807655401), 31: (1.16375891274626, 0.27444487939113144, 0.008001649379730247), 32: (0.4919021572730726, 0.6345631666685482, 0.0034994781017303467), 33: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 34: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 35: (0.39284214929279937, 0.7035784983380782, 0.0032848954200744296), 36: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 37: (0.2943463250143888, 0.7751692720323028, 0.0020151317119598278), 38: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 39: (0.7515970584041647, 0.4714896875247243, 0.006257641315460194), 40: (1.9945685680042773, 0.07722599577720352, 0.011468788981437772), 41: (1.8283585583843929, 0.100757866179108, 0.009940785169601452), 42: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 43: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 44: (1.4015295999610728, 0.19458579777046728, 0.008382248878478993), 45: (0.9456705601318364, 0.3690119307893027, 0.0041120052337646484), 46: (1.4931575226360712, 0.1695986699885476, 0.00494876503944397), 47: (0.9119512608914013, 0.38557024297308085, 0.004036653041839644), 48: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 49: (1.7729505719066518, 0.10999334378301455, 0.007531911134719849), 50: (2.275759927636859, 0.04890036967411337, 0.008783435821533181), 51: (1.9529107903724439, 0.08257973794432565, 0.009775882959365823), 52: (1.0998215140874008, 0.2999564894963836, 0.0051892101764678955), 53: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 54: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 55: (1.5999303300794778, 0.1440782652838534, 0.007100117206573497), 56: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 57: (1.0451172089021257, 0.3232257756949092, 0.003931707143783547), 58: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 59: (1.649514996205371, 0.13344141223553385, 0.008565112948417664), 60: (1.3295881275022043, 0.21636716175173012, 0.0024265706539154386), 61: (1.4173873856403525, 0.19004586466961876, 0.002405273914337147), 62: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 63: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 64: (1.2900967106111363, 0.2291768979657518, 0.0023651361465454324), 65: (-0.6394964562408798, 0.5384349236163265, -0.000882583856582686), 66: (0.9342767272221216, 0.3745482158358, 0.0027553081512451394), 67: (1.0726513518879688, 0.3113456338090456, 0.0038314461708068848), 68: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 69: (-0.5696281779426863, 0.5828662636022648, -0.0006015062332153098), 70: (-0.2745948075034577, 0.789826666639445, -0.00036676228046417236), 71: (0.7238457528963413, 0.4875475233179146, 0.0011965095996856467), 72: (0.29145151643111444, 0.7773117660790252, 0.00044726133346556507), 73: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 74: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 75: (2.3429201321187887, 0.04381023422088989, 0.0038973510265349898), 76: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 77: (0.9711736549982152, 0.3568371376290034, 0.0013588905334472878), 78: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 79: (1.5745269444394807, 0.1498180017765327, 0.002507019042968739), 80: (1.647618663053722, 0.13383478360750184, 0.011279281973838817), 81: (1.6738667774425795, 0.12848235166552172, 0.010548812150955211), 82: (1.1621081060048455, 0.27508109068113357, 0.002224203944206249), 83: (-0.25193420792078114, 0.8067507294293463, -0.0005240231752395297), 84: (0.0412125746255518, 0.968026221265819, 0.00013266801834105335), 85: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 86: (0.0412125746255518, 0.968026221265819, 0.00013266801834105335), 87: (-0.720503825232584, 0.48950445267006704, -0.002070850133895852), 88: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 89: (-1.3861570545292106, 0.19907553981411108, -0.004951936006545976), 90: (-0.41728301673572293, 0.6862493445482392, -0.001135048270225536), 91: (-0.25408668538698265, 0.8051384178876207, -0.000785475969314553), 92: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 93: (-0.8060328331453194, 0.441000463827958, -0.0032617926597595104), 94: (0.23516574824941155, 0.8193435829365103, 0.0006491661071777455), 95: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 96: (0.023711144409944818, 0.9816004121274581, 8.725523948671654e-05), 97: (-0.030663587039654484, 0.9762070583974061, -0.00010018348693846546), 98: (0.43235741452339665, 0.6756562027226033, 0.0015580117702483909), 99: (1.7325756622545776, 0.11721017975366686, 0.003394353389739979), 100: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 101: (-0.7524762354220925, 0.4709866065858548, -0.002182847261428811), 102: (1.225072725136357, 0.2516417794579439, 0.003177434206008911), 103: (-0.3083385384218838, 0.7648420365418974, -0.0006639301776886208), 104: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 105: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 106: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 107: (1.1827276187047462, 0.26721883946446306, 0.0033135801553725863), 108: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 109: (0.4677473899243689, 0.6510845019015793, 0.0023932158946990967), 110: (1.5543537521556468, 0.1545207289732867, 0.004358538985252347), 111: (0.33219457880664965, 0.7473483386668133, 0.0012724012136459129), 112: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 113: (0.5554129924996416, 0.5921477184652227, 0.0020010501146316417), 114: (0.5985961044900063, 0.5642019814694244, 0.002261972427368142), 115: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 116: (0.33314913057822887, 0.7466514536968696, 0.0012956619262695646), 117: (0.665602535423863, 0.5223561435016453, 0.002299004793167081), 118: (0.5939834392591454, 0.567151259835008, 0.0021283745765686146), 119: (0.9116082024107136, 0.3857414089404295, 0.0024249464273452537), 120: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 121: (0.6278400661121015, 0.545707397946765, 0.001571732759475697), 122: (0.8226113127614556, 0.4319832956577777, 0.002193671464920055), 123: (1.540657301774562, 0.15778809242077027, 0.003776147961616516), 124: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 125: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 126: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 127: (1.373430046054174, 0.2028595056854633, 0.006783539056777921), 128: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 129: (1.563314433817932, 0.15241582591032174, 0.004777562618255682), 130: (0.46319681215618164, 0.6542200303688117, 0.002609437704086337), 131: (-1.563110616436469, 0.1524634180112219, -0.005553948879241988), 132: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 133: (-0.12147064624780947, 0.9059871249278382, -0.00040838122367858887), 134: (0.1549571102303764, 0.8802746457107468, 0.0008236527442931907), 135: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 136: (-0.16280450007914946, 0.8742692334005663, -0.0007202357053757158), 137: (1.3272705868823882, 0.21710188904661032, 0.00633070766925814), 138: (1.4925421439116233, 0.16975665927954725, 0.00777252912521359), 139: (2.202101872751473, 0.0551512016007396, 0.009638789296150252), 140: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 141: (1.6469051935356773, 0.13398305609894848, 0.006484252214431718), 142: (1.5642716309109284, 0.1521924940783004, 0.006479287147521995), 143: (1.5814696097453336, 0.14822936067621462, 0.010344752669334434)}, 'original_noun_phrase': {0: (13.300901960438123, 3.703059968082085e-38, 0.004469728635417114)}}}\n",
            "EI_anger non_dann named\n",
            "named {0: (2.9999473312352563, 0.007362582726028531, 0.008020885288715363), 1: (2.8668234774506334, 0.009871819488213418, 0.007182353734970115), 2: (3.00154155812073, 0.007336638454529805, 0.007494708895683266), 3: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 4: (2.853858915689559, 0.010156129568114373, 0.007833994925022125), 5: (2.828059928865708, 0.010745468775168335, 0.007886649668216728), 6: (3.02450282733906, 0.0069726453063536685, 0.009996812045574177), 7: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 8: (2.8598644963104904, 0.010023463730840692, 0.008409228920936596), 9: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 10: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 11: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 12: (2.5998013893153016, 0.017592679093099937, 0.005481539666652657), 13: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 14: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 15: (2.9326297210120362, 0.008542831810533981, 0.005548548698425304), 16: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 17: (2.940704275103277, 0.008392164112157185, 0.0060208141803741455), 18: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 19: (2.287671406170058, 0.03379490281125083, 0.0036009758710860984), 20: (3.0413286735531093, 0.006717086069541028, 0.014911824464797996), 21: (3.069448373548996, 0.006310132602520979, 0.0162633419036865), 22: (3.0021290719939473, 0.007327099649944999, 0.012680222094059002), 23: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 24: (3.09062885600122, 0.006019513433223663, 0.01488557308912275), 25: (2.956577974398133, 0.008103409424171643, 0.010112801194191001), 26: (2.9864008493337164, 0.007586648071914168, 0.011366653442382801), 27: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 28: (2.973399068655112, 0.0078078980268998345, 0.012191291153430928), 29: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 30: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 31: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 32: (3.034858735548319, 0.006814261596819409, 0.014548160135746002), 33: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 34: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 35: (2.9552849489291804, 0.008126567023332459, 0.013237571716308572), 36: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 37: (2.936128540797658, 0.008477227650781594, 0.011788879334926616), 38: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 39: (2.7435982420580585, 0.012912897795710858, 0.009320780634880066), 40: (3.0405868166075836, 0.006728159852842062, 0.014032751321792603), 41: (3.060608797193966, 0.006435410659359709, 0.014685365557670582), 42: (2.9596167573971948, 0.008049236583012171, 0.012382332980632793), 43: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 44: (3.0580771508201217, 0.00647173267937202, 0.012605533003807068), 45: (2.82915858949088, 0.010719715967741848, 0.007928590476512898), 46: (2.9653970838404464, 0.007947153849508864, 0.01044829487800597), 47: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 48: (2.6817851636071484, 0.014757382691585958, 0.010272590816020943), 49: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 50: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 51: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 52: (2.86993360871273, 0.00980476144910828, 0.010063776373863242), 53: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 54: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 55: (2.916662743131008, 0.008848485787575558, 0.011233554780483268), 56: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 57: (2.743987235061825, 0.012902018625834148, 0.007454979419708241), 58: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 59: (2.594337170862807, 0.01779893404523318, 0.006748458743095376), 60: (2.914960139530747, 0.008881693746406093, 0.006869411468505837), 61: (2.8507182375464906, 0.010226178604013208, 0.006061425805091836), 62: (2.841321460699797, 0.010438544261103, 0.00706293582916262), 63: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 64: (2.885798778955244, 0.00946945042613447, 0.006650090217590332), 65: (2.7587514163403224, 0.012495511330205291, 0.006261751055717468), 66: (2.7396773779959207, 0.013023044734662113, 0.005785688757896423), 67: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 68: (2.812318299243337, 0.011121034507954432, 0.006750799715518951), 69: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 70: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 71: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 72: (2.8672447468516307, 0.00986271068265795, 0.006573969125747703), 73: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 74: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 75: (2.817415509292445, 0.010998067813829594, 0.006316101551055886), 76: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 77: (2.8505265911786, 0.010230468034281626, 0.00683790147304536), 78: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 79: (2.7607764414751292, 0.012440719137313207, 0.006465397775173187), 80: (0.8877558925020694, 0.3857725591115787, 0.001051077246665949), 81: (2.801768111398139, 0.01137974983785741, 0.0159024238586426), 82: (-1.5057271346943946, 0.1485817808527447, -0.0008097499608993308), 83: (1.9700449211964866, 0.06358531425883647, 0.002071602642536141), 84: (-22.95339218952276, 2.5686339773041418e-15, -0.022832025587558757), 85: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 86: (-22.95339218952276, 2.5686339773041418e-15, -0.022832025587558757), 87: (-22.86662846998824, 2.7536733557052225e-15, -0.01856851130723952), 88: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 89: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 90: (-16.768518789837042, 7.632218059649364e-13, -0.017311701178550742), 91: (-11.611768393844091, 4.5059977084044113e-10, -0.009996645152568817), 92: (-23.866649352713203, 1.2532478803632706e-15, -0.020687639713287354), 93: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 94: (-24.985537608446055, 5.384084308191323e-16, -0.01994720846414566), 95: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 96: (-19.523207824261274, 4.935981580158765e-14, -0.018804195523262013), 97: (-23.407583424122, 1.7917538260793985e-15, -0.014919529855251301), 98: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 99: (-26.935806838095047, 1.3401717265174718e-16, -0.023001615703105938), 100: (-29.05155747805047, 3.290875147552598e-17, -0.01941395699977877), 101: (-20.952578853822185, 1.364592063799021e-14, -0.015068808197975181), 102: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 103: (-17.6441388240095, 3.067566632212257e-13, -0.016543084383010886), 104: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 105: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 106: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 107: (-0.6701712489599319, 0.5108126714636266, -0.0008711561560630909), 108: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 109: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 110: (0.2285465785879622, 0.8216631703527664, 0.00027125924825666115), 111: (-1.1693969055389009, 0.25670543531739254, -0.0009917482733726724), 112: (-0.5160453000378008, 0.6117762825661357, -0.0006043612957000732), 113: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 114: (0.19566382318731748, 0.8469506675785499, 0.00022938847541809082), 115: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 116: (-0.4111320283226256, 0.6855758638595414, -0.0005095556378364452), 117: (-0.5973129280740079, 0.5573506555723196, -0.0007590308785438427), 118: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 119: (0.5016432707791384, 0.6216815313223802, 0.0006431668996810802), 120: (-0.7605603194798696, 0.4562551943917086, -0.0009556487202644348), 121: (0.41965824946936614, 0.6794421954453768, 0.0005446463823318703), 122: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 123: (0.44557433780413114, 0.6609402438153469, 0.0005996599793434143), 124: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 125: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 126: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 127: (2.7777349607598487, 0.011990792200825029, 0.005483637750148762), 128: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 129: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 130: (3.0837643105296504, 0.006112248291725002, 0.00957365781068803), 131: (3.1351439301505257, 0.00545050683362242, 0.011689433455467246), 132: (3.0435190939944987, 0.006684492448388404, 0.008233727514743827), 133: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 134: (3.0966805509407807, 0.00593889477616171, 0.00858931094408033), 135: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 136: (2.6523439052590074, 0.015721598320941495, 0.005913108587265015), 137: (3.0953838504201574, 0.005956080152424374, 0.009752169251441956), 138: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 139: (2.657918666541735, 0.015534546577635117, 0.0047150939702987615), 140: (2.5123701025065484, 0.02118038133567884, 0.004118752479553234), 141: (2.708947810106843, 0.013917895751007657, 0.003966300189495092), 142: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 143: (2.965174451191324, 0.007951062361363352, 0.007695800065994268)}\n",
            "EI_anger non_dann noun_phrase\n",
            "noun_phrase {0: (-1.823182545892379, 0.10158903404476334, -0.004346001148223921), 1: (0.6529009636033906, 0.5301427638966068, 0.0014689952135086282), 2: (-1.9147767672455225, 0.08778776509585746, -0.004484418034553517), 3: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 4: (-1.8334472014081977, 0.09994691694819158, -0.004479753971099842), 5: (-1.6768964109221438, 0.12787725657828872, -0.006013435125350963), 6: (-2.411025152323756, 0.03918309767510654, -0.006692928075790416), 7: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 8: (-2.50267900383429, 0.033713307978660734, -0.0077323317527771), 9: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 10: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 11: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 12: (-1.2215763576396204, 0.2528992657800809, -0.0032196015119552723), 13: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 14: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 15: (-2.0185489614696026, 0.07429584774720312, -0.005984464287757896), 16: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 17: (-2.502975107022215, 0.03369693249034014, -0.006320160627365068), 18: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 19: (-0.8907364716036349, 0.3962576215910443, -0.0022437006235123125), 20: (-2.8025004219687366, 0.020628201662149168, -0.011005443334579423), 21: (-2.107034519266076, 0.06437899542473148, -0.008922708034515447), 22: (-2.9943248021882756, 0.015094652822419669, -0.01277201473712919), 23: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 24: (-3.3972888461876676, 0.007907222552312736, -0.014024832844734214), 25: (-4.1748166136782485, 0.0023944354171003945, -0.012944406270980868), 26: (-4.1962723641365995, 0.002319461381102061, -0.01413772106170652), 27: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 28: (-2.695548618638821, 0.024572183386920812, -0.010166427493095376), 29: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 30: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 31: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 32: (-2.843643522099154, 0.01928815883461188, -0.009994786977767955), 33: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 34: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 35: (-3.7646858982640334, 0.004452747333857424, -0.011769860982894897), 36: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 37: (-3.7324423984291, 0.004679780409504576, -0.014772346615791354), 38: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 39: (-3.2359231103348005, 0.010225186578521093, -0.009877172112464916), 40: (-2.3616352973704924, 0.04248737155882671, -0.005944323539733842), 41: (-0.6965706128165664, 0.5036633522655196, -0.0019025415182113425), 42: (-2.1819167419098173, 0.056996194905469984, -0.006671530008315996), 43: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 44: (-1.8999677088194737, 0.08989273023884006, -0.006697237491607666), 45: (-2.887450189083182, 0.017958705520828136, -0.008153745532035794), 46: (-3.6971182160452765, 0.004942572347528052, -0.009962809085845903), 47: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 48: (-1.7840686423691154, 0.10807941437847667, -0.0044749736785889005), 49: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 50: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 51: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 52: (-2.012770107772844, 0.07499209109599361, -0.005148887634277344), 53: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 54: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 55: (-3.3358841727430946, 0.008717067187864204, -0.01025363802909851), 56: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 57: (-3.0324117689742924, 0.014191161631230537, -0.010666310787200928), 58: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 59: (-2.709559987535502, 0.024014793251107003, -0.008456566929817166), 60: (-5.612223135587408, 0.00032908633523572386, -0.007830667495727472), 61: (-3.044537086563111, 0.01391535583938568, -0.004270273447036788), 62: (-4.466322721932532, 0.0015629322764594033, -0.004947292804717951), 63: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 64: (-2.9114347754278325, 0.017270871413094906, -0.004106640815734863), 65: (-3.3891489240165122, 0.008009904035483717, -0.005237913131713845), 66: (-4.262865332703453, 0.00210226306433529, -0.006344011425972018), 67: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 68: (-2.183246878448915, 0.05687278800943512, -0.0033423155546188354), 69: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 70: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 71: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 72: (-2.5803190667246017, 0.029681082044315344, -0.0034605830907821766), 73: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 74: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 75: (-2.563400996839954, 0.03051638168160957, -0.004253998398780823), 76: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 77: (-2.2429798918897146, 0.051591542039749805, -0.004107862710952759), 78: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 79: (-2.8804512051376103, 0.018164657500522638, -0.004243367910385154), 80: (-3.446982093301815, 0.0073094595679489595, -0.0070547282695770375), 81: (-2.877209148867109, 0.018260874752402068, -0.009688264131546054), 82: (-4.375781209611118, 0.0017820273124026433, -0.005826637148857117), 83: (-4.227395409459643, 0.002215116531872715, -0.008993631601333596), 84: (-10.859157548412043, 1.794588733464013e-06, -0.028087219595909096), 85: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 86: (-10.859157548412043, 1.794588733464013e-06, -0.028087219595909096), 87: (-13.425354861271588, 2.9440041519051927e-07, -0.028906109929084767), 88: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 89: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 90: (-16.15389910304737, 5.917210079387948e-08, -0.026733589172363237), 91: (-10.429215707109279, 2.5193237097096096e-06, -0.02062155306339264), 92: (-15.871086648344093, 6.902775576577911e-08, -0.026989573240280174), 93: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 94: (-19.213494334267327, 1.2930383331406867e-08, -0.0260120451450348), 95: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 96: (-17.396849352413383, 3.094297015510351e-08, -0.027425014972686734), 97: (-11.630878088616768, 1.0043174949080335e-06, -0.023780626058578502), 98: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 99: (-16.256511459884884, 5.5990327970512016e-08, -0.03293475806713103), 100: (-14.966067719944089, 1.1505091346154834e-07, -0.02911258637905123), 101: (-8.9014716568645, 9.34175689190008e-06, -0.02603101432323457), 102: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 103: (-11.619817167101273, 1.0124617705326627e-06, -0.025008597970008872), 104: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 105: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 106: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 107: (-0.9049840849981707, 0.38905713129157093, -0.0011850744485855103), 108: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 109: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 110: (0.14430588605679148, 0.8884386416264047, 0.00020279288291930042), 111: (-0.9779956544240272, 0.3536312369845376, -0.0014018923044205045), 112: (-0.6577968932510961, 0.52713316186464, -0.000915491580963157), 113: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 114: (-1.73562370385813, 0.11665049488657796, -0.002127626538276639), 115: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 116: (-0.7291060706599362, 0.48447730366659025, -0.0009533852338791116), 117: (-1.276532090034004, 0.2337205874074845, -0.0018397957086563332), 118: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 119: (-0.11016672130588094, 0.9146946252160408, -0.00012453794479372338), 120: (0.6337352230370069, 0.5420222418426753, 0.000764855742454551), 121: (0.6038074636841404, 0.5608803614850112, 0.0007824897766113281), 122: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 123: (0.6025133045306404, 0.5617041956500191, 0.0009518772363663053), 124: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 125: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 126: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 127: (-1.5586334828057054, 0.15351219246910677, -0.003723523020744357), 128: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 129: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 130: (-0.8889873387795206, 0.39714807548087205, -0.0023648232221603616), 131: (1.8731454777310295, 0.09382679279907638, 0.005485448241233815), 132: (-1.4602469096557182, 0.17823119122489203, -0.00560216009616854), 133: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 134: (-1.2349301099673444, 0.24812404639205596, -0.0043000727891922), 135: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 136: (-0.48362082848308674, 0.6402040030990057, -0.0012556225061416404), 137: (-0.8892996750279333, 0.3969889665798001, -0.0023857623338698897), 138: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 139: (-4.450203928550094, 0.001599725184121625, -0.008053421974182129), 140: (-4.422306758793129, 0.0016656100862283232, -0.009977713227272034), 141: (-2.3976431527953115, 0.04005235709894506, -0.007599052786827071), 142: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 143: (-1.7572022516970278, 0.11275805348644675, -0.005658695101737943)}\n",
            "EI_anger non_dann original_noun_phrase\n",
            "original_noun_phrase {0: (-24.1029332123197, 4.213490669717725e-108, -0.007616794750922251)}\n",
            "non_dann original_noun_phrase {'named': {0: (2.9999473312352563, 0.007362582726028531, 0.008020885288715363), 1: (2.8668234774506334, 0.009871819488213418, 0.007182353734970115), 2: (3.00154155812073, 0.007336638454529805, 0.007494708895683266), 3: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 4: (2.853858915689559, 0.010156129568114373, 0.007833994925022125), 5: (2.828059928865708, 0.010745468775168335, 0.007886649668216728), 6: (3.02450282733906, 0.0069726453063536685, 0.009996812045574177), 7: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 8: (2.8598644963104904, 0.010023463730840692, 0.008409228920936596), 9: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 10: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 11: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 12: (2.5998013893153016, 0.017592679093099937, 0.005481539666652657), 13: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 14: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 15: (2.9326297210120362, 0.008542831810533981, 0.005548548698425304), 16: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 17: (2.940704275103277, 0.008392164112157185, 0.0060208141803741455), 18: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 19: (2.287671406170058, 0.03379490281125083, 0.0036009758710860984), 20: (3.0413286735531093, 0.006717086069541028, 0.014911824464797996), 21: (3.069448373548996, 0.006310132602520979, 0.0162633419036865), 22: (3.0021290719939473, 0.007327099649944999, 0.012680222094059002), 23: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 24: (3.09062885600122, 0.006019513433223663, 0.01488557308912275), 25: (2.956577974398133, 0.008103409424171643, 0.010112801194191001), 26: (2.9864008493337164, 0.007586648071914168, 0.011366653442382801), 27: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 28: (2.973399068655112, 0.0078078980268998345, 0.012191291153430928), 29: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 30: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 31: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 32: (3.034858735548319, 0.006814261596819409, 0.014548160135746002), 33: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 34: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 35: (2.9552849489291804, 0.008126567023332459, 0.013237571716308572), 36: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 37: (2.936128540797658, 0.008477227650781594, 0.011788879334926616), 38: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 39: (2.7435982420580585, 0.012912897795710858, 0.009320780634880066), 40: (3.0405868166075836, 0.006728159852842062, 0.014032751321792603), 41: (3.060608797193966, 0.006435410659359709, 0.014685365557670582), 42: (2.9596167573971948, 0.008049236583012171, 0.012382332980632793), 43: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 44: (3.0580771508201217, 0.00647173267937202, 0.012605533003807068), 45: (2.82915858949088, 0.010719715967741848, 0.007928590476512898), 46: (2.9653970838404464, 0.007947153849508864, 0.01044829487800597), 47: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 48: (2.6817851636071484, 0.014757382691585958, 0.010272590816020943), 49: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 50: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 51: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 52: (2.86993360871273, 0.00980476144910828, 0.010063776373863242), 53: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 54: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 55: (2.916662743131008, 0.008848485787575558, 0.011233554780483268), 56: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 57: (2.743987235061825, 0.012902018625834148, 0.007454979419708241), 58: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 59: (2.594337170862807, 0.01779893404523318, 0.006748458743095376), 60: (2.914960139530747, 0.008881693746406093, 0.006869411468505837), 61: (2.8507182375464906, 0.010226178604013208, 0.006061425805091836), 62: (2.841321460699797, 0.010438544261103, 0.00706293582916262), 63: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 64: (2.885798778955244, 0.00946945042613447, 0.006650090217590332), 65: (2.7587514163403224, 0.012495511330205291, 0.006261751055717468), 66: (2.7396773779959207, 0.013023044734662113, 0.005785688757896423), 67: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 68: (2.812318299243337, 0.011121034507954432, 0.006750799715518951), 69: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 70: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 71: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 72: (2.8672447468516307, 0.00986271068265795, 0.006573969125747703), 73: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 74: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 75: (2.817415509292445, 0.010998067813829594, 0.006316101551055886), 76: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 77: (2.8505265911786, 0.010230468034281626, 0.00683790147304536), 78: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 79: (2.7607764414751292, 0.012440719137313207, 0.006465397775173187), 80: (0.8877558925020694, 0.3857725591115787, 0.001051077246665949), 81: (2.801768111398139, 0.01137974983785741, 0.0159024238586426), 82: (-1.5057271346943946, 0.1485817808527447, -0.0008097499608993308), 83: (1.9700449211964866, 0.06358531425883647, 0.002071602642536141), 84: (-22.95339218952276, 2.5686339773041418e-15, -0.022832025587558757), 85: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 86: (-22.95339218952276, 2.5686339773041418e-15, -0.022832025587558757), 87: (-22.86662846998824, 2.7536733557052225e-15, -0.01856851130723952), 88: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 89: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 90: (-16.768518789837042, 7.632218059649364e-13, -0.017311701178550742), 91: (-11.611768393844091, 4.5059977084044113e-10, -0.009996645152568817), 92: (-23.866649352713203, 1.2532478803632706e-15, -0.020687639713287354), 93: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 94: (-24.985537608446055, 5.384084308191323e-16, -0.01994720846414566), 95: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 96: (-19.523207824261274, 4.935981580158765e-14, -0.018804195523262013), 97: (-23.407583424122, 1.7917538260793985e-15, -0.014919529855251301), 98: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 99: (-26.935806838095047, 1.3401717265174718e-16, -0.023001615703105938), 100: (-29.05155747805047, 3.290875147552598e-17, -0.01941395699977877), 101: (-20.952578853822185, 1.364592063799021e-14, -0.015068808197975181), 102: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 103: (-17.6441388240095, 3.067566632212257e-13, -0.016543084383010886), 104: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 105: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 106: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 107: (-0.6701712489599319, 0.5108126714636266, -0.0008711561560630909), 108: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 109: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 110: (0.2285465785879622, 0.8216631703527664, 0.00027125924825666115), 111: (-1.1693969055389009, 0.25670543531739254, -0.0009917482733726724), 112: (-0.5160453000378008, 0.6117762825661357, -0.0006043612957000732), 113: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 114: (0.19566382318731748, 0.8469506675785499, 0.00022938847541809082), 115: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 116: (-0.4111320283226256, 0.6855758638595414, -0.0005095556378364452), 117: (-0.5973129280740079, 0.5573506555723196, -0.0007590308785438427), 118: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 119: (0.5016432707791384, 0.6216815313223802, 0.0006431668996810802), 120: (-0.7605603194798696, 0.4562551943917086, -0.0009556487202644348), 121: (0.41965824946936614, 0.6794421954453768, 0.0005446463823318703), 122: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 123: (0.44557433780413114, 0.6609402438153469, 0.0005996599793434143), 124: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 125: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 126: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 127: (2.7777349607598487, 0.011990792200825029, 0.005483637750148762), 128: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 129: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 130: (3.0837643105296504, 0.006112248291725002, 0.00957365781068803), 131: (3.1351439301505257, 0.00545050683362242, 0.011689433455467246), 132: (3.0435190939944987, 0.006684492448388404, 0.008233727514743827), 133: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 134: (3.0966805509407807, 0.00593889477616171, 0.00858931094408033), 135: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 136: (2.6523439052590074, 0.015721598320941495, 0.005913108587265015), 137: (3.0953838504201574, 0.005956080152424374, 0.009752169251441956), 138: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 139: (2.657918666541735, 0.015534546577635117, 0.0047150939702987615), 140: (2.5123701025065484, 0.02118038133567884, 0.004118752479553234), 141: (2.708947810106843, 0.013917895751007657, 0.003966300189495092), 142: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 143: (2.965174451191324, 0.007951062361363352, 0.007695800065994268)}, 'noun_phrase': {0: (-1.823182545892379, 0.10158903404476334, -0.004346001148223921), 1: (0.6529009636033906, 0.5301427638966068, 0.0014689952135086282), 2: (-1.9147767672455225, 0.08778776509585746, -0.004484418034553517), 3: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 4: (-1.8334472014081977, 0.09994691694819158, -0.004479753971099842), 5: (-1.6768964109221438, 0.12787725657828872, -0.006013435125350963), 6: (-2.411025152323756, 0.03918309767510654, -0.006692928075790416), 7: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 8: (-2.50267900383429, 0.033713307978660734, -0.0077323317527771), 9: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 10: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 11: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 12: (-1.2215763576396204, 0.2528992657800809, -0.0032196015119552723), 13: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 14: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 15: (-2.0185489614696026, 0.07429584774720312, -0.005984464287757896), 16: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 17: (-2.502975107022215, 0.03369693249034014, -0.006320160627365068), 18: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 19: (-0.8907364716036349, 0.3962576215910443, -0.0022437006235123125), 20: (-2.8025004219687366, 0.020628201662149168, -0.011005443334579423), 21: (-2.107034519266076, 0.06437899542473148, -0.008922708034515447), 22: (-2.9943248021882756, 0.015094652822419669, -0.01277201473712919), 23: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 24: (-3.3972888461876676, 0.007907222552312736, -0.014024832844734214), 25: (-4.1748166136782485, 0.0023944354171003945, -0.012944406270980868), 26: (-4.1962723641365995, 0.002319461381102061, -0.01413772106170652), 27: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 28: (-2.695548618638821, 0.024572183386920812, -0.010166427493095376), 29: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 30: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 31: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 32: (-2.843643522099154, 0.01928815883461188, -0.009994786977767955), 33: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 34: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 35: (-3.7646858982640334, 0.004452747333857424, -0.011769860982894897), 36: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 37: (-3.7324423984291, 0.004679780409504576, -0.014772346615791354), 38: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 39: (-3.2359231103348005, 0.010225186578521093, -0.009877172112464916), 40: (-2.3616352973704924, 0.04248737155882671, -0.005944323539733842), 41: (-0.6965706128165664, 0.5036633522655196, -0.0019025415182113425), 42: (-2.1819167419098173, 0.056996194905469984, -0.006671530008315996), 43: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 44: (-1.8999677088194737, 0.08989273023884006, -0.006697237491607666), 45: (-2.887450189083182, 0.017958705520828136, -0.008153745532035794), 46: (-3.6971182160452765, 0.004942572347528052, -0.009962809085845903), 47: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 48: (-1.7840686423691154, 0.10807941437847667, -0.0044749736785889005), 49: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 50: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 51: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 52: (-2.012770107772844, 0.07499209109599361, -0.005148887634277344), 53: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 54: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 55: (-3.3358841727430946, 0.008717067187864204, -0.01025363802909851), 56: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 57: (-3.0324117689742924, 0.014191161631230537, -0.010666310787200928), 58: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 59: (-2.709559987535502, 0.024014793251107003, -0.008456566929817166), 60: (-5.612223135587408, 0.00032908633523572386, -0.007830667495727472), 61: (-3.044537086563111, 0.01391535583938568, -0.004270273447036788), 62: (-4.466322721932532, 0.0015629322764594033, -0.004947292804717951), 63: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 64: (-2.9114347754278325, 0.017270871413094906, -0.004106640815734863), 65: (-3.3891489240165122, 0.008009904035483717, -0.005237913131713845), 66: (-4.262865332703453, 0.00210226306433529, -0.006344011425972018), 67: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 68: (-2.183246878448915, 0.05687278800943512, -0.0033423155546188354), 69: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 70: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 71: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 72: (-2.5803190667246017, 0.029681082044315344, -0.0034605830907821766), 73: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 74: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 75: (-2.563400996839954, 0.03051638168160957, -0.004253998398780823), 76: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 77: (-2.2429798918897146, 0.051591542039749805, -0.004107862710952759), 78: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 79: (-2.8804512051376103, 0.018164657500522638, -0.004243367910385154), 80: (-3.446982093301815, 0.0073094595679489595, -0.0070547282695770375), 81: (-2.877209148867109, 0.018260874752402068, -0.009688264131546054), 82: (-4.375781209611118, 0.0017820273124026433, -0.005826637148857117), 83: (-4.227395409459643, 0.002215116531872715, -0.008993631601333596), 84: (-10.859157548412043, 1.794588733464013e-06, -0.028087219595909096), 85: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 86: (-10.859157548412043, 1.794588733464013e-06, -0.028087219595909096), 87: (-13.425354861271588, 2.9440041519051927e-07, -0.028906109929084767), 88: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 89: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 90: (-16.15389910304737, 5.917210079387948e-08, -0.026733589172363237), 91: (-10.429215707109279, 2.5193237097096096e-06, -0.02062155306339264), 92: (-15.871086648344093, 6.902775576577911e-08, -0.026989573240280174), 93: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 94: (-19.213494334267327, 1.2930383331406867e-08, -0.0260120451450348), 95: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 96: (-17.396849352413383, 3.094297015510351e-08, -0.027425014972686734), 97: (-11.630878088616768, 1.0043174949080335e-06, -0.023780626058578502), 98: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 99: (-16.256511459884884, 5.5990327970512016e-08, -0.03293475806713103), 100: (-14.966067719944089, 1.1505091346154834e-07, -0.02911258637905123), 101: (-8.9014716568645, 9.34175689190008e-06, -0.02603101432323457), 102: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 103: (-11.619817167101273, 1.0124617705326627e-06, -0.025008597970008872), 104: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 105: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 106: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 107: (-0.9049840849981707, 0.38905713129157093, -0.0011850744485855103), 108: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 109: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 110: (0.14430588605679148, 0.8884386416264047, 0.00020279288291930042), 111: (-0.9779956544240272, 0.3536312369845376, -0.0014018923044205045), 112: (-0.6577968932510961, 0.52713316186464, -0.000915491580963157), 113: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 114: (-1.73562370385813, 0.11665049488657796, -0.002127626538276639), 115: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 116: (-0.7291060706599362, 0.48447730366659025, -0.0009533852338791116), 117: (-1.276532090034004, 0.2337205874074845, -0.0018397957086563332), 118: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 119: (-0.11016672130588094, 0.9146946252160408, -0.00012453794479372338), 120: (0.6337352230370069, 0.5420222418426753, 0.000764855742454551), 121: (0.6038074636841404, 0.5608803614850112, 0.0007824897766113281), 122: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 123: (0.6025133045306404, 0.5617041956500191, 0.0009518772363663053), 124: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 125: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 126: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 127: (-1.5586334828057054, 0.15351219246910677, -0.003723523020744357), 128: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 129: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 130: (-0.8889873387795206, 0.39714807548087205, -0.0023648232221603616), 131: (1.8731454777310295, 0.09382679279907638, 0.005485448241233815), 132: (-1.4602469096557182, 0.17823119122489203, -0.00560216009616854), 133: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 134: (-1.2349301099673444, 0.24812404639205596, -0.0043000727891922), 135: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 136: (-0.48362082848308674, 0.6402040030990057, -0.0012556225061416404), 137: (-0.8892996750279333, 0.3969889665798001, -0.0023857623338698897), 138: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 139: (-4.450203928550094, 0.001599725184121625, -0.008053421974182129), 140: (-4.422306758793129, 0.0016656100862283232, -0.009977713227272034), 141: (-2.3976431527953115, 0.04005235709894506, -0.007599052786827071), 142: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 143: (-1.7572022516970278, 0.11275805348644675, -0.005658695101737943)}, 'original_noun_phrase': {0: (-24.1029332123197, 4.213490669717725e-108, -0.007616794750922251)}}\n",
            "EI_anger dann named\n",
            "named {0: (3.1541497723996583, 0.00522384135791289, 0.00986942350864406), 1: (3.181309271590398, 0.004915805191278162, 0.011457297205924977), 2: (3.127365912520824, 0.005546005199175376, 0.009590110182762124), 3: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 4: (3.1493556275382018, 0.005280135194466702, 0.010423678159713734), 5: (3.1297181110943186, 0.00551695442450737, 0.007126376032829285), 6: (3.08688816725016, 0.006069876481733846, 0.010540437698364269), 7: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 8: (3.1645958707087574, 0.00510319942100133, 0.011332765221595764), 9: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 10: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 11: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 12: (3.102131411924078, 0.005867179401643754, 0.0061305731534958), 13: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 14: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 15: (3.0000584814268514, 0.007360771004279333, 0.0068900465965270885), 16: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 17: (3.0841412656010374, 0.006107120088498524, 0.006706812977790855), 18: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 19: (2.5196051941625384, 0.020859143591782148, 0.0034940391778945923), 20: (3.139819430130235, 0.005393873686063911, 0.01886967718601229), 21: (3.154179233456912, 0.005223497234744051, 0.02260468602180482), 22: (3.1507253686005186, 0.0052639913739709054, 0.016295456886291526), 23: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 24: (3.1363917765936224, 0.005435335588589249, 0.01601330339908602), 25: (3.0916877516329184, 0.006005330815079033, 0.00884194076061251), 26: (3.017501051367118, 0.0070817458098385395, 0.007745276391506184), 27: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 28: (3.1120514476799865, 0.005738819214552431, 0.014920613169670083), 29: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 30: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 31: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 32: (3.1078968924867683, 0.005792241140374358, 0.01412692666053772), 33: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 34: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 35: (3.0885493840039504, 0.006047460221819138, 0.010939228534698475), 36: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 37: (3.1264344405222126, 0.005557550435755567, 0.013928736746311177), 38: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 39: (2.721222332581404, 0.013553645016633994, 0.005284606665372837), 40: (3.068690031293318, 0.00632078661724058, 0.019550687074661233), 41: (3.07003739288343, 0.006301869452771459, 0.02186712622642517), 42: (3.0644857210587464, 0.006380170879024848, 0.019834151864051797), 43: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 44: (3.0976252131600672, 0.005926405378420299, 0.018958660960197438), 45: (2.8919449369729633, 0.009342539390275371, 0.013088151812553406), 46: (3.0509629683921577, 0.00657486941097741, 0.010790181159973156), 47: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 48: (2.9229009235242236, 0.00872783447418792, 0.01431553065776825), 49: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 50: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 51: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 52: (2.9671368452040445, 0.007916674678809254, 0.01232005059719088), 53: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 54: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 55: (3.070699551364976, 0.006292592803193917, 0.016034062951803207), 56: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 57: (3.0368559536937076, 0.006784119775119118, 0.013155999779701222), 58: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 59: (2.875630327042398, 0.009683067762781711, 0.008324480056762673), 60: (2.7486062148184214, 0.012773504283733491, 0.006257417798042342), 61: (2.9762263224875394, 0.0077592624524099535, 0.007529836893081621), 62: (3.0148540153590613, 0.00712342071843546, 0.007782182097434975), 63: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 64: (2.989777061076018, 0.007530194496840481, 0.007344353199005116), 65: (2.777215221805072, 0.012004347233363365, 0.007192438840866067), 66: (2.614099029701356, 0.017063641108465092, 0.005363926291465759), 67: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 68: (2.8197206933326515, 0.010942885865725937, 0.007550516724586509), 69: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 70: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 71: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 72: (2.8579679658821235, 0.01006517796201528, 0.006564566493034385), 73: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 74: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 75: (2.8578427461103284, 0.010067938036389864, 0.00712003856897353), 76: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 77: (2.926692507871646, 0.008655278882439317, 0.00737684965133667), 78: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 79: (2.945411434880791, 0.008305517094331305, 0.007217621803283669), 80: (2.9549685909975034, 0.008132242598424416, 0.006953896582126612), 81: (3.005514411265544, 0.007272369120876625, 0.022204640507698048), 82: (2.707395073279422, 0.01396463290410086, 0.0027575403451919334), 83: (3.0113191208907693, 0.007179444602253337, 0.006955680251121532), 84: (-7.325341937968756, 6.045422435606574e-07, -0.0066195636987685935), 85: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 86: (-7.325341937968756, 6.045422435606574e-07, -0.0066195636987685935), 87: (-10.74069959243424, 1.6448399515383815e-09, -0.008333107829093911), 88: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 89: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 90: (0.18239733846981665, 0.8572029605082729, 0.00031536519527436413), 91: (1.7362255918841576, 0.09871197028057596, 0.003092189133167278), 92: (-0.940070890721652, 0.3589800896324369, -0.0012825310230255016), 93: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 94: (3.0357114102967033, 0.00680137726430756, 0.0038804203271866067), 95: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 96: (-1.1773778726816657, 0.253583294746442, -0.001657232642173767), 97: (3.5997084988096124, 0.0019097129950801056, 0.004591581225395214), 98: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 99: (1.1089465269046965, 0.28129319148179616, 0.0018455713987350353), 100: (-0.5295364484976446, 0.6025665348471472, -0.0007632926106452831), 101: (1.5058625044790652, 0.1485472554812579, 0.0018052011728286743), 102: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 103: (4.310195331721349, 0.00037750741628889866, 0.0077728137373924144), 104: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 105: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 106: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 107: (0.8210562490126309, 0.4217971975292646, 0.0006834208965301736), 108: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 109: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 110: (1.8198202333697628, 0.08457938902254086, 0.0013935327529907005), 111: (0.4377222947367746, 0.6665230539813192, 0.00022646188735964135), 112: (0.20970731813703808, 0.8361283611725413, 0.00012572109699249268), 113: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 114: (-0.15158254894517134, 0.881114123565619, -0.0001502901315689198), 115: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 116: (2.0145092513411904, 0.05833227460376728, 0.001291951537132241), 117: (0.2962314517029351, 0.7702669343080781, 0.0002052307128906361), 118: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 119: (1.8139215348766842, 0.08551538619544562, 0.0013534307479858287), 120: (-1.0397772310328304, 0.31149499050162893, -0.0004953473806381115), 121: (0.6238769147308664, 0.5401272059841066, 0.0005101948976516946), 122: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 123: (2.0842931669980382, 0.05087023026204952, 0.0026114106178283802), 124: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 125: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 126: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 127: (3.0149132102670153, 0.007122486163198851, 0.0069400861859321705), 128: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 129: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 130: (3.1269969622232923, 0.005550575401041034, 0.012137611210346233), 131: (3.161449722975462, 0.005139245536346475, 0.012094923853874229), 132: (2.670957543312085, 0.015105268001117854, 0.007432873547077168), 133: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 134: (3.1207491378829744, 0.005628526362397582, 0.011614325642585743), 135: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 136: (2.4076877774607914, 0.026379633477613015, 0.006028772145509731), 137: (3.099804932215983, 0.0058976846234807204, 0.013817815482616402), 138: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 139: (2.681860483691142, 0.014754989598012682, 0.005324845761060709), 140: (2.621741612879915, 0.01678706553181637, 0.005616920441389062), 141: (2.7503886062482987, 0.012724240238317888, 0.006610753387212759), 142: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 143: (3.0221484823384093, 0.00700914733911952, 0.008415175974369055)}\n",
            "EI_anger dann noun_phrase\n",
            "noun_phrase {0: (-1.7014712871235786, 0.12306423285839684, -0.008207207918167092), 1: (-1.209152251513571, 0.2574092262783106, -0.005838519334793069), 2: (-2.2741258510416813, 0.04903120185809948, -0.008376097679138161), 3: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 4: (-2.4336529698323597, 0.03775566225680258, -0.008571654558181763), 5: (-2.974418762892723, 0.01559018013610164, -0.011515149474144004), 6: (-3.7429218799226067, 0.0046046824226910355, -0.011580252647399869), 7: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 8: (-2.6748416167039375, 0.025419995672536787, -0.014063483476638772), 9: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 10: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 11: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 12: (-0.868794681803827, 0.4075302724955586, -0.0038636088371276633), 13: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 14: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 15: (-2.565657162667428, 0.030403638037283857, -0.011824379861354817), 16: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 17: (-2.258488164901773, 0.050300726156232385, -0.011758840084075906), 18: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 19: (-1.8880463081963228, 0.09162170682924786, -0.005839028954505887), 20: (-2.4604038966831085, 0.03613469573733932, -0.014544641971588113), 21: (-2.8847849291391174, 0.018036850527221066, -0.01452497243881229), 22: (-2.551173735937456, 0.031134730890924927, -0.01110403537750243), 23: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 24: (-2.9892038014917492, 0.015220565122441269, -0.013150608539581332), 25: (-3.0358582416996263, 0.01411219946624356, -0.009422686696052562), 26: (-1.8660285997449984, 0.0948974771938708, -0.006441432237625111), 27: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 28: (-3.132727195674896, 0.012068561763517274, -0.015891933441162076), 29: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 30: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 31: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 32: (-2.4382151236003016, 0.03747418958339082, -0.010398060083389282), 33: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 34: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 35: (-0.4020122275284977, 0.6970547964026097, -0.001407685875892628), 36: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 37: (-2.3817349153601155, 0.041110618220405626, -0.0105152428150177), 38: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 39: (-0.9283198803266377, 0.3774665466052247, -0.0028055936098098644), 40: (-1.2495992374355025, 0.24296399817492736, -0.006587874889373735), 41: (-0.8132883786986603, 0.4370386133713132, -0.0045864880084991455), 42: (-0.8147147390846828, 0.4362625933545672, -0.0038908302783965953), 43: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 44: (-0.8168166857324468, 0.43512071642609373, -0.004046750068664562), 45: (-1.849788845567326, 0.0973836808318736, -0.008271408081054699), 46: (-2.4891812967461524, 0.03446827802048692, -0.00911563038825991), 47: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 48: (-1.257612326326132, 0.24018284151870845, -0.005986660718917847), 49: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 50: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 51: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 52: (-0.5367128485920113, 0.6044777235518819, -0.0025108575820922963), 53: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 54: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 55: (-1.711825808156662, 0.1210863310467368, -0.008448512852191892), 56: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 57: (-1.4351811948788749, 0.18506084115120194, -0.008757948875427246), 58: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 59: (-1.7385832043518836, 0.11610942142227056, -0.007238876819610551), 60: (-4.4708621725623, 0.0015527346020212072, -0.01286238431930542), 61: (-3.093124421481188, 0.012864420613802998, -0.009431958198547363), 62: (-2.509957558117125, 0.03331307548642898, -0.006254076957702637), 63: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 64: (-2.446109453748497, 0.03699205282025611, -0.0065238356590270885), 65: (-2.7155631563309925, 0.02377991331523115, -0.008957451581954934), 66: (-3.1404512583916464, 0.011919362713343784, -0.008159947395324718), 67: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 68: (-2.258420562695312, 0.050306283711191606, -0.008272564411163308), 69: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 70: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 71: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 72: (-2.137501827401781, 0.06126964941052555, -0.005974161624908436), 73: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 74: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 75: (-2.2966378673043333, 0.04725865309097473, -0.007664144039154053), 76: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 77: (-1.7607714994671344, 0.11212588279486928, -0.006605374813079823), 78: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 79: (-2.1628039190842365, 0.058798454376853486, -0.007389742136001576), 80: (0.6104773495640484, 0.556645368160736, 0.0022977083921432717), 81: (-2.054564425384406, 0.07009414087799887, -0.012084883451461748), 82: (-1.3624638417474084, 0.20616905327334664, -0.00208470821380613), 83: (-0.41301183616454257, 0.6892641446549439, -0.002105090022087086), 84: (-5.627079331263414, 0.00032289986025933146, -0.014079564809799217), 85: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 86: (-5.627079331263414, 0.00032289986025933146, -0.014079564809799217), 87: (-6.451965022375558, 0.00011787055596603068, -0.018509644269943226), 88: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 89: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 90: (-2.7796784174762608, 0.021412021024543516, -0.011031919717788663), 91: (-1.9531050197523963, 0.08255397686718234, -0.007644644379615817), 92: (-3.974534561372611, 0.0032323967288151565, -0.010690855979919422), 93: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 94: (-2.4375247727498524, 0.03751664799519189, -0.0077206671237945446), 95: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 96: (-3.6971469313038927, 0.004942352518800177, -0.011132961511611916), 97: (-2.409284665271311, 0.03929508737744707, -0.006687295436859142), 98: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 99: (-3.391759907387993, 0.007976816914009565, -0.011295613646507274), 100: (-4.550282689732428, 0.0013853788483211749, -0.01318627893924712), 101: (-1.9228295311738242, 0.08666275825938953, -0.00699996352195742), 102: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 103: (-0.8379698118004049, 0.4237422752964861, -0.0032378971576690896), 104: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 105: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 106: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 107: (-0.6687090925180541, 0.520462228362796, -0.0008792579174041748), 108: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 109: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 110: (1.874027241162766, 0.09369492998386465, 0.001990282535552945), 111: (2.5304714659685534, 0.03221044502910602, 0.0031304419040680265), 112: (2.2681340275158024, 0.04951388216076753, 0.0024760425090789573), 113: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 114: (-2.67396811839617, 0.025456402645859244, -0.004358720779418956), 115: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 116: (2.059597017358291, 0.06952548788412616, 0.002565145492553711), 117: (1.7672393612030144, 0.11098865588675123, 0.0019124507904053067), 118: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 119: (2.3449763280740075, 0.043662927431223436, 0.003461676836013783), 120: (2.7105711284566856, 0.023975067436201904, 0.0033237516880035844), 121: (2.6924024594040903, 0.02469913117707077, 0.0031828880310058594), 122: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 123: (1.4427718860170107, 0.1829689959341304, 0.002938568592071533), 124: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 125: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 126: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 127: (0.35754748266078845, 0.7289232559385432, 0.0015774309635162576), 128: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 129: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 130: (0.4769284775470119, 0.6447804326307979, 0.002782040834426902), 131: (1.5916127249552854, 0.14593548854062083, 0.009280022978782665), 132: (-0.1372929792438153, 0.8938215875311126, -0.0008468002080916914), 133: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 134: (0.06349382805961021, 0.9507611144000385, 0.0003700375556945912), 135: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 136: (0.363499115120205, 0.7246239293824269, 0.0017059653997421043), 137: (0.616811119120338, 0.5526407815822985, 0.0034369230270385742), 138: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 139: (0.6503904569933501, 0.5316899776332693, 0.002200816571712505), 140: (0.6637485050695717, 0.5234884334503043, 0.0028235316276550293), 141: (0.033023209048875894, 0.9743768586929915, 0.00017293840646748215), 142: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 143: (0.6544397522467309, 0.5291957439553789, 0.002611398696899414)}\n",
            "EI_anger dann original_noun_phrase\n",
            "original_noun_phrase {0: (-15.686372171980068, 2.5908143315881138e-51, -0.00538205282969606)}\n",
            "dann original_noun_phrase {'named': {0: (3.1541497723996583, 0.00522384135791289, 0.00986942350864406), 1: (3.181309271590398, 0.004915805191278162, 0.011457297205924977), 2: (3.127365912520824, 0.005546005199175376, 0.009590110182762124), 3: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 4: (3.1493556275382018, 0.005280135194466702, 0.010423678159713734), 5: (3.1297181110943186, 0.00551695442450737, 0.007126376032829285), 6: (3.08688816725016, 0.006069876481733846, 0.010540437698364269), 7: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 8: (3.1645958707087574, 0.00510319942100133, 0.011332765221595764), 9: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 10: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 11: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 12: (3.102131411924078, 0.005867179401643754, 0.0061305731534958), 13: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 14: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 15: (3.0000584814268514, 0.007360771004279333, 0.0068900465965270885), 16: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 17: (3.0841412656010374, 0.006107120088498524, 0.006706812977790855), 18: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 19: (2.5196051941625384, 0.020859143591782148, 0.0034940391778945923), 20: (3.139819430130235, 0.005393873686063911, 0.01886967718601229), 21: (3.154179233456912, 0.005223497234744051, 0.02260468602180482), 22: (3.1507253686005186, 0.0052639913739709054, 0.016295456886291526), 23: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 24: (3.1363917765936224, 0.005435335588589249, 0.01601330339908602), 25: (3.0916877516329184, 0.006005330815079033, 0.00884194076061251), 26: (3.017501051367118, 0.0070817458098385395, 0.007745276391506184), 27: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 28: (3.1120514476799865, 0.005738819214552431, 0.014920613169670083), 29: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 30: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 31: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 32: (3.1078968924867683, 0.005792241140374358, 0.01412692666053772), 33: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 34: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 35: (3.0885493840039504, 0.006047460221819138, 0.010939228534698475), 36: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 37: (3.1264344405222126, 0.005557550435755567, 0.013928736746311177), 38: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 39: (2.721222332581404, 0.013553645016633994, 0.005284606665372837), 40: (3.068690031293318, 0.00632078661724058, 0.019550687074661233), 41: (3.07003739288343, 0.006301869452771459, 0.02186712622642517), 42: (3.0644857210587464, 0.006380170879024848, 0.019834151864051797), 43: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 44: (3.0976252131600672, 0.005926405378420299, 0.018958660960197438), 45: (2.8919449369729633, 0.009342539390275371, 0.013088151812553406), 46: (3.0509629683921577, 0.00657486941097741, 0.010790181159973156), 47: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 48: (2.9229009235242236, 0.00872783447418792, 0.01431553065776825), 49: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 50: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 51: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 52: (2.9671368452040445, 0.007916674678809254, 0.01232005059719088), 53: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 54: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 55: (3.070699551364976, 0.006292592803193917, 0.016034062951803207), 56: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 57: (3.0368559536937076, 0.006784119775119118, 0.013155999779701222), 58: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 59: (2.875630327042398, 0.009683067762781711, 0.008324480056762673), 60: (2.7486062148184214, 0.012773504283733491, 0.006257417798042342), 61: (2.9762263224875394, 0.0077592624524099535, 0.007529836893081621), 62: (3.0148540153590613, 0.00712342071843546, 0.007782182097434975), 63: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 64: (2.989777061076018, 0.007530194496840481, 0.007344353199005116), 65: (2.777215221805072, 0.012004347233363365, 0.007192438840866067), 66: (2.614099029701356, 0.017063641108465092, 0.005363926291465759), 67: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 68: (2.8197206933326515, 0.010942885865725937, 0.007550516724586509), 69: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 70: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 71: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 72: (2.8579679658821235, 0.01006517796201528, 0.006564566493034385), 73: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 74: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 75: (2.8578427461103284, 0.010067938036389864, 0.00712003856897353), 76: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 77: (2.926692507871646, 0.008655278882439317, 0.00737684965133667), 78: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 79: (2.945411434880791, 0.008305517094331305, 0.007217621803283669), 80: (2.9549685909975034, 0.008132242598424416, 0.006953896582126612), 81: (3.005514411265544, 0.007272369120876625, 0.022204640507698048), 82: (2.707395073279422, 0.01396463290410086, 0.0027575403451919334), 83: (3.0113191208907693, 0.007179444602253337, 0.006955680251121532), 84: (-7.325341937968756, 6.045422435606574e-07, -0.0066195636987685935), 85: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 86: (-7.325341937968756, 6.045422435606574e-07, -0.0066195636987685935), 87: (-10.74069959243424, 1.6448399515383815e-09, -0.008333107829093911), 88: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 89: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 90: (0.18239733846981665, 0.8572029605082729, 0.00031536519527436413), 91: (1.7362255918841576, 0.09871197028057596, 0.003092189133167278), 92: (-0.940070890721652, 0.3589800896324369, -0.0012825310230255016), 93: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 94: (3.0357114102967033, 0.00680137726430756, 0.0038804203271866067), 95: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 96: (-1.1773778726816657, 0.253583294746442, -0.001657232642173767), 97: (3.5997084988096124, 0.0019097129950801056, 0.004591581225395214), 98: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 99: (1.1089465269046965, 0.28129319148179616, 0.0018455713987350353), 100: (-0.5295364484976446, 0.6025665348471472, -0.0007632926106452831), 101: (1.5058625044790652, 0.1485472554812579, 0.0018052011728286743), 102: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 103: (4.310195331721349, 0.00037750741628889866, 0.0077728137373924144), 104: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 105: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 106: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 107: (0.8210562490126309, 0.4217971975292646, 0.0006834208965301736), 108: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 109: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 110: (1.8198202333697628, 0.08457938902254086, 0.0013935327529907005), 111: (0.4377222947367746, 0.6665230539813192, 0.00022646188735964135), 112: (0.20970731813703808, 0.8361283611725413, 0.00012572109699249268), 113: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 114: (-0.15158254894517134, 0.881114123565619, -0.0001502901315689198), 115: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 116: (2.0145092513411904, 0.05833227460376728, 0.001291951537132241), 117: (0.2962314517029351, 0.7702669343080781, 0.0002052307128906361), 118: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 119: (1.8139215348766842, 0.08551538619544562, 0.0013534307479858287), 120: (-1.0397772310328304, 0.31149499050162893, -0.0004953473806381115), 121: (0.6238769147308664, 0.5401272059841066, 0.0005101948976516946), 122: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 123: (2.0842931669980382, 0.05087023026204952, 0.0026114106178283802), 124: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 125: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 126: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 127: (3.0149132102670153, 0.007122486163198851, 0.0069400861859321705), 128: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 129: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 130: (3.1269969622232923, 0.005550575401041034, 0.012137611210346233), 131: (3.161449722975462, 0.005139245536346475, 0.012094923853874229), 132: (2.670957543312085, 0.015105268001117854, 0.007432873547077168), 133: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 134: (3.1207491378829744, 0.005628526362397582, 0.011614325642585743), 135: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 136: (2.4076877774607914, 0.026379633477613015, 0.006028772145509731), 137: (3.099804932215983, 0.0058976846234807204, 0.013817815482616402), 138: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 139: (2.681860483691142, 0.014754989598012682, 0.005324845761060709), 140: (2.621741612879915, 0.01678706553181637, 0.005616920441389062), 141: (2.7503886062482987, 0.012724240238317888, 0.006610753387212759), 142: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 143: (3.0221484823384093, 0.00700914733911952, 0.008415175974369055)}, 'noun_phrase': {0: (-1.7014712871235786, 0.12306423285839684, -0.008207207918167092), 1: (-1.209152251513571, 0.2574092262783106, -0.005838519334793069), 2: (-2.2741258510416813, 0.04903120185809948, -0.008376097679138161), 3: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 4: (-2.4336529698323597, 0.03775566225680258, -0.008571654558181763), 5: (-2.974418762892723, 0.01559018013610164, -0.011515149474144004), 6: (-3.7429218799226067, 0.0046046824226910355, -0.011580252647399869), 7: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 8: (-2.6748416167039375, 0.025419995672536787, -0.014063483476638772), 9: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 10: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 11: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 12: (-0.868794681803827, 0.4075302724955586, -0.0038636088371276633), 13: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 14: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 15: (-2.565657162667428, 0.030403638037283857, -0.011824379861354817), 16: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 17: (-2.258488164901773, 0.050300726156232385, -0.011758840084075906), 18: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 19: (-1.8880463081963228, 0.09162170682924786, -0.005839028954505887), 20: (-2.4604038966831085, 0.03613469573733932, -0.014544641971588113), 21: (-2.8847849291391174, 0.018036850527221066, -0.01452497243881229), 22: (-2.551173735937456, 0.031134730890924927, -0.01110403537750243), 23: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 24: (-2.9892038014917492, 0.015220565122441269, -0.013150608539581332), 25: (-3.0358582416996263, 0.01411219946624356, -0.009422686696052562), 26: (-1.8660285997449984, 0.0948974771938708, -0.006441432237625111), 27: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 28: (-3.132727195674896, 0.012068561763517274, -0.015891933441162076), 29: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 30: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 31: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 32: (-2.4382151236003016, 0.03747418958339082, -0.010398060083389282), 33: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 34: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 35: (-0.4020122275284977, 0.6970547964026097, -0.001407685875892628), 36: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 37: (-2.3817349153601155, 0.041110618220405626, -0.0105152428150177), 38: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 39: (-0.9283198803266377, 0.3774665466052247, -0.0028055936098098644), 40: (-1.2495992374355025, 0.24296399817492736, -0.006587874889373735), 41: (-0.8132883786986603, 0.4370386133713132, -0.0045864880084991455), 42: (-0.8147147390846828, 0.4362625933545672, -0.0038908302783965953), 43: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 44: (-0.8168166857324468, 0.43512071642609373, -0.004046750068664562), 45: (-1.849788845567326, 0.0973836808318736, -0.008271408081054699), 46: (-2.4891812967461524, 0.03446827802048692, -0.00911563038825991), 47: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 48: (-1.257612326326132, 0.24018284151870845, -0.005986660718917847), 49: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 50: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 51: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 52: (-0.5367128485920113, 0.6044777235518819, -0.0025108575820922963), 53: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 54: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 55: (-1.711825808156662, 0.1210863310467368, -0.008448512852191892), 56: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 57: (-1.4351811948788749, 0.18506084115120194, -0.008757948875427246), 58: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 59: (-1.7385832043518836, 0.11610942142227056, -0.007238876819610551), 60: (-4.4708621725623, 0.0015527346020212072, -0.01286238431930542), 61: (-3.093124421481188, 0.012864420613802998, -0.009431958198547363), 62: (-2.509957558117125, 0.03331307548642898, -0.006254076957702637), 63: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 64: (-2.446109453748497, 0.03699205282025611, -0.0065238356590270885), 65: (-2.7155631563309925, 0.02377991331523115, -0.008957451581954934), 66: (-3.1404512583916464, 0.011919362713343784, -0.008159947395324718), 67: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 68: (-2.258420562695312, 0.050306283711191606, -0.008272564411163308), 69: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 70: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 71: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 72: (-2.137501827401781, 0.06126964941052555, -0.005974161624908436), 73: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 74: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 75: (-2.2966378673043333, 0.04725865309097473, -0.007664144039154053), 76: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 77: (-1.7607714994671344, 0.11212588279486928, -0.006605374813079823), 78: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 79: (-2.1628039190842365, 0.058798454376853486, -0.007389742136001576), 80: (0.6104773495640484, 0.556645368160736, 0.0022977083921432717), 81: (-2.054564425384406, 0.07009414087799887, -0.012084883451461748), 82: (-1.3624638417474084, 0.20616905327334664, -0.00208470821380613), 83: (-0.41301183616454257, 0.6892641446549439, -0.002105090022087086), 84: (-5.627079331263414, 0.00032289986025933146, -0.014079564809799217), 85: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 86: (-5.627079331263414, 0.00032289986025933146, -0.014079564809799217), 87: (-6.451965022375558, 0.00011787055596603068, -0.018509644269943226), 88: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 89: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 90: (-2.7796784174762608, 0.021412021024543516, -0.011031919717788663), 91: (-1.9531050197523963, 0.08255397686718234, -0.007644644379615817), 92: (-3.974534561372611, 0.0032323967288151565, -0.010690855979919422), 93: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 94: (-2.4375247727498524, 0.03751664799519189, -0.0077206671237945446), 95: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 96: (-3.6971469313038927, 0.004942352518800177, -0.011132961511611916), 97: (-2.409284665271311, 0.03929508737744707, -0.006687295436859142), 98: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 99: (-3.391759907387993, 0.007976816914009565, -0.011295613646507274), 100: (-4.550282689732428, 0.0013853788483211749, -0.01318627893924712), 101: (-1.9228295311738242, 0.08666275825938953, -0.00699996352195742), 102: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 103: (-0.8379698118004049, 0.4237422752964861, -0.0032378971576690896), 104: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 105: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 106: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 107: (-0.6687090925180541, 0.520462228362796, -0.0008792579174041748), 108: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 109: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 110: (1.874027241162766, 0.09369492998386465, 0.001990282535552945), 111: (2.5304714659685534, 0.03221044502910602, 0.0031304419040680265), 112: (2.2681340275158024, 0.04951388216076753, 0.0024760425090789573), 113: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 114: (-2.67396811839617, 0.025456402645859244, -0.004358720779418956), 115: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 116: (2.059597017358291, 0.06952548788412616, 0.002565145492553711), 117: (1.7672393612030144, 0.11098865588675123, 0.0019124507904053067), 118: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 119: (2.3449763280740075, 0.043662927431223436, 0.003461676836013783), 120: (2.7105711284566856, 0.023975067436201904, 0.0033237516880035844), 121: (2.6924024594040903, 0.02469913117707077, 0.0031828880310058594), 122: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 123: (1.4427718860170107, 0.1829689959341304, 0.002938568592071533), 124: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 125: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 126: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 127: (0.35754748266078845, 0.7289232559385432, 0.0015774309635162576), 128: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 129: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 130: (0.4769284775470119, 0.6447804326307979, 0.002782040834426902), 131: (1.5916127249552854, 0.14593548854062083, 0.009280022978782665), 132: (-0.1372929792438153, 0.8938215875311126, -0.0008468002080916914), 133: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 134: (0.06349382805961021, 0.9507611144000385, 0.0003700375556945912), 135: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 136: (0.363499115120205, 0.7246239293824269, 0.0017059653997421043), 137: (0.616811119120338, 0.5526407815822985, 0.0034369230270385742), 138: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 139: (0.6503904569933501, 0.5316899776332693, 0.002200816571712505), 140: (0.6637485050695717, 0.5234884334503043, 0.0028235316276550293), 141: (0.033023209048875894, 0.9743768586929915, 0.00017293840646748215), 142: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 143: (0.6544397522467309, 0.5291957439553789, 0.002611398696899414)}, 'original_noun_phrase': {0: (-15.686372171980068, 2.5908143315881138e-51, -0.00538205282969606)}}\n",
            "EI_anger dann original_noun_phrase {'non_dann': {'named': {0: (2.9999473312352563, 0.007362582726028531, 0.008020885288715363), 1: (2.8668234774506334, 0.009871819488213418, 0.007182353734970115), 2: (3.00154155812073, 0.007336638454529805, 0.007494708895683266), 3: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 4: (2.853858915689559, 0.010156129568114373, 0.007833994925022125), 5: (2.828059928865708, 0.010745468775168335, 0.007886649668216728), 6: (3.02450282733906, 0.0069726453063536685, 0.009996812045574177), 7: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 8: (2.8598644963104904, 0.010023463730840692, 0.008409228920936596), 9: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 10: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 11: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 12: (2.5998013893153016, 0.017592679093099937, 0.005481539666652657), 13: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 14: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 15: (2.9326297210120362, 0.008542831810533981, 0.005548548698425304), 16: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 17: (2.940704275103277, 0.008392164112157185, 0.0060208141803741455), 18: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 19: (2.287671406170058, 0.03379490281125083, 0.0036009758710860984), 20: (3.0413286735531093, 0.006717086069541028, 0.014911824464797996), 21: (3.069448373548996, 0.006310132602520979, 0.0162633419036865), 22: (3.0021290719939473, 0.007327099649944999, 0.012680222094059002), 23: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 24: (3.09062885600122, 0.006019513433223663, 0.01488557308912275), 25: (2.956577974398133, 0.008103409424171643, 0.010112801194191001), 26: (2.9864008493337164, 0.007586648071914168, 0.011366653442382801), 27: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 28: (2.973399068655112, 0.0078078980268998345, 0.012191291153430928), 29: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 30: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 31: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 32: (3.034858735548319, 0.006814261596819409, 0.014548160135746002), 33: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 34: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 35: (2.9552849489291804, 0.008126567023332459, 0.013237571716308572), 36: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 37: (2.936128540797658, 0.008477227650781594, 0.011788879334926616), 38: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 39: (2.7435982420580585, 0.012912897795710858, 0.009320780634880066), 40: (3.0405868166075836, 0.006728159852842062, 0.014032751321792603), 41: (3.060608797193966, 0.006435410659359709, 0.014685365557670582), 42: (2.9596167573971948, 0.008049236583012171, 0.012382332980632793), 43: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 44: (3.0580771508201217, 0.00647173267937202, 0.012605533003807068), 45: (2.82915858949088, 0.010719715967741848, 0.007928590476512898), 46: (2.9653970838404464, 0.007947153849508864, 0.01044829487800597), 47: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 48: (2.6817851636071484, 0.014757382691585958, 0.010272590816020943), 49: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 50: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 51: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 52: (2.86993360871273, 0.00980476144910828, 0.010063776373863242), 53: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 54: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 55: (2.916662743131008, 0.008848485787575558, 0.011233554780483268), 56: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 57: (2.743987235061825, 0.012902018625834148, 0.007454979419708241), 58: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 59: (2.594337170862807, 0.01779893404523318, 0.006748458743095376), 60: (2.914960139530747, 0.008881693746406093, 0.006869411468505837), 61: (2.8507182375464906, 0.010226178604013208, 0.006061425805091836), 62: (2.841321460699797, 0.010438544261103, 0.00706293582916262), 63: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 64: (2.885798778955244, 0.00946945042613447, 0.006650090217590332), 65: (2.7587514163403224, 0.012495511330205291, 0.006261751055717468), 66: (2.7396773779959207, 0.013023044734662113, 0.005785688757896423), 67: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 68: (2.812318299243337, 0.011121034507954432, 0.006750799715518951), 69: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 70: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 71: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 72: (2.8672447468516307, 0.00986271068265795, 0.006573969125747703), 73: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 74: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 75: (2.817415509292445, 0.010998067813829594, 0.006316101551055886), 76: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 77: (2.8505265911786, 0.010230468034281626, 0.00683790147304536), 78: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 79: (2.7607764414751292, 0.012440719137313207, 0.006465397775173187), 80: (0.8877558925020694, 0.3857725591115787, 0.001051077246665949), 81: (2.801768111398139, 0.01137974983785741, 0.0159024238586426), 82: (-1.5057271346943946, 0.1485817808527447, -0.0008097499608993308), 83: (1.9700449211964866, 0.06358531425883647, 0.002071602642536141), 84: (-22.95339218952276, 2.5686339773041418e-15, -0.022832025587558757), 85: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 86: (-22.95339218952276, 2.5686339773041418e-15, -0.022832025587558757), 87: (-22.86662846998824, 2.7536733557052225e-15, -0.01856851130723952), 88: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 89: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 90: (-16.768518789837042, 7.632218059649364e-13, -0.017311701178550742), 91: (-11.611768393844091, 4.5059977084044113e-10, -0.009996645152568817), 92: (-23.866649352713203, 1.2532478803632706e-15, -0.020687639713287354), 93: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 94: (-24.985537608446055, 5.384084308191323e-16, -0.01994720846414566), 95: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 96: (-19.523207824261274, 4.935981580158765e-14, -0.018804195523262013), 97: (-23.407583424122, 1.7917538260793985e-15, -0.014919529855251301), 98: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 99: (-26.935806838095047, 1.3401717265174718e-16, -0.023001615703105938), 100: (-29.05155747805047, 3.290875147552598e-17, -0.01941395699977877), 101: (-20.952578853822185, 1.364592063799021e-14, -0.015068808197975181), 102: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 103: (-17.6441388240095, 3.067566632212257e-13, -0.016543084383010886), 104: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 105: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 106: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 107: (-0.6701712489599319, 0.5108126714636266, -0.0008711561560630909), 108: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 109: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 110: (0.2285465785879622, 0.8216631703527664, 0.00027125924825666115), 111: (-1.1693969055389009, 0.25670543531739254, -0.0009917482733726724), 112: (-0.5160453000378008, 0.6117762825661357, -0.0006043612957000732), 113: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 114: (0.19566382318731748, 0.8469506675785499, 0.00022938847541809082), 115: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 116: (-0.4111320283226256, 0.6855758638595414, -0.0005095556378364452), 117: (-0.5973129280740079, 0.5573506555723196, -0.0007590308785438427), 118: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 119: (0.5016432707791384, 0.6216815313223802, 0.0006431668996810802), 120: (-0.7605603194798696, 0.4562551943917086, -0.0009556487202644348), 121: (0.41965824946936614, 0.6794421954453768, 0.0005446463823318703), 122: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 123: (0.44557433780413114, 0.6609402438153469, 0.0005996599793434143), 124: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 125: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 126: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 127: (2.7777349607598487, 0.011990792200825029, 0.005483637750148762), 128: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 129: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 130: (3.0837643105296504, 0.006112248291725002, 0.00957365781068803), 131: (3.1351439301505257, 0.00545050683362242, 0.011689433455467246), 132: (3.0435190939944987, 0.006684492448388404, 0.008233727514743827), 133: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 134: (3.0966805509407807, 0.00593889477616171, 0.00858931094408033), 135: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 136: (2.6523439052590074, 0.015721598320941495, 0.005913108587265015), 137: (3.0953838504201574, 0.005956080152424374, 0.009752169251441956), 138: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 139: (2.657918666541735, 0.015534546577635117, 0.0047150939702987615), 140: (2.5123701025065484, 0.02118038133567884, 0.004118752479553234), 141: (2.708947810106843, 0.013917895751007657, 0.003966300189495092), 142: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 143: (2.965174451191324, 0.007951062361363352, 0.007695800065994268)}, 'noun_phrase': {0: (-1.823182545892379, 0.10158903404476334, -0.004346001148223921), 1: (0.6529009636033906, 0.5301427638966068, 0.0014689952135086282), 2: (-1.9147767672455225, 0.08778776509585746, -0.004484418034553517), 3: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 4: (-1.8334472014081977, 0.09994691694819158, -0.004479753971099842), 5: (-1.6768964109221438, 0.12787725657828872, -0.006013435125350963), 6: (-2.411025152323756, 0.03918309767510654, -0.006692928075790416), 7: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 8: (-2.50267900383429, 0.033713307978660734, -0.0077323317527771), 9: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 10: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 11: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 12: (-1.2215763576396204, 0.2528992657800809, -0.0032196015119552723), 13: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 14: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 15: (-2.0185489614696026, 0.07429584774720312, -0.005984464287757896), 16: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 17: (-2.502975107022215, 0.03369693249034014, -0.006320160627365068), 18: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 19: (-0.8907364716036349, 0.3962576215910443, -0.0022437006235123125), 20: (-2.8025004219687366, 0.020628201662149168, -0.011005443334579423), 21: (-2.107034519266076, 0.06437899542473148, -0.008922708034515447), 22: (-2.9943248021882756, 0.015094652822419669, -0.01277201473712919), 23: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 24: (-3.3972888461876676, 0.007907222552312736, -0.014024832844734214), 25: (-4.1748166136782485, 0.0023944354171003945, -0.012944406270980868), 26: (-4.1962723641365995, 0.002319461381102061, -0.01413772106170652), 27: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 28: (-2.695548618638821, 0.024572183386920812, -0.010166427493095376), 29: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 30: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 31: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 32: (-2.843643522099154, 0.01928815883461188, -0.009994786977767955), 33: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 34: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 35: (-3.7646858982640334, 0.004452747333857424, -0.011769860982894897), 36: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 37: (-3.7324423984291, 0.004679780409504576, -0.014772346615791354), 38: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 39: (-3.2359231103348005, 0.010225186578521093, -0.009877172112464916), 40: (-2.3616352973704924, 0.04248737155882671, -0.005944323539733842), 41: (-0.6965706128165664, 0.5036633522655196, -0.0019025415182113425), 42: (-2.1819167419098173, 0.056996194905469984, -0.006671530008315996), 43: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 44: (-1.8999677088194737, 0.08989273023884006, -0.006697237491607666), 45: (-2.887450189083182, 0.017958705520828136, -0.008153745532035794), 46: (-3.6971182160452765, 0.004942572347528052, -0.009962809085845903), 47: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 48: (-1.7840686423691154, 0.10807941437847667, -0.0044749736785889005), 49: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 50: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 51: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 52: (-2.012770107772844, 0.07499209109599361, -0.005148887634277344), 53: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 54: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 55: (-3.3358841727430946, 0.008717067187864204, -0.01025363802909851), 56: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 57: (-3.0324117689742924, 0.014191161631230537, -0.010666310787200928), 58: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 59: (-2.709559987535502, 0.024014793251107003, -0.008456566929817166), 60: (-5.612223135587408, 0.00032908633523572386, -0.007830667495727472), 61: (-3.044537086563111, 0.01391535583938568, -0.004270273447036788), 62: (-4.466322721932532, 0.0015629322764594033, -0.004947292804717951), 63: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 64: (-2.9114347754278325, 0.017270871413094906, -0.004106640815734863), 65: (-3.3891489240165122, 0.008009904035483717, -0.005237913131713845), 66: (-4.262865332703453, 0.00210226306433529, -0.006344011425972018), 67: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 68: (-2.183246878448915, 0.05687278800943512, -0.0033423155546188354), 69: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 70: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 71: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 72: (-2.5803190667246017, 0.029681082044315344, -0.0034605830907821766), 73: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 74: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 75: (-2.563400996839954, 0.03051638168160957, -0.004253998398780823), 76: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 77: (-2.2429798918897146, 0.051591542039749805, -0.004107862710952759), 78: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 79: (-2.8804512051376103, 0.018164657500522638, -0.004243367910385154), 80: (-3.446982093301815, 0.0073094595679489595, -0.0070547282695770375), 81: (-2.877209148867109, 0.018260874752402068, -0.009688264131546054), 82: (-4.375781209611118, 0.0017820273124026433, -0.005826637148857117), 83: (-4.227395409459643, 0.002215116531872715, -0.008993631601333596), 84: (-10.859157548412043, 1.794588733464013e-06, -0.028087219595909096), 85: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 86: (-10.859157548412043, 1.794588733464013e-06, -0.028087219595909096), 87: (-13.425354861271588, 2.9440041519051927e-07, -0.028906109929084767), 88: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 89: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 90: (-16.15389910304737, 5.917210079387948e-08, -0.026733589172363237), 91: (-10.429215707109279, 2.5193237097096096e-06, -0.02062155306339264), 92: (-15.871086648344093, 6.902775576577911e-08, -0.026989573240280174), 93: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 94: (-19.213494334267327, 1.2930383331406867e-08, -0.0260120451450348), 95: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 96: (-17.396849352413383, 3.094297015510351e-08, -0.027425014972686734), 97: (-11.630878088616768, 1.0043174949080335e-06, -0.023780626058578502), 98: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 99: (-16.256511459884884, 5.5990327970512016e-08, -0.03293475806713103), 100: (-14.966067719944089, 1.1505091346154834e-07, -0.02911258637905123), 101: (-8.9014716568645, 9.34175689190008e-06, -0.02603101432323457), 102: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 103: (-11.619817167101273, 1.0124617705326627e-06, -0.025008597970008872), 104: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 105: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 106: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 107: (-0.9049840849981707, 0.38905713129157093, -0.0011850744485855103), 108: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 109: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 110: (0.14430588605679148, 0.8884386416264047, 0.00020279288291930042), 111: (-0.9779956544240272, 0.3536312369845376, -0.0014018923044205045), 112: (-0.6577968932510961, 0.52713316186464, -0.000915491580963157), 113: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 114: (-1.73562370385813, 0.11665049488657796, -0.002127626538276639), 115: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 116: (-0.7291060706599362, 0.48447730366659025, -0.0009533852338791116), 117: (-1.276532090034004, 0.2337205874074845, -0.0018397957086563332), 118: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 119: (-0.11016672130588094, 0.9146946252160408, -0.00012453794479372338), 120: (0.6337352230370069, 0.5420222418426753, 0.000764855742454551), 121: (0.6038074636841404, 0.5608803614850112, 0.0007824897766113281), 122: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 123: (0.6025133045306404, 0.5617041956500191, 0.0009518772363663053), 124: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 125: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 126: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 127: (-1.5586334828057054, 0.15351219246910677, -0.003723523020744357), 128: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 129: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 130: (-0.8889873387795206, 0.39714807548087205, -0.0023648232221603616), 131: (1.8731454777310295, 0.09382679279907638, 0.005485448241233815), 132: (-1.4602469096557182, 0.17823119122489203, -0.00560216009616854), 133: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 134: (-1.2349301099673444, 0.24812404639205596, -0.0043000727891922), 135: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 136: (-0.48362082848308674, 0.6402040030990057, -0.0012556225061416404), 137: (-0.8892996750279333, 0.3969889665798001, -0.0023857623338698897), 138: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 139: (-4.450203928550094, 0.001599725184121625, -0.008053421974182129), 140: (-4.422306758793129, 0.0016656100862283232, -0.009977713227272034), 141: (-2.3976431527953115, 0.04005235709894506, -0.007599052786827071), 142: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 143: (-1.7572022516970278, 0.11275805348644675, -0.005658695101737943)}, 'original_noun_phrase': {0: (-24.1029332123197, 4.213490669717725e-108, -0.007616794750922251)}}, 'dann': {'named': {0: (3.1541497723996583, 0.00522384135791289, 0.00986942350864406), 1: (3.181309271590398, 0.004915805191278162, 0.011457297205924977), 2: (3.127365912520824, 0.005546005199175376, 0.009590110182762124), 3: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 4: (3.1493556275382018, 0.005280135194466702, 0.010423678159713734), 5: (3.1297181110943186, 0.00551695442450737, 0.007126376032829285), 6: (3.08688816725016, 0.006069876481733846, 0.010540437698364269), 7: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 8: (3.1645958707087574, 0.00510319942100133, 0.011332765221595764), 9: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 10: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 11: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 12: (3.102131411924078, 0.005867179401643754, 0.0061305731534958), 13: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 14: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 15: (3.0000584814268514, 0.007360771004279333, 0.0068900465965270885), 16: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 17: (3.0841412656010374, 0.006107120088498524, 0.006706812977790855), 18: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 19: (2.5196051941625384, 0.020859143591782148, 0.0034940391778945923), 20: (3.139819430130235, 0.005393873686063911, 0.01886967718601229), 21: (3.154179233456912, 0.005223497234744051, 0.02260468602180482), 22: (3.1507253686005186, 0.0052639913739709054, 0.016295456886291526), 23: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 24: (3.1363917765936224, 0.005435335588589249, 0.01601330339908602), 25: (3.0916877516329184, 0.006005330815079033, 0.00884194076061251), 26: (3.017501051367118, 0.0070817458098385395, 0.007745276391506184), 27: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 28: (3.1120514476799865, 0.005738819214552431, 0.014920613169670083), 29: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 30: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 31: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 32: (3.1078968924867683, 0.005792241140374358, 0.01412692666053772), 33: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 34: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 35: (3.0885493840039504, 0.006047460221819138, 0.010939228534698475), 36: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 37: (3.1264344405222126, 0.005557550435755567, 0.013928736746311177), 38: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 39: (2.721222332581404, 0.013553645016633994, 0.005284606665372837), 40: (3.068690031293318, 0.00632078661724058, 0.019550687074661233), 41: (3.07003739288343, 0.006301869452771459, 0.02186712622642517), 42: (3.0644857210587464, 0.006380170879024848, 0.019834151864051797), 43: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 44: (3.0976252131600672, 0.005926405378420299, 0.018958660960197438), 45: (2.8919449369729633, 0.009342539390275371, 0.013088151812553406), 46: (3.0509629683921577, 0.00657486941097741, 0.010790181159973156), 47: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 48: (2.9229009235242236, 0.00872783447418792, 0.01431553065776825), 49: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 50: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 51: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 52: (2.9671368452040445, 0.007916674678809254, 0.01232005059719088), 53: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 54: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 55: (3.070699551364976, 0.006292592803193917, 0.016034062951803207), 56: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 57: (3.0368559536937076, 0.006784119775119118, 0.013155999779701222), 58: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 59: (2.875630327042398, 0.009683067762781711, 0.008324480056762673), 60: (2.7486062148184214, 0.012773504283733491, 0.006257417798042342), 61: (2.9762263224875394, 0.0077592624524099535, 0.007529836893081621), 62: (3.0148540153590613, 0.00712342071843546, 0.007782182097434975), 63: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 64: (2.989777061076018, 0.007530194496840481, 0.007344353199005116), 65: (2.777215221805072, 0.012004347233363365, 0.007192438840866067), 66: (2.614099029701356, 0.017063641108465092, 0.005363926291465759), 67: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 68: (2.8197206933326515, 0.010942885865725937, 0.007550516724586509), 69: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 70: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 71: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 72: (2.8579679658821235, 0.01006517796201528, 0.006564566493034385), 73: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 74: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 75: (2.8578427461103284, 0.010067938036389864, 0.00712003856897353), 76: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 77: (2.926692507871646, 0.008655278882439317, 0.00737684965133667), 78: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 79: (2.945411434880791, 0.008305517094331305, 0.007217621803283669), 80: (2.9549685909975034, 0.008132242598424416, 0.006953896582126612), 81: (3.005514411265544, 0.007272369120876625, 0.022204640507698048), 82: (2.707395073279422, 0.01396463290410086, 0.0027575403451919334), 83: (3.0113191208907693, 0.007179444602253337, 0.006955680251121532), 84: (-7.325341937968756, 6.045422435606574e-07, -0.0066195636987685935), 85: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 86: (-7.325341937968756, 6.045422435606574e-07, -0.0066195636987685935), 87: (-10.74069959243424, 1.6448399515383815e-09, -0.008333107829093911), 88: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 89: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 90: (0.18239733846981665, 0.8572029605082729, 0.00031536519527436413), 91: (1.7362255918841576, 0.09871197028057596, 0.003092189133167278), 92: (-0.940070890721652, 0.3589800896324369, -0.0012825310230255016), 93: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 94: (3.0357114102967033, 0.00680137726430756, 0.0038804203271866067), 95: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 96: (-1.1773778726816657, 0.253583294746442, -0.001657232642173767), 97: (3.5997084988096124, 0.0019097129950801056, 0.004591581225395214), 98: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 99: (1.1089465269046965, 0.28129319148179616, 0.0018455713987350353), 100: (-0.5295364484976446, 0.6025665348471472, -0.0007632926106452831), 101: (1.5058625044790652, 0.1485472554812579, 0.0018052011728286743), 102: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 103: (4.310195331721349, 0.00037750741628889866, 0.0077728137373924144), 104: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 105: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 106: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 107: (0.8210562490126309, 0.4217971975292646, 0.0006834208965301736), 108: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 109: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 110: (1.8198202333697628, 0.08457938902254086, 0.0013935327529907005), 111: (0.4377222947367746, 0.6665230539813192, 0.00022646188735964135), 112: (0.20970731813703808, 0.8361283611725413, 0.00012572109699249268), 113: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 114: (-0.15158254894517134, 0.881114123565619, -0.0001502901315689198), 115: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 116: (2.0145092513411904, 0.05833227460376728, 0.001291951537132241), 117: (0.2962314517029351, 0.7702669343080781, 0.0002052307128906361), 118: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 119: (1.8139215348766842, 0.08551538619544562, 0.0013534307479858287), 120: (-1.0397772310328304, 0.31149499050162893, -0.0004953473806381115), 121: (0.6238769147308664, 0.5401272059841066, 0.0005101948976516946), 122: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 123: (2.0842931669980382, 0.05087023026204952, 0.0026114106178283802), 124: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 125: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 126: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 127: (3.0149132102670153, 0.007122486163198851, 0.0069400861859321705), 128: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 129: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 130: (3.1269969622232923, 0.005550575401041034, 0.012137611210346233), 131: (3.161449722975462, 0.005139245536346475, 0.012094923853874229), 132: (2.670957543312085, 0.015105268001117854, 0.007432873547077168), 133: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 134: (3.1207491378829744, 0.005628526362397582, 0.011614325642585743), 135: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 136: (2.4076877774607914, 0.026379633477613015, 0.006028772145509731), 137: (3.099804932215983, 0.0058976846234807204, 0.013817815482616402), 138: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 139: (2.681860483691142, 0.014754989598012682, 0.005324845761060709), 140: (2.621741612879915, 0.01678706553181637, 0.005616920441389062), 141: (2.7503886062482987, 0.012724240238317888, 0.006610753387212759), 142: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 143: (3.0221484823384093, 0.00700914733911952, 0.008415175974369055)}, 'noun_phrase': {0: (-1.7014712871235786, 0.12306423285839684, -0.008207207918167092), 1: (-1.209152251513571, 0.2574092262783106, -0.005838519334793069), 2: (-2.2741258510416813, 0.04903120185809948, -0.008376097679138161), 3: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 4: (-2.4336529698323597, 0.03775566225680258, -0.008571654558181763), 5: (-2.974418762892723, 0.01559018013610164, -0.011515149474144004), 6: (-3.7429218799226067, 0.0046046824226910355, -0.011580252647399869), 7: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 8: (-2.6748416167039375, 0.025419995672536787, -0.014063483476638772), 9: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 10: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 11: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 12: (-0.868794681803827, 0.4075302724955586, -0.0038636088371276633), 13: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 14: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 15: (-2.565657162667428, 0.030403638037283857, -0.011824379861354817), 16: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 17: (-2.258488164901773, 0.050300726156232385, -0.011758840084075906), 18: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 19: (-1.8880463081963228, 0.09162170682924786, -0.005839028954505887), 20: (-2.4604038966831085, 0.03613469573733932, -0.014544641971588113), 21: (-2.8847849291391174, 0.018036850527221066, -0.01452497243881229), 22: (-2.551173735937456, 0.031134730890924927, -0.01110403537750243), 23: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 24: (-2.9892038014917492, 0.015220565122441269, -0.013150608539581332), 25: (-3.0358582416996263, 0.01411219946624356, -0.009422686696052562), 26: (-1.8660285997449984, 0.0948974771938708, -0.006441432237625111), 27: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 28: (-3.132727195674896, 0.012068561763517274, -0.015891933441162076), 29: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 30: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 31: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 32: (-2.4382151236003016, 0.03747418958339082, -0.010398060083389282), 33: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 34: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 35: (-0.4020122275284977, 0.6970547964026097, -0.001407685875892628), 36: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 37: (-2.3817349153601155, 0.041110618220405626, -0.0105152428150177), 38: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 39: (-0.9283198803266377, 0.3774665466052247, -0.0028055936098098644), 40: (-1.2495992374355025, 0.24296399817492736, -0.006587874889373735), 41: (-0.8132883786986603, 0.4370386133713132, -0.0045864880084991455), 42: (-0.8147147390846828, 0.4362625933545672, -0.0038908302783965953), 43: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 44: (-0.8168166857324468, 0.43512071642609373, -0.004046750068664562), 45: (-1.849788845567326, 0.0973836808318736, -0.008271408081054699), 46: (-2.4891812967461524, 0.03446827802048692, -0.00911563038825991), 47: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 48: (-1.257612326326132, 0.24018284151870845, -0.005986660718917847), 49: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 50: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 51: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 52: (-0.5367128485920113, 0.6044777235518819, -0.0025108575820922963), 53: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 54: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 55: (-1.711825808156662, 0.1210863310467368, -0.008448512852191892), 56: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 57: (-1.4351811948788749, 0.18506084115120194, -0.008757948875427246), 58: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 59: (-1.7385832043518836, 0.11610942142227056, -0.007238876819610551), 60: (-4.4708621725623, 0.0015527346020212072, -0.01286238431930542), 61: (-3.093124421481188, 0.012864420613802998, -0.009431958198547363), 62: (-2.509957558117125, 0.03331307548642898, -0.006254076957702637), 63: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 64: (-2.446109453748497, 0.03699205282025611, -0.0065238356590270885), 65: (-2.7155631563309925, 0.02377991331523115, -0.008957451581954934), 66: (-3.1404512583916464, 0.011919362713343784, -0.008159947395324718), 67: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 68: (-2.258420562695312, 0.050306283711191606, -0.008272564411163308), 69: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 70: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 71: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 72: (-2.137501827401781, 0.06126964941052555, -0.005974161624908436), 73: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 74: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 75: (-2.2966378673043333, 0.04725865309097473, -0.007664144039154053), 76: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 77: (-1.7607714994671344, 0.11212588279486928, -0.006605374813079823), 78: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 79: (-2.1628039190842365, 0.058798454376853486, -0.007389742136001576), 80: (0.6104773495640484, 0.556645368160736, 0.0022977083921432717), 81: (-2.054564425384406, 0.07009414087799887, -0.012084883451461748), 82: (-1.3624638417474084, 0.20616905327334664, -0.00208470821380613), 83: (-0.41301183616454257, 0.6892641446549439, -0.002105090022087086), 84: (-5.627079331263414, 0.00032289986025933146, -0.014079564809799217), 85: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 86: (-5.627079331263414, 0.00032289986025933146, -0.014079564809799217), 87: (-6.451965022375558, 0.00011787055596603068, -0.018509644269943226), 88: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 89: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 90: (-2.7796784174762608, 0.021412021024543516, -0.011031919717788663), 91: (-1.9531050197523963, 0.08255397686718234, -0.007644644379615817), 92: (-3.974534561372611, 0.0032323967288151565, -0.010690855979919422), 93: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 94: (-2.4375247727498524, 0.03751664799519189, -0.0077206671237945446), 95: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 96: (-3.6971469313038927, 0.004942352518800177, -0.011132961511611916), 97: (-2.409284665271311, 0.03929508737744707, -0.006687295436859142), 98: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 99: (-3.391759907387993, 0.007976816914009565, -0.011295613646507274), 100: (-4.550282689732428, 0.0013853788483211749, -0.01318627893924712), 101: (-1.9228295311738242, 0.08666275825938953, -0.00699996352195742), 102: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 103: (-0.8379698118004049, 0.4237422752964861, -0.0032378971576690896), 104: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 105: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 106: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 107: (-0.6687090925180541, 0.520462228362796, -0.0008792579174041748), 108: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 109: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 110: (1.874027241162766, 0.09369492998386465, 0.001990282535552945), 111: (2.5304714659685534, 0.03221044502910602, 0.0031304419040680265), 112: (2.2681340275158024, 0.04951388216076753, 0.0024760425090789573), 113: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 114: (-2.67396811839617, 0.025456402645859244, -0.004358720779418956), 115: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 116: (2.059597017358291, 0.06952548788412616, 0.002565145492553711), 117: (1.7672393612030144, 0.11098865588675123, 0.0019124507904053067), 118: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 119: (2.3449763280740075, 0.043662927431223436, 0.003461676836013783), 120: (2.7105711284566856, 0.023975067436201904, 0.0033237516880035844), 121: (2.6924024594040903, 0.02469913117707077, 0.0031828880310058594), 122: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 123: (1.4427718860170107, 0.1829689959341304, 0.002938568592071533), 124: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 125: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 126: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 127: (0.35754748266078845, 0.7289232559385432, 0.0015774309635162576), 128: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 129: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 130: (0.4769284775470119, 0.6447804326307979, 0.002782040834426902), 131: (1.5916127249552854, 0.14593548854062083, 0.009280022978782665), 132: (-0.1372929792438153, 0.8938215875311126, -0.0008468002080916914), 133: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 134: (0.06349382805961021, 0.9507611144000385, 0.0003700375556945912), 135: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 136: (0.363499115120205, 0.7246239293824269, 0.0017059653997421043), 137: (0.616811119120338, 0.5526407815822985, 0.0034369230270385742), 138: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 139: (0.6503904569933501, 0.5316899776332693, 0.002200816571712505), 140: (0.6637485050695717, 0.5234884334503043, 0.0028235316276550293), 141: (0.033023209048875894, 0.9743768586929915, 0.00017293840646748215), 142: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 143: (0.6544397522467309, 0.5291957439553789, 0.002611398696899414)}, 'original_noun_phrase': {0: (-15.686372171980068, 2.5908143315881138e-51, -0.00538205282969606)}}}\n",
            "EI_fear non_dann named\n",
            "named {0: (-0.4536497069305446, 0.6552198366825752, -0.0006526619195937888), 1: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 2: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 3: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 4: (-0.8453837185365612, 0.40841698204839616, -0.0013110116124153137), 5: (-0.46427452094332555, 0.6477266892107059, -0.0005847990512848344), 6: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 7: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 8: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 9: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 10: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 11: (-0.11691070026151802, 0.9081577799239737, -0.000165420770645186), 12: (2.0225299646717434, 0.05742722578522014, 0.0029049932956695113), 13: (-0.26262674405991276, 0.7956648531240026, -0.0004011809825897883), 14: (-0.08360809439295097, 0.9342426460836097, -0.00010615885257725388), 15: (-0.709156222445198, 0.4868391424395345, -0.0005408331751823314), 16: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 17: (-0.7959533420944594, 0.43589159110764286, -0.0009384125471115223), 18: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 19: (-0.7651367147586945, 0.45358998697214725, -0.0010971322655677906), 20: (1.5288874046307366, 0.142770504358485, 0.00632993280887606), 21: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 22: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 23: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 24: (1.5561215016818635, 0.13617942425602786, 0.006514191627502441), 25: (1.324449090361284, 0.2010655150470798, 0.005747288465499878), 26: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 27: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 28: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 29: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 30: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 31: (1.7506147286030171, 0.09614284165925152, 0.00850757062435159), 32: (1.8548369171790378, 0.07920530825837102, 0.00788701772689826), 33: (1.6009913019210766, 0.12587473375451938, 0.007541057467460699), 34: (1.3659567646552007, 0.18790451769818264, 0.005688571929931663), 35: (1.26755219530107, 0.2202730831554025, 0.005834899842739105), 36: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 37: (1.7587175226377527, 0.09472170382020201, 0.008782824873924289), 38: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 39: (0.9488361249336216, 0.3546174940700779, 0.004171487689018205), 40: (0.8765149718311525, 0.39169780491417294, 0.0021890848875045776), 41: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 42: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 43: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 44: (2.1282274166381874, 0.046626328415440045, 0.005648136138916016), 45: (1.6609366785571051, 0.11313885567545193, 0.003921613097190857), 46: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 47: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 48: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 49: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 50: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 51: (1.3946870168456393, 0.17920377269488452, 0.0037615150213241577), 52: (1.6353851739741603, 0.11842713792866925, 0.00456748902797699), 53: (1.3315823913263836, 0.1987533241794981, 0.004584217071533225), 54: (1.3371353570410147, 0.19696795927159966, 0.00397452712059021), 55: (1.779829401688538, 0.09110404205894308, 0.00402035713195803), 56: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 57: (2.019810376955619, 0.05773267898214161, 0.0058293938636779785), 58: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 59: (1.0962920103537341, 0.2866524914517763, 0.0026982814073562844), 60: (0.4370111105755945, 0.6670297001581933, 0.0006972163915633933), 61: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 62: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 63: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 64: (1.1558873260782965, 0.2620559169228458, 0.0017056524753570335), 65: (0.6085352404626192, 0.5500392182921371, 0.0007077634334564653), 66: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 67: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 68: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 69: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 70: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 71: (0.9027840809592305, 0.37794389342005685, 0.0010757893323898315), 72: (1.0765194416528894, 0.29517481630283526, 0.0015606909990311113), 73: (1.1249983108921853, 0.27460131610512795, 0.0015131771564483865), 74: (0.9582896293698356, 0.3499530367509156, 0.0014142096042633057), 75: (-0.3494323682524638, 0.730607818680475, -0.00045030713081362084), 76: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 77: (0.43274634082896746, 0.6700713494942525, 0.0006299242377281189), 78: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 79: (0.831870192340097, 0.41581548127152934, 0.0010650619864464028), 80: (-0.6664111234719685, 0.5131597453870926, -0.0017973065376281627), 81: (0.7780572169891012, 0.44611673216458503, 0.0031023234128951804), 82: (-0.2686312437643469, 0.791108615990461, -0.00022893697023396165), 83: (-0.5457728927551285, 0.5915729724571748, -0.0005119055509567594), 84: (4.561650474819855, 0.00021319750591981511, 0.00431002676486969), 85: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 86: (4.561650474819855, 0.00021319750591981511, 0.00431002676486969), 87: (11.010716603364635, 1.0921299854413208e-09, 0.010568979382514998), 88: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 89: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 90: (1.0637210480277177, 0.30078805955814025, 0.0011882573366165383), 91: (3.6135226538296683, 0.001850616143029893, 0.003304967284202487), 92: (7.622456405355375, 3.410296896217827e-07, 0.008830562233924866), 93: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 94: (4.938953194680991, 9.112475991874698e-05, 0.005027011036872864), 95: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 96: (6.714881111632672, 2.0348426215097325e-06, 0.007181236147880576), 97: (-0.07731640103103846, 0.9391802415001849, -8.628368377683326e-05), 98: (2.79280976762182, 0.011603946348508714, 0.0030788868665695412), 99: (2.903617370634803, 0.009106010357973707, 0.0030685946345329063), 100: (0.9654002716532625, 0.34647240489796993, 0.001101657748222351), 101: (10.59050226920981, 2.0724162787332835e-09, 0.010231825709342945), 102: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 103: (5.4843523237003415, 2.7265752737184905e-05, 0.005727683007717144), 104: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 105: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 106: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 107: (-0.3181158623863308, 0.7538670422588711, -0.00038250088691710316), 108: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 109: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 110: (-0.17260063152673366, 0.8647906774407321, -0.00022515356540681042), 111: (0.1146094829785176, 0.9099571640600346, 0.00012142956256866455), 112: (0.08284497610249766, 0.9348413845528509, 9.741485118863746e-05), 113: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 114: (0.09216537345907856, 0.9275314887096396, 0.00010473728179938302), 115: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 116: (0.4632378052925146, 0.648456153909601, 0.0005057364702224509), 117: (-0.26042867881764836, 0.7973346475213109, -0.00029164254665370315), 118: (-0.3750008117769526, 0.7118131023296859, -0.000460708141326871), 119: (-0.6469147561060148, 0.5254261353596277, -0.000817073881626107), 120: (-0.29662372851395696, 0.7699719612142981, -0.000387200713157676), 121: (-0.7101735937355383, 0.4862224188674523, -0.0010588198900222667), 122: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 123: (-0.4844556320193189, 0.6336001696899646, -0.000822190940380052), 124: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 125: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 126: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 127: (2.2785103645191667, 0.03443363204092818, 0.009405428171157859), 128: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 129: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 130: (2.359254180695483, 0.029168546047316737, 0.012106120586395264), 131: (2.665775666150978, 0.015274503537344412, 0.01082838624715804), 132: (2.640052341317529, 0.016141602275097384, 0.011544437706470456), 133: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 134: (2.359823315241125, 0.029134239010212195, 0.00960416495800015), 135: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 136: (2.2719409701125346, 0.03489850091617077, 0.008913415670394942), 137: (2.410227595417562, 0.026240474368313817, 0.01069551706314087), 138: (2.4547128014911554, 0.023911529244833432, 0.009658692777156863), 139: (2.243085573650609, 0.03700966254519938, 0.010553579777479172), 140: (1.861165540870239, 0.07826668314737244, 0.007289951294660535), 141: (2.410029655559331, 0.026251295034089187, 0.007672186940908404), 142: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 143: (2.314148366568837, 0.032009860677918146, 0.009153123199939772)}\n",
            "EI_fear non_dann noun_phrase\n",
            "noun_phrase {0: (-1.9359197349281163, 0.08486303438398021, -0.003261709213256858), 1: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 2: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 3: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 4: (-3.5921207239926622, 0.005819472709776622, -0.006515455245971691), 5: (-2.2405191888039777, 0.05179931555336764, -0.004590368270874001), 6: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 7: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 8: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 9: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 10: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 11: (-2.177198306446434, 0.057436064972269864, -0.005819413065910328), 12: (-1.400019122170044, 0.19502306335776495, -0.004387927055358953), 13: (-3.8281904159932822, 0.004038902993587881, -0.005794817209243797), 14: (-3.0418381044554876, 0.013976266320790747, -0.005463844537734963), 15: (-3.002390611876544, 0.014898501704831292, -0.006923961639404286), 16: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 17: (-1.3965123499270924, 0.19604149075064167, -0.005478394031524669), 18: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 19: (-1.5320431542102702, 0.15987433423037603, -0.0039048135280608798), 20: (-1.2667867509932154, 0.23703097219497393, -0.004638332128524714), 21: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 22: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 23: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 24: (-1.5919673048439271, 0.1458558787983439, -0.005894666910171487), 25: (-1.9018239322840609, 0.08962630091028609, -0.007182878255844183), 26: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 27: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 28: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 29: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 30: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 31: (-2.231033720451802, 0.0526079234418805, -0.009498947858810391), 32: (-1.6722600173987396, 0.12880431947882942, -0.006319308280944846), 33: (-1.319104625064257, 0.21970751880154052, -0.005503982305526733), 34: (-1.6426000220670234, 0.13488091489643397, -0.006459599733352639), 35: (-1.8324739912421704, 0.10010153939268446, -0.007775568962097135), 36: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 37: (-0.5298460653769466, 0.6090391421319425, -0.0025731325149536133), 38: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 39: (-2.0563281051041438, 0.06989434897981066, -0.0072820544242858665), 40: (-1.832659175296179, 0.100072100381963, -0.0058657824993133545), 41: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 42: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 43: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 44: (-1.8572277489032036, 0.09623735235256793, -0.00609959363937379), 45: (-2.0199085350068584, 0.07413294507286643, -0.005737346410751298), 46: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 47: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 48: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 49: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 50: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 51: (-2.0686929741325577, 0.06850892653616228, -0.010611301660537698), 52: (-2.1190318543237137, 0.06313680911664347, -0.0076056540012360285), 53: (-1.371374748224241, 0.20347631027779792, -0.00496740341186519), 54: (-1.670398188835708, 0.1291783171103463, -0.00642936229705815), 55: (-1.805673082097354, 0.10444831758628668, -0.009003195166587818), 56: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 57: (-1.1323926277300542, 0.28673600315853054, -0.005244308710098289), 58: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 59: (-1.2051019454800171, 0.2588935665517983, -0.00519210398197173), 60: (-1.3078051242097022, 0.2233563599285183, -0.004501736164092995), 61: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 62: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 63: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 64: (-1.1304764999833943, 0.287500792462536, -0.004508697986602805), 65: (-1.5776562043906954, 0.14910008006886857, -0.0054946839809417725), 66: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 67: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 68: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 69: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 70: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 71: (-1.1228025481450146, 0.2905799023735358, -0.0034292519092559703), 72: (-1.1442219595457226, 0.2820501762831403, -0.002496677637100153), 73: (-1.39756984179323, 0.19573389666515814, -0.004101824760437078), 74: (-1.782239322829124, 0.10839218870326162, -0.0048563122749327725), 75: (-1.1621705134074587, 0.2750570177717406, -0.004986456036567721), 76: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 77: (-1.4777500256179747, 0.17359339189294204, -0.005267497897148143), 78: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 79: (-1.3501294547030407, 0.20994633313990962, -0.004570305347442627), 80: (-0.41295578067509575, 0.6893037499489926, -0.002099892497062661), 81: (-1.7588500575013584, 0.11246579297791844, -0.009781035780906722), 82: (-2.0992377338378363, 0.06519888246336426, -0.0063045263290405495), 83: (-1.7171570209124136, 0.12007936926462812, -0.006237471103668235), 84: (-0.5440740393161605, 0.5996079486356212, -0.002246636152267445), 85: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 86: (-0.5440740393161605, 0.5996079486356212, -0.002246636152267445), 87: (1.0527153678445045, 0.3199131931574301, 0.0033967435359955056), 88: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 89: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 90: (-2.143597626272751, 0.06066523565255921, -0.008459538221359253), 91: (-2.2476325636466004, 0.05120091292210157, -0.008722174167633101), 92: (-0.2006075316872387, 0.8454655225549137, -0.0007541239261626975), 93: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 94: (-1.4932396741932192, 0.1695775885824614, -0.005306470394134455), 95: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 96: (-0.3191203618275257, 0.7569175311159203, -0.0011212348937987837), 97: (-2.458981689327924, 0.03621910324283895, -0.008778303861618042), 98: (-1.5699961994350784, 0.15086293024603037, -0.005399644374847412), 99: (-1.7332341226006547, 0.11708906381472337, -0.006020474433898915), 100: (-2.2478215731877276, 0.051185105408517506, -0.007329279184341453), 101: (0.9242215188599492, 0.3794839117881119, 0.004103481769561768), 102: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 103: (-1.2435497732198746, 0.2450811694075575, -0.005191737413406394), 104: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 105: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 106: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 107: (-1.2584645766151907, 0.23988859851978603, -0.004110944271087658), 108: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 109: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 110: (-1.8641328798290278, 0.09518460229791703, -0.006200087070465099), 111: (-1.4991464007925193, 0.1680678520803307, -0.00435821413993831), 112: (-1.755279233997687, 0.11310001292177661, -0.005064666271209717), 113: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 114: (-1.6163046921037871, 0.14048400703572178, -0.005287939310073764), 115: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 116: (-1.9711279406636157, 0.08019647814243068, -0.005731606483459495), 117: (-1.75484549632847, 0.11317727421496512, -0.00519672632217405), 118: (-1.6664082176427797, 0.12998313803203515, -0.0054109632968902255), 119: (-2.0442336127660954, 0.07127548469948174, -0.005514851212501504), 120: (-1.7009025325257665, 0.12317372724695272, -0.004597327113151528), 121: (-1.5645063437346745, 0.15213777580428148, -0.004361140727996815), 122: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 123: (-1.6247418312440645, 0.13866371532846725, -0.004948762059211742), 124: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 125: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 126: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 127: (-1.3397891031696871, 0.2131580692015096, -0.004450920224189747), 128: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 129: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 130: (-0.6524386219967896, 0.5304275011164683, -0.0029736489057540783), 131: (-1.5652547968287462, 0.15196340716105336, -0.005452835559844937), 132: (-0.9499563651753489, 0.366944972691554, -0.003337734937667869), 133: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 134: (-0.331746526385179, 0.7476755298176827, -0.0010100603103637695), 135: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 136: (-1.5532771974724229, 0.15477535040653126, -0.005829417705535911), 137: (0.173603556619036, 0.8660189516820461, 0.0005457580089569536), 138: (-0.7888195050695923, 0.45049597174297196, -0.003311330080032371), 139: (-1.5504657179857682, 0.15544206940481914, -0.006878229975700367), 140: (-1.7797678230287417, 0.10881609423344454, -0.008536991477012645), 141: (-1.4496366575264021, 0.18109491798781593, -0.007257568836212147), 142: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 143: (-0.9296431144648586, 0.37681686074414045, -0.004888626933097828)}\n",
            "EI_fear non_dann original_noun_phrase\n",
            "original_noun_phrase {0: (-15.651398643433845, 4.1453538019672003e-51, -0.0055239785255657425)}\n",
            "non_dann original_noun_phrase {'named': {0: (-0.4536497069305446, 0.6552198366825752, -0.0006526619195937888), 1: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 2: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 3: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 4: (-0.8453837185365612, 0.40841698204839616, -0.0013110116124153137), 5: (-0.46427452094332555, 0.6477266892107059, -0.0005847990512848344), 6: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 7: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 8: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 9: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 10: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 11: (-0.11691070026151802, 0.9081577799239737, -0.000165420770645186), 12: (2.0225299646717434, 0.05742722578522014, 0.0029049932956695113), 13: (-0.26262674405991276, 0.7956648531240026, -0.0004011809825897883), 14: (-0.08360809439295097, 0.9342426460836097, -0.00010615885257725388), 15: (-0.709156222445198, 0.4868391424395345, -0.0005408331751823314), 16: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 17: (-0.7959533420944594, 0.43589159110764286, -0.0009384125471115223), 18: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 19: (-0.7651367147586945, 0.45358998697214725, -0.0010971322655677906), 20: (1.5288874046307366, 0.142770504358485, 0.00632993280887606), 21: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 22: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 23: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 24: (1.5561215016818635, 0.13617942425602786, 0.006514191627502441), 25: (1.324449090361284, 0.2010655150470798, 0.005747288465499878), 26: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 27: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 28: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 29: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 30: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 31: (1.7506147286030171, 0.09614284165925152, 0.00850757062435159), 32: (1.8548369171790378, 0.07920530825837102, 0.00788701772689826), 33: (1.6009913019210766, 0.12587473375451938, 0.007541057467460699), 34: (1.3659567646552007, 0.18790451769818264, 0.005688571929931663), 35: (1.26755219530107, 0.2202730831554025, 0.005834899842739105), 36: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 37: (1.7587175226377527, 0.09472170382020201, 0.008782824873924289), 38: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 39: (0.9488361249336216, 0.3546174940700779, 0.004171487689018205), 40: (0.8765149718311525, 0.39169780491417294, 0.0021890848875045776), 41: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 42: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 43: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 44: (2.1282274166381874, 0.046626328415440045, 0.005648136138916016), 45: (1.6609366785571051, 0.11313885567545193, 0.003921613097190857), 46: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 47: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 48: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 49: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 50: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 51: (1.3946870168456393, 0.17920377269488452, 0.0037615150213241577), 52: (1.6353851739741603, 0.11842713792866925, 0.00456748902797699), 53: (1.3315823913263836, 0.1987533241794981, 0.004584217071533225), 54: (1.3371353570410147, 0.19696795927159966, 0.00397452712059021), 55: (1.779829401688538, 0.09110404205894308, 0.00402035713195803), 56: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 57: (2.019810376955619, 0.05773267898214161, 0.0058293938636779785), 58: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 59: (1.0962920103537341, 0.2866524914517763, 0.0026982814073562844), 60: (0.4370111105755945, 0.6670297001581933, 0.0006972163915633933), 61: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 62: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 63: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 64: (1.1558873260782965, 0.2620559169228458, 0.0017056524753570335), 65: (0.6085352404626192, 0.5500392182921371, 0.0007077634334564653), 66: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 67: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 68: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 69: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 70: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 71: (0.9027840809592305, 0.37794389342005685, 0.0010757893323898315), 72: (1.0765194416528894, 0.29517481630283526, 0.0015606909990311113), 73: (1.1249983108921853, 0.27460131610512795, 0.0015131771564483865), 74: (0.9582896293698356, 0.3499530367509156, 0.0014142096042633057), 75: (-0.3494323682524638, 0.730607818680475, -0.00045030713081362084), 76: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 77: (0.43274634082896746, 0.6700713494942525, 0.0006299242377281189), 78: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 79: (0.831870192340097, 0.41581548127152934, 0.0010650619864464028), 80: (-0.6664111234719685, 0.5131597453870926, -0.0017973065376281627), 81: (0.7780572169891012, 0.44611673216458503, 0.0031023234128951804), 82: (-0.2686312437643469, 0.791108615990461, -0.00022893697023396165), 83: (-0.5457728927551285, 0.5915729724571748, -0.0005119055509567594), 84: (4.561650474819855, 0.00021319750591981511, 0.00431002676486969), 85: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 86: (4.561650474819855, 0.00021319750591981511, 0.00431002676486969), 87: (11.010716603364635, 1.0921299854413208e-09, 0.010568979382514998), 88: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 89: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 90: (1.0637210480277177, 0.30078805955814025, 0.0011882573366165383), 91: (3.6135226538296683, 0.001850616143029893, 0.003304967284202487), 92: (7.622456405355375, 3.410296896217827e-07, 0.008830562233924866), 93: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 94: (4.938953194680991, 9.112475991874698e-05, 0.005027011036872864), 95: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 96: (6.714881111632672, 2.0348426215097325e-06, 0.007181236147880576), 97: (-0.07731640103103846, 0.9391802415001849, -8.628368377683326e-05), 98: (2.79280976762182, 0.011603946348508714, 0.0030788868665695412), 99: (2.903617370634803, 0.009106010357973707, 0.0030685946345329063), 100: (0.9654002716532625, 0.34647240489796993, 0.001101657748222351), 101: (10.59050226920981, 2.0724162787332835e-09, 0.010231825709342945), 102: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 103: (5.4843523237003415, 2.7265752737184905e-05, 0.005727683007717144), 104: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 105: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 106: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 107: (-0.3181158623863308, 0.7538670422588711, -0.00038250088691710316), 108: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 109: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 110: (-0.17260063152673366, 0.8647906774407321, -0.00022515356540681042), 111: (0.1146094829785176, 0.9099571640600346, 0.00012142956256866455), 112: (0.08284497610249766, 0.9348413845528509, 9.741485118863746e-05), 113: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 114: (0.09216537345907856, 0.9275314887096396, 0.00010473728179938302), 115: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 116: (0.4632378052925146, 0.648456153909601, 0.0005057364702224509), 117: (-0.26042867881764836, 0.7973346475213109, -0.00029164254665370315), 118: (-0.3750008117769526, 0.7118131023296859, -0.000460708141326871), 119: (-0.6469147561060148, 0.5254261353596277, -0.000817073881626107), 120: (-0.29662372851395696, 0.7699719612142981, -0.000387200713157676), 121: (-0.7101735937355383, 0.4862224188674523, -0.0010588198900222667), 122: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 123: (-0.4844556320193189, 0.6336001696899646, -0.000822190940380052), 124: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 125: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 126: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 127: (2.2785103645191667, 0.03443363204092818, 0.009405428171157859), 128: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 129: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 130: (2.359254180695483, 0.029168546047316737, 0.012106120586395264), 131: (2.665775666150978, 0.015274503537344412, 0.01082838624715804), 132: (2.640052341317529, 0.016141602275097384, 0.011544437706470456), 133: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 134: (2.359823315241125, 0.029134239010212195, 0.00960416495800015), 135: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 136: (2.2719409701125346, 0.03489850091617077, 0.008913415670394942), 137: (2.410227595417562, 0.026240474368313817, 0.01069551706314087), 138: (2.4547128014911554, 0.023911529244833432, 0.009658692777156863), 139: (2.243085573650609, 0.03700966254519938, 0.010553579777479172), 140: (1.861165540870239, 0.07826668314737244, 0.007289951294660535), 141: (2.410029655559331, 0.026251295034089187, 0.007672186940908404), 142: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 143: (2.314148366568837, 0.032009860677918146, 0.009153123199939772)}, 'noun_phrase': {0: (-1.9359197349281163, 0.08486303438398021, -0.003261709213256858), 1: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 2: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 3: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 4: (-3.5921207239926622, 0.005819472709776622, -0.006515455245971691), 5: (-2.2405191888039777, 0.05179931555336764, -0.004590368270874001), 6: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 7: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 8: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 9: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 10: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 11: (-2.177198306446434, 0.057436064972269864, -0.005819413065910328), 12: (-1.400019122170044, 0.19502306335776495, -0.004387927055358953), 13: (-3.8281904159932822, 0.004038902993587881, -0.005794817209243797), 14: (-3.0418381044554876, 0.013976266320790747, -0.005463844537734963), 15: (-3.002390611876544, 0.014898501704831292, -0.006923961639404286), 16: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 17: (-1.3965123499270924, 0.19604149075064167, -0.005478394031524669), 18: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 19: (-1.5320431542102702, 0.15987433423037603, -0.0039048135280608798), 20: (-1.2667867509932154, 0.23703097219497393, -0.004638332128524714), 21: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 22: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 23: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 24: (-1.5919673048439271, 0.1458558787983439, -0.005894666910171487), 25: (-1.9018239322840609, 0.08962630091028609, -0.007182878255844183), 26: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 27: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 28: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 29: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 30: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 31: (-2.231033720451802, 0.0526079234418805, -0.009498947858810391), 32: (-1.6722600173987396, 0.12880431947882942, -0.006319308280944846), 33: (-1.319104625064257, 0.21970751880154052, -0.005503982305526733), 34: (-1.6426000220670234, 0.13488091489643397, -0.006459599733352639), 35: (-1.8324739912421704, 0.10010153939268446, -0.007775568962097135), 36: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 37: (-0.5298460653769466, 0.6090391421319425, -0.0025731325149536133), 38: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 39: (-2.0563281051041438, 0.06989434897981066, -0.0072820544242858665), 40: (-1.832659175296179, 0.100072100381963, -0.0058657824993133545), 41: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 42: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 43: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 44: (-1.8572277489032036, 0.09623735235256793, -0.00609959363937379), 45: (-2.0199085350068584, 0.07413294507286643, -0.005737346410751298), 46: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 47: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 48: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 49: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 50: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 51: (-2.0686929741325577, 0.06850892653616228, -0.010611301660537698), 52: (-2.1190318543237137, 0.06313680911664347, -0.0076056540012360285), 53: (-1.371374748224241, 0.20347631027779792, -0.00496740341186519), 54: (-1.670398188835708, 0.1291783171103463, -0.00642936229705815), 55: (-1.805673082097354, 0.10444831758628668, -0.009003195166587818), 56: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 57: (-1.1323926277300542, 0.28673600315853054, -0.005244308710098289), 58: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 59: (-1.2051019454800171, 0.2588935665517983, -0.00519210398197173), 60: (-1.3078051242097022, 0.2233563599285183, -0.004501736164092995), 61: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 62: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 63: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 64: (-1.1304764999833943, 0.287500792462536, -0.004508697986602805), 65: (-1.5776562043906954, 0.14910008006886857, -0.0054946839809417725), 66: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 67: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 68: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 69: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 70: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 71: (-1.1228025481450146, 0.2905799023735358, -0.0034292519092559703), 72: (-1.1442219595457226, 0.2820501762831403, -0.002496677637100153), 73: (-1.39756984179323, 0.19573389666515814, -0.004101824760437078), 74: (-1.782239322829124, 0.10839218870326162, -0.0048563122749327725), 75: (-1.1621705134074587, 0.2750570177717406, -0.004986456036567721), 76: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 77: (-1.4777500256179747, 0.17359339189294204, -0.005267497897148143), 78: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 79: (-1.3501294547030407, 0.20994633313990962, -0.004570305347442627), 80: (-0.41295578067509575, 0.6893037499489926, -0.002099892497062661), 81: (-1.7588500575013584, 0.11246579297791844, -0.009781035780906722), 82: (-2.0992377338378363, 0.06519888246336426, -0.0063045263290405495), 83: (-1.7171570209124136, 0.12007936926462812, -0.006237471103668235), 84: (-0.5440740393161605, 0.5996079486356212, -0.002246636152267445), 85: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 86: (-0.5440740393161605, 0.5996079486356212, -0.002246636152267445), 87: (1.0527153678445045, 0.3199131931574301, 0.0033967435359955056), 88: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 89: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 90: (-2.143597626272751, 0.06066523565255921, -0.008459538221359253), 91: (-2.2476325636466004, 0.05120091292210157, -0.008722174167633101), 92: (-0.2006075316872387, 0.8454655225549137, -0.0007541239261626975), 93: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 94: (-1.4932396741932192, 0.1695775885824614, -0.005306470394134455), 95: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 96: (-0.3191203618275257, 0.7569175311159203, -0.0011212348937987837), 97: (-2.458981689327924, 0.03621910324283895, -0.008778303861618042), 98: (-1.5699961994350784, 0.15086293024603037, -0.005399644374847412), 99: (-1.7332341226006547, 0.11708906381472337, -0.006020474433898915), 100: (-2.2478215731877276, 0.051185105408517506, -0.007329279184341453), 101: (0.9242215188599492, 0.3794839117881119, 0.004103481769561768), 102: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 103: (-1.2435497732198746, 0.2450811694075575, -0.005191737413406394), 104: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 105: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 106: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 107: (-1.2584645766151907, 0.23988859851978603, -0.004110944271087658), 108: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 109: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 110: (-1.8641328798290278, 0.09518460229791703, -0.006200087070465099), 111: (-1.4991464007925193, 0.1680678520803307, -0.00435821413993831), 112: (-1.755279233997687, 0.11310001292177661, -0.005064666271209717), 113: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 114: (-1.6163046921037871, 0.14048400703572178, -0.005287939310073764), 115: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 116: (-1.9711279406636157, 0.08019647814243068, -0.005731606483459495), 117: (-1.75484549632847, 0.11317727421496512, -0.00519672632217405), 118: (-1.6664082176427797, 0.12998313803203515, -0.0054109632968902255), 119: (-2.0442336127660954, 0.07127548469948174, -0.005514851212501504), 120: (-1.7009025325257665, 0.12317372724695272, -0.004597327113151528), 121: (-1.5645063437346745, 0.15213777580428148, -0.004361140727996815), 122: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 123: (-1.6247418312440645, 0.13866371532846725, -0.004948762059211742), 124: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 125: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 126: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 127: (-1.3397891031696871, 0.2131580692015096, -0.004450920224189747), 128: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 129: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 130: (-0.6524386219967896, 0.5304275011164683, -0.0029736489057540783), 131: (-1.5652547968287462, 0.15196340716105336, -0.005452835559844937), 132: (-0.9499563651753489, 0.366944972691554, -0.003337734937667869), 133: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 134: (-0.331746526385179, 0.7476755298176827, -0.0010100603103637695), 135: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 136: (-1.5532771974724229, 0.15477535040653126, -0.005829417705535911), 137: (0.173603556619036, 0.8660189516820461, 0.0005457580089569536), 138: (-0.7888195050695923, 0.45049597174297196, -0.003311330080032371), 139: (-1.5504657179857682, 0.15544206940481914, -0.006878229975700367), 140: (-1.7797678230287417, 0.10881609423344454, -0.008536991477012645), 141: (-1.4496366575264021, 0.18109491798781593, -0.007257568836212147), 142: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 143: (-0.9296431144648586, 0.37681686074414045, -0.004888626933097828)}, 'original_noun_phrase': {0: (-15.651398643433845, 4.1453538019672003e-51, -0.0055239785255657425)}}\n",
            "EI_fear dann named\n",
            "named {0: (2.6103947862637535, 0.017199240439614367, 0.011928600072860673), 1: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 2: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 3: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 4: (2.5092497314562876, 0.02132035644276648, 0.012463900446891829), 5: (2.4316064916403395, 0.025095964440944134, 0.008169776201248147), 6: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 7: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 8: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 9: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 10: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 11: (2.7206603552292705, 0.013570121500998907, 0.013262182474136297), 12: (2.7599366760771935, 0.012463413199479363, 0.01201771795749662), 13: (2.6464823962790263, 0.015920576282805184, 0.011606037616729736), 14: (2.6262554643444473, 0.016625715351621783, 0.010800504684448264), 15: (2.7682107276193775, 0.012241527969973616, 0.013800801336765312), 16: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 17: (2.7885554708035167, 0.011711892006608559, 0.015732431411743186), 18: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 19: (2.8352092689387693, 0.01057894462261949, 0.014922112226486206), 20: (2.768750157576103, 0.012227193965040123, 0.02500676512718203), 21: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 22: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 23: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 24: (2.93476167269629, 0.008502798875669767, 0.02939695119857788), 25: (2.738110081888915, 0.013067324427681905, 0.024826714396476768), 26: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 27: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 28: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 29: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 30: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 31: (2.9634761132464313, 0.007980939359257441, 0.02686447501182554), 32: (2.8407244653589068, 0.010452178354582193, 0.023518845438957214), 33: (2.775746657367221, 0.01204272733819661, 0.023004117608070307), 34: (2.8277620864617217, 0.01075246047528403, 0.022682592272758484), 35: (2.88838547813698, 0.009415837176344984, 0.03005471378564839), 36: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 37: (2.9733964670835156, 0.007807942915604268, 0.032468363642692566), 38: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 39: (2.906120344098321, 0.009056046626788558, 0.0336635142564774), 40: (2.80842970923703, 0.01121572907259239, 0.016974622011184715), 41: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 42: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 43: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 44: (2.8924318658678465, 0.009332555163869228, 0.02221711874008181), 45: (2.7029789524994787, 0.014098376107217862, 0.015183374285697937), 46: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 47: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 48: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 49: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 50: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 51: (2.89587139025981, 0.00926232144924328, 0.018863475322723477), 52: (2.7971994432079477, 0.011493564463744527, 0.018300509452819802), 53: (2.6929188384939966, 0.014407617635695123, 0.01884995400905609), 54: (2.6942788236446074, 0.014365438288492592, 0.016951128840446472), 55: (2.9185447165477694, 0.008811918607938592, 0.02505885809659958), 56: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 57: (3.007555189737084, 0.007239567422610325, 0.029827497899532318), 58: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 59: (2.9490198539813113, 0.008239680467161007, 0.027023300528526306), 60: (2.9294106654178353, 0.0086036226508914, 0.010767298936843916), 61: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 62: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 63: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 64: (2.993402595610379, 0.00747002451250534, 0.012270322442054837), 65: (3.0601517867655366, 0.006441952809940185, 0.009791091084480286), 66: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 67: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 68: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 69: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 70: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 71: (2.92869606273098, 0.008617174175294142, 0.0105192571878433), 72: (2.277012206788573, 0.034539140453250156, 0.004699712991714455), 73: (2.831245353720505, 0.010670964778161365, 0.005946746468543984), 74: (2.7295140761014784, 0.013312747369964885, 0.007367062568664595), 75: (3.1058799658170972, 0.005818350172150957, 0.015683779120445274), 76: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 77: (3.0892671588081413, 0.006037799562176934, 0.012800940871238675), 78: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 79: (3.087240881130914, 0.006065110261453242, 0.01329335868358611), 80: (2.8087124765138056, 0.01120881718131954, 0.016108225286006916), 81: (2.833286273988715, 0.010623490409024463, 0.03871192485094066), 82: (2.4806467052111603, 0.02264456595340393, 0.0043008685111999845), 83: (2.5829132463192552, 0.018237557057120556, 0.0034660324454307556), 84: (4.54841894094851, 0.00021968685465574152, 0.008455431461334206), 85: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 86: (4.54841894094851, 0.00021968685465574152, 0.008455431461334206), 87: (5.769151100855724, 1.4705844669189098e-05, 0.011277711391449008), 88: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 89: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 90: (0.6769574592804383, 0.5065920974120413, 0.001984626054763794), 91: (5.214256783919982, 4.93760898309573e-05, 0.008036795258521967), 92: (6.056797547522969, 7.960969881339664e-06, 0.015093770623207048), 93: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 94: (-0.40490702966731434, 0.6900682738154246, -0.0008324533700942993), 95: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 96: (-0.8440543326556268, 0.40914102931184826, -0.0015962868928909302), 97: (-0.021390838269522268, 0.9831569308204549, -4.547834396362305e-05), 98: (0.7861387266930034, 0.4414810991375544, 0.001652923226356462), 99: (4.285325112910361, 0.00039951537069062866, 0.012760338187217724), 100: (1.694590781568973, 0.10648142938167585, 0.004414877295494057), 101: (10.453739572099726, 2.563048905570711e-09, 0.02470133304595945), 102: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 103: (4.785291290120442, 0.00012865458575207346, 0.01689343750476835), 104: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 105: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 106: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 107: (3.0197293044775324, 0.00704684737387263, 0.01095967292785649), 108: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 109: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 110: (2.9887173755638496, 0.007547869581981068, 0.011412858963012695), 111: (2.9949393585336357, 0.007444660741199274, 0.007520383596420266), 112: (2.980428862712128, 0.007687509455643306, 0.010929712653160062), 113: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 114: (2.997267297196953, 0.007406397525509584, 0.009008508920669511), 115: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 116: (3.033095790477973, 0.006840975632980589, 0.00854403972625728), 117: (3.0559468368881655, 0.006502450773708548, 0.008743429183959983), 118: (2.969353781850851, 0.007877999433326299, 0.009646764397621133), 119: (2.972589402427881, 0.00782188042582904, 0.016182480752468142), 120: (2.9553898016429723, 0.008124686772074364, 0.014857803285121929), 121: (2.9641410442447254, 0.007969229059858068, 0.01723177880048754), 122: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 123: (3.0032123391275105, 0.007309543316983239, 0.019232231378555287), 124: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 125: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 126: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 127: (2.9494314684755825, 0.008232202534938372, 0.02923231124877934), 128: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 129: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 130: (2.9540510825213544, 0.008148724687837637, 0.03158140927553177), 131: (2.9624466600605515, 0.007999102167196308, 0.018715083599090576), 132: (2.9772518632058964, 0.007741693167467521, 0.024149402976036072), 133: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 134: (2.903855433908864, 0.009101246795777613, 0.021802923083305337), 135: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 136: (2.8992656468373372, 0.009193511126860463, 0.02026311159133909), 137: (2.8638766807406766, 0.00993576246164692, 0.022373551130294822), 138: (2.969027447358234, 0.007883680972499465, 0.023039574921131123), 139: (2.9340452536128225, 0.008516231274771047, 0.023069849610328652), 140: (2.7874978474710157, 0.011738876234434108, 0.01798664256930349), 141: (2.9419650728470366, 0.008368870824083162, 0.01703978329896927), 142: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 143: (2.978592531378548, 0.00771878319812897, 0.022190096974372853)}\n",
            "EI_fear dann noun_phrase\n",
            "noun_phrase {0: (1.5211540598530064, 0.16254649062513027, 0.013754546642303467), 1: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 2: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 3: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 4: (0.984389581324264, 0.35064594117528725, 0.010298395156860374), 5: (0.6632973004745379, 0.5237642157995729, 0.0070555627346039484), 6: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 7: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 8: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 9: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 10: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 11: (1.0370261838546138, 0.3267820036368045, 0.00874339342117314), 12: (0.7119061384825222, 0.49456175001248004, 0.00777845382690423), 13: (0.9371818472161219, 0.3731309140840815, 0.007833760976791448), 14: (0.7366210450925132, 0.48011253405961407, 0.006562745571136452), 15: (0.8343167047122595, 0.42569261636890376, 0.00946221351623533), 16: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 17: (1.0512282292622237, 0.32055948669042184, 0.011078271269798268), 18: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 19: (1.2560797061214373, 0.2407127354829818, 0.013133659958839417), 20: (0.6197988663892074, 0.5507575215599216, 0.006505572795867831), 21: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 22: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 23: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 24: (0.5314321588295272, 0.6079839464159491, 0.00651753544807443), 25: (0.24727993101355267, 0.8102403009419568, 0.0027957201004028542), 26: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 27: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 28: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 29: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 30: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 31: (0.1951014486755195, 0.8496468908155581, 0.001885938644409224), 32: (0.08608159956273065, 0.9332862053204737, 0.0009582996368407981), 33: (0.2217119948379997, 0.8294870516588212, 0.002364957332610995), 34: (0.21154382157197257, 0.8371756779670659, 0.0023240566253661665), 35: (0.1916696407314516, 0.8522555602874563, 0.0023022353649139737), 36: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 37: (0.4321730201921053, 0.6757853326831198, 0.006130862236022994), 38: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 39: (0.5592106457537548, 0.5896603452047438, 0.0070510983467102495), 40: (0.5588404923242061, 0.5899025381298684, 0.003660529851913452), 41: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 42: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 43: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 44: (0.42468507042674847, 0.6810385105607396, 0.003106921911239624), 45: (0.3690807508222823, 0.7206011217092118, 0.002863657474517778), 46: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 47: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 48: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 49: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 50: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 51: (-0.0456223529946904, 0.9646075102381807, -0.0002785503864287664), 52: (0.34091357953439466, 0.7409919871619243, 0.002949303388595559), 53: (0.403134994753946, 0.6962578372112948, 0.0028584957122802512), 54: (0.4314506422032745, 0.676291316411693, 0.003278601169586115), 55: (0.38669031903503914, 0.7079694287177205, 0.004895877838134777), 56: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 57: (0.6550953575024775, 0.5287925704894851, 0.009386119246482805), 58: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 59: (0.8946026226027306, 0.3942944517389294, 0.010824564099311818), 60: (1.1624310662910748, 0.2749565308335149, 0.007143926620483354), 61: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 62: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 63: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 64: (1.4330775157735591, 0.18564423250548448, 0.008354246616363525), 65: (0.8255857678848993, 0.43037880484130064, 0.0045719683170318826), 66: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 67: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 68: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 69: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 70: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 71: (1.1667007049543583, 0.27331405258306735, 0.007150387763977029), 72: (0.9869411346801803, 0.34945987843095927, 0.0037491142749787043), 73: (0.3678523045748961, 0.7214857162649786, 0.0013000845909117986), 74: (0.9355467605241871, 0.37392813308729367, 0.004311722517013505), 75: (1.363227620845901, 0.20593706702985365, 0.012658190727233898), 76: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 77: (1.1017450409429674, 0.2991626799477583, 0.008647975325584423), 78: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 79: (1.2084413701632182, 0.2576692452722908, 0.008719712495803833), 80: (1.0238216628886645, 0.33264965296422166, 0.008077451586723294), 81: (1.9762407340205639, 0.0795394356984782, 0.02270984649658203), 82: (-0.42703446245694854, 0.6793883185306457, -0.0011041760444641113), 83: (1.532474106169191, 0.159769384656312, 0.003844776749610923), 84: (0.0879514617355009, 0.9318411509571123, 0.0004138648509979248), 85: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 86: (0.0879514617355009, 0.9318411509571123, 0.0004138648509979248), 87: (0.15523944596788888, 0.8800584383822896, 0.0006642401218414085), 88: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 89: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 90: (-3.8067711291385278, 0.004173753314251861, -0.011523932218551636), 91: (-1.8029863878824919, 0.10489362292100228, -0.0071912050247191495), 92: (0.6411449947774289, 0.5374110134642108, 0.0022733032703399214), 93: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 94: (-3.226996053276431, 0.01037244631668259, -0.01360133886337278), 95: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 96: (-2.6142804669791535, 0.028073003494980337, -0.012852048873901345), 97: (-3.146274040846368, 0.011808150660718974, -0.012410771846771307), 98: (-2.969709395751982, 0.015709836245144774, -0.010099208354949929), 99: (1.0111902941119997, 0.33833698593321376, 0.002703249454498291), 100: (-2.1807562313493887, 0.05710407711809115, -0.005730390548706055), 101: (3.3910737644395006, 0.007985498073479418, 0.014367562532424938), 102: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 103: (0.845008227377277, 0.4200018922314048, 0.0031632542610168235), 104: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 105: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 106: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 107: (1.503233054789495, 0.1670302522608637, 0.00598465204238896), 108: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 109: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 110: (1.0801275731534432, 0.3081788129116308, 0.0037075042724609153), 111: (1.350372644189398, 0.20987129463835882, 0.003793919086456321), 112: (1.1409329986178038, 0.2833468488875774, 0.004815900325775169), 113: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 114: (0.938123675525445, 0.3726722678854003, 0.0029580175876616766), 115: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 116: (0.4418870143661213, 0.6689980321458654, 0.0010536968708039218), 117: (1.0624567868430348, 0.31570439647134824, 0.003084981441497825), 118: (1.1882036759612773, 0.26516151588337133, 0.004590630531311035), 119: (1.4773634891828835, 0.17369466132529682, 0.010195052623748757), 120: (1.541044659092739, 0.15769484933100672, 0.009900480508804321), 121: (1.6310872575661979, 0.13730875715834567, 0.011794212460517872), 122: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 123: (1.325637573581265, 0.21762086424884264, 0.01053376793861388), 124: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 125: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 126: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 127: (0.5937930491209755, 0.5672731790101048, 0.0066876918077469205), 128: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 129: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 130: (0.5426486941236397, 0.6005492527688636, 0.0045343518257141), 131: (-0.8042424472502977, 0.44198180042872004, -0.007315874099731445), 132: (0.3308831718874723, 0.7483061466157258, 0.0032659351825714), 133: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 134: (0.4624585358432608, 0.6547294103416952, 0.004889035224914573), 135: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 136: (-0.34970864312488015, 0.7346011484279015, -0.002848786115646429), 137: (0.6464103164936413, 0.534148417373987, 0.005934375524520863), 138: (0.2691786656064834, 0.793861518024378, 0.003027743101119973), 139: (0.17735762485888104, 0.863154868091551, 0.001822125911712602), 140: (-0.37085576442258356, 0.7193237238085466, -0.003197172284126304), 141: (-0.05608851125686871, 0.9564967421496149, -0.00037859976291654274), 142: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 143: (-0.23611853117342302, 0.8186265508544835, -0.0017890527844429127)}\n",
            "EI_fear dann original_noun_phrase\n",
            "original_noun_phrase {0: (3.7088827910822393, 0.00021609055086301644, 0.0022620030678809355)}\n",
            "dann original_noun_phrase {'named': {0: (2.6103947862637535, 0.017199240439614367, 0.011928600072860673), 1: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 2: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 3: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 4: (2.5092497314562876, 0.02132035644276648, 0.012463900446891829), 5: (2.4316064916403395, 0.025095964440944134, 0.008169776201248147), 6: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 7: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 8: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 9: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 10: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 11: (2.7206603552292705, 0.013570121500998907, 0.013262182474136297), 12: (2.7599366760771935, 0.012463413199479363, 0.01201771795749662), 13: (2.6464823962790263, 0.015920576282805184, 0.011606037616729736), 14: (2.6262554643444473, 0.016625715351621783, 0.010800504684448264), 15: (2.7682107276193775, 0.012241527969973616, 0.013800801336765312), 16: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 17: (2.7885554708035167, 0.011711892006608559, 0.015732431411743186), 18: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 19: (2.8352092689387693, 0.01057894462261949, 0.014922112226486206), 20: (2.768750157576103, 0.012227193965040123, 0.02500676512718203), 21: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 22: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 23: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 24: (2.93476167269629, 0.008502798875669767, 0.02939695119857788), 25: (2.738110081888915, 0.013067324427681905, 0.024826714396476768), 26: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 27: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 28: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 29: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 30: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 31: (2.9634761132464313, 0.007980939359257441, 0.02686447501182554), 32: (2.8407244653589068, 0.010452178354582193, 0.023518845438957214), 33: (2.775746657367221, 0.01204272733819661, 0.023004117608070307), 34: (2.8277620864617217, 0.01075246047528403, 0.022682592272758484), 35: (2.88838547813698, 0.009415837176344984, 0.03005471378564839), 36: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 37: (2.9733964670835156, 0.007807942915604268, 0.032468363642692566), 38: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 39: (2.906120344098321, 0.009056046626788558, 0.0336635142564774), 40: (2.80842970923703, 0.01121572907259239, 0.016974622011184715), 41: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 42: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 43: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 44: (2.8924318658678465, 0.009332555163869228, 0.02221711874008181), 45: (2.7029789524994787, 0.014098376107217862, 0.015183374285697937), 46: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 47: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 48: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 49: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 50: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 51: (2.89587139025981, 0.00926232144924328, 0.018863475322723477), 52: (2.7971994432079477, 0.011493564463744527, 0.018300509452819802), 53: (2.6929188384939966, 0.014407617635695123, 0.01884995400905609), 54: (2.6942788236446074, 0.014365438288492592, 0.016951128840446472), 55: (2.9185447165477694, 0.008811918607938592, 0.02505885809659958), 56: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 57: (3.007555189737084, 0.007239567422610325, 0.029827497899532318), 58: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 59: (2.9490198539813113, 0.008239680467161007, 0.027023300528526306), 60: (2.9294106654178353, 0.0086036226508914, 0.010767298936843916), 61: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 62: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 63: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 64: (2.993402595610379, 0.00747002451250534, 0.012270322442054837), 65: (3.0601517867655366, 0.006441952809940185, 0.009791091084480286), 66: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 67: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 68: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 69: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 70: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 71: (2.92869606273098, 0.008617174175294142, 0.0105192571878433), 72: (2.277012206788573, 0.034539140453250156, 0.004699712991714455), 73: (2.831245353720505, 0.010670964778161365, 0.005946746468543984), 74: (2.7295140761014784, 0.013312747369964885, 0.007367062568664595), 75: (3.1058799658170972, 0.005818350172150957, 0.015683779120445274), 76: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 77: (3.0892671588081413, 0.006037799562176934, 0.012800940871238675), 78: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 79: (3.087240881130914, 0.006065110261453242, 0.01329335868358611), 80: (2.8087124765138056, 0.01120881718131954, 0.016108225286006916), 81: (2.833286273988715, 0.010623490409024463, 0.03871192485094066), 82: (2.4806467052111603, 0.02264456595340393, 0.0043008685111999845), 83: (2.5829132463192552, 0.018237557057120556, 0.0034660324454307556), 84: (4.54841894094851, 0.00021968685465574152, 0.008455431461334206), 85: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 86: (4.54841894094851, 0.00021968685465574152, 0.008455431461334206), 87: (5.769151100855724, 1.4705844669189098e-05, 0.011277711391449008), 88: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 89: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 90: (0.6769574592804383, 0.5065920974120413, 0.001984626054763794), 91: (5.214256783919982, 4.93760898309573e-05, 0.008036795258521967), 92: (6.056797547522969, 7.960969881339664e-06, 0.015093770623207048), 93: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 94: (-0.40490702966731434, 0.6900682738154246, -0.0008324533700942993), 95: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 96: (-0.8440543326556268, 0.40914102931184826, -0.0015962868928909302), 97: (-0.021390838269522268, 0.9831569308204549, -4.547834396362305e-05), 98: (0.7861387266930034, 0.4414810991375544, 0.001652923226356462), 99: (4.285325112910361, 0.00039951537069062866, 0.012760338187217724), 100: (1.694590781568973, 0.10648142938167585, 0.004414877295494057), 101: (10.453739572099726, 2.563048905570711e-09, 0.02470133304595945), 102: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 103: (4.785291290120442, 0.00012865458575207346, 0.01689343750476835), 104: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 105: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 106: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 107: (3.0197293044775324, 0.00704684737387263, 0.01095967292785649), 108: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 109: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 110: (2.9887173755638496, 0.007547869581981068, 0.011412858963012695), 111: (2.9949393585336357, 0.007444660741199274, 0.007520383596420266), 112: (2.980428862712128, 0.007687509455643306, 0.010929712653160062), 113: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 114: (2.997267297196953, 0.007406397525509584, 0.009008508920669511), 115: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 116: (3.033095790477973, 0.006840975632980589, 0.00854403972625728), 117: (3.0559468368881655, 0.006502450773708548, 0.008743429183959983), 118: (2.969353781850851, 0.007877999433326299, 0.009646764397621133), 119: (2.972589402427881, 0.00782188042582904, 0.016182480752468142), 120: (2.9553898016429723, 0.008124686772074364, 0.014857803285121929), 121: (2.9641410442447254, 0.007969229059858068, 0.01723177880048754), 122: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 123: (3.0032123391275105, 0.007309543316983239, 0.019232231378555287), 124: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 125: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 126: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 127: (2.9494314684755825, 0.008232202534938372, 0.02923231124877934), 128: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 129: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 130: (2.9540510825213544, 0.008148724687837637, 0.03158140927553177), 131: (2.9624466600605515, 0.007999102167196308, 0.018715083599090576), 132: (2.9772518632058964, 0.007741693167467521, 0.024149402976036072), 133: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 134: (2.903855433908864, 0.009101246795777613, 0.021802923083305337), 135: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 136: (2.8992656468373372, 0.009193511126860463, 0.02026311159133909), 137: (2.8638766807406766, 0.00993576246164692, 0.022373551130294822), 138: (2.969027447358234, 0.007883680972499465, 0.023039574921131123), 139: (2.9340452536128225, 0.008516231274771047, 0.023069849610328652), 140: (2.7874978474710157, 0.011738876234434108, 0.01798664256930349), 141: (2.9419650728470366, 0.008368870824083162, 0.01703978329896927), 142: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 143: (2.978592531378548, 0.00771878319812897, 0.022190096974372853)}, 'noun_phrase': {0: (1.5211540598530064, 0.16254649062513027, 0.013754546642303467), 1: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 2: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 3: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 4: (0.984389581324264, 0.35064594117528725, 0.010298395156860374), 5: (0.6632973004745379, 0.5237642157995729, 0.0070555627346039484), 6: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 7: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 8: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 9: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 10: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 11: (1.0370261838546138, 0.3267820036368045, 0.00874339342117314), 12: (0.7119061384825222, 0.49456175001248004, 0.00777845382690423), 13: (0.9371818472161219, 0.3731309140840815, 0.007833760976791448), 14: (0.7366210450925132, 0.48011253405961407, 0.006562745571136452), 15: (0.8343167047122595, 0.42569261636890376, 0.00946221351623533), 16: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 17: (1.0512282292622237, 0.32055948669042184, 0.011078271269798268), 18: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 19: (1.2560797061214373, 0.2407127354829818, 0.013133659958839417), 20: (0.6197988663892074, 0.5507575215599216, 0.006505572795867831), 21: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 22: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 23: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 24: (0.5314321588295272, 0.6079839464159491, 0.00651753544807443), 25: (0.24727993101355267, 0.8102403009419568, 0.0027957201004028542), 26: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 27: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 28: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 29: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 30: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 31: (0.1951014486755195, 0.8496468908155581, 0.001885938644409224), 32: (0.08608159956273065, 0.9332862053204737, 0.0009582996368407981), 33: (0.2217119948379997, 0.8294870516588212, 0.002364957332610995), 34: (0.21154382157197257, 0.8371756779670659, 0.0023240566253661665), 35: (0.1916696407314516, 0.8522555602874563, 0.0023022353649139737), 36: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 37: (0.4321730201921053, 0.6757853326831198, 0.006130862236022994), 38: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 39: (0.5592106457537548, 0.5896603452047438, 0.0070510983467102495), 40: (0.5588404923242061, 0.5899025381298684, 0.003660529851913452), 41: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 42: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 43: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 44: (0.42468507042674847, 0.6810385105607396, 0.003106921911239624), 45: (0.3690807508222823, 0.7206011217092118, 0.002863657474517778), 46: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 47: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 48: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 49: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 50: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 51: (-0.0456223529946904, 0.9646075102381807, -0.0002785503864287664), 52: (0.34091357953439466, 0.7409919871619243, 0.002949303388595559), 53: (0.403134994753946, 0.6962578372112948, 0.0028584957122802512), 54: (0.4314506422032745, 0.676291316411693, 0.003278601169586115), 55: (0.38669031903503914, 0.7079694287177205, 0.004895877838134777), 56: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 57: (0.6550953575024775, 0.5287925704894851, 0.009386119246482805), 58: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 59: (0.8946026226027306, 0.3942944517389294, 0.010824564099311818), 60: (1.1624310662910748, 0.2749565308335149, 0.007143926620483354), 61: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 62: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 63: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 64: (1.4330775157735591, 0.18564423250548448, 0.008354246616363525), 65: (0.8255857678848993, 0.43037880484130064, 0.0045719683170318826), 66: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 67: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 68: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 69: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 70: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 71: (1.1667007049543583, 0.27331405258306735, 0.007150387763977029), 72: (0.9869411346801803, 0.34945987843095927, 0.0037491142749787043), 73: (0.3678523045748961, 0.7214857162649786, 0.0013000845909117986), 74: (0.9355467605241871, 0.37392813308729367, 0.004311722517013505), 75: (1.363227620845901, 0.20593706702985365, 0.012658190727233898), 76: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 77: (1.1017450409429674, 0.2991626799477583, 0.008647975325584423), 78: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 79: (1.2084413701632182, 0.2576692452722908, 0.008719712495803833), 80: (1.0238216628886645, 0.33264965296422166, 0.008077451586723294), 81: (1.9762407340205639, 0.0795394356984782, 0.02270984649658203), 82: (-0.42703446245694854, 0.6793883185306457, -0.0011041760444641113), 83: (1.532474106169191, 0.159769384656312, 0.003844776749610923), 84: (0.0879514617355009, 0.9318411509571123, 0.0004138648509979248), 85: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 86: (0.0879514617355009, 0.9318411509571123, 0.0004138648509979248), 87: (0.15523944596788888, 0.8800584383822896, 0.0006642401218414085), 88: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 89: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 90: (-3.8067711291385278, 0.004173753314251861, -0.011523932218551636), 91: (-1.8029863878824919, 0.10489362292100228, -0.0071912050247191495), 92: (0.6411449947774289, 0.5374110134642108, 0.0022733032703399214), 93: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 94: (-3.226996053276431, 0.01037244631668259, -0.01360133886337278), 95: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 96: (-2.6142804669791535, 0.028073003494980337, -0.012852048873901345), 97: (-3.146274040846368, 0.011808150660718974, -0.012410771846771307), 98: (-2.969709395751982, 0.015709836245144774, -0.010099208354949929), 99: (1.0111902941119997, 0.33833698593321376, 0.002703249454498291), 100: (-2.1807562313493887, 0.05710407711809115, -0.005730390548706055), 101: (3.3910737644395006, 0.007985498073479418, 0.014367562532424938), 102: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 103: (0.845008227377277, 0.4200018922314048, 0.0031632542610168235), 104: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 105: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 106: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 107: (1.503233054789495, 0.1670302522608637, 0.00598465204238896), 108: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 109: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 110: (1.0801275731534432, 0.3081788129116308, 0.0037075042724609153), 111: (1.350372644189398, 0.20987129463835882, 0.003793919086456321), 112: (1.1409329986178038, 0.2833468488875774, 0.004815900325775169), 113: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 114: (0.938123675525445, 0.3726722678854003, 0.0029580175876616766), 115: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 116: (0.4418870143661213, 0.6689980321458654, 0.0010536968708039218), 117: (1.0624567868430348, 0.31570439647134824, 0.003084981441497825), 118: (1.1882036759612773, 0.26516151588337133, 0.004590630531311035), 119: (1.4773634891828835, 0.17369466132529682, 0.010195052623748757), 120: (1.541044659092739, 0.15769484933100672, 0.009900480508804321), 121: (1.6310872575661979, 0.13730875715834567, 0.011794212460517872), 122: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 123: (1.325637573581265, 0.21762086424884264, 0.01053376793861388), 124: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 125: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 126: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 127: (0.5937930491209755, 0.5672731790101048, 0.0066876918077469205), 128: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 129: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 130: (0.5426486941236397, 0.6005492527688636, 0.0045343518257141), 131: (-0.8042424472502977, 0.44198180042872004, -0.007315874099731445), 132: (0.3308831718874723, 0.7483061466157258, 0.0032659351825714), 133: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 134: (0.4624585358432608, 0.6547294103416952, 0.004889035224914573), 135: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 136: (-0.34970864312488015, 0.7346011484279015, -0.002848786115646429), 137: (0.6464103164936413, 0.534148417373987, 0.005934375524520863), 138: (0.2691786656064834, 0.793861518024378, 0.003027743101119973), 139: (0.17735762485888104, 0.863154868091551, 0.001822125911712602), 140: (-0.37085576442258356, 0.7193237238085466, -0.003197172284126304), 141: (-0.05608851125686871, 0.9564967421496149, -0.00037859976291654274), 142: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 143: (-0.23611853117342302, 0.8186265508544835, -0.0017890527844429127)}, 'original_noun_phrase': {0: (3.7088827910822393, 0.00021609055086301644, 0.0022620030678809355)}}\n",
            "EI_fear dann original_noun_phrase {'non_dann': {'named': {0: (-0.4536497069305446, 0.6552198366825752, -0.0006526619195937888), 1: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 2: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 3: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 4: (-0.8453837185365612, 0.40841698204839616, -0.0013110116124153137), 5: (-0.46427452094332555, 0.6477266892107059, -0.0005847990512848344), 6: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 7: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 8: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 9: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 10: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 11: (-0.11691070026151802, 0.9081577799239737, -0.000165420770645186), 12: (2.0225299646717434, 0.05742722578522014, 0.0029049932956695113), 13: (-0.26262674405991276, 0.7956648531240026, -0.0004011809825897883), 14: (-0.08360809439295097, 0.9342426460836097, -0.00010615885257725388), 15: (-0.709156222445198, 0.4868391424395345, -0.0005408331751823314), 16: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 17: (-0.7959533420944594, 0.43589159110764286, -0.0009384125471115223), 18: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 19: (-0.7651367147586945, 0.45358998697214725, -0.0010971322655677906), 20: (1.5288874046307366, 0.142770504358485, 0.00632993280887606), 21: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 22: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 23: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 24: (1.5561215016818635, 0.13617942425602786, 0.006514191627502441), 25: (1.324449090361284, 0.2010655150470798, 0.005747288465499878), 26: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 27: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 28: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 29: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 30: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 31: (1.7506147286030171, 0.09614284165925152, 0.00850757062435159), 32: (1.8548369171790378, 0.07920530825837102, 0.00788701772689826), 33: (1.6009913019210766, 0.12587473375451938, 0.007541057467460699), 34: (1.3659567646552007, 0.18790451769818264, 0.005688571929931663), 35: (1.26755219530107, 0.2202730831554025, 0.005834899842739105), 36: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 37: (1.7587175226377527, 0.09472170382020201, 0.008782824873924289), 38: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 39: (0.9488361249336216, 0.3546174940700779, 0.004171487689018205), 40: (0.8765149718311525, 0.39169780491417294, 0.0021890848875045776), 41: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 42: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 43: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 44: (2.1282274166381874, 0.046626328415440045, 0.005648136138916016), 45: (1.6609366785571051, 0.11313885567545193, 0.003921613097190857), 46: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 47: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 48: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 49: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 50: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 51: (1.3946870168456393, 0.17920377269488452, 0.0037615150213241577), 52: (1.6353851739741603, 0.11842713792866925, 0.00456748902797699), 53: (1.3315823913263836, 0.1987533241794981, 0.004584217071533225), 54: (1.3371353570410147, 0.19696795927159966, 0.00397452712059021), 55: (1.779829401688538, 0.09110404205894308, 0.00402035713195803), 56: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 57: (2.019810376955619, 0.05773267898214161, 0.0058293938636779785), 58: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 59: (1.0962920103537341, 0.2866524914517763, 0.0026982814073562844), 60: (0.4370111105755945, 0.6670297001581933, 0.0006972163915633933), 61: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 62: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 63: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 64: (1.1558873260782965, 0.2620559169228458, 0.0017056524753570335), 65: (0.6085352404626192, 0.5500392182921371, 0.0007077634334564653), 66: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 67: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 68: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 69: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 70: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 71: (0.9027840809592305, 0.37794389342005685, 0.0010757893323898315), 72: (1.0765194416528894, 0.29517481630283526, 0.0015606909990311113), 73: (1.1249983108921853, 0.27460131610512795, 0.0015131771564483865), 74: (0.9582896293698356, 0.3499530367509156, 0.0014142096042633057), 75: (-0.3494323682524638, 0.730607818680475, -0.00045030713081362084), 76: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 77: (0.43274634082896746, 0.6700713494942525, 0.0006299242377281189), 78: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 79: (0.831870192340097, 0.41581548127152934, 0.0010650619864464028), 80: (-0.6664111234719685, 0.5131597453870926, -0.0017973065376281627), 81: (0.7780572169891012, 0.44611673216458503, 0.0031023234128951804), 82: (-0.2686312437643469, 0.791108615990461, -0.00022893697023396165), 83: (-0.5457728927551285, 0.5915729724571748, -0.0005119055509567594), 84: (4.561650474819855, 0.00021319750591981511, 0.00431002676486969), 85: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 86: (4.561650474819855, 0.00021319750591981511, 0.00431002676486969), 87: (11.010716603364635, 1.0921299854413208e-09, 0.010568979382514998), 88: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 89: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 90: (1.0637210480277177, 0.30078805955814025, 0.0011882573366165383), 91: (3.6135226538296683, 0.001850616143029893, 0.003304967284202487), 92: (7.622456405355375, 3.410296896217827e-07, 0.008830562233924866), 93: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 94: (4.938953194680991, 9.112475991874698e-05, 0.005027011036872864), 95: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 96: (6.714881111632672, 2.0348426215097325e-06, 0.007181236147880576), 97: (-0.07731640103103846, 0.9391802415001849, -8.628368377683326e-05), 98: (2.79280976762182, 0.011603946348508714, 0.0030788868665695412), 99: (2.903617370634803, 0.009106010357973707, 0.0030685946345329063), 100: (0.9654002716532625, 0.34647240489796993, 0.001101657748222351), 101: (10.59050226920981, 2.0724162787332835e-09, 0.010231825709342945), 102: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 103: (5.4843523237003415, 2.7265752737184905e-05, 0.005727683007717144), 104: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 105: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 106: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 107: (-0.3181158623863308, 0.7538670422588711, -0.00038250088691710316), 108: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 109: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 110: (-0.17260063152673366, 0.8647906774407321, -0.00022515356540681042), 111: (0.1146094829785176, 0.9099571640600346, 0.00012142956256866455), 112: (0.08284497610249766, 0.9348413845528509, 9.741485118863746e-05), 113: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 114: (0.09216537345907856, 0.9275314887096396, 0.00010473728179938302), 115: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 116: (0.4632378052925146, 0.648456153909601, 0.0005057364702224509), 117: (-0.26042867881764836, 0.7973346475213109, -0.00029164254665370315), 118: (-0.3750008117769526, 0.7118131023296859, -0.000460708141326871), 119: (-0.6469147561060148, 0.5254261353596277, -0.000817073881626107), 120: (-0.29662372851395696, 0.7699719612142981, -0.000387200713157676), 121: (-0.7101735937355383, 0.4862224188674523, -0.0010588198900222667), 122: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 123: (-0.4844556320193189, 0.6336001696899646, -0.000822190940380052), 124: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 125: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 126: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 127: (2.2785103645191667, 0.03443363204092818, 0.009405428171157859), 128: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 129: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 130: (2.359254180695483, 0.029168546047316737, 0.012106120586395264), 131: (2.665775666150978, 0.015274503537344412, 0.01082838624715804), 132: (2.640052341317529, 0.016141602275097384, 0.011544437706470456), 133: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 134: (2.359823315241125, 0.029134239010212195, 0.00960416495800015), 135: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 136: (2.2719409701125346, 0.03489850091617077, 0.008913415670394942), 137: (2.410227595417562, 0.026240474368313817, 0.01069551706314087), 138: (2.4547128014911554, 0.023911529244833432, 0.009658692777156863), 139: (2.243085573650609, 0.03700966254519938, 0.010553579777479172), 140: (1.861165540870239, 0.07826668314737244, 0.007289951294660535), 141: (2.410029655559331, 0.026251295034089187, 0.007672186940908404), 142: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 143: (2.314148366568837, 0.032009860677918146, 0.009153123199939772)}, 'noun_phrase': {0: (-1.9359197349281163, 0.08486303438398021, -0.003261709213256858), 1: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 2: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 3: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 4: (-3.5921207239926622, 0.005819472709776622, -0.006515455245971691), 5: (-2.2405191888039777, 0.05179931555336764, -0.004590368270874001), 6: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 7: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 8: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 9: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 10: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 11: (-2.177198306446434, 0.057436064972269864, -0.005819413065910328), 12: (-1.400019122170044, 0.19502306335776495, -0.004387927055358953), 13: (-3.8281904159932822, 0.004038902993587881, -0.005794817209243797), 14: (-3.0418381044554876, 0.013976266320790747, -0.005463844537734963), 15: (-3.002390611876544, 0.014898501704831292, -0.006923961639404286), 16: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 17: (-1.3965123499270924, 0.19604149075064167, -0.005478394031524669), 18: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 19: (-1.5320431542102702, 0.15987433423037603, -0.0039048135280608798), 20: (-1.2667867509932154, 0.23703097219497393, -0.004638332128524714), 21: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 22: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 23: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 24: (-1.5919673048439271, 0.1458558787983439, -0.005894666910171487), 25: (-1.9018239322840609, 0.08962630091028609, -0.007182878255844183), 26: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 27: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 28: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 29: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 30: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 31: (-2.231033720451802, 0.0526079234418805, -0.009498947858810391), 32: (-1.6722600173987396, 0.12880431947882942, -0.006319308280944846), 33: (-1.319104625064257, 0.21970751880154052, -0.005503982305526733), 34: (-1.6426000220670234, 0.13488091489643397, -0.006459599733352639), 35: (-1.8324739912421704, 0.10010153939268446, -0.007775568962097135), 36: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 37: (-0.5298460653769466, 0.6090391421319425, -0.0025731325149536133), 38: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 39: (-2.0563281051041438, 0.06989434897981066, -0.0072820544242858665), 40: (-1.832659175296179, 0.100072100381963, -0.0058657824993133545), 41: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 42: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 43: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 44: (-1.8572277489032036, 0.09623735235256793, -0.00609959363937379), 45: (-2.0199085350068584, 0.07413294507286643, -0.005737346410751298), 46: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 47: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 48: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 49: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 50: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 51: (-2.0686929741325577, 0.06850892653616228, -0.010611301660537698), 52: (-2.1190318543237137, 0.06313680911664347, -0.0076056540012360285), 53: (-1.371374748224241, 0.20347631027779792, -0.00496740341186519), 54: (-1.670398188835708, 0.1291783171103463, -0.00642936229705815), 55: (-1.805673082097354, 0.10444831758628668, -0.009003195166587818), 56: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 57: (-1.1323926277300542, 0.28673600315853054, -0.005244308710098289), 58: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 59: (-1.2051019454800171, 0.2588935665517983, -0.00519210398197173), 60: (-1.3078051242097022, 0.2233563599285183, -0.004501736164092995), 61: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 62: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 63: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 64: (-1.1304764999833943, 0.287500792462536, -0.004508697986602805), 65: (-1.5776562043906954, 0.14910008006886857, -0.0054946839809417725), 66: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 67: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 68: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 69: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 70: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 71: (-1.1228025481450146, 0.2905799023735358, -0.0034292519092559703), 72: (-1.1442219595457226, 0.2820501762831403, -0.002496677637100153), 73: (-1.39756984179323, 0.19573389666515814, -0.004101824760437078), 74: (-1.782239322829124, 0.10839218870326162, -0.0048563122749327725), 75: (-1.1621705134074587, 0.2750570177717406, -0.004986456036567721), 76: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 77: (-1.4777500256179747, 0.17359339189294204, -0.005267497897148143), 78: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 79: (-1.3501294547030407, 0.20994633313990962, -0.004570305347442627), 80: (-0.41295578067509575, 0.6893037499489926, -0.002099892497062661), 81: (-1.7588500575013584, 0.11246579297791844, -0.009781035780906722), 82: (-2.0992377338378363, 0.06519888246336426, -0.0063045263290405495), 83: (-1.7171570209124136, 0.12007936926462812, -0.006237471103668235), 84: (-0.5440740393161605, 0.5996079486356212, -0.002246636152267445), 85: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 86: (-0.5440740393161605, 0.5996079486356212, -0.002246636152267445), 87: (1.0527153678445045, 0.3199131931574301, 0.0033967435359955056), 88: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 89: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 90: (-2.143597626272751, 0.06066523565255921, -0.008459538221359253), 91: (-2.2476325636466004, 0.05120091292210157, -0.008722174167633101), 92: (-0.2006075316872387, 0.8454655225549137, -0.0007541239261626975), 93: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 94: (-1.4932396741932192, 0.1695775885824614, -0.005306470394134455), 95: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 96: (-0.3191203618275257, 0.7569175311159203, -0.0011212348937987837), 97: (-2.458981689327924, 0.03621910324283895, -0.008778303861618042), 98: (-1.5699961994350784, 0.15086293024603037, -0.005399644374847412), 99: (-1.7332341226006547, 0.11708906381472337, -0.006020474433898915), 100: (-2.2478215731877276, 0.051185105408517506, -0.007329279184341453), 101: (0.9242215188599492, 0.3794839117881119, 0.004103481769561768), 102: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 103: (-1.2435497732198746, 0.2450811694075575, -0.005191737413406394), 104: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 105: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 106: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 107: (-1.2584645766151907, 0.23988859851978603, -0.004110944271087658), 108: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 109: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 110: (-1.8641328798290278, 0.09518460229791703, -0.006200087070465099), 111: (-1.4991464007925193, 0.1680678520803307, -0.00435821413993831), 112: (-1.755279233997687, 0.11310001292177661, -0.005064666271209717), 113: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 114: (-1.6163046921037871, 0.14048400703572178, -0.005287939310073764), 115: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 116: (-1.9711279406636157, 0.08019647814243068, -0.005731606483459495), 117: (-1.75484549632847, 0.11317727421496512, -0.00519672632217405), 118: (-1.6664082176427797, 0.12998313803203515, -0.0054109632968902255), 119: (-2.0442336127660954, 0.07127548469948174, -0.005514851212501504), 120: (-1.7009025325257665, 0.12317372724695272, -0.004597327113151528), 121: (-1.5645063437346745, 0.15213777580428148, -0.004361140727996815), 122: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 123: (-1.6247418312440645, 0.13866371532846725, -0.004948762059211742), 124: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 125: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 126: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 127: (-1.3397891031696871, 0.2131580692015096, -0.004450920224189747), 128: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 129: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 130: (-0.6524386219967896, 0.5304275011164683, -0.0029736489057540783), 131: (-1.5652547968287462, 0.15196340716105336, -0.005452835559844937), 132: (-0.9499563651753489, 0.366944972691554, -0.003337734937667869), 133: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 134: (-0.331746526385179, 0.7476755298176827, -0.0010100603103637695), 135: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 136: (-1.5532771974724229, 0.15477535040653126, -0.005829417705535911), 137: (0.173603556619036, 0.8660189516820461, 0.0005457580089569536), 138: (-0.7888195050695923, 0.45049597174297196, -0.003311330080032371), 139: (-1.5504657179857682, 0.15544206940481914, -0.006878229975700367), 140: (-1.7797678230287417, 0.10881609423344454, -0.008536991477012645), 141: (-1.4496366575264021, 0.18109491798781593, -0.007257568836212147), 142: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 143: (-0.9296431144648586, 0.37681686074414045, -0.004888626933097828)}, 'original_noun_phrase': {0: (-15.651398643433845, 4.1453538019672003e-51, -0.0055239785255657425)}}, 'dann': {'named': {0: (2.6103947862637535, 0.017199240439614367, 0.011928600072860673), 1: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 2: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 3: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 4: (2.5092497314562876, 0.02132035644276648, 0.012463900446891829), 5: (2.4316064916403395, 0.025095964440944134, 0.008169776201248147), 6: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 7: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 8: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 9: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 10: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 11: (2.7206603552292705, 0.013570121500998907, 0.013262182474136297), 12: (2.7599366760771935, 0.012463413199479363, 0.01201771795749662), 13: (2.6464823962790263, 0.015920576282805184, 0.011606037616729736), 14: (2.6262554643444473, 0.016625715351621783, 0.010800504684448264), 15: (2.7682107276193775, 0.012241527969973616, 0.013800801336765312), 16: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 17: (2.7885554708035167, 0.011711892006608559, 0.015732431411743186), 18: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 19: (2.8352092689387693, 0.01057894462261949, 0.014922112226486206), 20: (2.768750157576103, 0.012227193965040123, 0.02500676512718203), 21: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 22: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 23: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 24: (2.93476167269629, 0.008502798875669767, 0.02939695119857788), 25: (2.738110081888915, 0.013067324427681905, 0.024826714396476768), 26: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 27: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 28: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 29: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 30: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 31: (2.9634761132464313, 0.007980939359257441, 0.02686447501182554), 32: (2.8407244653589068, 0.010452178354582193, 0.023518845438957214), 33: (2.775746657367221, 0.01204272733819661, 0.023004117608070307), 34: (2.8277620864617217, 0.01075246047528403, 0.022682592272758484), 35: (2.88838547813698, 0.009415837176344984, 0.03005471378564839), 36: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 37: (2.9733964670835156, 0.007807942915604268, 0.032468363642692566), 38: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 39: (2.906120344098321, 0.009056046626788558, 0.0336635142564774), 40: (2.80842970923703, 0.01121572907259239, 0.016974622011184715), 41: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 42: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 43: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 44: (2.8924318658678465, 0.009332555163869228, 0.02221711874008181), 45: (2.7029789524994787, 0.014098376107217862, 0.015183374285697937), 46: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 47: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 48: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 49: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 50: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 51: (2.89587139025981, 0.00926232144924328, 0.018863475322723477), 52: (2.7971994432079477, 0.011493564463744527, 0.018300509452819802), 53: (2.6929188384939966, 0.014407617635695123, 0.01884995400905609), 54: (2.6942788236446074, 0.014365438288492592, 0.016951128840446472), 55: (2.9185447165477694, 0.008811918607938592, 0.02505885809659958), 56: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 57: (3.007555189737084, 0.007239567422610325, 0.029827497899532318), 58: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 59: (2.9490198539813113, 0.008239680467161007, 0.027023300528526306), 60: (2.9294106654178353, 0.0086036226508914, 0.010767298936843916), 61: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 62: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 63: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 64: (2.993402595610379, 0.00747002451250534, 0.012270322442054837), 65: (3.0601517867655366, 0.006441952809940185, 0.009791091084480286), 66: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 67: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 68: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 69: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 70: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 71: (2.92869606273098, 0.008617174175294142, 0.0105192571878433), 72: (2.277012206788573, 0.034539140453250156, 0.004699712991714455), 73: (2.831245353720505, 0.010670964778161365, 0.005946746468543984), 74: (2.7295140761014784, 0.013312747369964885, 0.007367062568664595), 75: (3.1058799658170972, 0.005818350172150957, 0.015683779120445274), 76: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 77: (3.0892671588081413, 0.006037799562176934, 0.012800940871238675), 78: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 79: (3.087240881130914, 0.006065110261453242, 0.01329335868358611), 80: (2.8087124765138056, 0.01120881718131954, 0.016108225286006916), 81: (2.833286273988715, 0.010623490409024463, 0.03871192485094066), 82: (2.4806467052111603, 0.02264456595340393, 0.0043008685111999845), 83: (2.5829132463192552, 0.018237557057120556, 0.0034660324454307556), 84: (4.54841894094851, 0.00021968685465574152, 0.008455431461334206), 85: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 86: (4.54841894094851, 0.00021968685465574152, 0.008455431461334206), 87: (5.769151100855724, 1.4705844669189098e-05, 0.011277711391449008), 88: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 89: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 90: (0.6769574592804383, 0.5065920974120413, 0.001984626054763794), 91: (5.214256783919982, 4.93760898309573e-05, 0.008036795258521967), 92: (6.056797547522969, 7.960969881339664e-06, 0.015093770623207048), 93: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 94: (-0.40490702966731434, 0.6900682738154246, -0.0008324533700942993), 95: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 96: (-0.8440543326556268, 0.40914102931184826, -0.0015962868928909302), 97: (-0.021390838269522268, 0.9831569308204549, -4.547834396362305e-05), 98: (0.7861387266930034, 0.4414810991375544, 0.001652923226356462), 99: (4.285325112910361, 0.00039951537069062866, 0.012760338187217724), 100: (1.694590781568973, 0.10648142938167585, 0.004414877295494057), 101: (10.453739572099726, 2.563048905570711e-09, 0.02470133304595945), 102: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 103: (4.785291290120442, 0.00012865458575207346, 0.01689343750476835), 104: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 105: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 106: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 107: (3.0197293044775324, 0.00704684737387263, 0.01095967292785649), 108: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 109: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 110: (2.9887173755638496, 0.007547869581981068, 0.011412858963012695), 111: (2.9949393585336357, 0.007444660741199274, 0.007520383596420266), 112: (2.980428862712128, 0.007687509455643306, 0.010929712653160062), 113: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 114: (2.997267297196953, 0.007406397525509584, 0.009008508920669511), 115: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 116: (3.033095790477973, 0.006840975632980589, 0.00854403972625728), 117: (3.0559468368881655, 0.006502450773708548, 0.008743429183959983), 118: (2.969353781850851, 0.007877999433326299, 0.009646764397621133), 119: (2.972589402427881, 0.00782188042582904, 0.016182480752468142), 120: (2.9553898016429723, 0.008124686772074364, 0.014857803285121929), 121: (2.9641410442447254, 0.007969229059858068, 0.01723177880048754), 122: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 123: (3.0032123391275105, 0.007309543316983239, 0.019232231378555287), 124: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 125: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 126: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 127: (2.9494314684755825, 0.008232202534938372, 0.02923231124877934), 128: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 129: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 130: (2.9540510825213544, 0.008148724687837637, 0.03158140927553177), 131: (2.9624466600605515, 0.007999102167196308, 0.018715083599090576), 132: (2.9772518632058964, 0.007741693167467521, 0.024149402976036072), 133: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 134: (2.903855433908864, 0.009101246795777613, 0.021802923083305337), 135: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 136: (2.8992656468373372, 0.009193511126860463, 0.02026311159133909), 137: (2.8638766807406766, 0.00993576246164692, 0.022373551130294822), 138: (2.969027447358234, 0.007883680972499465, 0.023039574921131123), 139: (2.9340452536128225, 0.008516231274771047, 0.023069849610328652), 140: (2.7874978474710157, 0.011738876234434108, 0.01798664256930349), 141: (2.9419650728470366, 0.008368870824083162, 0.01703978329896927), 142: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 143: (2.978592531378548, 0.00771878319812897, 0.022190096974372853)}, 'noun_phrase': {0: (1.5211540598530064, 0.16254649062513027, 0.013754546642303467), 1: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 2: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 3: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 4: (0.984389581324264, 0.35064594117528725, 0.010298395156860374), 5: (0.6632973004745379, 0.5237642157995729, 0.0070555627346039484), 6: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 7: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 8: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 9: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 10: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 11: (1.0370261838546138, 0.3267820036368045, 0.00874339342117314), 12: (0.7119061384825222, 0.49456175001248004, 0.00777845382690423), 13: (0.9371818472161219, 0.3731309140840815, 0.007833760976791448), 14: (0.7366210450925132, 0.48011253405961407, 0.006562745571136452), 15: (0.8343167047122595, 0.42569261636890376, 0.00946221351623533), 16: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 17: (1.0512282292622237, 0.32055948669042184, 0.011078271269798268), 18: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 19: (1.2560797061214373, 0.2407127354829818, 0.013133659958839417), 20: (0.6197988663892074, 0.5507575215599216, 0.006505572795867831), 21: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 22: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 23: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 24: (0.5314321588295272, 0.6079839464159491, 0.00651753544807443), 25: (0.24727993101355267, 0.8102403009419568, 0.0027957201004028542), 26: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 27: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 28: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 29: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 30: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 31: (0.1951014486755195, 0.8496468908155581, 0.001885938644409224), 32: (0.08608159956273065, 0.9332862053204737, 0.0009582996368407981), 33: (0.2217119948379997, 0.8294870516588212, 0.002364957332610995), 34: (0.21154382157197257, 0.8371756779670659, 0.0023240566253661665), 35: (0.1916696407314516, 0.8522555602874563, 0.0023022353649139737), 36: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 37: (0.4321730201921053, 0.6757853326831198, 0.006130862236022994), 38: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 39: (0.5592106457537548, 0.5896603452047438, 0.0070510983467102495), 40: (0.5588404923242061, 0.5899025381298684, 0.003660529851913452), 41: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 42: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 43: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 44: (0.42468507042674847, 0.6810385105607396, 0.003106921911239624), 45: (0.3690807508222823, 0.7206011217092118, 0.002863657474517778), 46: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 47: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 48: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 49: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 50: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 51: (-0.0456223529946904, 0.9646075102381807, -0.0002785503864287664), 52: (0.34091357953439466, 0.7409919871619243, 0.002949303388595559), 53: (0.403134994753946, 0.6962578372112948, 0.0028584957122802512), 54: (0.4314506422032745, 0.676291316411693, 0.003278601169586115), 55: (0.38669031903503914, 0.7079694287177205, 0.004895877838134777), 56: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 57: (0.6550953575024775, 0.5287925704894851, 0.009386119246482805), 58: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 59: (0.8946026226027306, 0.3942944517389294, 0.010824564099311818), 60: (1.1624310662910748, 0.2749565308335149, 0.007143926620483354), 61: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 62: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 63: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 64: (1.4330775157735591, 0.18564423250548448, 0.008354246616363525), 65: (0.8255857678848993, 0.43037880484130064, 0.0045719683170318826), 66: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 67: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 68: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 69: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 70: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 71: (1.1667007049543583, 0.27331405258306735, 0.007150387763977029), 72: (0.9869411346801803, 0.34945987843095927, 0.0037491142749787043), 73: (0.3678523045748961, 0.7214857162649786, 0.0013000845909117986), 74: (0.9355467605241871, 0.37392813308729367, 0.004311722517013505), 75: (1.363227620845901, 0.20593706702985365, 0.012658190727233898), 76: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 77: (1.1017450409429674, 0.2991626799477583, 0.008647975325584423), 78: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 79: (1.2084413701632182, 0.2576692452722908, 0.008719712495803833), 80: (1.0238216628886645, 0.33264965296422166, 0.008077451586723294), 81: (1.9762407340205639, 0.0795394356984782, 0.02270984649658203), 82: (-0.42703446245694854, 0.6793883185306457, -0.0011041760444641113), 83: (1.532474106169191, 0.159769384656312, 0.003844776749610923), 84: (0.0879514617355009, 0.9318411509571123, 0.0004138648509979248), 85: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 86: (0.0879514617355009, 0.9318411509571123, 0.0004138648509979248), 87: (0.15523944596788888, 0.8800584383822896, 0.0006642401218414085), 88: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 89: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 90: (-3.8067711291385278, 0.004173753314251861, -0.011523932218551636), 91: (-1.8029863878824919, 0.10489362292100228, -0.0071912050247191495), 92: (0.6411449947774289, 0.5374110134642108, 0.0022733032703399214), 93: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 94: (-3.226996053276431, 0.01037244631668259, -0.01360133886337278), 95: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 96: (-2.6142804669791535, 0.028073003494980337, -0.012852048873901345), 97: (-3.146274040846368, 0.011808150660718974, -0.012410771846771307), 98: (-2.969709395751982, 0.015709836245144774, -0.010099208354949929), 99: (1.0111902941119997, 0.33833698593321376, 0.002703249454498291), 100: (-2.1807562313493887, 0.05710407711809115, -0.005730390548706055), 101: (3.3910737644395006, 0.007985498073479418, 0.014367562532424938), 102: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 103: (0.845008227377277, 0.4200018922314048, 0.0031632542610168235), 104: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 105: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 106: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 107: (1.503233054789495, 0.1670302522608637, 0.00598465204238896), 108: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 109: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 110: (1.0801275731534432, 0.3081788129116308, 0.0037075042724609153), 111: (1.350372644189398, 0.20987129463835882, 0.003793919086456321), 112: (1.1409329986178038, 0.2833468488875774, 0.004815900325775169), 113: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 114: (0.938123675525445, 0.3726722678854003, 0.0029580175876616766), 115: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 116: (0.4418870143661213, 0.6689980321458654, 0.0010536968708039218), 117: (1.0624567868430348, 0.31570439647134824, 0.003084981441497825), 118: (1.1882036759612773, 0.26516151588337133, 0.004590630531311035), 119: (1.4773634891828835, 0.17369466132529682, 0.010195052623748757), 120: (1.541044659092739, 0.15769484933100672, 0.009900480508804321), 121: (1.6310872575661979, 0.13730875715834567, 0.011794212460517872), 122: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 123: (1.325637573581265, 0.21762086424884264, 0.01053376793861388), 124: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 125: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 126: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 127: (0.5937930491209755, 0.5672731790101048, 0.0066876918077469205), 128: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 129: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 130: (0.5426486941236397, 0.6005492527688636, 0.0045343518257141), 131: (-0.8042424472502977, 0.44198180042872004, -0.007315874099731445), 132: (0.3308831718874723, 0.7483061466157258, 0.0032659351825714), 133: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 134: (0.4624585358432608, 0.6547294103416952, 0.004889035224914573), 135: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 136: (-0.34970864312488015, 0.7346011484279015, -0.002848786115646429), 137: (0.6464103164936413, 0.534148417373987, 0.005934375524520863), 138: (0.2691786656064834, 0.793861518024378, 0.003027743101119973), 139: (0.17735762485888104, 0.863154868091551, 0.001822125911712602), 140: (-0.37085576442258356, 0.7193237238085466, -0.003197172284126304), 141: (-0.05608851125686871, 0.9564967421496149, -0.00037859976291654274), 142: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 143: (-0.23611853117342302, 0.8186265508544835, -0.0017890527844429127)}, 'original_noun_phrase': {0: (3.7088827910822393, 0.00021609055086301644, 0.0022620030678809355)}}}\n",
            "V non_dann named\n",
            "named {0: (-0.9655923860156405, 0.34637869716316627, -0.0015480875968932883), 1: (-1.0741254413589432, 0.29621899886218495, -0.0027149647474289385), 2: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 3: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 4: (-1.4234530845842852, 0.1708193823853086, -0.003223337233066559), 5: (-1.5367179668232358, 0.1408488075207118, -0.0025740280747413857), 6: (-1.5653479220113784, 0.13400480155122227, -0.003283877670764934), 7: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 8: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 9: (-0.19233909614082156, 0.8495174542603634, -0.00021315068006516613), 10: (-0.6684984996365081, 0.5118560541591541, -0.0013095587491989136), 11: (-1.2836756202863069, 0.21469064283757888, -0.00285803079605107), 12: (-0.8837081892317356, 0.38789931416530654, -0.001225583255290985), 13: (-0.8763453230689969, 0.3917876841049176, -0.0012641027569770813), 14: (-1.3433628505638238, 0.19498083875532457, -0.0034336537122726107), 15: (0.377900318366655, 0.7096933759595105, 0.0003549039363861528), 16: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 17: (-0.05933765597262203, 0.9533028378635036, -7.38292932510598e-05), 18: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 19: (0.16783953998993503, 0.868483143283497, 0.00023294389247896508), 20: (-1.1688359416468115, 0.25692596352159114, -0.0050141215324401855), 21: (-1.2758445829146297, 0.21738809261491887, -0.0051697060465812905), 22: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 23: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 24: (-1.0716031571730849, 0.29732202084518844, -0.0033075079321860934), 25: (-0.5141001827100227, 0.6131096575601327, -0.0010094165802002064), 26: (-0.5208521245718691, 0.6084871752646555, -0.0013989895582199319), 27: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 28: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 29: (-0.3114520296882886, 0.7588485536702768, -0.0008454144001007191), 30: (-0.9634391729116165, 0.3474299688920349, -0.0041394203901291005), 31: (-0.7690457529567646, 0.45132096835816593, -0.002662914991378773), 32: (-0.24857079295377213, 0.8063597754513214, -0.0007111832499503978), 33: (-1.066340630532658, 0.2996329236992549, -0.00435973554849628), 34: (-1.307672890608816, 0.20658688109408466, -0.00601511597633364), 35: (-0.5262795155351091, 0.6047837023066619, -0.0013959199190139993), 36: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 37: (-0.8581215038175375, 0.4015212353378972, -0.0022276133298874123), 38: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 39: (-0.40565957624116045, 0.6895245499887311, -0.0012224495410918523), 40: (-0.6406568097495143, 0.5293975255966856, -0.001646570861339569), 41: (-0.5853719355747461, 0.5651861053067004, -0.0012907266616821733), 42: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 43: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 44: (-0.9489773932241476, 0.35454747994196123, -0.0018764838576317278), 45: (-1.0683973297232137, 0.29872824067494963, -0.002051642537116982), 46: (-0.8369860284159304, 0.4130046106090671, -0.0017104774713516402), 47: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 48: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 49: (0.5390550742003877, 0.5961094768520061, 0.0008374646306038014), 50: (0.509430426665857, 0.6163164151134919, 0.0011194631457328574), 51: (-0.45904937428945386, 0.6514069685670868, -0.0009944468736648449), 52: (1.1920889842998112, 0.24790333603105016, 0.002108065783977542), 53: (-0.009001506377464103, 0.9929117780184165, -1.8292665481589587e-05), 54: (-0.8938228627007104, 0.38259927561296503, -0.002469956874847412), 55: (-0.7409374164917433, 0.4677905743603378, -0.001771286129951477), 56: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 57: (-0.189613780755316, 0.8516227653999674, -0.0003061845898628124), 58: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 59: (0.08010588384267328, 0.9369907869551097, 0.0001716315746307373), 60: (-0.9435540905679378, 0.357242098328733, -0.0012674659490585216), 61: (-1.2222354524469252, 0.23656526113198942, -0.001689542829990398), 62: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 63: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 64: (-1.3416058181990118, 0.1955398725124688, -0.002584441006183602), 65: (-1.2939101689160104, 0.2112046721344239, -0.0025296270847320335), 66: (-1.4975951364366846, 0.15066797542950505, -0.0037569612264632957), 67: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 68: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 69: (-0.8947327697137233, 0.38212485012500774, -0.0015893116593361234), 70: (-1.0199802863067917, 0.3205505139576873, -0.0019883275032043235), 71: (-1.3887894251705182, 0.180962945560633, -0.003145812451839425), 72: (-1.2717330742018775, 0.21881483186899028, -0.0028291866183280945), 73: (-1.084865105941498, 0.29155555727453236, -0.001851402223110199), 74: (-1.2807612794340442, 0.2156914457213122, -0.0027890652418136597), 75: (-1.4655199689952445, 0.1591328174936716, -0.0026975095272063765), 76: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 77: (-1.4125480364169472, 0.17395977281344674, -0.0030394211411476357), 78: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 79: (-1.4035888265476122, 0.176574526295681, -0.002786573767662115), 80: (-1.9538375680947229, 0.06560263570584778, -0.006239135563373577), 81: (-0.9555164699181337, 0.35131696447438465, -0.0018814221024513467), 82: (-1.4043987162522156, 0.17633686542850158, -0.0023443534970283286), 83: (-1.3046279959727924, 0.2076016532099325, -0.002861945331096627), 84: (-1.246634060229046, 0.2276824161542174, -0.0028518155217170382), 85: (-1.2310797936963471, 0.2333151456783793, -0.002442561089992523), 86: (-1.246634060229046, 0.2276824161542174, -0.0028518155217170382), 87: (-1.4632692623450536, 0.15974111921326092, -0.003566589951515209), 88: (-1.2310797936963471, 0.2333151456783793, -0.002442561089992523), 89: (-1.2807847243339239, 0.21568338013571242, -0.002878469228744518), 90: (-1.2411009999355749, 0.22967401659770825, -0.0032365977764129417), 91: (-1.1329422043775377, 0.27133332550132955, -0.0024403154850006215), 92: (-1.055336190633787, 0.3045069992126154, -0.0014326974749565013), 93: (-0.6382379117121378, 0.5309370059872862, -0.0010516881942749245), 94: (-1.1996396689886066, 0.245025619144954, -0.0029798626899719682), 95: (-1.2310797936963471, 0.2333151456783793, -0.002442561089992523), 96: (-1.2310797936963471, 0.2333151456783793, -0.002442561089992523), 97: (-1.271510855089988, 0.21889215087735586, -0.003068020939826943), 98: (-1.0164371450382368, 0.32219065132596236, -0.002341513335704759), 99: (-0.9881038745787944, 0.3355189738627954, -0.0011626988649369174), 100: (-0.8175833210164493, 0.42372975642343835, -0.0007899612188339011), 101: (-1.150245744457188, 0.26431475506382923, -0.0010504990816117), 102: (-1.2234240597852992, 0.23612647059559277, -0.001219561696052529), 103: (-1.2639236569598344, 0.22154477783200927, -0.002597288787364982), 104: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 105: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 106: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 107: (-1.332367803736338, 0.1985000284689346, -0.001470197737216905), 108: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 109: (-1.3690711449738875, 0.18694535568447485, -0.0014252185821533314), 110: (-1.6968615586433409, 0.10604454993733481, -0.0023965135216712508), 111: (-1.6758607670247108, 0.11014403108217709, -0.002761526405811343), 112: (-1.5574983394136035, 0.13585305366721342, -0.0022291630506515725), 113: (-1.6498443215634835, 0.1154094152679027, -0.0020239174365997425), 114: (-1.325939630449497, 0.2005806263497934, -0.002689258754253354), 115: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 116: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 117: (-1.646052449263405, 0.11619442187733992, -0.002467985451221455), 118: (-1.373653937332699, 0.1855410625486748, -0.0016740038990974426), 119: (-1.5631035035748297, 0.13453111299807083, -0.0018570870161056519), 120: (-1.478995505174084, 0.15553026955592655, -0.0020525068044662254), 121: (-1.6110691092376708, 0.12365268822716842, -0.0019341111183166504), 122: (-1.6883917964470028, 0.10768191643732017, -0.0021598696708680087), 123: (-1.7141499880632753, 0.1027684786821179, -0.0023778527975082397), 124: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 125: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 126: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 127: (-1.4938415223919361, 0.1516390377816124, -0.004572677612304676), 128: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 129: (-1.3129193599736824, 0.20484752227316716, -0.005023352801799774), 130: (-0.8583608420079871, 0.40139239324552134, -0.0018903136253357045), 131: (-1.1694472967345249, 0.2566856322955363, -0.003980498015880574), 132: (-1.1360628993963449, 0.2700574248940383, -0.004126235842704773), 133: (-1.6375335808188212, 0.11797456457409827, -0.008619713783264149), 134: (-1.2194356546819272, 0.23760130541591554, -0.004548071324825265), 135: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 136: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 137: (-1.2420058982862812, 0.22934738959328374, -0.004030098021030437), 138: (-1.482196568669953, 0.15468439566172634, -0.005874595046043374), 139: (-0.7966730898476708, 0.4354834302361442, -0.0019156903028488825), 140: (-1.1543869206064388, 0.2626552548600694, -0.003194358944892839), 141: (-0.8446116343344517, 0.408837395689407, -0.0022171080112457497), 142: (-0.7843797043545297, 0.4424875467617805, -0.00208998024463658), 143: (-1.146384039521738, 0.2658692879092321, -0.003623847663402624)}\n",
            "V non_dann noun_phrase\n",
            "noun_phrase {0: (-0.059404197340731345, 0.9539283013239545, -0.00019150078296659157), 1: (0.2827684870284805, 0.7837500846544987, 0.0008828014135360829), 2: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 3: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 4: (-0.16916203883537503, 0.8694101768114813, -0.0005187541246414185), 5: (2.7188632924943, 0.023651784815193116, 0.009782162308692888), 6: (0.9267465809386195, 0.37824006408654753, 0.002643743157386802), 7: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 8: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 9: (0.1553281404548011, 0.8799905200007769, 0.0003853142261505127), 10: (2.042603803188805, 0.0714635916706121, 0.006852361559867848), 11: (0.11978999502582492, 0.9072809340766865, 0.00039168596267702416), 12: (1.7358431873080704, 0.11661028811499242, 0.004445970058441162), 13: (0.8352533038787895, 0.4251919941350234, 0.0024510204792022594), 14: (0.5276772076773238, 0.6104835859831382, 0.0019851565361022616), 15: (2.623989835452748, 0.027629588005256053, 0.00590239763259881), 16: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 17: (0.899885400693973, 0.39162312295973234, 0.0020317703485489003), 18: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 19: (1.8851414600043277, 0.09204771789818098, 0.0039432287216186745), 20: (1.9481795871832923, 0.0832096083036202, 0.009910395741462685), 21: (2.388838197839223, 0.04063470690309999, 0.01304997205734254), 22: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 23: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 24: (1.986108497553953, 0.07828579223734768, 0.01063987910747527), 25: (2.9010552828347635, 0.01756516657000177, 0.019302809238433805), 26: (1.9697079522156795, 0.08037987401938257, 0.01144414544105532), 27: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 28: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 29: (3.006855421113125, 0.014791051196128932, 0.015049532055854797), 30: (2.9458463949005096, 0.016330797875466346, 0.018118998408317544), 31: (2.225783411275994, 0.05306078927680244, 0.011234748363494917), 32: (2.1868453891588238, 0.05654022876027513, 0.014129883050918557), 33: (2.1398547510197097, 0.06103566280519921, 0.012387192249298129), 34: (2.1289182482555518, 0.062130632265630044, 0.012444487214088418), 35: (2.3401750146396885, 0.04400766060431682, 0.010740101337432861), 36: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 37: (1.4027578105564296, 0.19423086702518264, 0.006725168228149392), 38: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 39: (2.715432548796257, 0.023784998621499708, 0.011719131469726474), 40: (1.8455854852216242, 0.09803705351169469, 0.010246938467025735), 41: (2.0597568391954546, 0.06950750176674306, 0.011862045526504494), 42: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 43: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 44: (2.1407480241861574, 0.06094705815844367, 0.013551744818687428), 45: (2.821955705898563, 0.019983100357021288, 0.019610649347305276), 46: (2.0108135877737956, 0.0752292248974375, 0.012152406573295582), 47: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 48: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 49: (2.791275816514626, 0.021009994005131174, 0.01436871588230132), 50: (2.899181632937714, 0.01761883543492855, 0.018244919180870012), 51: (2.2642454795287064, 0.049829620819470316, 0.011322516202926625), 52: (2.2631511194968574, 0.04991883509184121, 0.014994788169860884), 53: (2.1331777822232794, 0.06170192204766262, 0.01296908259391788), 54: (1.9738854362225988, 0.07984147514886539, 0.012726208567619357), 55: (3.2270638975436823, 0.010371318905303612, 0.01232927441596987), 56: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 57: (2.3059216084842595, 0.04654615390440723, 0.011069506406784002), 58: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 59: (3.1403809797085143, 0.011920711594495271, 0.012796837091445856), 60: (2.4737431050712355, 0.035352498876522205, 0.002737727761268627), 61: (1.6113005706540569, 0.14157378011701163, 0.0019494831562042458), 62: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 63: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 64: (1.53743688384478, 0.1585651949016809, 0.002130213379859913), 65: (0.6035986920680937, 0.5610132145872897, 0.0003473639488220659), 66: (1.0077287109915996, 0.339908305180983, 0.0011471956968307495), 67: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 68: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 69: (1.6021500606216978, 0.14358622667512966, 0.001747226715087924), 70: (1.578855155842922, 0.14882583018562254, 0.001754996180534374), 71: (-0.9331597303937214, 0.3750941961722959, -0.0008138090372085682), 72: (1.5852476471891817, 0.14737120760043948, 0.0018799364566802756), 73: (-0.7271373912647012, 0.4856248944885333, -0.0007396459579467329), 74: (-1.0830423046211022, 0.30695096566719837, -0.0011349618434906006), 75: (0.4343451327383166, 0.6742649308269852, 0.0006856977939605491), 76: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 77: (0.3532057330609686, 0.7320659772780975, 0.0005770146846771351), 78: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 79: (-0.788284034364636, 0.45079352471553624, -0.0010927379131316917), 80: (1.3336937425899686, 0.21507071520896828, 0.0051479965448379406), 81: (1.9807669198305076, 0.07896205495606001, 0.012577366828918468), 82: (0.06972756394308077, 0.9459352163099144, 0.00014835894107817493), 83: (-1.1598508863626962, 0.27595292042177283, -0.003138679265975941), 84: (2.502593835967706, 0.03371801951659803, 0.009901961684226968), 85: (2.4071531852960577, 0.039432667879380455, 0.007238087058067333), 86: (2.502593835967706, 0.03371801951659803, 0.009901961684226968), 87: (2.1891701035114055, 0.05632639574221948, 0.007605543732643161), 88: (2.4071531852960577, 0.039432667879380455, 0.007238087058067333), 89: (2.2839470834087656, 0.04825000877654509, 0.006980195641517639), 90: (2.1001663651565923, 0.06510070417770156, 0.007232803106307972), 91: (2.217126449173077, 0.05381582701278829, 0.006793224811553922), 92: (2.9082126428940716, 0.01736168617648168, 0.005811285972595226), 93: (2.4387499353236053, 0.03744133009823867, 0.007071739435195901), 94: (2.007662306976666, 0.07561267274669725, 0.006354174017906167), 95: (2.4071531852960577, 0.039432667879380455, 0.007238087058067333), 96: (2.4071531852960577, 0.039432667879380455, 0.007238087058067333), 97: (1.634712174810816, 0.13654009651084903, 0.005153477191925049), 98: (2.630136944145678, 0.02735251082667631, 0.008446931838989258), 99: (1.9107332969923785, 0.0883578431583259, 0.0041037440299988015), 100: (2.290628562098974, 0.047725556273620136, 0.004274970293045022), 101: (2.0933821116848716, 0.06582125831616147, 0.006068557500839233), 102: (1.627603621013519, 0.13805114116980974, 0.0039044678211211936), 103: (2.585950424413753, 0.02940816869870594, 0.008170664310455322), 104: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 105: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 106: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 107: (0.7945155788504622, 0.4473388366512432, 0.0008462429046631192), 108: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 109: (0.9942624898356804, 0.34607322156604914, 0.0014589965343475009), 110: (1.455289556974256, 0.17956424097009338, 0.0012722373008727805), 111: (-0.40311714339013993, 0.6962705053346807, -0.00046482086181637294), 112: (-1.3995010159139174, 0.19517324349859388, -0.0016595661640167458), 113: (0.31736921700451814, 0.7582025900197144, 0.0004179954528809038), 114: (0.48352736126717916, 0.6402678088763173, 0.0006562054157256969), 115: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 116: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 117: (-0.2685125850444927, 0.794358177071614, -0.0003172755241394043), 118: (0.17690180491132731, 0.8635025141437851, 0.0002203315496444591), 119: (0.10644048277647157, 0.9175676736078939, 0.0001502275466918057), 120: (-1.1374421926809106, 0.2847282753703033, -0.0014120459556580256), 121: (-0.26892403073778803, 0.7940513734382388, -0.00041577816009519264), 122: (-0.7879673008075888, 0.4509695902220746, -0.000934952497482322), 123: (0.7339417108574199, 0.48166582547252856, 0.001118680834770236), 124: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 125: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 126: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 127: (1.3815475677348008, 0.20043898665844112, 0.008650729060173057), 128: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 129: (1.7839885104955835, 0.10809309766930562, 0.012804698944091808), 130: (1.993676236811333, 0.07733712993116255, 0.010171923041343678), 131: (1.3413500042767823, 0.2126705993332483, 0.009033668041229292), 132: (2.9252832409934597, 0.01688605966307884, 0.01588654816150664), 133: (0.8375860459376804, 0.42394687359118666, 0.006955501437187206), 134: (2.509183986679589, 0.033355385995736835, 0.014794331789016735), 135: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 136: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 137: (2.432500896362837, 0.03782707395283051, 0.015213516354560863), 138: (1.9800145535322926, 0.07905775295108508, 0.014388802647590682), 139: (2.494999426821794, 0.03414080250541585, 0.009348744153976463), 140: (2.19400933262387, 0.055883794369236614, 0.009948676824569791), 141: (2.526709734292557, 0.03240986873880421, 0.01006338596343992), 142: (2.5065990466500887, 0.03349715943896789, 0.009605455398559615), 143: (1.9558113216511723, 0.0821958264018493, 0.0078074634075164795)}\n",
            "V non_dann original_noun_phrase\n",
            "original_noun_phrase {0: (17.928403897297034, 4.934233780617678e-65, 0.006354058471818802)}\n",
            "non_dann original_noun_phrase {'named': {0: (-0.9655923860156405, 0.34637869716316627, -0.0015480875968932883), 1: (-1.0741254413589432, 0.29621899886218495, -0.0027149647474289385), 2: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 3: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 4: (-1.4234530845842852, 0.1708193823853086, -0.003223337233066559), 5: (-1.5367179668232358, 0.1408488075207118, -0.0025740280747413857), 6: (-1.5653479220113784, 0.13400480155122227, -0.003283877670764934), 7: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 8: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 9: (-0.19233909614082156, 0.8495174542603634, -0.00021315068006516613), 10: (-0.6684984996365081, 0.5118560541591541, -0.0013095587491989136), 11: (-1.2836756202863069, 0.21469064283757888, -0.00285803079605107), 12: (-0.8837081892317356, 0.38789931416530654, -0.001225583255290985), 13: (-0.8763453230689969, 0.3917876841049176, -0.0012641027569770813), 14: (-1.3433628505638238, 0.19498083875532457, -0.0034336537122726107), 15: (0.377900318366655, 0.7096933759595105, 0.0003549039363861528), 16: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 17: (-0.05933765597262203, 0.9533028378635036, -7.38292932510598e-05), 18: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 19: (0.16783953998993503, 0.868483143283497, 0.00023294389247896508), 20: (-1.1688359416468115, 0.25692596352159114, -0.0050141215324401855), 21: (-1.2758445829146297, 0.21738809261491887, -0.0051697060465812905), 22: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 23: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 24: (-1.0716031571730849, 0.29732202084518844, -0.0033075079321860934), 25: (-0.5141001827100227, 0.6131096575601327, -0.0010094165802002064), 26: (-0.5208521245718691, 0.6084871752646555, -0.0013989895582199319), 27: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 28: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 29: (-0.3114520296882886, 0.7588485536702768, -0.0008454144001007191), 30: (-0.9634391729116165, 0.3474299688920349, -0.0041394203901291005), 31: (-0.7690457529567646, 0.45132096835816593, -0.002662914991378773), 32: (-0.24857079295377213, 0.8063597754513214, -0.0007111832499503978), 33: (-1.066340630532658, 0.2996329236992549, -0.00435973554849628), 34: (-1.307672890608816, 0.20658688109408466, -0.00601511597633364), 35: (-0.5262795155351091, 0.6047837023066619, -0.0013959199190139993), 36: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 37: (-0.8581215038175375, 0.4015212353378972, -0.0022276133298874123), 38: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 39: (-0.40565957624116045, 0.6895245499887311, -0.0012224495410918523), 40: (-0.6406568097495143, 0.5293975255966856, -0.001646570861339569), 41: (-0.5853719355747461, 0.5651861053067004, -0.0012907266616821733), 42: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 43: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 44: (-0.9489773932241476, 0.35454747994196123, -0.0018764838576317278), 45: (-1.0683973297232137, 0.29872824067494963, -0.002051642537116982), 46: (-0.8369860284159304, 0.4130046106090671, -0.0017104774713516402), 47: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 48: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 49: (0.5390550742003877, 0.5961094768520061, 0.0008374646306038014), 50: (0.509430426665857, 0.6163164151134919, 0.0011194631457328574), 51: (-0.45904937428945386, 0.6514069685670868, -0.0009944468736648449), 52: (1.1920889842998112, 0.24790333603105016, 0.002108065783977542), 53: (-0.009001506377464103, 0.9929117780184165, -1.8292665481589587e-05), 54: (-0.8938228627007104, 0.38259927561296503, -0.002469956874847412), 55: (-0.7409374164917433, 0.4677905743603378, -0.001771286129951477), 56: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 57: (-0.189613780755316, 0.8516227653999674, -0.0003061845898628124), 58: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 59: (0.08010588384267328, 0.9369907869551097, 0.0001716315746307373), 60: (-0.9435540905679378, 0.357242098328733, -0.0012674659490585216), 61: (-1.2222354524469252, 0.23656526113198942, -0.001689542829990398), 62: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 63: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 64: (-1.3416058181990118, 0.1955398725124688, -0.002584441006183602), 65: (-1.2939101689160104, 0.2112046721344239, -0.0025296270847320335), 66: (-1.4975951364366846, 0.15066797542950505, -0.0037569612264632957), 67: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 68: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 69: (-0.8947327697137233, 0.38212485012500774, -0.0015893116593361234), 70: (-1.0199802863067917, 0.3205505139576873, -0.0019883275032043235), 71: (-1.3887894251705182, 0.180962945560633, -0.003145812451839425), 72: (-1.2717330742018775, 0.21881483186899028, -0.0028291866183280945), 73: (-1.084865105941498, 0.29155555727453236, -0.001851402223110199), 74: (-1.2807612794340442, 0.2156914457213122, -0.0027890652418136597), 75: (-1.4655199689952445, 0.1591328174936716, -0.0026975095272063765), 76: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 77: (-1.4125480364169472, 0.17395977281344674, -0.0030394211411476357), 78: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 79: (-1.4035888265476122, 0.176574526295681, -0.002786573767662115), 80: (-1.9538375680947229, 0.06560263570584778, -0.006239135563373577), 81: (-0.9555164699181337, 0.35131696447438465, -0.0018814221024513467), 82: (-1.4043987162522156, 0.17633686542850158, -0.0023443534970283286), 83: (-1.3046279959727924, 0.2076016532099325, -0.002861945331096627), 84: (-1.246634060229046, 0.2276824161542174, -0.0028518155217170382), 85: (-1.2310797936963471, 0.2333151456783793, -0.002442561089992523), 86: (-1.246634060229046, 0.2276824161542174, -0.0028518155217170382), 87: (-1.4632692623450536, 0.15974111921326092, -0.003566589951515209), 88: (-1.2310797936963471, 0.2333151456783793, -0.002442561089992523), 89: (-1.2807847243339239, 0.21568338013571242, -0.002878469228744518), 90: (-1.2411009999355749, 0.22967401659770825, -0.0032365977764129417), 91: (-1.1329422043775377, 0.27133332550132955, -0.0024403154850006215), 92: (-1.055336190633787, 0.3045069992126154, -0.0014326974749565013), 93: (-0.6382379117121378, 0.5309370059872862, -0.0010516881942749245), 94: (-1.1996396689886066, 0.245025619144954, -0.0029798626899719682), 95: (-1.2310797936963471, 0.2333151456783793, -0.002442561089992523), 96: (-1.2310797936963471, 0.2333151456783793, -0.002442561089992523), 97: (-1.271510855089988, 0.21889215087735586, -0.003068020939826943), 98: (-1.0164371450382368, 0.32219065132596236, -0.002341513335704759), 99: (-0.9881038745787944, 0.3355189738627954, -0.0011626988649369174), 100: (-0.8175833210164493, 0.42372975642343835, -0.0007899612188339011), 101: (-1.150245744457188, 0.26431475506382923, -0.0010504990816117), 102: (-1.2234240597852992, 0.23612647059559277, -0.001219561696052529), 103: (-1.2639236569598344, 0.22154477783200927, -0.002597288787364982), 104: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 105: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 106: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 107: (-1.332367803736338, 0.1985000284689346, -0.001470197737216905), 108: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 109: (-1.3690711449738875, 0.18694535568447485, -0.0014252185821533314), 110: (-1.6968615586433409, 0.10604454993733481, -0.0023965135216712508), 111: (-1.6758607670247108, 0.11014403108217709, -0.002761526405811343), 112: (-1.5574983394136035, 0.13585305366721342, -0.0022291630506515725), 113: (-1.6498443215634835, 0.1154094152679027, -0.0020239174365997425), 114: (-1.325939630449497, 0.2005806263497934, -0.002689258754253354), 115: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 116: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 117: (-1.646052449263405, 0.11619442187733992, -0.002467985451221455), 118: (-1.373653937332699, 0.1855410625486748, -0.0016740038990974426), 119: (-1.5631035035748297, 0.13453111299807083, -0.0018570870161056519), 120: (-1.478995505174084, 0.15553026955592655, -0.0020525068044662254), 121: (-1.6110691092376708, 0.12365268822716842, -0.0019341111183166504), 122: (-1.6883917964470028, 0.10768191643732017, -0.0021598696708680087), 123: (-1.7141499880632753, 0.1027684786821179, -0.0023778527975082397), 124: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 125: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 126: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 127: (-1.4938415223919361, 0.1516390377816124, -0.004572677612304676), 128: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 129: (-1.3129193599736824, 0.20484752227316716, -0.005023352801799774), 130: (-0.8583608420079871, 0.40139239324552134, -0.0018903136253357045), 131: (-1.1694472967345249, 0.2566856322955363, -0.003980498015880574), 132: (-1.1360628993963449, 0.2700574248940383, -0.004126235842704773), 133: (-1.6375335808188212, 0.11797456457409827, -0.008619713783264149), 134: (-1.2194356546819272, 0.23760130541591554, -0.004548071324825265), 135: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 136: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 137: (-1.2420058982862812, 0.22934738959328374, -0.004030098021030437), 138: (-1.482196568669953, 0.15468439566172634, -0.005874595046043374), 139: (-0.7966730898476708, 0.4354834302361442, -0.0019156903028488825), 140: (-1.1543869206064388, 0.2626552548600694, -0.003194358944892839), 141: (-0.8446116343344517, 0.408837395689407, -0.0022171080112457497), 142: (-0.7843797043545297, 0.4424875467617805, -0.00208998024463658), 143: (-1.146384039521738, 0.2658692879092321, -0.003623847663402624)}, 'noun_phrase': {0: (-0.059404197340731345, 0.9539283013239545, -0.00019150078296659157), 1: (0.2827684870284805, 0.7837500846544987, 0.0008828014135360829), 2: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 3: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 4: (-0.16916203883537503, 0.8694101768114813, -0.0005187541246414185), 5: (2.7188632924943, 0.023651784815193116, 0.009782162308692888), 6: (0.9267465809386195, 0.37824006408654753, 0.002643743157386802), 7: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 8: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 9: (0.1553281404548011, 0.8799905200007769, 0.0003853142261505127), 10: (2.042603803188805, 0.0714635916706121, 0.006852361559867848), 11: (0.11978999502582492, 0.9072809340766865, 0.00039168596267702416), 12: (1.7358431873080704, 0.11661028811499242, 0.004445970058441162), 13: (0.8352533038787895, 0.4251919941350234, 0.0024510204792022594), 14: (0.5276772076773238, 0.6104835859831382, 0.0019851565361022616), 15: (2.623989835452748, 0.027629588005256053, 0.00590239763259881), 16: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 17: (0.899885400693973, 0.39162312295973234, 0.0020317703485489003), 18: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 19: (1.8851414600043277, 0.09204771789818098, 0.0039432287216186745), 20: (1.9481795871832923, 0.0832096083036202, 0.009910395741462685), 21: (2.388838197839223, 0.04063470690309999, 0.01304997205734254), 22: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 23: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 24: (1.986108497553953, 0.07828579223734768, 0.01063987910747527), 25: (2.9010552828347635, 0.01756516657000177, 0.019302809238433805), 26: (1.9697079522156795, 0.08037987401938257, 0.01144414544105532), 27: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 28: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 29: (3.006855421113125, 0.014791051196128932, 0.015049532055854797), 30: (2.9458463949005096, 0.016330797875466346, 0.018118998408317544), 31: (2.225783411275994, 0.05306078927680244, 0.011234748363494917), 32: (2.1868453891588238, 0.05654022876027513, 0.014129883050918557), 33: (2.1398547510197097, 0.06103566280519921, 0.012387192249298129), 34: (2.1289182482555518, 0.062130632265630044, 0.012444487214088418), 35: (2.3401750146396885, 0.04400766060431682, 0.010740101337432861), 36: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 37: (1.4027578105564296, 0.19423086702518264, 0.006725168228149392), 38: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 39: (2.715432548796257, 0.023784998621499708, 0.011719131469726474), 40: (1.8455854852216242, 0.09803705351169469, 0.010246938467025735), 41: (2.0597568391954546, 0.06950750176674306, 0.011862045526504494), 42: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 43: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 44: (2.1407480241861574, 0.06094705815844367, 0.013551744818687428), 45: (2.821955705898563, 0.019983100357021288, 0.019610649347305276), 46: (2.0108135877737956, 0.0752292248974375, 0.012152406573295582), 47: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 48: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 49: (2.791275816514626, 0.021009994005131174, 0.01436871588230132), 50: (2.899181632937714, 0.01761883543492855, 0.018244919180870012), 51: (2.2642454795287064, 0.049829620819470316, 0.011322516202926625), 52: (2.2631511194968574, 0.04991883509184121, 0.014994788169860884), 53: (2.1331777822232794, 0.06170192204766262, 0.01296908259391788), 54: (1.9738854362225988, 0.07984147514886539, 0.012726208567619357), 55: (3.2270638975436823, 0.010371318905303612, 0.01232927441596987), 56: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 57: (2.3059216084842595, 0.04654615390440723, 0.011069506406784002), 58: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 59: (3.1403809797085143, 0.011920711594495271, 0.012796837091445856), 60: (2.4737431050712355, 0.035352498876522205, 0.002737727761268627), 61: (1.6113005706540569, 0.14157378011701163, 0.0019494831562042458), 62: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 63: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 64: (1.53743688384478, 0.1585651949016809, 0.002130213379859913), 65: (0.6035986920680937, 0.5610132145872897, 0.0003473639488220659), 66: (1.0077287109915996, 0.339908305180983, 0.0011471956968307495), 67: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 68: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 69: (1.6021500606216978, 0.14358622667512966, 0.001747226715087924), 70: (1.578855155842922, 0.14882583018562254, 0.001754996180534374), 71: (-0.9331597303937214, 0.3750941961722959, -0.0008138090372085682), 72: (1.5852476471891817, 0.14737120760043948, 0.0018799364566802756), 73: (-0.7271373912647012, 0.4856248944885333, -0.0007396459579467329), 74: (-1.0830423046211022, 0.30695096566719837, -0.0011349618434906006), 75: (0.4343451327383166, 0.6742649308269852, 0.0006856977939605491), 76: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 77: (0.3532057330609686, 0.7320659772780975, 0.0005770146846771351), 78: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 79: (-0.788284034364636, 0.45079352471553624, -0.0010927379131316917), 80: (1.3336937425899686, 0.21507071520896828, 0.0051479965448379406), 81: (1.9807669198305076, 0.07896205495606001, 0.012577366828918468), 82: (0.06972756394308077, 0.9459352163099144, 0.00014835894107817493), 83: (-1.1598508863626962, 0.27595292042177283, -0.003138679265975941), 84: (2.502593835967706, 0.03371801951659803, 0.009901961684226968), 85: (2.4071531852960577, 0.039432667879380455, 0.007238087058067333), 86: (2.502593835967706, 0.03371801951659803, 0.009901961684226968), 87: (2.1891701035114055, 0.05632639574221948, 0.007605543732643161), 88: (2.4071531852960577, 0.039432667879380455, 0.007238087058067333), 89: (2.2839470834087656, 0.04825000877654509, 0.006980195641517639), 90: (2.1001663651565923, 0.06510070417770156, 0.007232803106307972), 91: (2.217126449173077, 0.05381582701278829, 0.006793224811553922), 92: (2.9082126428940716, 0.01736168617648168, 0.005811285972595226), 93: (2.4387499353236053, 0.03744133009823867, 0.007071739435195901), 94: (2.007662306976666, 0.07561267274669725, 0.006354174017906167), 95: (2.4071531852960577, 0.039432667879380455, 0.007238087058067333), 96: (2.4071531852960577, 0.039432667879380455, 0.007238087058067333), 97: (1.634712174810816, 0.13654009651084903, 0.005153477191925049), 98: (2.630136944145678, 0.02735251082667631, 0.008446931838989258), 99: (1.9107332969923785, 0.0883578431583259, 0.0041037440299988015), 100: (2.290628562098974, 0.047725556273620136, 0.004274970293045022), 101: (2.0933821116848716, 0.06582125831616147, 0.006068557500839233), 102: (1.627603621013519, 0.13805114116980974, 0.0039044678211211936), 103: (2.585950424413753, 0.02940816869870594, 0.008170664310455322), 104: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 105: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 106: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 107: (0.7945155788504622, 0.4473388366512432, 0.0008462429046631192), 108: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 109: (0.9942624898356804, 0.34607322156604914, 0.0014589965343475009), 110: (1.455289556974256, 0.17956424097009338, 0.0012722373008727805), 111: (-0.40311714339013993, 0.6962705053346807, -0.00046482086181637294), 112: (-1.3995010159139174, 0.19517324349859388, -0.0016595661640167458), 113: (0.31736921700451814, 0.7582025900197144, 0.0004179954528809038), 114: (0.48352736126717916, 0.6402678088763173, 0.0006562054157256969), 115: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 116: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 117: (-0.2685125850444927, 0.794358177071614, -0.0003172755241394043), 118: (0.17690180491132731, 0.8635025141437851, 0.0002203315496444591), 119: (0.10644048277647157, 0.9175676736078939, 0.0001502275466918057), 120: (-1.1374421926809106, 0.2847282753703033, -0.0014120459556580256), 121: (-0.26892403073778803, 0.7940513734382388, -0.00041577816009519264), 122: (-0.7879673008075888, 0.4509695902220746, -0.000934952497482322), 123: (0.7339417108574199, 0.48166582547252856, 0.001118680834770236), 124: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 125: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 126: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 127: (1.3815475677348008, 0.20043898665844112, 0.008650729060173057), 128: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 129: (1.7839885104955835, 0.10809309766930562, 0.012804698944091808), 130: (1.993676236811333, 0.07733712993116255, 0.010171923041343678), 131: (1.3413500042767823, 0.2126705993332483, 0.009033668041229292), 132: (2.9252832409934597, 0.01688605966307884, 0.01588654816150664), 133: (0.8375860459376804, 0.42394687359118666, 0.006955501437187206), 134: (2.509183986679589, 0.033355385995736835, 0.014794331789016735), 135: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 136: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 137: (2.432500896362837, 0.03782707395283051, 0.015213516354560863), 138: (1.9800145535322926, 0.07905775295108508, 0.014388802647590682), 139: (2.494999426821794, 0.03414080250541585, 0.009348744153976463), 140: (2.19400933262387, 0.055883794369236614, 0.009948676824569791), 141: (2.526709734292557, 0.03240986873880421, 0.01006338596343992), 142: (2.5065990466500887, 0.03349715943896789, 0.009605455398559615), 143: (1.9558113216511723, 0.0821958264018493, 0.0078074634075164795)}, 'original_noun_phrase': {0: (17.928403897297034, 4.934233780617678e-65, 0.006354058471818802)}}\n",
            "V dann named\n",
            "named {0: (-0.9867294195570829, 0.33617516260200075, -0.0019272759556770436), 1: (-1.2452428997465435, 0.22818190293584237, -0.0020560294389724842), 2: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 3: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 4: (-1.3566084641525438, 0.19080716428991337, -0.0022081181406974904), 5: (-0.5066744948466355, 0.6182126598611556, -0.000635613501071941), 6: (-0.9807570537436318, 0.33903683077187907, -0.0012205190956592615), 7: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 8: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 9: (0.38819756753275503, 0.7021851295065851, 0.0004766911268234142), 10: (-1.1755225931788928, 0.2543065174989379, -0.0024114027619361877), 11: (-0.9917942822648065, 0.3337615197075454, -0.0015550673007965199), 12: (0.2774077479936443, 0.7844628678651137, 0.00034148842096326515), 13: (-0.8339300993217477, 0.41468221042413356, -0.0013277605175971985), 14: (-1.5202488930803648, 0.14491564372650456, -0.004161497950553872), 15: (1.6972759586236632, 0.10596498853229763, 0.003535556793212935), 16: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 17: (1.0016507492575955, 0.32909914609836277, 0.0016669109463691711), 18: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 19: (1.6277437842392213, 0.12004876977937895, 0.0032092809677123357), 20: (-1.5009283623118574, 0.1498099651304123, -0.006498688459396329), 21: (-1.6109635955680688, 0.12367578045278915, -0.0068493291735649), 22: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 23: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 24: (-1.2891871810569517, 0.2128078300899039, -0.003938792645931222), 25: (-0.999616474258732, 0.33005766075827636, -0.0026086762547492537), 26: (-1.3924457849298786, 0.17987067769141993, -0.002627967298030859), 27: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 28: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 29: (-0.636680581148872, 0.5319294488758434, -0.001921905577182803), 30: (-1.0990154391398026, 0.2854928455095363, -0.0049043744802474976), 31: (-0.8290505966473767, 0.41736989975170846, -0.0022954314947128407), 32: (-1.0159921359762007, 0.3223970657175921, -0.003628966212272655), 33: (-1.2450212130949883, 0.22826157586858026, -0.005895775556564287), 34: (-1.5628644896088133, 0.13458726283948527, -0.00883282124996182), 35: (-0.2775163543425045, 0.7843807340327269, -0.0007456392049789429), 36: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 37: (-0.9951581226427932, 0.33216518239122017, -0.0030926048755645086), 38: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 39: (-0.5049933294009642, 0.6193707536363772, -0.0014558494091033714), 40: (-1.234195357923345, 0.23217842040776085, -0.0030386224389076233), 41: (-0.8005745624243296, 0.4332750978182609, -0.0009572267532348411), 42: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 43: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 44: (-0.8849455143984167, 0.38724837754189967, -0.0010334163904189841), 45: (-0.9632115368751197, 0.34754123658872504, -0.0013620018959045743), 46: (-0.844525009189141, 0.4088845819892061, -0.0009750984609127045), 47: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 48: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 49: (1.712832759416862, 0.10301499218695129, 0.0028555780649184848), 50: (-0.7001632511296481, 0.49231043837757804, -0.001612403988838218), 51: (0.6411093264132413, 0.5291097990458402, 0.0008345276117325273), 52: (1.1293342529446195, 0.2728139954959521, 0.0013565793633460999), 53: (-0.7914214552349883, 0.43846704390652413, -0.0017496749758720287), 54: (-1.79079785815554, 0.08927231509681495, -0.007404179871082328), 55: (0.7673307186972625, 0.45231561228052297, 0.001419311761856057), 56: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 57: (-0.4790246383966941, 0.6373879263513851, -0.0009406745433807595), 58: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 59: (1.5413282816567015, 0.13972747015120882, 0.003300172090530351), 60: (1.4461796521472785, 0.16442213427483043, 0.002373544871807076), 61: (-0.2265780880045984, 0.8231716551620237, -0.00032638758420944214), 62: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 63: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 64: (-0.31440272416523185, 0.7566414288566066, -0.0005337491631507985), 65: (0.2993878914671035, 0.7678944669361603, 0.00044022351503375523), 66: (0.8265976246560939, 0.41872520800562596, 0.0012576088309287914), 67: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 68: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 69: (0.6203414116234746, 0.5424028527539361, 0.000960740447044417), 70: (1.1831346456523935, 0.25134904624556187, 0.0019820079207419905), 71: (-0.12513206171331423, 0.9017335024184183, -0.0002091348171234242), 72: (-0.32320675290749284, 0.7500688061850564, -0.0005764201283455228), 73: (-0.193046127530431, 0.8489714599354892, -0.0003615766763687134), 74: (1.032006781588347, 0.3150274090199826, 0.0016774669289589261), 75: (-0.6984208369015937, 0.49337462560092404, -0.0011544555425644365), 76: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 77: (-1.0163627556385761, 0.3222251498500238, -0.0023468554019928645), 78: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 79: (-1.0705322236151504, 0.29779124771528703, -0.0024858862161636353), 80: (-1.1413412513774956, 0.26790949133339526, -0.002657562494277954), 81: (-0.09323127588617472, 0.9266959202957807, -0.00019965767860408157), 82: (-0.7831413718382906, 0.44319692478841277, -0.0005579873919487333), 83: (-0.7579305183526267, 0.45779103790086184, -0.0009718358516693226), 84: (-1.207548910324389, 0.2420384788511308, -0.001984208822250366), 85: (-1.099413381128593, 0.285323687638757, -0.0022287189960479847), 86: (-1.207548910324389, 0.2420384788511308, -0.001984208822250366), 87: (-1.3110906270483678, 0.2054524906583894, -0.003223280608654011), 88: (-1.099413381128593, 0.285323687638757, -0.0022287189960479847), 89: (-0.9996574191640942, 0.3300383489980435, -0.0009561315178870933), 90: (-1.0516519210829798, 0.3061514750470006, -0.003016935288906064), 91: (-0.9291803564474265, 0.3644510598083106, -0.00220405012369157), 92: (-1.175401010869867, 0.25435396664616183, -0.0025725707411766163), 93: (-0.06055605743868714, 0.9523452087615489, -5.860477685926124e-05), 94: (-0.8225972785487173, 0.42094146169540103, -0.0009408704936504364), 95: (-1.099413381128593, 0.285323687638757, -0.0022287189960479847), 96: (-1.099413381128593, 0.285323687638757, -0.0022287189960479847), 97: (-1.0611938437803885, 0.30190549491190133, -0.0031025767326355314), 98: (-0.9387229241331986, 0.359654215235255, -0.002385479211807262), 99: (-1.1336384223047353, 0.2710482898355101, -0.0032224059104919434), 100: (-1.2423371530833762, 0.2292279110528442, -0.0028264492750167403), 101: (-1.1430441857127842, 0.267219224605781, -0.002621537446975797), 102: (-1.326179076988338, 0.2005028178374613, -0.003980955481529214), 103: (-1.0804694565232806, 0.29345778095464037, -0.0030408114194869773), 104: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 105: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 106: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 107: (1.1370978192396186, 0.2696352794586459, 0.0008532896637916676), 108: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 109: (0.9661359028032891, 0.34611368012936705, 0.0007006019353866799), 110: (0.5873272559320057, 0.5638991515795342, 0.0004150778055190818), 111: (-0.8768927671793506, 0.3914976991308914, -0.0010815337300300598), 112: (-0.492585715107945, 0.6279493442868852, -0.0005521222949028126), 113: (-0.8876679884782366, 0.38581866399279774, -0.001028694212436676), 114: (1.9919857371898635, 0.0609425756947396, 0.0017527014017104658), 115: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 116: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 117: (-0.17356616266441877, 0.8640422455233715, -0.000146953761577584), 118: (0.05262973954484554, 0.9585763353239114, 4.5648217201210706e-05), 119: (-0.7937550885372135, 0.4371396708869957, -0.0008041948080061978), 120: (-0.8291497433376069, 0.4173151783385879, -0.0008700907230377863), 121: (-0.8888635595557307, 0.38519190917442914, -0.0010397553443908247), 122: (-0.8103474787237596, 0.4277741836374571, -0.0008706778287886907), 123: (-0.4076069663627391, 0.6881183421446317, -0.00041303932666780785), 124: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 125: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 126: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 127: (-1.6980236247314056, 0.10582157209362963, -0.00728069692850114), 128: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 129: (-1.6014726497406484, 0.1257678426921422, -0.0063585817813873735), 130: (-1.389156437285621, 0.18085306802344098, -0.006095549464225802), 131: (-1.203391578778529, 0.2436051407849732, -0.004777027666568789), 132: (-0.7429261706312886, 0.4666135784705717, -0.0031158849596977234), 133: (-1.4174551873754109, 0.17254091080034387, -0.007822236418724038), 134: (-0.4066098503932951, 0.6888382131061397, -0.0015775725245475991), 135: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 136: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 137: (-1.0550217584710877, 0.30464709845296795, -0.003987997770309448), 138: (-1.216419014520918, 0.23872146186183196, -0.005159489810466766), 139: (-0.6273524459348028, 0.537895201199105, -0.002090153098106451), 140: (-1.2759050472617128, 0.21736716485991053, -0.004835143685340881), 141: (-1.0370987066099429, 0.3127094386870817, -0.004217070341110185), 142: (-0.929467327108055, 0.3643061783437216, -0.0036227077245712946), 143: (-0.7362115149546515, 0.47059460455096525, -0.0027908772230148537)}\n",
            "V dann noun_phrase\n",
            "noun_phrase {0: (0.32176681478719044, 0.7549769578469689, 0.0012385070323944203), 1: (1.4766226480042148, 0.17388890034183893, 0.003506255149841264), 2: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 3: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 4: (0.918911793265633, 0.38210910874179593, 0.0020947158336639404), 5: (3.0882193176317525, 0.012966697975766992, 0.011990487575531006), 6: (2.5676534241826596, 0.03030423078546963, 0.008141294121742249), 7: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 8: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 9: (1.547013422172735, 0.15626424608948053, 0.005729144811630227), 10: (2.4550281157408893, 0.036454779706908066, 0.007289832830429055), 11: (1.2572070136387585, 0.2403228820471545, 0.002852046489715543), 12: (1.9338228423299695, 0.08514892994019035, 0.0050469756126403365), 13: (1.2390259791919658, 0.24667429300686539, 0.0036864370107650424), 14: (0.48488703356022395, 0.6393399309607386, 0.0018735915422439797), 15: (3.0180731625373034, 0.014524585908708705, 0.01241346597671511), 16: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 17: (2.2035700060731642, 0.05501930296620119, 0.008129897713661238), 18: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 19: (4.048596889831625, 0.002891013585731314, 0.012764960527420044), 20: (1.8079507621380635, 0.1040721817494618, 0.01314978301525116), 21: (2.081269316041891, 0.0671269195079296, 0.013036927580833413), 22: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 23: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 24: (1.3841530851981612, 0.19966731786475816, 0.009304749965667769), 25: (2.3562657772333533, 0.042862819899520725, 0.019379985332489025), 26: (1.8930350680862085, 0.09089440506378557, 0.012767596542835213), 27: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 28: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 29: (2.0354686831459485, 0.07229272014087024, 0.013244447112083446), 30: (2.1598565954741553, 0.059081257858952, 0.015896314382553067), 31: (1.6534747802033491, 0.13262338073074043, 0.01141127645969392), 32: (1.6880543973538569, 0.12567104980516453, 0.012637373805046093), 33: (1.8361282815985915, 0.09952210292351608, 0.012894773483276334), 34: (1.3368283216280035, 0.21408532009435072, 0.011225506663322449), 35: (2.609624460078373, 0.028288172896220688, 0.013556665182113625), 36: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 37: (1.91107212434062, 0.08830993899457726, 0.011751183867454551), 38: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 39: (2.148512238971029, 0.06018215146050553, 0.01318176388740544), 40: (2.0638939744849827, 0.06904346740782925, 0.015153759717941251), 41: (2.218225015959506, 0.05371943471530602, 0.01549370288848878), 42: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 43: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 44: (1.9175249664254657, 0.08740228790429246, 0.012787795066833474), 45: (2.826005784456006, 0.01985141251957473, 0.022183382511138938), 46: (2.400549429629872, 0.03986196143203122, 0.015419840812683105), 47: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 48: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 49: (2.505639567322363, 0.033549936127881964, 0.01723344624042511), 50: (2.986644043743537, 0.015283907001565568, 0.01938931047916409), 51: (2.1677734204354304, 0.05832459243502238, 0.013565436005592346), 52: (2.0865272636900545, 0.06655711860338713, 0.014177829027175903), 53: (2.297315725716465, 0.04720626843717642, 0.01569410562515261), 54: (1.9330067188475204, 0.08526044851507485, 0.014338076114654541), 55: (3.033151673430603, 0.01417417142098351, 0.015086108446121238), 56: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 57: (2.593551740533509, 0.029043780052692654, 0.014308959245681763), 58: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 59: (2.8422391488625958, 0.019332403640807263, 0.014718294143676758), 60: (2.3478558177617104, 0.04345746117319644, 0.0067915409803390725), 61: (1.534866195829691, 0.15918794928573535, 0.004163616895675626), 62: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 63: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 64: (0.9611441817827515, 0.3615892674135417, 0.0029742836952209584), 65: (0.44765017690051645, 0.6649861385672213, 0.0012340337038039828), 66: (0.3762017454388502, 0.7154820324859454, 0.0012960508465766907), 67: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 68: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 69: (1.1967354350238133, 0.2619817086495285, 0.003531971573829673), 70: (1.0992289864297298, 0.30020134809395893, 0.0034032225608826017), 71: (0.1587061438478116, 0.8774045689061314, 0.0005131065845489502), 72: (0.42648493986091657, 0.6797741371312713, 0.0011184543371201006), 73: (0.2655708572575739, 0.7965528273142781, 0.0005496859550476185), 74: (0.5492131345664989, 0.5962205939672494, 0.0016154468059539906), 75: (0.9987362359534044, 0.3440158964787752, 0.0021351695060729536), 76: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 77: (0.8569000352002448, 0.41373418445484245, 0.0017239898443222046), 78: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 79: (0.3522973172050324, 0.7327241932984645, 0.0006840765476227029), 80: (2.0212493136417597, 0.07397262899774995, 0.01194812059402467), 81: (1.9236852369726192, 0.08654401633515652, 0.016407230496406577), 82: (-0.47962578956006724, 0.6429340136693806, -0.0010462343692779763), 83: (-0.4520202174282119, 0.6619515180399729, -0.0013109803199767844), 84: (2.009315991639913, 0.07541122018983434, 0.009284698963165305), 85: (1.5616612390054907, 0.15280223525836137, 0.007558640837669384), 86: (2.009315991639913, 0.07541122018983434, 0.009284698963165305), 87: (2.3551862846862446, 0.04293869528060917, 0.01398688554763794), 88: (1.5616612390054907, 0.15280223525836137, 0.007558640837669384), 89: (3.543341051175119, 0.006281111838804635, 0.009105792641639743), 90: (2.2544266917526525, 0.050635688571756834, 0.012923109531402577), 91: (2.0509966809881472, 0.07049997744201736, 0.012570643424987815), 92: (2.3482289288575813, 0.04343090776926426, 0.010921820998191833), 93: (2.525897669445957, 0.0324530813336942, 0.013159114122390791), 94: (3.6530274871827073, 0.005292558199144063, 0.009726472198963165), 95: (1.5616612390054907, 0.15280223525836137, 0.007558640837669384), 96: (1.5616612390054907, 0.15280223525836137, 0.007558640837669384), 97: (2.2342428055361436, 0.052332986349772126, 0.011621281504631042), 98: (2.3261772832574, 0.04502809643828023, 0.011416107416152954), 99: (1.0855901183385217, 0.3058808007821362, 0.006092655658721857), 100: (0.8517691553364071, 0.4164304708552665, 0.0038651406764984575), 101: (0.9305035802434644, 0.37639482087757603, 0.005082440376281805), 102: (0.42063581835907643, 0.6838868765081995, 0.002441000938415594), 103: (1.2413875033361923, 0.24584158719440594, 0.007783329486847013), 104: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 105: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 106: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 107: (1.1461810666772596, 0.28128004427667713, 0.0015783518552780484), 108: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 109: (0.8042179440478184, 0.4419952411606892, 0.0012110233306885099), 110: (2.0171298896813723, 0.07446624454084619, 0.0024679958820342796), 111: (-0.1809439041613578, 0.8604207660427458, -0.0002918362617492787), 112: (-0.03487466536290386, 0.972940919216302, -4.5865774154663086e-05), 113: (0.020356245105158034, 0.9842033447427685, 3.411471843722813e-05), 114: (1.9362754442796217, 0.08481462632865278, 0.0023768365383148304), 115: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 116: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 117: (0.656416529562219, 0.5279806554409376, 0.0010124027729034313), 118: (-0.06917983691502126, 0.946359153392213, -0.0001084446907043124), 119: (-0.6111665245114267, 0.5562088264271956, -0.00088534355163572), 120: (-0.29190712472856417, 0.7769744306615747, -0.0003355681896209939), 121: (-1.2099638399245203, 0.2571126327336307, -0.0020089626312256303), 122: (-1.2187103075203463, 0.25393388037841697, -0.0016553401947021484), 123: (-0.11224944773483804, 0.9130893441532238, -0.0001426100730895774), 124: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 125: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 126: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 127: (0.8188097810674024, 0.43403984418158514, 0.008369690179824807), 128: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 129: (1.344219612684229, 0.21177688246528342, 0.014003562927246083), 130: (1.2182077609202966, 0.25411565011020515, 0.010748335719108604), 131: (1.4284011620972987, 0.18694678849210875, 0.014729905128478993), 132: (2.448221417005585, 0.03686411709070902, 0.02071168422698977), 133: (1.307978779444202, 0.22329990056242666, 0.014133965969085671), 134: (1.8508231489365952, 0.0972235339920648, 0.016493734717369035), 135: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 136: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 137: (1.9938421708074958, 0.07731645237533531, 0.018206831812858537), 138: (1.3997402905893874, 0.19510387413129615, 0.01494083404541019), 139: (1.6489407277297508, 0.13356042651209937, 0.00880095362663269), 140: (1.617289305446289, 0.1402704745815911, 0.009810954332351685), 141: (2.1037029662360927, 0.06472810658485369, 0.012146306037902899), 142: (2.1458646654182423, 0.06044193061800628, 0.01131196022033687), 143: (2.2606987112332058, 0.05011932905829078, 0.016005074977874734)}\n",
            "V dann original_noun_phrase\n",
            "original_noun_phrase {0: (17.549553772156735, 1.2464257307518868e-62, 0.008245057353956864)}\n",
            "dann original_noun_phrase {'named': {0: (-0.9867294195570829, 0.33617516260200075, -0.0019272759556770436), 1: (-1.2452428997465435, 0.22818190293584237, -0.0020560294389724842), 2: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 3: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 4: (-1.3566084641525438, 0.19080716428991337, -0.0022081181406974904), 5: (-0.5066744948466355, 0.6182126598611556, -0.000635613501071941), 6: (-0.9807570537436318, 0.33903683077187907, -0.0012205190956592615), 7: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 8: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 9: (0.38819756753275503, 0.7021851295065851, 0.0004766911268234142), 10: (-1.1755225931788928, 0.2543065174989379, -0.0024114027619361877), 11: (-0.9917942822648065, 0.3337615197075454, -0.0015550673007965199), 12: (0.2774077479936443, 0.7844628678651137, 0.00034148842096326515), 13: (-0.8339300993217477, 0.41468221042413356, -0.0013277605175971985), 14: (-1.5202488930803648, 0.14491564372650456, -0.004161497950553872), 15: (1.6972759586236632, 0.10596498853229763, 0.003535556793212935), 16: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 17: (1.0016507492575955, 0.32909914609836277, 0.0016669109463691711), 18: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 19: (1.6277437842392213, 0.12004876977937895, 0.0032092809677123357), 20: (-1.5009283623118574, 0.1498099651304123, -0.006498688459396329), 21: (-1.6109635955680688, 0.12367578045278915, -0.0068493291735649), 22: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 23: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 24: (-1.2891871810569517, 0.2128078300899039, -0.003938792645931222), 25: (-0.999616474258732, 0.33005766075827636, -0.0026086762547492537), 26: (-1.3924457849298786, 0.17987067769141993, -0.002627967298030859), 27: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 28: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 29: (-0.636680581148872, 0.5319294488758434, -0.001921905577182803), 30: (-1.0990154391398026, 0.2854928455095363, -0.0049043744802474976), 31: (-0.8290505966473767, 0.41736989975170846, -0.0022954314947128407), 32: (-1.0159921359762007, 0.3223970657175921, -0.003628966212272655), 33: (-1.2450212130949883, 0.22826157586858026, -0.005895775556564287), 34: (-1.5628644896088133, 0.13458726283948527, -0.00883282124996182), 35: (-0.2775163543425045, 0.7843807340327269, -0.0007456392049789429), 36: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 37: (-0.9951581226427932, 0.33216518239122017, -0.0030926048755645086), 38: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 39: (-0.5049933294009642, 0.6193707536363772, -0.0014558494091033714), 40: (-1.234195357923345, 0.23217842040776085, -0.0030386224389076233), 41: (-0.8005745624243296, 0.4332750978182609, -0.0009572267532348411), 42: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 43: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 44: (-0.8849455143984167, 0.38724837754189967, -0.0010334163904189841), 45: (-0.9632115368751197, 0.34754123658872504, -0.0013620018959045743), 46: (-0.844525009189141, 0.4088845819892061, -0.0009750984609127045), 47: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 48: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 49: (1.712832759416862, 0.10301499218695129, 0.0028555780649184848), 50: (-0.7001632511296481, 0.49231043837757804, -0.001612403988838218), 51: (0.6411093264132413, 0.5291097990458402, 0.0008345276117325273), 52: (1.1293342529446195, 0.2728139954959521, 0.0013565793633460999), 53: (-0.7914214552349883, 0.43846704390652413, -0.0017496749758720287), 54: (-1.79079785815554, 0.08927231509681495, -0.007404179871082328), 55: (0.7673307186972625, 0.45231561228052297, 0.001419311761856057), 56: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 57: (-0.4790246383966941, 0.6373879263513851, -0.0009406745433807595), 58: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 59: (1.5413282816567015, 0.13972747015120882, 0.003300172090530351), 60: (1.4461796521472785, 0.16442213427483043, 0.002373544871807076), 61: (-0.2265780880045984, 0.8231716551620237, -0.00032638758420944214), 62: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 63: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 64: (-0.31440272416523185, 0.7566414288566066, -0.0005337491631507985), 65: (0.2993878914671035, 0.7678944669361603, 0.00044022351503375523), 66: (0.8265976246560939, 0.41872520800562596, 0.0012576088309287914), 67: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 68: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 69: (0.6203414116234746, 0.5424028527539361, 0.000960740447044417), 70: (1.1831346456523935, 0.25134904624556187, 0.0019820079207419905), 71: (-0.12513206171331423, 0.9017335024184183, -0.0002091348171234242), 72: (-0.32320675290749284, 0.7500688061850564, -0.0005764201283455228), 73: (-0.193046127530431, 0.8489714599354892, -0.0003615766763687134), 74: (1.032006781588347, 0.3150274090199826, 0.0016774669289589261), 75: (-0.6984208369015937, 0.49337462560092404, -0.0011544555425644365), 76: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 77: (-1.0163627556385761, 0.3222251498500238, -0.0023468554019928645), 78: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 79: (-1.0705322236151504, 0.29779124771528703, -0.0024858862161636353), 80: (-1.1413412513774956, 0.26790949133339526, -0.002657562494277954), 81: (-0.09323127588617472, 0.9266959202957807, -0.00019965767860408157), 82: (-0.7831413718382906, 0.44319692478841277, -0.0005579873919487333), 83: (-0.7579305183526267, 0.45779103790086184, -0.0009718358516693226), 84: (-1.207548910324389, 0.2420384788511308, -0.001984208822250366), 85: (-1.099413381128593, 0.285323687638757, -0.0022287189960479847), 86: (-1.207548910324389, 0.2420384788511308, -0.001984208822250366), 87: (-1.3110906270483678, 0.2054524906583894, -0.003223280608654011), 88: (-1.099413381128593, 0.285323687638757, -0.0022287189960479847), 89: (-0.9996574191640942, 0.3300383489980435, -0.0009561315178870933), 90: (-1.0516519210829798, 0.3061514750470006, -0.003016935288906064), 91: (-0.9291803564474265, 0.3644510598083106, -0.00220405012369157), 92: (-1.175401010869867, 0.25435396664616183, -0.0025725707411766163), 93: (-0.06055605743868714, 0.9523452087615489, -5.860477685926124e-05), 94: (-0.8225972785487173, 0.42094146169540103, -0.0009408704936504364), 95: (-1.099413381128593, 0.285323687638757, -0.0022287189960479847), 96: (-1.099413381128593, 0.285323687638757, -0.0022287189960479847), 97: (-1.0611938437803885, 0.30190549491190133, -0.0031025767326355314), 98: (-0.9387229241331986, 0.359654215235255, -0.002385479211807262), 99: (-1.1336384223047353, 0.2710482898355101, -0.0032224059104919434), 100: (-1.2423371530833762, 0.2292279110528442, -0.0028264492750167403), 101: (-1.1430441857127842, 0.267219224605781, -0.002621537446975797), 102: (-1.326179076988338, 0.2005028178374613, -0.003980955481529214), 103: (-1.0804694565232806, 0.29345778095464037, -0.0030408114194869773), 104: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 105: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 106: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 107: (1.1370978192396186, 0.2696352794586459, 0.0008532896637916676), 108: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 109: (0.9661359028032891, 0.34611368012936705, 0.0007006019353866799), 110: (0.5873272559320057, 0.5638991515795342, 0.0004150778055190818), 111: (-0.8768927671793506, 0.3914976991308914, -0.0010815337300300598), 112: (-0.492585715107945, 0.6279493442868852, -0.0005521222949028126), 113: (-0.8876679884782366, 0.38581866399279774, -0.001028694212436676), 114: (1.9919857371898635, 0.0609425756947396, 0.0017527014017104658), 115: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 116: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 117: (-0.17356616266441877, 0.8640422455233715, -0.000146953761577584), 118: (0.05262973954484554, 0.9585763353239114, 4.5648217201210706e-05), 119: (-0.7937550885372135, 0.4371396708869957, -0.0008041948080061978), 120: (-0.8291497433376069, 0.4173151783385879, -0.0008700907230377863), 121: (-0.8888635595557307, 0.38519190917442914, -0.0010397553443908247), 122: (-0.8103474787237596, 0.4277741836374571, -0.0008706778287886907), 123: (-0.4076069663627391, 0.6881183421446317, -0.00041303932666780785), 124: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 125: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 126: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 127: (-1.6980236247314056, 0.10582157209362963, -0.00728069692850114), 128: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 129: (-1.6014726497406484, 0.1257678426921422, -0.0063585817813873735), 130: (-1.389156437285621, 0.18085306802344098, -0.006095549464225802), 131: (-1.203391578778529, 0.2436051407849732, -0.004777027666568789), 132: (-0.7429261706312886, 0.4666135784705717, -0.0031158849596977234), 133: (-1.4174551873754109, 0.17254091080034387, -0.007822236418724038), 134: (-0.4066098503932951, 0.6888382131061397, -0.0015775725245475991), 135: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 136: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 137: (-1.0550217584710877, 0.30464709845296795, -0.003987997770309448), 138: (-1.216419014520918, 0.23872146186183196, -0.005159489810466766), 139: (-0.6273524459348028, 0.537895201199105, -0.002090153098106451), 140: (-1.2759050472617128, 0.21736716485991053, -0.004835143685340881), 141: (-1.0370987066099429, 0.3127094386870817, -0.004217070341110185), 142: (-0.929467327108055, 0.3643061783437216, -0.0036227077245712946), 143: (-0.7362115149546515, 0.47059460455096525, -0.0027908772230148537)}, 'noun_phrase': {0: (0.32176681478719044, 0.7549769578469689, 0.0012385070323944203), 1: (1.4766226480042148, 0.17388890034183893, 0.003506255149841264), 2: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 3: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 4: (0.918911793265633, 0.38210910874179593, 0.0020947158336639404), 5: (3.0882193176317525, 0.012966697975766992, 0.011990487575531006), 6: (2.5676534241826596, 0.03030423078546963, 0.008141294121742249), 7: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 8: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 9: (1.547013422172735, 0.15626424608948053, 0.005729144811630227), 10: (2.4550281157408893, 0.036454779706908066, 0.007289832830429055), 11: (1.2572070136387585, 0.2403228820471545, 0.002852046489715543), 12: (1.9338228423299695, 0.08514892994019035, 0.0050469756126403365), 13: (1.2390259791919658, 0.24667429300686539, 0.0036864370107650424), 14: (0.48488703356022395, 0.6393399309607386, 0.0018735915422439797), 15: (3.0180731625373034, 0.014524585908708705, 0.01241346597671511), 16: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 17: (2.2035700060731642, 0.05501930296620119, 0.008129897713661238), 18: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 19: (4.048596889831625, 0.002891013585731314, 0.012764960527420044), 20: (1.8079507621380635, 0.1040721817494618, 0.01314978301525116), 21: (2.081269316041891, 0.0671269195079296, 0.013036927580833413), 22: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 23: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 24: (1.3841530851981612, 0.19966731786475816, 0.009304749965667769), 25: (2.3562657772333533, 0.042862819899520725, 0.019379985332489025), 26: (1.8930350680862085, 0.09089440506378557, 0.012767596542835213), 27: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 28: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 29: (2.0354686831459485, 0.07229272014087024, 0.013244447112083446), 30: (2.1598565954741553, 0.059081257858952, 0.015896314382553067), 31: (1.6534747802033491, 0.13262338073074043, 0.01141127645969392), 32: (1.6880543973538569, 0.12567104980516453, 0.012637373805046093), 33: (1.8361282815985915, 0.09952210292351608, 0.012894773483276334), 34: (1.3368283216280035, 0.21408532009435072, 0.011225506663322449), 35: (2.609624460078373, 0.028288172896220688, 0.013556665182113625), 36: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 37: (1.91107212434062, 0.08830993899457726, 0.011751183867454551), 38: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 39: (2.148512238971029, 0.06018215146050553, 0.01318176388740544), 40: (2.0638939744849827, 0.06904346740782925, 0.015153759717941251), 41: (2.218225015959506, 0.05371943471530602, 0.01549370288848878), 42: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 43: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 44: (1.9175249664254657, 0.08740228790429246, 0.012787795066833474), 45: (2.826005784456006, 0.01985141251957473, 0.022183382511138938), 46: (2.400549429629872, 0.03986196143203122, 0.015419840812683105), 47: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 48: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 49: (2.505639567322363, 0.033549936127881964, 0.01723344624042511), 50: (2.986644043743537, 0.015283907001565568, 0.01938931047916409), 51: (2.1677734204354304, 0.05832459243502238, 0.013565436005592346), 52: (2.0865272636900545, 0.06655711860338713, 0.014177829027175903), 53: (2.297315725716465, 0.04720626843717642, 0.01569410562515261), 54: (1.9330067188475204, 0.08526044851507485, 0.014338076114654541), 55: (3.033151673430603, 0.01417417142098351, 0.015086108446121238), 56: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 57: (2.593551740533509, 0.029043780052692654, 0.014308959245681763), 58: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 59: (2.8422391488625958, 0.019332403640807263, 0.014718294143676758), 60: (2.3478558177617104, 0.04345746117319644, 0.0067915409803390725), 61: (1.534866195829691, 0.15918794928573535, 0.004163616895675626), 62: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 63: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 64: (0.9611441817827515, 0.3615892674135417, 0.0029742836952209584), 65: (0.44765017690051645, 0.6649861385672213, 0.0012340337038039828), 66: (0.3762017454388502, 0.7154820324859454, 0.0012960508465766907), 67: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 68: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 69: (1.1967354350238133, 0.2619817086495285, 0.003531971573829673), 70: (1.0992289864297298, 0.30020134809395893, 0.0034032225608826017), 71: (0.1587061438478116, 0.8774045689061314, 0.0005131065845489502), 72: (0.42648493986091657, 0.6797741371312713, 0.0011184543371201006), 73: (0.2655708572575739, 0.7965528273142781, 0.0005496859550476185), 74: (0.5492131345664989, 0.5962205939672494, 0.0016154468059539906), 75: (0.9987362359534044, 0.3440158964787752, 0.0021351695060729536), 76: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 77: (0.8569000352002448, 0.41373418445484245, 0.0017239898443222046), 78: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 79: (0.3522973172050324, 0.7327241932984645, 0.0006840765476227029), 80: (2.0212493136417597, 0.07397262899774995, 0.01194812059402467), 81: (1.9236852369726192, 0.08654401633515652, 0.016407230496406577), 82: (-0.47962578956006724, 0.6429340136693806, -0.0010462343692779763), 83: (-0.4520202174282119, 0.6619515180399729, -0.0013109803199767844), 84: (2.009315991639913, 0.07541122018983434, 0.009284698963165305), 85: (1.5616612390054907, 0.15280223525836137, 0.007558640837669384), 86: (2.009315991639913, 0.07541122018983434, 0.009284698963165305), 87: (2.3551862846862446, 0.04293869528060917, 0.01398688554763794), 88: (1.5616612390054907, 0.15280223525836137, 0.007558640837669384), 89: (3.543341051175119, 0.006281111838804635, 0.009105792641639743), 90: (2.2544266917526525, 0.050635688571756834, 0.012923109531402577), 91: (2.0509966809881472, 0.07049997744201736, 0.012570643424987815), 92: (2.3482289288575813, 0.04343090776926426, 0.010921820998191833), 93: (2.525897669445957, 0.0324530813336942, 0.013159114122390791), 94: (3.6530274871827073, 0.005292558199144063, 0.009726472198963165), 95: (1.5616612390054907, 0.15280223525836137, 0.007558640837669384), 96: (1.5616612390054907, 0.15280223525836137, 0.007558640837669384), 97: (2.2342428055361436, 0.052332986349772126, 0.011621281504631042), 98: (2.3261772832574, 0.04502809643828023, 0.011416107416152954), 99: (1.0855901183385217, 0.3058808007821362, 0.006092655658721857), 100: (0.8517691553364071, 0.4164304708552665, 0.0038651406764984575), 101: (0.9305035802434644, 0.37639482087757603, 0.005082440376281805), 102: (0.42063581835907643, 0.6838868765081995, 0.002441000938415594), 103: (1.2413875033361923, 0.24584158719440594, 0.007783329486847013), 104: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 105: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 106: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 107: (1.1461810666772596, 0.28128004427667713, 0.0015783518552780484), 108: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 109: (0.8042179440478184, 0.4419952411606892, 0.0012110233306885099), 110: (2.0171298896813723, 0.07446624454084619, 0.0024679958820342796), 111: (-0.1809439041613578, 0.8604207660427458, -0.0002918362617492787), 112: (-0.03487466536290386, 0.972940919216302, -4.5865774154663086e-05), 113: (0.020356245105158034, 0.9842033447427685, 3.411471843722813e-05), 114: (1.9362754442796217, 0.08481462632865278, 0.0023768365383148304), 115: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 116: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 117: (0.656416529562219, 0.5279806554409376, 0.0010124027729034313), 118: (-0.06917983691502126, 0.946359153392213, -0.0001084446907043124), 119: (-0.6111665245114267, 0.5562088264271956, -0.00088534355163572), 120: (-0.29190712472856417, 0.7769744306615747, -0.0003355681896209939), 121: (-1.2099638399245203, 0.2571126327336307, -0.0020089626312256303), 122: (-1.2187103075203463, 0.25393388037841697, -0.0016553401947021484), 123: (-0.11224944773483804, 0.9130893441532238, -0.0001426100730895774), 124: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 125: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 126: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 127: (0.8188097810674024, 0.43403984418158514, 0.008369690179824807), 128: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 129: (1.344219612684229, 0.21177688246528342, 0.014003562927246083), 130: (1.2182077609202966, 0.25411565011020515, 0.010748335719108604), 131: (1.4284011620972987, 0.18694678849210875, 0.014729905128478993), 132: (2.448221417005585, 0.03686411709070902, 0.02071168422698977), 133: (1.307978779444202, 0.22329990056242666, 0.014133965969085671), 134: (1.8508231489365952, 0.0972235339920648, 0.016493734717369035), 135: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 136: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 137: (1.9938421708074958, 0.07731645237533531, 0.018206831812858537), 138: (1.3997402905893874, 0.19510387413129615, 0.01494083404541019), 139: (1.6489407277297508, 0.13356042651209937, 0.00880095362663269), 140: (1.617289305446289, 0.1402704745815911, 0.009810954332351685), 141: (2.1037029662360927, 0.06472810658485369, 0.012146306037902899), 142: (2.1458646654182423, 0.06044193061800628, 0.01131196022033687), 143: (2.2606987112332058, 0.05011932905829078, 0.016005074977874734)}, 'original_noun_phrase': {0: (17.549553772156735, 1.2464257307518868e-62, 0.008245057353956864)}}\n",
            "V dann original_noun_phrase {'non_dann': {'named': {0: (-0.9655923860156405, 0.34637869716316627, -0.0015480875968932883), 1: (-1.0741254413589432, 0.29621899886218495, -0.0027149647474289385), 2: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 3: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 4: (-1.4234530845842852, 0.1708193823853086, -0.003223337233066559), 5: (-1.5367179668232358, 0.1408488075207118, -0.0025740280747413857), 6: (-1.5653479220113784, 0.13400480155122227, -0.003283877670764934), 7: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 8: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 9: (-0.19233909614082156, 0.8495174542603634, -0.00021315068006516613), 10: (-0.6684984996365081, 0.5118560541591541, -0.0013095587491989136), 11: (-1.2836756202863069, 0.21469064283757888, -0.00285803079605107), 12: (-0.8837081892317356, 0.38789931416530654, -0.001225583255290985), 13: (-0.8763453230689969, 0.3917876841049176, -0.0012641027569770813), 14: (-1.3433628505638238, 0.19498083875532457, -0.0034336537122726107), 15: (0.377900318366655, 0.7096933759595105, 0.0003549039363861528), 16: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 17: (-0.05933765597262203, 0.9533028378635036, -7.38292932510598e-05), 18: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 19: (0.16783953998993503, 0.868483143283497, 0.00023294389247896508), 20: (-1.1688359416468115, 0.25692596352159114, -0.0050141215324401855), 21: (-1.2758445829146297, 0.21738809261491887, -0.0051697060465812905), 22: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 23: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 24: (-1.0716031571730849, 0.29732202084518844, -0.0033075079321860934), 25: (-0.5141001827100227, 0.6131096575601327, -0.0010094165802002064), 26: (-0.5208521245718691, 0.6084871752646555, -0.0013989895582199319), 27: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 28: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 29: (-0.3114520296882886, 0.7588485536702768, -0.0008454144001007191), 30: (-0.9634391729116165, 0.3474299688920349, -0.0041394203901291005), 31: (-0.7690457529567646, 0.45132096835816593, -0.002662914991378773), 32: (-0.24857079295377213, 0.8063597754513214, -0.0007111832499503978), 33: (-1.066340630532658, 0.2996329236992549, -0.00435973554849628), 34: (-1.307672890608816, 0.20658688109408466, -0.00601511597633364), 35: (-0.5262795155351091, 0.6047837023066619, -0.0013959199190139993), 36: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 37: (-0.8581215038175375, 0.4015212353378972, -0.0022276133298874123), 38: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 39: (-0.40565957624116045, 0.6895245499887311, -0.0012224495410918523), 40: (-0.6406568097495143, 0.5293975255966856, -0.001646570861339569), 41: (-0.5853719355747461, 0.5651861053067004, -0.0012907266616821733), 42: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 43: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 44: (-0.9489773932241476, 0.35454747994196123, -0.0018764838576317278), 45: (-1.0683973297232137, 0.29872824067494963, -0.002051642537116982), 46: (-0.8369860284159304, 0.4130046106090671, -0.0017104774713516402), 47: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 48: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 49: (0.5390550742003877, 0.5961094768520061, 0.0008374646306038014), 50: (0.509430426665857, 0.6163164151134919, 0.0011194631457328574), 51: (-0.45904937428945386, 0.6514069685670868, -0.0009944468736648449), 52: (1.1920889842998112, 0.24790333603105016, 0.002108065783977542), 53: (-0.009001506377464103, 0.9929117780184165, -1.8292665481589587e-05), 54: (-0.8938228627007104, 0.38259927561296503, -0.002469956874847412), 55: (-0.7409374164917433, 0.4677905743603378, -0.001771286129951477), 56: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 57: (-0.189613780755316, 0.8516227653999674, -0.0003061845898628124), 58: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 59: (0.08010588384267328, 0.9369907869551097, 0.0001716315746307373), 60: (-0.9435540905679378, 0.357242098328733, -0.0012674659490585216), 61: (-1.2222354524469252, 0.23656526113198942, -0.001689542829990398), 62: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 63: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 64: (-1.3416058181990118, 0.1955398725124688, -0.002584441006183602), 65: (-1.2939101689160104, 0.2112046721344239, -0.0025296270847320335), 66: (-1.4975951364366846, 0.15066797542950505, -0.0037569612264632957), 67: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 68: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 69: (-0.8947327697137233, 0.38212485012500774, -0.0015893116593361234), 70: (-1.0199802863067917, 0.3205505139576873, -0.0019883275032043235), 71: (-1.3887894251705182, 0.180962945560633, -0.003145812451839425), 72: (-1.2717330742018775, 0.21881483186899028, -0.0028291866183280945), 73: (-1.084865105941498, 0.29155555727453236, -0.001851402223110199), 74: (-1.2807612794340442, 0.2156914457213122, -0.0027890652418136597), 75: (-1.4655199689952445, 0.1591328174936716, -0.0026975095272063765), 76: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 77: (-1.4125480364169472, 0.17395977281344674, -0.0030394211411476357), 78: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 79: (-1.4035888265476122, 0.176574526295681, -0.002786573767662115), 80: (-1.9538375680947229, 0.06560263570584778, -0.006239135563373577), 81: (-0.9555164699181337, 0.35131696447438465, -0.0018814221024513467), 82: (-1.4043987162522156, 0.17633686542850158, -0.0023443534970283286), 83: (-1.3046279959727924, 0.2076016532099325, -0.002861945331096627), 84: (-1.246634060229046, 0.2276824161542174, -0.0028518155217170382), 85: (-1.2310797936963471, 0.2333151456783793, -0.002442561089992523), 86: (-1.246634060229046, 0.2276824161542174, -0.0028518155217170382), 87: (-1.4632692623450536, 0.15974111921326092, -0.003566589951515209), 88: (-1.2310797936963471, 0.2333151456783793, -0.002442561089992523), 89: (-1.2807847243339239, 0.21568338013571242, -0.002878469228744518), 90: (-1.2411009999355749, 0.22967401659770825, -0.0032365977764129417), 91: (-1.1329422043775377, 0.27133332550132955, -0.0024403154850006215), 92: (-1.055336190633787, 0.3045069992126154, -0.0014326974749565013), 93: (-0.6382379117121378, 0.5309370059872862, -0.0010516881942749245), 94: (-1.1996396689886066, 0.245025619144954, -0.0029798626899719682), 95: (-1.2310797936963471, 0.2333151456783793, -0.002442561089992523), 96: (-1.2310797936963471, 0.2333151456783793, -0.002442561089992523), 97: (-1.271510855089988, 0.21889215087735586, -0.003068020939826943), 98: (-1.0164371450382368, 0.32219065132596236, -0.002341513335704759), 99: (-0.9881038745787944, 0.3355189738627954, -0.0011626988649369174), 100: (-0.8175833210164493, 0.42372975642343835, -0.0007899612188339011), 101: (-1.150245744457188, 0.26431475506382923, -0.0010504990816117), 102: (-1.2234240597852992, 0.23612647059559277, -0.001219561696052529), 103: (-1.2639236569598344, 0.22154477783200927, -0.002597288787364982), 104: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 105: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 106: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 107: (-1.332367803736338, 0.1985000284689346, -0.001470197737216905), 108: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 109: (-1.3690711449738875, 0.18694535568447485, -0.0014252185821533314), 110: (-1.6968615586433409, 0.10604454993733481, -0.0023965135216712508), 111: (-1.6758607670247108, 0.11014403108217709, -0.002761526405811343), 112: (-1.5574983394136035, 0.13585305366721342, -0.0022291630506515725), 113: (-1.6498443215634835, 0.1154094152679027, -0.0020239174365997425), 114: (-1.325939630449497, 0.2005806263497934, -0.002689258754253354), 115: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 116: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 117: (-1.646052449263405, 0.11619442187733992, -0.002467985451221455), 118: (-1.373653937332699, 0.1855410625486748, -0.0016740038990974426), 119: (-1.5631035035748297, 0.13453111299807083, -0.0018570870161056519), 120: (-1.478995505174084, 0.15553026955592655, -0.0020525068044662254), 121: (-1.6110691092376708, 0.12365268822716842, -0.0019341111183166504), 122: (-1.6883917964470028, 0.10768191643732017, -0.0021598696708680087), 123: (-1.7141499880632753, 0.1027684786821179, -0.0023778527975082397), 124: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 125: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 126: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 127: (-1.4938415223919361, 0.1516390377816124, -0.004572677612304676), 128: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 129: (-1.3129193599736824, 0.20484752227316716, -0.005023352801799774), 130: (-0.8583608420079871, 0.40139239324552134, -0.0018903136253357045), 131: (-1.1694472967345249, 0.2566856322955363, -0.003980498015880574), 132: (-1.1360628993963449, 0.2700574248940383, -0.004126235842704773), 133: (-1.6375335808188212, 0.11797456457409827, -0.008619713783264149), 134: (-1.2194356546819272, 0.23760130541591554, -0.004548071324825265), 135: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 136: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 137: (-1.2420058982862812, 0.22934738959328374, -0.004030098021030437), 138: (-1.482196568669953, 0.15468439566172634, -0.005874595046043374), 139: (-0.7966730898476708, 0.4354834302361442, -0.0019156903028488825), 140: (-1.1543869206064388, 0.2626552548600694, -0.003194358944892839), 141: (-0.8446116343344517, 0.408837395689407, -0.0022171080112457497), 142: (-0.7843797043545297, 0.4424875467617805, -0.00208998024463658), 143: (-1.146384039521738, 0.2658692879092321, -0.003623847663402624)}, 'noun_phrase': {0: (-0.059404197340731345, 0.9539283013239545, -0.00019150078296659157), 1: (0.2827684870284805, 0.7837500846544987, 0.0008828014135360829), 2: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 3: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 4: (-0.16916203883537503, 0.8694101768114813, -0.0005187541246414185), 5: (2.7188632924943, 0.023651784815193116, 0.009782162308692888), 6: (0.9267465809386195, 0.37824006408654753, 0.002643743157386802), 7: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 8: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 9: (0.1553281404548011, 0.8799905200007769, 0.0003853142261505127), 10: (2.042603803188805, 0.0714635916706121, 0.006852361559867848), 11: (0.11978999502582492, 0.9072809340766865, 0.00039168596267702416), 12: (1.7358431873080704, 0.11661028811499242, 0.004445970058441162), 13: (0.8352533038787895, 0.4251919941350234, 0.0024510204792022594), 14: (0.5276772076773238, 0.6104835859831382, 0.0019851565361022616), 15: (2.623989835452748, 0.027629588005256053, 0.00590239763259881), 16: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 17: (0.899885400693973, 0.39162312295973234, 0.0020317703485489003), 18: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 19: (1.8851414600043277, 0.09204771789818098, 0.0039432287216186745), 20: (1.9481795871832923, 0.0832096083036202, 0.009910395741462685), 21: (2.388838197839223, 0.04063470690309999, 0.01304997205734254), 22: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 23: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 24: (1.986108497553953, 0.07828579223734768, 0.01063987910747527), 25: (2.9010552828347635, 0.01756516657000177, 0.019302809238433805), 26: (1.9697079522156795, 0.08037987401938257, 0.01144414544105532), 27: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 28: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 29: (3.006855421113125, 0.014791051196128932, 0.015049532055854797), 30: (2.9458463949005096, 0.016330797875466346, 0.018118998408317544), 31: (2.225783411275994, 0.05306078927680244, 0.011234748363494917), 32: (2.1868453891588238, 0.05654022876027513, 0.014129883050918557), 33: (2.1398547510197097, 0.06103566280519921, 0.012387192249298129), 34: (2.1289182482555518, 0.062130632265630044, 0.012444487214088418), 35: (2.3401750146396885, 0.04400766060431682, 0.010740101337432861), 36: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 37: (1.4027578105564296, 0.19423086702518264, 0.006725168228149392), 38: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 39: (2.715432548796257, 0.023784998621499708, 0.011719131469726474), 40: (1.8455854852216242, 0.09803705351169469, 0.010246938467025735), 41: (2.0597568391954546, 0.06950750176674306, 0.011862045526504494), 42: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 43: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 44: (2.1407480241861574, 0.06094705815844367, 0.013551744818687428), 45: (2.821955705898563, 0.019983100357021288, 0.019610649347305276), 46: (2.0108135877737956, 0.0752292248974375, 0.012152406573295582), 47: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 48: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 49: (2.791275816514626, 0.021009994005131174, 0.01436871588230132), 50: (2.899181632937714, 0.01761883543492855, 0.018244919180870012), 51: (2.2642454795287064, 0.049829620819470316, 0.011322516202926625), 52: (2.2631511194968574, 0.04991883509184121, 0.014994788169860884), 53: (2.1331777822232794, 0.06170192204766262, 0.01296908259391788), 54: (1.9738854362225988, 0.07984147514886539, 0.012726208567619357), 55: (3.2270638975436823, 0.010371318905303612, 0.01232927441596987), 56: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 57: (2.3059216084842595, 0.04654615390440723, 0.011069506406784002), 58: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 59: (3.1403809797085143, 0.011920711594495271, 0.012796837091445856), 60: (2.4737431050712355, 0.035352498876522205, 0.002737727761268627), 61: (1.6113005706540569, 0.14157378011701163, 0.0019494831562042458), 62: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 63: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 64: (1.53743688384478, 0.1585651949016809, 0.002130213379859913), 65: (0.6035986920680937, 0.5610132145872897, 0.0003473639488220659), 66: (1.0077287109915996, 0.339908305180983, 0.0011471956968307495), 67: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 68: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 69: (1.6021500606216978, 0.14358622667512966, 0.001747226715087924), 70: (1.578855155842922, 0.14882583018562254, 0.001754996180534374), 71: (-0.9331597303937214, 0.3750941961722959, -0.0008138090372085682), 72: (1.5852476471891817, 0.14737120760043948, 0.0018799364566802756), 73: (-0.7271373912647012, 0.4856248944885333, -0.0007396459579467329), 74: (-1.0830423046211022, 0.30695096566719837, -0.0011349618434906006), 75: (0.4343451327383166, 0.6742649308269852, 0.0006856977939605491), 76: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 77: (0.3532057330609686, 0.7320659772780975, 0.0005770146846771351), 78: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 79: (-0.788284034364636, 0.45079352471553624, -0.0010927379131316917), 80: (1.3336937425899686, 0.21507071520896828, 0.0051479965448379406), 81: (1.9807669198305076, 0.07896205495606001, 0.012577366828918468), 82: (0.06972756394308077, 0.9459352163099144, 0.00014835894107817493), 83: (-1.1598508863626962, 0.27595292042177283, -0.003138679265975941), 84: (2.502593835967706, 0.03371801951659803, 0.009901961684226968), 85: (2.4071531852960577, 0.039432667879380455, 0.007238087058067333), 86: (2.502593835967706, 0.03371801951659803, 0.009901961684226968), 87: (2.1891701035114055, 0.05632639574221948, 0.007605543732643161), 88: (2.4071531852960577, 0.039432667879380455, 0.007238087058067333), 89: (2.2839470834087656, 0.04825000877654509, 0.006980195641517639), 90: (2.1001663651565923, 0.06510070417770156, 0.007232803106307972), 91: (2.217126449173077, 0.05381582701278829, 0.006793224811553922), 92: (2.9082126428940716, 0.01736168617648168, 0.005811285972595226), 93: (2.4387499353236053, 0.03744133009823867, 0.007071739435195901), 94: (2.007662306976666, 0.07561267274669725, 0.006354174017906167), 95: (2.4071531852960577, 0.039432667879380455, 0.007238087058067333), 96: (2.4071531852960577, 0.039432667879380455, 0.007238087058067333), 97: (1.634712174810816, 0.13654009651084903, 0.005153477191925049), 98: (2.630136944145678, 0.02735251082667631, 0.008446931838989258), 99: (1.9107332969923785, 0.0883578431583259, 0.0041037440299988015), 100: (2.290628562098974, 0.047725556273620136, 0.004274970293045022), 101: (2.0933821116848716, 0.06582125831616147, 0.006068557500839233), 102: (1.627603621013519, 0.13805114116980974, 0.0039044678211211936), 103: (2.585950424413753, 0.02940816869870594, 0.008170664310455322), 104: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 105: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 106: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 107: (0.7945155788504622, 0.4473388366512432, 0.0008462429046631192), 108: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 109: (0.9942624898356804, 0.34607322156604914, 0.0014589965343475009), 110: (1.455289556974256, 0.17956424097009338, 0.0012722373008727805), 111: (-0.40311714339013993, 0.6962705053346807, -0.00046482086181637294), 112: (-1.3995010159139174, 0.19517324349859388, -0.0016595661640167458), 113: (0.31736921700451814, 0.7582025900197144, 0.0004179954528809038), 114: (0.48352736126717916, 0.6402678088763173, 0.0006562054157256969), 115: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 116: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 117: (-0.2685125850444927, 0.794358177071614, -0.0003172755241394043), 118: (0.17690180491132731, 0.8635025141437851, 0.0002203315496444591), 119: (0.10644048277647157, 0.9175676736078939, 0.0001502275466918057), 120: (-1.1374421926809106, 0.2847282753703033, -0.0014120459556580256), 121: (-0.26892403073778803, 0.7940513734382388, -0.00041577816009519264), 122: (-0.7879673008075888, 0.4509695902220746, -0.000934952497482322), 123: (0.7339417108574199, 0.48166582547252856, 0.001118680834770236), 124: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 125: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 126: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 127: (1.3815475677348008, 0.20043898665844112, 0.008650729060173057), 128: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 129: (1.7839885104955835, 0.10809309766930562, 0.012804698944091808), 130: (1.993676236811333, 0.07733712993116255, 0.010171923041343678), 131: (1.3413500042767823, 0.2126705993332483, 0.009033668041229292), 132: (2.9252832409934597, 0.01688605966307884, 0.01588654816150664), 133: (0.8375860459376804, 0.42394687359118666, 0.006955501437187206), 134: (2.509183986679589, 0.033355385995736835, 0.014794331789016735), 135: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 136: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 137: (2.432500896362837, 0.03782707395283051, 0.015213516354560863), 138: (1.9800145535322926, 0.07905775295108508, 0.014388802647590682), 139: (2.494999426821794, 0.03414080250541585, 0.009348744153976463), 140: (2.19400933262387, 0.055883794369236614, 0.009948676824569791), 141: (2.526709734292557, 0.03240986873880421, 0.01006338596343992), 142: (2.5065990466500887, 0.03349715943896789, 0.009605455398559615), 143: (1.9558113216511723, 0.0821958264018493, 0.0078074634075164795)}, 'original_noun_phrase': {0: (17.928403897297034, 4.934233780617678e-65, 0.006354058471818802)}}, 'dann': {'named': {0: (-0.9867294195570829, 0.33617516260200075, -0.0019272759556770436), 1: (-1.2452428997465435, 0.22818190293584237, -0.0020560294389724842), 2: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 3: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 4: (-1.3566084641525438, 0.19080716428991337, -0.0022081181406974904), 5: (-0.5066744948466355, 0.6182126598611556, -0.000635613501071941), 6: (-0.9807570537436318, 0.33903683077187907, -0.0012205190956592615), 7: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 8: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 9: (0.38819756753275503, 0.7021851295065851, 0.0004766911268234142), 10: (-1.1755225931788928, 0.2543065174989379, -0.0024114027619361877), 11: (-0.9917942822648065, 0.3337615197075454, -0.0015550673007965199), 12: (0.2774077479936443, 0.7844628678651137, 0.00034148842096326515), 13: (-0.8339300993217477, 0.41468221042413356, -0.0013277605175971985), 14: (-1.5202488930803648, 0.14491564372650456, -0.004161497950553872), 15: (1.6972759586236632, 0.10596498853229763, 0.003535556793212935), 16: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 17: (1.0016507492575955, 0.32909914609836277, 0.0016669109463691711), 18: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 19: (1.6277437842392213, 0.12004876977937895, 0.0032092809677123357), 20: (-1.5009283623118574, 0.1498099651304123, -0.006498688459396329), 21: (-1.6109635955680688, 0.12367578045278915, -0.0068493291735649), 22: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 23: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 24: (-1.2891871810569517, 0.2128078300899039, -0.003938792645931222), 25: (-0.999616474258732, 0.33005766075827636, -0.0026086762547492537), 26: (-1.3924457849298786, 0.17987067769141993, -0.002627967298030859), 27: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 28: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 29: (-0.636680581148872, 0.5319294488758434, -0.001921905577182803), 30: (-1.0990154391398026, 0.2854928455095363, -0.0049043744802474976), 31: (-0.8290505966473767, 0.41736989975170846, -0.0022954314947128407), 32: (-1.0159921359762007, 0.3223970657175921, -0.003628966212272655), 33: (-1.2450212130949883, 0.22826157586858026, -0.005895775556564287), 34: (-1.5628644896088133, 0.13458726283948527, -0.00883282124996182), 35: (-0.2775163543425045, 0.7843807340327269, -0.0007456392049789429), 36: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 37: (-0.9951581226427932, 0.33216518239122017, -0.0030926048755645086), 38: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 39: (-0.5049933294009642, 0.6193707536363772, -0.0014558494091033714), 40: (-1.234195357923345, 0.23217842040776085, -0.0030386224389076233), 41: (-0.8005745624243296, 0.4332750978182609, -0.0009572267532348411), 42: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 43: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 44: (-0.8849455143984167, 0.38724837754189967, -0.0010334163904189841), 45: (-0.9632115368751197, 0.34754123658872504, -0.0013620018959045743), 46: (-0.844525009189141, 0.4088845819892061, -0.0009750984609127045), 47: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 48: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 49: (1.712832759416862, 0.10301499218695129, 0.0028555780649184848), 50: (-0.7001632511296481, 0.49231043837757804, -0.001612403988838218), 51: (0.6411093264132413, 0.5291097990458402, 0.0008345276117325273), 52: (1.1293342529446195, 0.2728139954959521, 0.0013565793633460999), 53: (-0.7914214552349883, 0.43846704390652413, -0.0017496749758720287), 54: (-1.79079785815554, 0.08927231509681495, -0.007404179871082328), 55: (0.7673307186972625, 0.45231561228052297, 0.001419311761856057), 56: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 57: (-0.4790246383966941, 0.6373879263513851, -0.0009406745433807595), 58: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 59: (1.5413282816567015, 0.13972747015120882, 0.003300172090530351), 60: (1.4461796521472785, 0.16442213427483043, 0.002373544871807076), 61: (-0.2265780880045984, 0.8231716551620237, -0.00032638758420944214), 62: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 63: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 64: (-0.31440272416523185, 0.7566414288566066, -0.0005337491631507985), 65: (0.2993878914671035, 0.7678944669361603, 0.00044022351503375523), 66: (0.8265976246560939, 0.41872520800562596, 0.0012576088309287914), 67: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 68: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 69: (0.6203414116234746, 0.5424028527539361, 0.000960740447044417), 70: (1.1831346456523935, 0.25134904624556187, 0.0019820079207419905), 71: (-0.12513206171331423, 0.9017335024184183, -0.0002091348171234242), 72: (-0.32320675290749284, 0.7500688061850564, -0.0005764201283455228), 73: (-0.193046127530431, 0.8489714599354892, -0.0003615766763687134), 74: (1.032006781588347, 0.3150274090199826, 0.0016774669289589261), 75: (-0.6984208369015937, 0.49337462560092404, -0.0011544555425644365), 76: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 77: (-1.0163627556385761, 0.3222251498500238, -0.0023468554019928645), 78: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 79: (-1.0705322236151504, 0.29779124771528703, -0.0024858862161636353), 80: (-1.1413412513774956, 0.26790949133339526, -0.002657562494277954), 81: (-0.09323127588617472, 0.9266959202957807, -0.00019965767860408157), 82: (-0.7831413718382906, 0.44319692478841277, -0.0005579873919487333), 83: (-0.7579305183526267, 0.45779103790086184, -0.0009718358516693226), 84: (-1.207548910324389, 0.2420384788511308, -0.001984208822250366), 85: (-1.099413381128593, 0.285323687638757, -0.0022287189960479847), 86: (-1.207548910324389, 0.2420384788511308, -0.001984208822250366), 87: (-1.3110906270483678, 0.2054524906583894, -0.003223280608654011), 88: (-1.099413381128593, 0.285323687638757, -0.0022287189960479847), 89: (-0.9996574191640942, 0.3300383489980435, -0.0009561315178870933), 90: (-1.0516519210829798, 0.3061514750470006, -0.003016935288906064), 91: (-0.9291803564474265, 0.3644510598083106, -0.00220405012369157), 92: (-1.175401010869867, 0.25435396664616183, -0.0025725707411766163), 93: (-0.06055605743868714, 0.9523452087615489, -5.860477685926124e-05), 94: (-0.8225972785487173, 0.42094146169540103, -0.0009408704936504364), 95: (-1.099413381128593, 0.285323687638757, -0.0022287189960479847), 96: (-1.099413381128593, 0.285323687638757, -0.0022287189960479847), 97: (-1.0611938437803885, 0.30190549491190133, -0.0031025767326355314), 98: (-0.9387229241331986, 0.359654215235255, -0.002385479211807262), 99: (-1.1336384223047353, 0.2710482898355101, -0.0032224059104919434), 100: (-1.2423371530833762, 0.2292279110528442, -0.0028264492750167403), 101: (-1.1430441857127842, 0.267219224605781, -0.002621537446975797), 102: (-1.326179076988338, 0.2005028178374613, -0.003980955481529214), 103: (-1.0804694565232806, 0.29345778095464037, -0.0030408114194869773), 104: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 105: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 106: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 107: (1.1370978192396186, 0.2696352794586459, 0.0008532896637916676), 108: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 109: (0.9661359028032891, 0.34611368012936705, 0.0007006019353866799), 110: (0.5873272559320057, 0.5638991515795342, 0.0004150778055190818), 111: (-0.8768927671793506, 0.3914976991308914, -0.0010815337300300598), 112: (-0.492585715107945, 0.6279493442868852, -0.0005521222949028126), 113: (-0.8876679884782366, 0.38581866399279774, -0.001028694212436676), 114: (1.9919857371898635, 0.0609425756947396, 0.0017527014017104658), 115: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 116: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 117: (-0.17356616266441877, 0.8640422455233715, -0.000146953761577584), 118: (0.05262973954484554, 0.9585763353239114, 4.5648217201210706e-05), 119: (-0.7937550885372135, 0.4371396708869957, -0.0008041948080061978), 120: (-0.8291497433376069, 0.4173151783385879, -0.0008700907230377863), 121: (-0.8888635595557307, 0.38519190917442914, -0.0010397553443908247), 122: (-0.8103474787237596, 0.4277741836374571, -0.0008706778287886907), 123: (-0.4076069663627391, 0.6881183421446317, -0.00041303932666780785), 124: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 125: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 126: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 127: (-1.6980236247314056, 0.10582157209362963, -0.00728069692850114), 128: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 129: (-1.6014726497406484, 0.1257678426921422, -0.0063585817813873735), 130: (-1.389156437285621, 0.18085306802344098, -0.006095549464225802), 131: (-1.203391578778529, 0.2436051407849732, -0.004777027666568789), 132: (-0.7429261706312886, 0.4666135784705717, -0.0031158849596977234), 133: (-1.4174551873754109, 0.17254091080034387, -0.007822236418724038), 134: (-0.4066098503932951, 0.6888382131061397, -0.0015775725245475991), 135: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 136: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 137: (-1.0550217584710877, 0.30464709845296795, -0.003987997770309448), 138: (-1.216419014520918, 0.23872146186183196, -0.005159489810466766), 139: (-0.6273524459348028, 0.537895201199105, -0.002090153098106451), 140: (-1.2759050472617128, 0.21736716485991053, -0.004835143685340881), 141: (-1.0370987066099429, 0.3127094386870817, -0.004217070341110185), 142: (-0.929467327108055, 0.3643061783437216, -0.0036227077245712946), 143: (-0.7362115149546515, 0.47059460455096525, -0.0027908772230148537)}, 'noun_phrase': {0: (0.32176681478719044, 0.7549769578469689, 0.0012385070323944203), 1: (1.4766226480042148, 0.17388890034183893, 0.003506255149841264), 2: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 3: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 4: (0.918911793265633, 0.38210910874179593, 0.0020947158336639404), 5: (3.0882193176317525, 0.012966697975766992, 0.011990487575531006), 6: (2.5676534241826596, 0.03030423078546963, 0.008141294121742249), 7: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 8: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 9: (1.547013422172735, 0.15626424608948053, 0.005729144811630227), 10: (2.4550281157408893, 0.036454779706908066, 0.007289832830429055), 11: (1.2572070136387585, 0.2403228820471545, 0.002852046489715543), 12: (1.9338228423299695, 0.08514892994019035, 0.0050469756126403365), 13: (1.2390259791919658, 0.24667429300686539, 0.0036864370107650424), 14: (0.48488703356022395, 0.6393399309607386, 0.0018735915422439797), 15: (3.0180731625373034, 0.014524585908708705, 0.01241346597671511), 16: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 17: (2.2035700060731642, 0.05501930296620119, 0.008129897713661238), 18: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 19: (4.048596889831625, 0.002891013585731314, 0.012764960527420044), 20: (1.8079507621380635, 0.1040721817494618, 0.01314978301525116), 21: (2.081269316041891, 0.0671269195079296, 0.013036927580833413), 22: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 23: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 24: (1.3841530851981612, 0.19966731786475816, 0.009304749965667769), 25: (2.3562657772333533, 0.042862819899520725, 0.019379985332489025), 26: (1.8930350680862085, 0.09089440506378557, 0.012767596542835213), 27: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 28: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 29: (2.0354686831459485, 0.07229272014087024, 0.013244447112083446), 30: (2.1598565954741553, 0.059081257858952, 0.015896314382553067), 31: (1.6534747802033491, 0.13262338073074043, 0.01141127645969392), 32: (1.6880543973538569, 0.12567104980516453, 0.012637373805046093), 33: (1.8361282815985915, 0.09952210292351608, 0.012894773483276334), 34: (1.3368283216280035, 0.21408532009435072, 0.011225506663322449), 35: (2.609624460078373, 0.028288172896220688, 0.013556665182113625), 36: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 37: (1.91107212434062, 0.08830993899457726, 0.011751183867454551), 38: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 39: (2.148512238971029, 0.06018215146050553, 0.01318176388740544), 40: (2.0638939744849827, 0.06904346740782925, 0.015153759717941251), 41: (2.218225015959506, 0.05371943471530602, 0.01549370288848878), 42: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 43: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 44: (1.9175249664254657, 0.08740228790429246, 0.012787795066833474), 45: (2.826005784456006, 0.01985141251957473, 0.022183382511138938), 46: (2.400549429629872, 0.03986196143203122, 0.015419840812683105), 47: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 48: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 49: (2.505639567322363, 0.033549936127881964, 0.01723344624042511), 50: (2.986644043743537, 0.015283907001565568, 0.01938931047916409), 51: (2.1677734204354304, 0.05832459243502238, 0.013565436005592346), 52: (2.0865272636900545, 0.06655711860338713, 0.014177829027175903), 53: (2.297315725716465, 0.04720626843717642, 0.01569410562515261), 54: (1.9330067188475204, 0.08526044851507485, 0.014338076114654541), 55: (3.033151673430603, 0.01417417142098351, 0.015086108446121238), 56: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 57: (2.593551740533509, 0.029043780052692654, 0.014308959245681763), 58: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 59: (2.8422391488625958, 0.019332403640807263, 0.014718294143676758), 60: (2.3478558177617104, 0.04345746117319644, 0.0067915409803390725), 61: (1.534866195829691, 0.15918794928573535, 0.004163616895675626), 62: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 63: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 64: (0.9611441817827515, 0.3615892674135417, 0.0029742836952209584), 65: (0.44765017690051645, 0.6649861385672213, 0.0012340337038039828), 66: (0.3762017454388502, 0.7154820324859454, 0.0012960508465766907), 67: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 68: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 69: (1.1967354350238133, 0.2619817086495285, 0.003531971573829673), 70: (1.0992289864297298, 0.30020134809395893, 0.0034032225608826017), 71: (0.1587061438478116, 0.8774045689061314, 0.0005131065845489502), 72: (0.42648493986091657, 0.6797741371312713, 0.0011184543371201006), 73: (0.2655708572575739, 0.7965528273142781, 0.0005496859550476185), 74: (0.5492131345664989, 0.5962205939672494, 0.0016154468059539906), 75: (0.9987362359534044, 0.3440158964787752, 0.0021351695060729536), 76: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 77: (0.8569000352002448, 0.41373418445484245, 0.0017239898443222046), 78: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 79: (0.3522973172050324, 0.7327241932984645, 0.0006840765476227029), 80: (2.0212493136417597, 0.07397262899774995, 0.01194812059402467), 81: (1.9236852369726192, 0.08654401633515652, 0.016407230496406577), 82: (-0.47962578956006724, 0.6429340136693806, -0.0010462343692779763), 83: (-0.4520202174282119, 0.6619515180399729, -0.0013109803199767844), 84: (2.009315991639913, 0.07541122018983434, 0.009284698963165305), 85: (1.5616612390054907, 0.15280223525836137, 0.007558640837669384), 86: (2.009315991639913, 0.07541122018983434, 0.009284698963165305), 87: (2.3551862846862446, 0.04293869528060917, 0.01398688554763794), 88: (1.5616612390054907, 0.15280223525836137, 0.007558640837669384), 89: (3.543341051175119, 0.006281111838804635, 0.009105792641639743), 90: (2.2544266917526525, 0.050635688571756834, 0.012923109531402577), 91: (2.0509966809881472, 0.07049997744201736, 0.012570643424987815), 92: (2.3482289288575813, 0.04343090776926426, 0.010921820998191833), 93: (2.525897669445957, 0.0324530813336942, 0.013159114122390791), 94: (3.6530274871827073, 0.005292558199144063, 0.009726472198963165), 95: (1.5616612390054907, 0.15280223525836137, 0.007558640837669384), 96: (1.5616612390054907, 0.15280223525836137, 0.007558640837669384), 97: (2.2342428055361436, 0.052332986349772126, 0.011621281504631042), 98: (2.3261772832574, 0.04502809643828023, 0.011416107416152954), 99: (1.0855901183385217, 0.3058808007821362, 0.006092655658721857), 100: (0.8517691553364071, 0.4164304708552665, 0.0038651406764984575), 101: (0.9305035802434644, 0.37639482087757603, 0.005082440376281805), 102: (0.42063581835907643, 0.6838868765081995, 0.002441000938415594), 103: (1.2413875033361923, 0.24584158719440594, 0.007783329486847013), 104: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 105: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 106: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 107: (1.1461810666772596, 0.28128004427667713, 0.0015783518552780484), 108: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 109: (0.8042179440478184, 0.4419952411606892, 0.0012110233306885099), 110: (2.0171298896813723, 0.07446624454084619, 0.0024679958820342796), 111: (-0.1809439041613578, 0.8604207660427458, -0.0002918362617492787), 112: (-0.03487466536290386, 0.972940919216302, -4.5865774154663086e-05), 113: (0.020356245105158034, 0.9842033447427685, 3.411471843722813e-05), 114: (1.9362754442796217, 0.08481462632865278, 0.0023768365383148304), 115: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 116: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 117: (0.656416529562219, 0.5279806554409376, 0.0010124027729034313), 118: (-0.06917983691502126, 0.946359153392213, -0.0001084446907043124), 119: (-0.6111665245114267, 0.5562088264271956, -0.00088534355163572), 120: (-0.29190712472856417, 0.7769744306615747, -0.0003355681896209939), 121: (-1.2099638399245203, 0.2571126327336307, -0.0020089626312256303), 122: (-1.2187103075203463, 0.25393388037841697, -0.0016553401947021484), 123: (-0.11224944773483804, 0.9130893441532238, -0.0001426100730895774), 124: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 125: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 126: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 127: (0.8188097810674024, 0.43403984418158514, 0.008369690179824807), 128: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 129: (1.344219612684229, 0.21177688246528342, 0.014003562927246083), 130: (1.2182077609202966, 0.25411565011020515, 0.010748335719108604), 131: (1.4284011620972987, 0.18694678849210875, 0.014729905128478993), 132: (2.448221417005585, 0.03686411709070902, 0.02071168422698977), 133: (1.307978779444202, 0.22329990056242666, 0.014133965969085671), 134: (1.8508231489365952, 0.0972235339920648, 0.016493734717369035), 135: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 136: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 137: (1.9938421708074958, 0.07731645237533531, 0.018206831812858537), 138: (1.3997402905893874, 0.19510387413129615, 0.01494083404541019), 139: (1.6489407277297508, 0.13356042651209937, 0.00880095362663269), 140: (1.617289305446289, 0.1402704745815911, 0.009810954332351685), 141: (2.1037029662360927, 0.06472810658485369, 0.012146306037902899), 142: (2.1458646654182423, 0.06044193061800628, 0.01131196022033687), 143: (2.2606987112332058, 0.05011932905829078, 0.016005074977874734)}, 'original_noun_phrase': {0: (17.549553772156735, 1.2464257307518868e-62, 0.008245057353956864)}}}\n",
            "EI_joy non_dann named\n",
            "named {0: (0.927130428671265, 0.3654871282317217, 0.0019718199968338235), 1: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 2: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 3: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 4: (1.2531693205122543, 0.22534723569449014, 0.0024197280406951793), 5: (1.3174986525278145, 0.2033387726702804, 0.002490296959877014), 6: (1.2436306837720958, 0.22876181256660205, 0.0026371866464614535), 7: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 8: (1.2518942904312103, 0.22580137345216844, 0.0027422189712524303), 9: (1.3042185510974138, 0.20773840637308943, 0.0022828787565231656), 10: (1.327117228975429, 0.2001981932160694, 0.002530188858509086), 11: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 12: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 13: (1.7428398231497895, 0.09752375837723172, 0.0035621538758278115), 14: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 15: (1.3023021240927164, 0.20837942480472954, 0.0024586543440818787), 16: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 17: (1.4053663411093298, 0.17605325521431578, 0.003052820265293077), 18: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 19: (1.334510768970373, 0.1978102185004114, 0.0025804668664932695), 20: (0.9648078850354396, 0.3467614634674251, 0.003345428407192208), 21: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 22: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 23: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 24: (1.1873555411945744, 0.24972033392870072, 0.004133212566375721), 25: (1.0341270734019747, 0.31406072117995326, 0.0029740095138549583), 26: (0.9954487568479664, 0.33202751012987175, 0.003394991159439087), 27: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 28: (1.0148970597038753, 0.3229054061666836, 0.0026780590415000916), 29: (1.1008968462715054, 0.2846937389346681, 0.0031256809830665366), 30: (0.8866396932483286, 0.38635826577565757, 0.0030403494834900235), 31: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 32: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 33: (1.1269897254497583, 0.2737793640146807, 0.0042918160557746665), 34: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 35: (1.2727763621511254, 0.21845211311102822, 0.0037619456648826377), 36: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 37: (1.1800763632204914, 0.2525341357443367, 0.003792211413383484), 38: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 39: (1.5784224237170579, 0.13097295691677174, 0.004382364451885223), 40: (0.3706764954299417, 0.7149789285371184, 0.001211306452751193), 41: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 42: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 43: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 44: (0.9510429695683896, 0.35352483469689244, 0.0026509761810302734), 45: (0.20321680659982658, 0.841126104553226, 0.00047275424003601074), 46: (0.8183434249209005, 0.42330630820690995, 0.002901148796081554), 47: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 48: (0.4200164302561195, 0.6791850216458598, 0.0011544987559318765), 49: (0.830738390994025, 0.4164389892854148, 0.0021710038185119296), 50: (0.6555897233779486, 0.5199482142674257, 0.0017548486590385215), 51: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 52: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 53: (0.6798450782278772, 0.5048022137549548, 0.0026078701019287), 54: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 55: (0.6442546163386427, 0.5271122871918197, 0.0016197532415390126), 56: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 57: (0.89902941782725, 0.3798898471765356, 0.0022531986236571933), 58: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 59: (1.0190850405952432, 0.3209643699571856, 0.002433972060680356), 60: (-0.16750666698169162, 0.868741420067475, -0.00022678077220916748), 61: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 62: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 63: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 64: (0.13922735134295997, 0.8907356759180701, 0.00022866278886796154), 65: (-0.3883816998147466, 0.702051151937652, -0.0005484327673912492), 66: (0.02193939548499107, 0.9827250708715176, 3.829300403596081e-05), 67: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 68: (0.002459680991383545, 0.9980631026103626, 5.003809928849634e-06), 69: (0.07916098447993353, 0.9377323781415308, 0.00017678886651989467), 70: (0.006313327585592243, 0.995028544256352, 1.0462105274222644e-05), 71: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 72: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 73: (-0.01168985920938709, 0.9907949286564823, -1.865178346632801e-05), 74: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 75: (-0.014069772953011189, 0.9889210067818506, -2.6115775108359607e-05), 76: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 77: (0.09061458634754754, 0.9287473156172278, 0.00014399439096451916), 78: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 79: (0.25524464813261455, 0.8012767347316481, 0.00042962878942487404), 80: (0.4944255105613076, 0.6266738386933018, 0.0017054766416549905), 81: (0.4015194111857567, 0.6925180059712798, 0.0017325416207313316), 82: (1.4284660577708723, 0.16939123354105534, 0.002391044795513164), 83: (-0.18513527348539655, 0.8550848963311205, -0.00041247904300689697), 84: (13.623805562448862, 2.953336520776732e-11, 0.03214100897312164), 85: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 86: (13.623805562448862, 2.953336520776732e-11, 0.03214100897312164), 87: (15.613965963745777, 2.712259102454546e-12, 0.03620264828205105), 88: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 89: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 90: (10.718198861612956, 1.7025185784121546e-09, 0.022274567186832434), 91: (10.278879605250857, 3.372997728430722e-09, 0.020667591691017145), 92: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 93: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 94: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 95: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 96: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 97: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 98: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 99: (17.278907545928526, 4.4642351018466344e-13, 0.0325523316860199), 100: (16.329994079427188, 1.2240473252310238e-12, 0.033421632647514354), 101: (11.532314218673289, 5.055202430326208e-10, 0.03211134076118466), 102: (12.56817891732165, 1.1814773159409936e-10, 0.031706155836582206), 103: (11.170516588095923, 8.600819301000773e-10, 0.03029073625802997), 104: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 105: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 106: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 107: (0.614321461231838, 0.5462894812398227, 0.0005189865827560092), 108: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 109: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 110: (0.5925784878290913, 0.5604504593874424, 0.0006949156522750854), 111: (0.8340101957405962, 0.41463818475912606, 0.000979569554328874), 112: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 113: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 114: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 115: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 116: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 117: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 118: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 119: (-0.7430493506231183, 0.46654073581996913, -0.0005963727831840293), 120: (-0.26132919939103477, 0.7966504313686013, -0.00017389357089997448), 121: (0.31679291549251387, 0.7548551317327175, 0.0003045767545700184), 122: (0.07705892819510853, 0.939382356282123, 7.993727922439575e-05), 123: (0.8649705897222905, 0.39784479313516674, 0.0009871751070022472), 124: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 125: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 126: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 127: (0.7967294372923546, 0.4354514862913137, 0.002296024560928367), 128: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 129: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 130: (0.06481738391512246, 0.9489965212009155, 0.00022808462381362915), 131: (-0.1603517261507232, 0.8742965632691216, -0.00047191381454469994), 132: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 133: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 134: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 135: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 136: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 137: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 138: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 139: (1.6146163075395017, 0.12287847791729747, 0.002999137341976099), 140: (0.305663680124198, 0.7631844067937561, 0.00042246878147123024), 141: (1.177061576252688, 0.2537064835219378, 0.0021041855216026306), 142: (1.292512658049242, 0.21167805377758545, 0.002440388500690449), 143: (0.6976292430085975, 0.4938585345161479, 0.0015214845538139565)}\n",
            "EI_joy non_dann noun_phrase\n",
            "noun_phrase {0: (1.0442462382617037, 0.3236071648886541, 0.014378252625465382), 1: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 2: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 3: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 4: (1.068156744919508, 0.3132615708445877, 0.014390206336975131), 5: (1.3491836965086035, 0.21023837259357941, 0.012973392009735074), 6: (0.9708868300900558, 0.3569723962343001, 0.013869673013687134), 7: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 8: (1.045011391917099, 0.32327209352648073, 0.011891117691993691), 9: (0.9160655739855799, 0.38352168605558257, 0.011061030626296986), 10: (1.0883179518932313, 0.3047382407508142, 0.016650095582008362), 11: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 12: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 13: (0.9440753908926197, 0.3697834154061227, 0.012642705440521207), 14: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 15: (1.165337749743547, 0.2738375081624465, 0.00994248390197755), 16: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 17: (1.133729001318476, 0.28620356708756717, 0.012761783599853471), 18: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 19: (1.2184084745740587, 0.2540430397974921, 0.010925072431564375), 20: (1.1388626197105938, 0.2841655220278896, 0.01670609414577484), 21: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 22: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 23: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 24: (1.1641932803103003, 0.27427767268470876, 0.015377753973007224), 25: (1.1262662246092308, 0.2891869208633998, 0.009866386651992798), 26: (1.2568708386939498, 0.24043908581333848, 0.01549505889415742), 27: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 28: (1.2433798432522831, 0.24514085951029313, 0.011981427669525146), 29: (1.3581336617829196, 0.207488489275405, 0.01307763159275055), 30: (1.1808753318955496, 0.267917640443583, 0.01745899319648747), 31: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 32: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 33: (1.0007150294443807, 0.3431088428374259, 0.014056950807571411), 34: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 35: (1.4016136397818648, 0.1945614940081961, 0.011961144208908037), 36: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 37: (0.9822887533739848, 0.35162473484834944, 0.010575699806213368), 38: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 39: (1.4805615219909143, 0.17285835724538356, 0.012446621060371421), 40: (1.336733785263309, 0.21411498289476213, 0.01734680533409122), 41: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 42: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 43: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 44: (1.4575218114632529, 0.17896290802183798, 0.018263766169548057), 45: (1.4098880955745712, 0.19218133050503033, 0.010134425759315446), 46: (1.2970440402707297, 0.2268784565464511, 0.01639994382858273), 47: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 48: (1.5330357203201004, 0.15963270659450096, 0.0149380803108215), 49: (1.4428694090638343, 0.18294225463509742, 0.015479716658592213), 50: (1.4701353173956515, 0.17559793168985668, 0.01913298964500426), 51: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 52: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 53: (1.3821539006476835, 0.20025918322600134, 0.01667459011077882), 54: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 55: (1.647582437547556, 0.13384230836982, 0.014032915234565735), 56: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 57: (1.2127117011155155, 0.25611049657681384, 0.014113497734069835), 58: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 59: (1.7320749330356022, 0.11730236002488194, 0.016027060151100225), 60: (1.2250884396890571, 0.2516361392022721, 0.011804008483886741), 61: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 62: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 63: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 64: (1.3524324432160792, 0.20923663597226802, 0.013714814186096214), 65: (1.207983408257259, 0.2578368670848937, 0.010349023342132613), 66: (1.0696250703873664, 0.3126346616232136, 0.011452090740203869), 67: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 68: (1.1524813196613046, 0.2788147386507328, 0.011603665351867687), 69: (1.137393340057812, 0.2847476458175558, 0.012291699647903442), 70: (1.2444432373122565, 0.24476752527845105, 0.01334513425827022), 71: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 72: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 73: (1.0440114176934014, 0.3237100492876409, 0.010314714908599809), 74: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 75: (1.2216244908644383, 0.25288191962479567, 0.009443157911300626), 76: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 77: (1.1135252547197316, 0.29433701718942434, 0.010051617026329063), 78: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 79: (1.2417819182356142, 0.24570273655767255, 0.010424494743347168), 80: (0.9263823178852398, 0.37841931834657005, 0.011364886164665244), 81: (1.4483064567465782, 0.18145675239684195, 0.021374163031578075), 82: (1.0548361444603112, 0.3189932578383086, 0.012901440262794495), 83: (0.5076225527682159, 0.6239236805496233, 0.008730635046958923), 84: (2.894407698951993, 0.017756339567876013, 0.04467357993125917), 85: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 86: (2.894407698951993, 0.017756339567876013, 0.04467357993125917), 87: (3.013370645684239, 0.014635682938161457, 0.048122626543045055), 88: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 89: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 90: (2.1060548842956632, 0.06448146155403724, 0.03805774450302124), 91: (2.2033485657290766, 0.05503917768304185, 0.03558392226696011), 92: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 93: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 94: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 95: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 96: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 97: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 98: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 99: (2.446681986732943, 0.03695732707497869, 0.03842214047908782), 100: (2.6467234192687865, 0.026618778820155658, 0.04456419050693511), 101: (2.408500757573574, 0.03934563074886143, 0.041527169942855824), 102: (2.4687665745111733, 0.03564232369271045, 0.04298895597457886), 103: (2.3990178598115888, 0.0399621853187128, 0.04548675417900083), 104: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 105: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 106: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 107: (0.6103225142216757, 0.5567434719512216, 0.005341175198555037), 108: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 109: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 110: (0.6937772128886472, 0.5053323531323073, 0.00855525135993962), 111: (0.5108092118769949, 0.6217780356510954, 0.005040934681892428), 112: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 113: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 114: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 115: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 116: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 117: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 118: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 119: (0.2727569654588054, 0.7911950651209458, 0.0022219240665435347), 120: (0.42626033178645306, 0.6799318623249235, 0.003239944577217102), 121: (0.6279777912011617, 0.5456211375801904, 0.005877128243446328), 122: (0.5451556351158041, 0.598894181824965, 0.004873842000961304), 123: (0.662451935092854, 0.5242811499325539, 0.007979604601860035), 124: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 125: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 126: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 127: (0.9574801818219801, 0.36333689627181376, 0.012267100811004683), 128: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 129: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 130: (1.0147349169071984, 0.3367336492726059, 0.013134312629699718), 131: (0.7447867858634004, 0.47539841641828073, 0.010077017545700062), 132: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 133: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 134: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 135: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 136: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 137: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 138: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 139: (1.395036536895664, 0.1964714564670174, 0.00986538827419281), 140: (0.9961169162223953, 0.3452193196021255, 0.008227831125259488), 141: (1.6477864903052022, 0.133799927532065, 0.010629636049270696), 142: (2.0859573671878606, 0.06661865209177747, 0.009048116207122758), 143: (1.732440577383377, 0.11723504124284848, 0.011292290687561046)}\n",
            "EI_joy non_dann original_noun_phrase\n",
            "original_noun_phrase {0: (16.221565546438303, 1.7798935688953818e-54, 0.017407106111447024)}\n",
            "non_dann original_noun_phrase {'named': {0: (0.927130428671265, 0.3654871282317217, 0.0019718199968338235), 1: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 2: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 3: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 4: (1.2531693205122543, 0.22534723569449014, 0.0024197280406951793), 5: (1.3174986525278145, 0.2033387726702804, 0.002490296959877014), 6: (1.2436306837720958, 0.22876181256660205, 0.0026371866464614535), 7: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 8: (1.2518942904312103, 0.22580137345216844, 0.0027422189712524303), 9: (1.3042185510974138, 0.20773840637308943, 0.0022828787565231656), 10: (1.327117228975429, 0.2001981932160694, 0.002530188858509086), 11: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 12: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 13: (1.7428398231497895, 0.09752375837723172, 0.0035621538758278115), 14: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 15: (1.3023021240927164, 0.20837942480472954, 0.0024586543440818787), 16: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 17: (1.4053663411093298, 0.17605325521431578, 0.003052820265293077), 18: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 19: (1.334510768970373, 0.1978102185004114, 0.0025804668664932695), 20: (0.9648078850354396, 0.3467614634674251, 0.003345428407192208), 21: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 22: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 23: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 24: (1.1873555411945744, 0.24972033392870072, 0.004133212566375721), 25: (1.0341270734019747, 0.31406072117995326, 0.0029740095138549583), 26: (0.9954487568479664, 0.33202751012987175, 0.003394991159439087), 27: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 28: (1.0148970597038753, 0.3229054061666836, 0.0026780590415000916), 29: (1.1008968462715054, 0.2846937389346681, 0.0031256809830665366), 30: (0.8866396932483286, 0.38635826577565757, 0.0030403494834900235), 31: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 32: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 33: (1.1269897254497583, 0.2737793640146807, 0.0042918160557746665), 34: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 35: (1.2727763621511254, 0.21845211311102822, 0.0037619456648826377), 36: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 37: (1.1800763632204914, 0.2525341357443367, 0.003792211413383484), 38: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 39: (1.5784224237170579, 0.13097295691677174, 0.004382364451885223), 40: (0.3706764954299417, 0.7149789285371184, 0.001211306452751193), 41: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 42: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 43: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 44: (0.9510429695683896, 0.35352483469689244, 0.0026509761810302734), 45: (0.20321680659982658, 0.841126104553226, 0.00047275424003601074), 46: (0.8183434249209005, 0.42330630820690995, 0.002901148796081554), 47: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 48: (0.4200164302561195, 0.6791850216458598, 0.0011544987559318765), 49: (0.830738390994025, 0.4164389892854148, 0.0021710038185119296), 50: (0.6555897233779486, 0.5199482142674257, 0.0017548486590385215), 51: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 52: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 53: (0.6798450782278772, 0.5048022137549548, 0.0026078701019287), 54: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 55: (0.6442546163386427, 0.5271122871918197, 0.0016197532415390126), 56: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 57: (0.89902941782725, 0.3798898471765356, 0.0022531986236571933), 58: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 59: (1.0190850405952432, 0.3209643699571856, 0.002433972060680356), 60: (-0.16750666698169162, 0.868741420067475, -0.00022678077220916748), 61: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 62: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 63: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 64: (0.13922735134295997, 0.8907356759180701, 0.00022866278886796154), 65: (-0.3883816998147466, 0.702051151937652, -0.0005484327673912492), 66: (0.02193939548499107, 0.9827250708715176, 3.829300403596081e-05), 67: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 68: (0.002459680991383545, 0.9980631026103626, 5.003809928849634e-06), 69: (0.07916098447993353, 0.9377323781415308, 0.00017678886651989467), 70: (0.006313327585592243, 0.995028544256352, 1.0462105274222644e-05), 71: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 72: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 73: (-0.01168985920938709, 0.9907949286564823, -1.865178346632801e-05), 74: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 75: (-0.014069772953011189, 0.9889210067818506, -2.6115775108359607e-05), 76: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 77: (0.09061458634754754, 0.9287473156172278, 0.00014399439096451916), 78: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 79: (0.25524464813261455, 0.8012767347316481, 0.00042962878942487404), 80: (0.4944255105613076, 0.6266738386933018, 0.0017054766416549905), 81: (0.4015194111857567, 0.6925180059712798, 0.0017325416207313316), 82: (1.4284660577708723, 0.16939123354105534, 0.002391044795513164), 83: (-0.18513527348539655, 0.8550848963311205, -0.00041247904300689697), 84: (13.623805562448862, 2.953336520776732e-11, 0.03214100897312164), 85: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 86: (13.623805562448862, 2.953336520776732e-11, 0.03214100897312164), 87: (15.613965963745777, 2.712259102454546e-12, 0.03620264828205105), 88: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 89: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 90: (10.718198861612956, 1.7025185784121546e-09, 0.022274567186832434), 91: (10.278879605250857, 3.372997728430722e-09, 0.020667591691017145), 92: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 93: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 94: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 95: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 96: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 97: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 98: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 99: (17.278907545928526, 4.4642351018466344e-13, 0.0325523316860199), 100: (16.329994079427188, 1.2240473252310238e-12, 0.033421632647514354), 101: (11.532314218673289, 5.055202430326208e-10, 0.03211134076118466), 102: (12.56817891732165, 1.1814773159409936e-10, 0.031706155836582206), 103: (11.170516588095923, 8.600819301000773e-10, 0.03029073625802997), 104: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 105: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 106: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 107: (0.614321461231838, 0.5462894812398227, 0.0005189865827560092), 108: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 109: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 110: (0.5925784878290913, 0.5604504593874424, 0.0006949156522750854), 111: (0.8340101957405962, 0.41463818475912606, 0.000979569554328874), 112: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 113: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 114: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 115: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 116: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 117: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 118: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 119: (-0.7430493506231183, 0.46654073581996913, -0.0005963727831840293), 120: (-0.26132919939103477, 0.7966504313686013, -0.00017389357089997448), 121: (0.31679291549251387, 0.7548551317327175, 0.0003045767545700184), 122: (0.07705892819510853, 0.939382356282123, 7.993727922439575e-05), 123: (0.8649705897222905, 0.39784479313516674, 0.0009871751070022472), 124: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 125: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 126: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 127: (0.7967294372923546, 0.4354514862913137, 0.002296024560928367), 128: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 129: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 130: (0.06481738391512246, 0.9489965212009155, 0.00022808462381362915), 131: (-0.1603517261507232, 0.8742965632691216, -0.00047191381454469994), 132: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 133: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 134: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 135: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 136: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 137: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 138: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 139: (1.6146163075395017, 0.12287847791729747, 0.002999137341976099), 140: (0.305663680124198, 0.7631844067937561, 0.00042246878147123024), 141: (1.177061576252688, 0.2537064835219378, 0.0021041855216026306), 142: (1.292512658049242, 0.21167805377758545, 0.002440388500690449), 143: (0.6976292430085975, 0.4938585345161479, 0.0015214845538139565)}, 'noun_phrase': {0: (1.0442462382617037, 0.3236071648886541, 0.014378252625465382), 1: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 2: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 3: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 4: (1.068156744919508, 0.3132615708445877, 0.014390206336975131), 5: (1.3491836965086035, 0.21023837259357941, 0.012973392009735074), 6: (0.9708868300900558, 0.3569723962343001, 0.013869673013687134), 7: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 8: (1.045011391917099, 0.32327209352648073, 0.011891117691993691), 9: (0.9160655739855799, 0.38352168605558257, 0.011061030626296986), 10: (1.0883179518932313, 0.3047382407508142, 0.016650095582008362), 11: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 12: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 13: (0.9440753908926197, 0.3697834154061227, 0.012642705440521207), 14: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 15: (1.165337749743547, 0.2738375081624465, 0.00994248390197755), 16: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 17: (1.133729001318476, 0.28620356708756717, 0.012761783599853471), 18: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 19: (1.2184084745740587, 0.2540430397974921, 0.010925072431564375), 20: (1.1388626197105938, 0.2841655220278896, 0.01670609414577484), 21: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 22: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 23: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 24: (1.1641932803103003, 0.27427767268470876, 0.015377753973007224), 25: (1.1262662246092308, 0.2891869208633998, 0.009866386651992798), 26: (1.2568708386939498, 0.24043908581333848, 0.01549505889415742), 27: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 28: (1.2433798432522831, 0.24514085951029313, 0.011981427669525146), 29: (1.3581336617829196, 0.207488489275405, 0.01307763159275055), 30: (1.1808753318955496, 0.267917640443583, 0.01745899319648747), 31: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 32: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 33: (1.0007150294443807, 0.3431088428374259, 0.014056950807571411), 34: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 35: (1.4016136397818648, 0.1945614940081961, 0.011961144208908037), 36: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 37: (0.9822887533739848, 0.35162473484834944, 0.010575699806213368), 38: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 39: (1.4805615219909143, 0.17285835724538356, 0.012446621060371421), 40: (1.336733785263309, 0.21411498289476213, 0.01734680533409122), 41: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 42: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 43: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 44: (1.4575218114632529, 0.17896290802183798, 0.018263766169548057), 45: (1.4098880955745712, 0.19218133050503033, 0.010134425759315446), 46: (1.2970440402707297, 0.2268784565464511, 0.01639994382858273), 47: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 48: (1.5330357203201004, 0.15963270659450096, 0.0149380803108215), 49: (1.4428694090638343, 0.18294225463509742, 0.015479716658592213), 50: (1.4701353173956515, 0.17559793168985668, 0.01913298964500426), 51: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 52: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 53: (1.3821539006476835, 0.20025918322600134, 0.01667459011077882), 54: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 55: (1.647582437547556, 0.13384230836982, 0.014032915234565735), 56: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 57: (1.2127117011155155, 0.25611049657681384, 0.014113497734069835), 58: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 59: (1.7320749330356022, 0.11730236002488194, 0.016027060151100225), 60: (1.2250884396890571, 0.2516361392022721, 0.011804008483886741), 61: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 62: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 63: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 64: (1.3524324432160792, 0.20923663597226802, 0.013714814186096214), 65: (1.207983408257259, 0.2578368670848937, 0.010349023342132613), 66: (1.0696250703873664, 0.3126346616232136, 0.011452090740203869), 67: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 68: (1.1524813196613046, 0.2788147386507328, 0.011603665351867687), 69: (1.137393340057812, 0.2847476458175558, 0.012291699647903442), 70: (1.2444432373122565, 0.24476752527845105, 0.01334513425827022), 71: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 72: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 73: (1.0440114176934014, 0.3237100492876409, 0.010314714908599809), 74: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 75: (1.2216244908644383, 0.25288191962479567, 0.009443157911300626), 76: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 77: (1.1135252547197316, 0.29433701718942434, 0.010051617026329063), 78: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 79: (1.2417819182356142, 0.24570273655767255, 0.010424494743347168), 80: (0.9263823178852398, 0.37841931834657005, 0.011364886164665244), 81: (1.4483064567465782, 0.18145675239684195, 0.021374163031578075), 82: (1.0548361444603112, 0.3189932578383086, 0.012901440262794495), 83: (0.5076225527682159, 0.6239236805496233, 0.008730635046958923), 84: (2.894407698951993, 0.017756339567876013, 0.04467357993125917), 85: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 86: (2.894407698951993, 0.017756339567876013, 0.04467357993125917), 87: (3.013370645684239, 0.014635682938161457, 0.048122626543045055), 88: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 89: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 90: (2.1060548842956632, 0.06448146155403724, 0.03805774450302124), 91: (2.2033485657290766, 0.05503917768304185, 0.03558392226696011), 92: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 93: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 94: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 95: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 96: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 97: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 98: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 99: (2.446681986732943, 0.03695732707497869, 0.03842214047908782), 100: (2.6467234192687865, 0.026618778820155658, 0.04456419050693511), 101: (2.408500757573574, 0.03934563074886143, 0.041527169942855824), 102: (2.4687665745111733, 0.03564232369271045, 0.04298895597457886), 103: (2.3990178598115888, 0.0399621853187128, 0.04548675417900083), 104: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 105: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 106: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 107: (0.6103225142216757, 0.5567434719512216, 0.005341175198555037), 108: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 109: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 110: (0.6937772128886472, 0.5053323531323073, 0.00855525135993962), 111: (0.5108092118769949, 0.6217780356510954, 0.005040934681892428), 112: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 113: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 114: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 115: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 116: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 117: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 118: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 119: (0.2727569654588054, 0.7911950651209458, 0.0022219240665435347), 120: (0.42626033178645306, 0.6799318623249235, 0.003239944577217102), 121: (0.6279777912011617, 0.5456211375801904, 0.005877128243446328), 122: (0.5451556351158041, 0.598894181824965, 0.004873842000961304), 123: (0.662451935092854, 0.5242811499325539, 0.007979604601860035), 124: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 125: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 126: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 127: (0.9574801818219801, 0.36333689627181376, 0.012267100811004683), 128: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 129: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 130: (1.0147349169071984, 0.3367336492726059, 0.013134312629699718), 131: (0.7447867858634004, 0.47539841641828073, 0.010077017545700062), 132: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 133: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 134: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 135: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 136: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 137: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 138: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 139: (1.395036536895664, 0.1964714564670174, 0.00986538827419281), 140: (0.9961169162223953, 0.3452193196021255, 0.008227831125259488), 141: (1.6477864903052022, 0.133799927532065, 0.010629636049270696), 142: (2.0859573671878606, 0.06661865209177747, 0.009048116207122758), 143: (1.732440577383377, 0.11723504124284848, 0.011292290687561046)}, 'original_noun_phrase': {0: (16.221565546438303, 1.7798935688953818e-54, 0.017407106111447024)}}\n",
            "EI_joy dann named\n",
            "named {0: (1.5746176509057868, 0.13184926305204134, 0.003214170038700115), 1: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 2: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 3: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 4: (1.1920304712684011, 0.2479257359003706, 0.0031599968671798817), 5: (0.5200779651273902, 0.6090163247364517, 0.001845601201057423), 6: (1.206054210112573, 0.2426008625369625, 0.0025039613246917725), 7: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 8: (0.47155000450879714, 0.6426177506381161, 0.001234653592109669), 9: (1.2247327050016992, 0.23564408602523226, 0.002675969898700725), 10: (1.201259793210151, 0.24441146928813237, 0.0026369109749793673), 11: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 12: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 13: (1.54360541965144, 0.13917635992975658, 0.003611910343170155), 14: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 15: (1.0152075004772516, 0.3227612405256235, 0.002449846267700173), 16: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 17: (1.3775253617312382, 0.18436132922410667, 0.002908635139465321), 18: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 19: (0.6885673381448015, 0.4994176755029808, 0.0023790627717971136), 20: (1.046287630939748, 0.3085571843597269, 0.0037065461277961953), 21: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 22: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 23: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 24: (1.000591330100209, 0.32959808234996657, 0.004472000896930728), 25: (0.6392760200676714, 0.5302760130804316, 0.0023487478494644276), 26: (0.9890859229571602, 0.3350506730103011, 0.002826303243637085), 27: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 28: (0.4683580328936644, 0.6448569654083677, 0.0015384703874588013), 29: (1.0974643109697455, 0.2861529009170869, 0.004083611071109772), 30: (0.772173717216145, 0.4495103347018585, 0.0024793311953544284), 31: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 32: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 33: (1.2597847802601188, 0.22300226077128277, 0.003996308147907246), 34: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 35: (0.8985685678690583, 0.38012915270937386, 0.0037279784679413175), 36: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 37: (0.8712614738398252, 0.3944873573023582, 0.00401660650968555), 38: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 39: (0.780226275068901, 0.4448696063686006, 0.003804713487625122), 40: (1.3736051264808316, 0.18555597501671686, 0.003539626300334886), 41: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 42: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 43: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 44: (1.758097856772768, 0.09482974069465373, 0.004365961253643025), 45: (0.6825310836190819, 0.5031405378657355, 0.0013693228363990673), 46: (1.2041798717946441, 0.24330748797831458, 0.003801923990249645), 47: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 48: (0.695622925269803, 0.49508624427654435, 0.001280762255191803), 49: (1.8432428301415822, 0.08095054192318862, 0.004096028208732561), 50: (1.2878114195851305, 0.21327659581953656, 0.0027184084057808033), 51: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 52: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 53: (1.1744685509945894, 0.25471809302092624, 0.0031597867608070263), 54: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 55: (1.2757436705880303, 0.21742302360071808, 0.0034473687410354614), 56: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 57: (1.1007890922782775, 0.2847394620571734, 0.0032403737306594405), 58: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 59: (1.1131175543418497, 0.279542918384095, 0.0035638988018036333), 60: (0.1915994441670849, 0.8500887228278105, 0.0004647448658943176), 61: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 62: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 63: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 64: (0.3236575634047364, 0.7497327770576242, 0.0005711078643799161), 65: (-0.02577210760347539, 0.97970787065479, -4.477500915528454e-05), 66: (0.0041640076295935205, 0.996721022131268, 8.733570575680805e-06), 67: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 68: (0.07551291390310966, 0.9405960573805252, 0.0001857444643973971), 69: (0.24132461775516534, 0.8118887345903755, 0.0005281269550323486), 70: (-0.06500013956140589, 0.9488529272401054, -0.00013585984706876442), 71: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 72: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 73: (0.10544986645382569, 0.9171241677134373, 0.00019060075283050537), 74: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 75: (0.23750506615096562, 0.8148072250609198, 0.0004985958337783702), 76: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 77: (0.10373080844069092, 0.9184700871410466, 0.00020903944969175026), 78: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 79: (0.27946346039111863, 0.782908671272175, 0.0005801826715469693), 80: (1.0969986685495292, 0.28635126411165845, 0.002052229642868053), 81: (-0.18077443392733775, 0.8584589639021618, -0.0003736421465873607), 82: (0.5641841057069512, 0.579228588538415, 0.0013973131775855685), 83: (-0.40957301815228875, 0.6866998398235482, -0.001840552687644964), 84: (18.638280067115037, 1.142972942234969e-13, 0.04939173609018327), 85: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 86: (18.638280067115037, 1.142972942234969e-13, 0.04939173609018327), 87: (11.157918897342645, 8.763492435193886e-10, 0.044972747564315796), 88: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 89: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 90: (4.726836066232786, 0.00014676288203370317, 0.017240795493125932), 91: (3.9594122265237783, 0.0008406267631679267, 0.01665395945310591), 92: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 93: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 94: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 95: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 96: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 97: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 98: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 99: (2.9341613190767846, 0.008514053732936375, 0.017832455039024364), 100: (3.5015153952942675, 0.0023871232577999447, 0.02095486819744108), 101: (6.307254546414991, 4.705343143619616e-06, 0.035032862424850486), 102: (4.06992614386743, 0.0006530922463351053, 0.027826881408691384), 103: (3.5060261182862744, 0.002362809989338763, 0.02212550938129426), 104: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 105: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 106: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 107: (0.3758450788003259, 0.71119563930838, 0.0008998572826385554), 108: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 109: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 110: (0.6371552716925805, 0.531626834397389, 0.001433078944683086), 111: (0.4111837667433367, 0.6855385756628762, 0.0012284129858016746), 112: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 113: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 114: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 115: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 116: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 117: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 118: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 119: (-0.12399070660995282, 0.9026249668448508, -0.0003284260630607161), 120: (0.17936267587213414, 0.8595518712321037, 0.0005316376686095858), 121: (0.15088294245970307, 0.8816584474742779, 0.0005153030157089011), 122: (0.114409037303277, 0.9101139217444596, 0.0004038020968437084), 123: (0.3521864721013083, 0.7285746708127132, 0.0013536572456359641), 124: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 125: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 126: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 127: (0.12741347758222332, 0.899951987034084, 0.00035066604614258923), 128: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 129: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 130: (-0.04000747782820315, 0.9685045710815892, -0.0001647800207138228), 131: (-0.5102105433586565, 0.6157801497262663, -0.0024284034967422263), 132: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 133: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 134: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 135: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 136: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 137: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 138: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 139: (0.7164851927341819, 0.482406587480824, 0.0013258129358292292), 140: (-0.2512823221189932, 0.8042934998481082, -0.0005313009023666382), 141: (0.6968525837654181, 0.49433358037327757, 0.001538360118865989), 142: (0.27887685878877855, 0.7833520692090833, 0.0006208002567290594), 143: (0.37910217570356103, 0.7088154508059843, 0.0008354976773262135)}\n",
            "EI_joy dann noun_phrase\n",
            "noun_phrase {0: (1.1897449735598769, 0.2645847721167414, 0.017573592066764843), 1: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 2: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 3: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 4: (1.3464183864381682, 0.2110942468276465, 0.015633621811866727), 5: (1.605223219851201, 0.14290750452770626, 0.01092868745326997), 6: (0.8667449107217655, 0.40859471483042686, 0.014970198273658752), 7: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 8: (0.9424232596385866, 0.37058368822804644, 0.010049128532409635), 9: (1.0004200719366165, 0.3432439337875035, 0.009594476222991921), 10: (1.1263998455452213, 0.2891332885974849, 0.017584273219108604), 11: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 12: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 13: (0.9933854623068189, 0.346477613185726, 0.012622731924056996), 14: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 15: (1.6032436342688692, 0.1433443742741506, 0.009411555528640703), 16: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 17: (1.158788151635794, 0.2763641563963549, 0.011333853006362915), 18: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 19: (1.9863680270079935, 0.0782530758132061, 0.010486459732055642), 20: (0.9766771897685304, 0.35424916066525003, 0.016275402903556835), 21: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 22: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 23: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 24: (1.0100004178142068, 0.3388764893485081, 0.01473958194255831), 25: (0.5924654780939295, 0.5681237185281386, 0.0063070297241210604), 26: (1.306671575638153, 0.22372519722970752, 0.016701975464820884), 27: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 28: (1.1269562258989507, 0.2889100552558865, 0.012668097019195523), 29: (0.9463363969787679, 0.3686902544162144, 0.010827291011810292), 30: (0.9142781364825604, 0.3844107065168235, 0.013658565282821666), 31: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 32: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 33: (0.7457949494362639, 0.4748184715790391, 0.011152601242065463), 34: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 35: (1.0477120521213235, 0.3220915580067869, 0.010223370790481523), 36: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 37: (0.8485865491571907, 0.4181090388139137, 0.009635287523269587), 38: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 39: (0.9280528929400361, 0.37759773112532347, 0.008633512258529707), 40: (1.3766956038409206, 0.20188278038668978, 0.020707631111145042), 41: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 42: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 43: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 44: (1.5073811767069394, 0.16598281928998024, 0.020500892400741544), 45: (1.6723040652145509, 0.12879548325218018, 0.012533774971962008), 46: (1.4666378051409232, 0.1765254005834594, 0.018508574366569497), 47: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 48: (1.6064828699659295, 0.14263013911645447, 0.018048653006553617), 49: (1.5463738395386384, 0.1564169887486045, 0.01534509062767031), 50: (1.3981889279428554, 0.19555401474455414, 0.018730267882347107), 51: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 52: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 53: (1.4226334528464957, 0.1885642222067885, 0.018465134501457203), 54: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 55: (1.9228400604573, 0.08666129622920002, 0.016585296392440774), 56: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 57: (1.623967010507898, 0.13882998936749893, 0.016398394107818515), 58: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 59: (1.7481542345898482, 0.11437536821340766, 0.015274143218994118), 60: (1.0490359251389871, 0.32151406212143196, 0.01535017490386964), 61: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 62: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 63: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 64: (1.3047798270533912, 0.22434187708656605, 0.014936184883117654), 65: (1.317960548529843, 0.22007466973219486, 0.014754098653793346), 66: (1.2617869061939107, 0.2387443941887252, 0.01689254939556123), 67: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 68: (1.2272916074156415, 0.25084640292998533, 0.015773829817771923), 69: (1.2265172246768885, 0.2511237527348488, 0.016437518596649214), 70: (1.2861969294266826, 0.23047558902018975, 0.018531644344329856), 71: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 72: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 73: (0.9991969600185951, 0.3438045461857614, 0.012521785497665416), 74: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 75: (1.1056965806537367, 0.2975371016397301, 0.012827748060226407), 76: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 77: (0.9991693337686526, 0.34381721657381925, 0.011863422393798895), 78: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 79: (0.9921678862571092, 0.3470396146081677, 0.01198123693466191), 80: (1.8635080862094613, 0.09527941169686853, 0.021046599745750405), 81: (1.6094299972768031, 0.14198309472141932, 0.019358164072036776), 82: (1.0479494079279454, 0.3219879609813807, 0.016733840107917786), 83: (0.35083009179502844, 0.7337877965758419, 0.009211620688438421), 84: (3.5329053514837865, 0.006384768927710425, 0.06510847508907319), 85: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 86: (3.5329053514837865, 0.006384768927710425, 0.06510847508907319), 87: (2.6267171046948254, 0.027506310672767986, 0.06364955008029938), 88: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 89: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 90: (1.5052862576449235, 0.16651107699783177, 0.03877555429935453), 91: (1.3697137814615197, 0.2039759430223708, 0.03738438487052914), 92: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 93: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 94: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 95: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 96: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 97: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 98: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 99: (1.1970975045130632, 0.2618474505146704, 0.03918782472610477), 100: (1.42327253408557, 0.18838441099826245, 0.04804551005363461), 101: (1.764074502143604, 0.11154378618190322, 0.05211286842823026), 102: (1.5055933118825775, 0.16643355723289166, 0.053024378418922435), 103: (1.4646924927840583, 0.17704310755118133, 0.04850500524044038), 104: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 105: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 106: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 107: (0.6855236510909435, 0.5102836227256967, 0.009697437286376953), 108: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 109: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 110: (0.7425844256789769, 0.47666690700989356, 0.010901454091072071), 111: (0.6136733515292012, 0.5546225904841315, 0.009904259443283103), 112: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 113: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 114: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 115: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 116: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 117: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 118: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 119: (0.39073274802622565, 0.7050828077030289, 0.005680873990058899), 120: (0.5407255398252496, 0.6018205539470665, 0.008237031102180492), 121: (0.5927683310151038, 0.5679296256557602, 0.009882375597953796), 122: (0.5545378575092976, 0.5927217120499975, 0.009223684668540955), 123: (0.6759325534544358, 0.5160745360370119, 0.01272856295108793), 124: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 125: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 126: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 127: (0.7646227956182511, 0.4640717712710538, 0.01223587095737455), 128: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 129: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 130: (0.5039502108633557, 0.6264009988096672, 0.011995682120323148), 131: (0.4691593497211488, 0.6501130732324156, 0.009293520450592008), 132: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 133: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 134: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 135: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 136: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 137: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 138: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 139: (0.5229328421333339, 0.6136495085651087, 0.004523599147796653), 140: (0.3408077548751734, 0.7410690126275548, 0.0035796403884887917), 141: (0.7773226886502699, 0.4569132428552066, 0.005795419216156006), 142: (0.7700970896635472, 0.4609771275072918, 0.004961055517196611), 143: (1.0665779213271587, 0.31393674043831626, 0.008490797877311718)}\n",
            "EI_joy dann original_noun_phrase\n",
            "original_noun_phrase {0: (14.357576138134364, 8.680784544688741e-44, 0.01910032100147674)}\n",
            "dann original_noun_phrase {'named': {0: (1.5746176509057868, 0.13184926305204134, 0.003214170038700115), 1: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 2: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 3: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 4: (1.1920304712684011, 0.2479257359003706, 0.0031599968671798817), 5: (0.5200779651273902, 0.6090163247364517, 0.001845601201057423), 6: (1.206054210112573, 0.2426008625369625, 0.0025039613246917725), 7: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 8: (0.47155000450879714, 0.6426177506381161, 0.001234653592109669), 9: (1.2247327050016992, 0.23564408602523226, 0.002675969898700725), 10: (1.201259793210151, 0.24441146928813237, 0.0026369109749793673), 11: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 12: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 13: (1.54360541965144, 0.13917635992975658, 0.003611910343170155), 14: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 15: (1.0152075004772516, 0.3227612405256235, 0.002449846267700173), 16: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 17: (1.3775253617312382, 0.18436132922410667, 0.002908635139465321), 18: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 19: (0.6885673381448015, 0.4994176755029808, 0.0023790627717971136), 20: (1.046287630939748, 0.3085571843597269, 0.0037065461277961953), 21: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 22: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 23: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 24: (1.000591330100209, 0.32959808234996657, 0.004472000896930728), 25: (0.6392760200676714, 0.5302760130804316, 0.0023487478494644276), 26: (0.9890859229571602, 0.3350506730103011, 0.002826303243637085), 27: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 28: (0.4683580328936644, 0.6448569654083677, 0.0015384703874588013), 29: (1.0974643109697455, 0.2861529009170869, 0.004083611071109772), 30: (0.772173717216145, 0.4495103347018585, 0.0024793311953544284), 31: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 32: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 33: (1.2597847802601188, 0.22300226077128277, 0.003996308147907246), 34: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 35: (0.8985685678690583, 0.38012915270937386, 0.0037279784679413175), 36: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 37: (0.8712614738398252, 0.3944873573023582, 0.00401660650968555), 38: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 39: (0.780226275068901, 0.4448696063686006, 0.003804713487625122), 40: (1.3736051264808316, 0.18555597501671686, 0.003539626300334886), 41: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 42: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 43: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 44: (1.758097856772768, 0.09482974069465373, 0.004365961253643025), 45: (0.6825310836190819, 0.5031405378657355, 0.0013693228363990673), 46: (1.2041798717946441, 0.24330748797831458, 0.003801923990249645), 47: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 48: (0.695622925269803, 0.49508624427654435, 0.001280762255191803), 49: (1.8432428301415822, 0.08095054192318862, 0.004096028208732561), 50: (1.2878114195851305, 0.21327659581953656, 0.0027184084057808033), 51: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 52: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 53: (1.1744685509945894, 0.25471809302092624, 0.0031597867608070263), 54: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 55: (1.2757436705880303, 0.21742302360071808, 0.0034473687410354614), 56: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 57: (1.1007890922782775, 0.2847394620571734, 0.0032403737306594405), 58: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 59: (1.1131175543418497, 0.279542918384095, 0.0035638988018036333), 60: (0.1915994441670849, 0.8500887228278105, 0.0004647448658943176), 61: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 62: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 63: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 64: (0.3236575634047364, 0.7497327770576242, 0.0005711078643799161), 65: (-0.02577210760347539, 0.97970787065479, -4.477500915528454e-05), 66: (0.0041640076295935205, 0.996721022131268, 8.733570575680805e-06), 67: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 68: (0.07551291390310966, 0.9405960573805252, 0.0001857444643973971), 69: (0.24132461775516534, 0.8118887345903755, 0.0005281269550323486), 70: (-0.06500013956140589, 0.9488529272401054, -0.00013585984706876442), 71: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 72: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 73: (0.10544986645382569, 0.9171241677134373, 0.00019060075283050537), 74: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 75: (0.23750506615096562, 0.8148072250609198, 0.0004985958337783702), 76: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 77: (0.10373080844069092, 0.9184700871410466, 0.00020903944969175026), 78: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 79: (0.27946346039111863, 0.782908671272175, 0.0005801826715469693), 80: (1.0969986685495292, 0.28635126411165845, 0.002052229642868053), 81: (-0.18077443392733775, 0.8584589639021618, -0.0003736421465873607), 82: (0.5641841057069512, 0.579228588538415, 0.0013973131775855685), 83: (-0.40957301815228875, 0.6866998398235482, -0.001840552687644964), 84: (18.638280067115037, 1.142972942234969e-13, 0.04939173609018327), 85: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 86: (18.638280067115037, 1.142972942234969e-13, 0.04939173609018327), 87: (11.157918897342645, 8.763492435193886e-10, 0.044972747564315796), 88: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 89: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 90: (4.726836066232786, 0.00014676288203370317, 0.017240795493125932), 91: (3.9594122265237783, 0.0008406267631679267, 0.01665395945310591), 92: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 93: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 94: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 95: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 96: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 97: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 98: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 99: (2.9341613190767846, 0.008514053732936375, 0.017832455039024364), 100: (3.5015153952942675, 0.0023871232577999447, 0.02095486819744108), 101: (6.307254546414991, 4.705343143619616e-06, 0.035032862424850486), 102: (4.06992614386743, 0.0006530922463351053, 0.027826881408691384), 103: (3.5060261182862744, 0.002362809989338763, 0.02212550938129426), 104: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 105: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 106: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 107: (0.3758450788003259, 0.71119563930838, 0.0008998572826385554), 108: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 109: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 110: (0.6371552716925805, 0.531626834397389, 0.001433078944683086), 111: (0.4111837667433367, 0.6855385756628762, 0.0012284129858016746), 112: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 113: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 114: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 115: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 116: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 117: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 118: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 119: (-0.12399070660995282, 0.9026249668448508, -0.0003284260630607161), 120: (0.17936267587213414, 0.8595518712321037, 0.0005316376686095858), 121: (0.15088294245970307, 0.8816584474742779, 0.0005153030157089011), 122: (0.114409037303277, 0.9101139217444596, 0.0004038020968437084), 123: (0.3521864721013083, 0.7285746708127132, 0.0013536572456359641), 124: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 125: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 126: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 127: (0.12741347758222332, 0.899951987034084, 0.00035066604614258923), 128: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 129: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 130: (-0.04000747782820315, 0.9685045710815892, -0.0001647800207138228), 131: (-0.5102105433586565, 0.6157801497262663, -0.0024284034967422263), 132: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 133: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 134: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 135: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 136: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 137: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 138: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 139: (0.7164851927341819, 0.482406587480824, 0.0013258129358292292), 140: (-0.2512823221189932, 0.8042934998481082, -0.0005313009023666382), 141: (0.6968525837654181, 0.49433358037327757, 0.001538360118865989), 142: (0.27887685878877855, 0.7833520692090833, 0.0006208002567290594), 143: (0.37910217570356103, 0.7088154508059843, 0.0008354976773262135)}, 'noun_phrase': {0: (1.1897449735598769, 0.2645847721167414, 0.017573592066764843), 1: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 2: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 3: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 4: (1.3464183864381682, 0.2110942468276465, 0.015633621811866727), 5: (1.605223219851201, 0.14290750452770626, 0.01092868745326997), 6: (0.8667449107217655, 0.40859471483042686, 0.014970198273658752), 7: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 8: (0.9424232596385866, 0.37058368822804644, 0.010049128532409635), 9: (1.0004200719366165, 0.3432439337875035, 0.009594476222991921), 10: (1.1263998455452213, 0.2891332885974849, 0.017584273219108604), 11: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 12: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 13: (0.9933854623068189, 0.346477613185726, 0.012622731924056996), 14: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 15: (1.6032436342688692, 0.1433443742741506, 0.009411555528640703), 16: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 17: (1.158788151635794, 0.2763641563963549, 0.011333853006362915), 18: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 19: (1.9863680270079935, 0.0782530758132061, 0.010486459732055642), 20: (0.9766771897685304, 0.35424916066525003, 0.016275402903556835), 21: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 22: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 23: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 24: (1.0100004178142068, 0.3388764893485081, 0.01473958194255831), 25: (0.5924654780939295, 0.5681237185281386, 0.0063070297241210604), 26: (1.306671575638153, 0.22372519722970752, 0.016701975464820884), 27: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 28: (1.1269562258989507, 0.2889100552558865, 0.012668097019195523), 29: (0.9463363969787679, 0.3686902544162144, 0.010827291011810292), 30: (0.9142781364825604, 0.3844107065168235, 0.013658565282821666), 31: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 32: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 33: (0.7457949494362639, 0.4748184715790391, 0.011152601242065463), 34: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 35: (1.0477120521213235, 0.3220915580067869, 0.010223370790481523), 36: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 37: (0.8485865491571907, 0.4181090388139137, 0.009635287523269587), 38: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 39: (0.9280528929400361, 0.37759773112532347, 0.008633512258529707), 40: (1.3766956038409206, 0.20188278038668978, 0.020707631111145042), 41: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 42: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 43: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 44: (1.5073811767069394, 0.16598281928998024, 0.020500892400741544), 45: (1.6723040652145509, 0.12879548325218018, 0.012533774971962008), 46: (1.4666378051409232, 0.1765254005834594, 0.018508574366569497), 47: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 48: (1.6064828699659295, 0.14263013911645447, 0.018048653006553617), 49: (1.5463738395386384, 0.1564169887486045, 0.01534509062767031), 50: (1.3981889279428554, 0.19555401474455414, 0.018730267882347107), 51: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 52: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 53: (1.4226334528464957, 0.1885642222067885, 0.018465134501457203), 54: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 55: (1.9228400604573, 0.08666129622920002, 0.016585296392440774), 56: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 57: (1.623967010507898, 0.13882998936749893, 0.016398394107818515), 58: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 59: (1.7481542345898482, 0.11437536821340766, 0.015274143218994118), 60: (1.0490359251389871, 0.32151406212143196, 0.01535017490386964), 61: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 62: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 63: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 64: (1.3047798270533912, 0.22434187708656605, 0.014936184883117654), 65: (1.317960548529843, 0.22007466973219486, 0.014754098653793346), 66: (1.2617869061939107, 0.2387443941887252, 0.01689254939556123), 67: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 68: (1.2272916074156415, 0.25084640292998533, 0.015773829817771923), 69: (1.2265172246768885, 0.2511237527348488, 0.016437518596649214), 70: (1.2861969294266826, 0.23047558902018975, 0.018531644344329856), 71: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 72: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 73: (0.9991969600185951, 0.3438045461857614, 0.012521785497665416), 74: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 75: (1.1056965806537367, 0.2975371016397301, 0.012827748060226407), 76: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 77: (0.9991693337686526, 0.34381721657381925, 0.011863422393798895), 78: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 79: (0.9921678862571092, 0.3470396146081677, 0.01198123693466191), 80: (1.8635080862094613, 0.09527941169686853, 0.021046599745750405), 81: (1.6094299972768031, 0.14198309472141932, 0.019358164072036776), 82: (1.0479494079279454, 0.3219879609813807, 0.016733840107917786), 83: (0.35083009179502844, 0.7337877965758419, 0.009211620688438421), 84: (3.5329053514837865, 0.006384768927710425, 0.06510847508907319), 85: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 86: (3.5329053514837865, 0.006384768927710425, 0.06510847508907319), 87: (2.6267171046948254, 0.027506310672767986, 0.06364955008029938), 88: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 89: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 90: (1.5052862576449235, 0.16651107699783177, 0.03877555429935453), 91: (1.3697137814615197, 0.2039759430223708, 0.03738438487052914), 92: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 93: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 94: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 95: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 96: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 97: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 98: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 99: (1.1970975045130632, 0.2618474505146704, 0.03918782472610477), 100: (1.42327253408557, 0.18838441099826245, 0.04804551005363461), 101: (1.764074502143604, 0.11154378618190322, 0.05211286842823026), 102: (1.5055933118825775, 0.16643355723289166, 0.053024378418922435), 103: (1.4646924927840583, 0.17704310755118133, 0.04850500524044038), 104: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 105: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 106: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 107: (0.6855236510909435, 0.5102836227256967, 0.009697437286376953), 108: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 109: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 110: (0.7425844256789769, 0.47666690700989356, 0.010901454091072071), 111: (0.6136733515292012, 0.5546225904841315, 0.009904259443283103), 112: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 113: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 114: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 115: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 116: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 117: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 118: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 119: (0.39073274802622565, 0.7050828077030289, 0.005680873990058899), 120: (0.5407255398252496, 0.6018205539470665, 0.008237031102180492), 121: (0.5927683310151038, 0.5679296256557602, 0.009882375597953796), 122: (0.5545378575092976, 0.5927217120499975, 0.009223684668540955), 123: (0.6759325534544358, 0.5160745360370119, 0.01272856295108793), 124: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 125: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 126: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 127: (0.7646227956182511, 0.4640717712710538, 0.01223587095737455), 128: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 129: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 130: (0.5039502108633557, 0.6264009988096672, 0.011995682120323148), 131: (0.4691593497211488, 0.6501130732324156, 0.009293520450592008), 132: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 133: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 134: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 135: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 136: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 137: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 138: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 139: (0.5229328421333339, 0.6136495085651087, 0.004523599147796653), 140: (0.3408077548751734, 0.7410690126275548, 0.0035796403884887917), 141: (0.7773226886502699, 0.4569132428552066, 0.005795419216156006), 142: (0.7700970896635472, 0.4609771275072918, 0.004961055517196611), 143: (1.0665779213271587, 0.31393674043831626, 0.008490797877311718)}, 'original_noun_phrase': {0: (14.357576138134364, 8.680784544688741e-44, 0.01910032100147674)}}\n",
            "EI_joy dann original_noun_phrase {'non_dann': {'named': {0: (0.927130428671265, 0.3654871282317217, 0.0019718199968338235), 1: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 2: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 3: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 4: (1.2531693205122543, 0.22534723569449014, 0.0024197280406951793), 5: (1.3174986525278145, 0.2033387726702804, 0.002490296959877014), 6: (1.2436306837720958, 0.22876181256660205, 0.0026371866464614535), 7: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 8: (1.2518942904312103, 0.22580137345216844, 0.0027422189712524303), 9: (1.3042185510974138, 0.20773840637308943, 0.0022828787565231656), 10: (1.327117228975429, 0.2001981932160694, 0.002530188858509086), 11: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 12: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 13: (1.7428398231497895, 0.09752375837723172, 0.0035621538758278115), 14: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 15: (1.3023021240927164, 0.20837942480472954, 0.0024586543440818787), 16: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 17: (1.4053663411093298, 0.17605325521431578, 0.003052820265293077), 18: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 19: (1.334510768970373, 0.1978102185004114, 0.0025804668664932695), 20: (0.9648078850354396, 0.3467614634674251, 0.003345428407192208), 21: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 22: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 23: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 24: (1.1873555411945744, 0.24972033392870072, 0.004133212566375721), 25: (1.0341270734019747, 0.31406072117995326, 0.0029740095138549583), 26: (0.9954487568479664, 0.33202751012987175, 0.003394991159439087), 27: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 28: (1.0148970597038753, 0.3229054061666836, 0.0026780590415000916), 29: (1.1008968462715054, 0.2846937389346681, 0.0031256809830665366), 30: (0.8866396932483286, 0.38635826577565757, 0.0030403494834900235), 31: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 32: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 33: (1.1269897254497583, 0.2737793640146807, 0.0042918160557746665), 34: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 35: (1.2727763621511254, 0.21845211311102822, 0.0037619456648826377), 36: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 37: (1.1800763632204914, 0.2525341357443367, 0.003792211413383484), 38: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 39: (1.5784224237170579, 0.13097295691677174, 0.004382364451885223), 40: (0.3706764954299417, 0.7149789285371184, 0.001211306452751193), 41: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 42: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 43: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 44: (0.9510429695683896, 0.35352483469689244, 0.0026509761810302734), 45: (0.20321680659982658, 0.841126104553226, 0.00047275424003601074), 46: (0.8183434249209005, 0.42330630820690995, 0.002901148796081554), 47: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 48: (0.4200164302561195, 0.6791850216458598, 0.0011544987559318765), 49: (0.830738390994025, 0.4164389892854148, 0.0021710038185119296), 50: (0.6555897233779486, 0.5199482142674257, 0.0017548486590385215), 51: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 52: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 53: (0.6798450782278772, 0.5048022137549548, 0.0026078701019287), 54: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 55: (0.6442546163386427, 0.5271122871918197, 0.0016197532415390126), 56: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 57: (0.89902941782725, 0.3798898471765356, 0.0022531986236571933), 58: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 59: (1.0190850405952432, 0.3209643699571856, 0.002433972060680356), 60: (-0.16750666698169162, 0.868741420067475, -0.00022678077220916748), 61: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 62: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 63: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 64: (0.13922735134295997, 0.8907356759180701, 0.00022866278886796154), 65: (-0.3883816998147466, 0.702051151937652, -0.0005484327673912492), 66: (0.02193939548499107, 0.9827250708715176, 3.829300403596081e-05), 67: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 68: (0.002459680991383545, 0.9980631026103626, 5.003809928849634e-06), 69: (0.07916098447993353, 0.9377323781415308, 0.00017678886651989467), 70: (0.006313327585592243, 0.995028544256352, 1.0462105274222644e-05), 71: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 72: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 73: (-0.01168985920938709, 0.9907949286564823, -1.865178346632801e-05), 74: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 75: (-0.014069772953011189, 0.9889210067818506, -2.6115775108359607e-05), 76: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 77: (0.09061458634754754, 0.9287473156172278, 0.00014399439096451916), 78: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 79: (0.25524464813261455, 0.8012767347316481, 0.00042962878942487404), 80: (0.4944255105613076, 0.6266738386933018, 0.0017054766416549905), 81: (0.4015194111857567, 0.6925180059712798, 0.0017325416207313316), 82: (1.4284660577708723, 0.16939123354105534, 0.002391044795513164), 83: (-0.18513527348539655, 0.8550848963311205, -0.00041247904300689697), 84: (13.623805562448862, 2.953336520776732e-11, 0.03214100897312164), 85: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 86: (13.623805562448862, 2.953336520776732e-11, 0.03214100897312164), 87: (15.613965963745777, 2.712259102454546e-12, 0.03620264828205105), 88: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 89: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 90: (10.718198861612956, 1.7025185784121546e-09, 0.022274567186832434), 91: (10.278879605250857, 3.372997728430722e-09, 0.020667591691017145), 92: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 93: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 94: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 95: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 96: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 97: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 98: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 99: (17.278907545928526, 4.4642351018466344e-13, 0.0325523316860199), 100: (16.329994079427188, 1.2240473252310238e-12, 0.033421632647514354), 101: (11.532314218673289, 5.055202430326208e-10, 0.03211134076118466), 102: (12.56817891732165, 1.1814773159409936e-10, 0.031706155836582206), 103: (11.170516588095923, 8.600819301000773e-10, 0.03029073625802997), 104: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 105: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 106: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 107: (0.614321461231838, 0.5462894812398227, 0.0005189865827560092), 108: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 109: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 110: (0.5925784878290913, 0.5604504593874424, 0.0006949156522750854), 111: (0.8340101957405962, 0.41463818475912606, 0.000979569554328874), 112: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 113: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 114: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 115: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 116: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 117: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 118: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 119: (-0.7430493506231183, 0.46654073581996913, -0.0005963727831840293), 120: (-0.26132919939103477, 0.7966504313686013, -0.00017389357089997448), 121: (0.31679291549251387, 0.7548551317327175, 0.0003045767545700184), 122: (0.07705892819510853, 0.939382356282123, 7.993727922439575e-05), 123: (0.8649705897222905, 0.39784479313516674, 0.0009871751070022472), 124: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 125: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 126: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 127: (0.7967294372923546, 0.4354514862913137, 0.002296024560928367), 128: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 129: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 130: (0.06481738391512246, 0.9489965212009155, 0.00022808462381362915), 131: (-0.1603517261507232, 0.8742965632691216, -0.00047191381454469994), 132: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 133: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 134: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 135: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 136: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 137: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 138: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 139: (1.6146163075395017, 0.12287847791729747, 0.002999137341976099), 140: (0.305663680124198, 0.7631844067937561, 0.00042246878147123024), 141: (1.177061576252688, 0.2537064835219378, 0.0021041855216026306), 142: (1.292512658049242, 0.21167805377758545, 0.002440388500690449), 143: (0.6976292430085975, 0.4938585345161479, 0.0015214845538139565)}, 'noun_phrase': {0: (1.0442462382617037, 0.3236071648886541, 0.014378252625465382), 1: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 2: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 3: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 4: (1.068156744919508, 0.3132615708445877, 0.014390206336975131), 5: (1.3491836965086035, 0.21023837259357941, 0.012973392009735074), 6: (0.9708868300900558, 0.3569723962343001, 0.013869673013687134), 7: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 8: (1.045011391917099, 0.32327209352648073, 0.011891117691993691), 9: (0.9160655739855799, 0.38352168605558257, 0.011061030626296986), 10: (1.0883179518932313, 0.3047382407508142, 0.016650095582008362), 11: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 12: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 13: (0.9440753908926197, 0.3697834154061227, 0.012642705440521207), 14: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 15: (1.165337749743547, 0.2738375081624465, 0.00994248390197755), 16: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 17: (1.133729001318476, 0.28620356708756717, 0.012761783599853471), 18: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 19: (1.2184084745740587, 0.2540430397974921, 0.010925072431564375), 20: (1.1388626197105938, 0.2841655220278896, 0.01670609414577484), 21: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 22: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 23: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 24: (1.1641932803103003, 0.27427767268470876, 0.015377753973007224), 25: (1.1262662246092308, 0.2891869208633998, 0.009866386651992798), 26: (1.2568708386939498, 0.24043908581333848, 0.01549505889415742), 27: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 28: (1.2433798432522831, 0.24514085951029313, 0.011981427669525146), 29: (1.3581336617829196, 0.207488489275405, 0.01307763159275055), 30: (1.1808753318955496, 0.267917640443583, 0.01745899319648747), 31: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 32: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 33: (1.0007150294443807, 0.3431088428374259, 0.014056950807571411), 34: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 35: (1.4016136397818648, 0.1945614940081961, 0.011961144208908037), 36: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 37: (0.9822887533739848, 0.35162473484834944, 0.010575699806213368), 38: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 39: (1.4805615219909143, 0.17285835724538356, 0.012446621060371421), 40: (1.336733785263309, 0.21411498289476213, 0.01734680533409122), 41: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 42: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 43: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 44: (1.4575218114632529, 0.17896290802183798, 0.018263766169548057), 45: (1.4098880955745712, 0.19218133050503033, 0.010134425759315446), 46: (1.2970440402707297, 0.2268784565464511, 0.01639994382858273), 47: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 48: (1.5330357203201004, 0.15963270659450096, 0.0149380803108215), 49: (1.4428694090638343, 0.18294225463509742, 0.015479716658592213), 50: (1.4701353173956515, 0.17559793168985668, 0.01913298964500426), 51: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 52: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 53: (1.3821539006476835, 0.20025918322600134, 0.01667459011077882), 54: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 55: (1.647582437547556, 0.13384230836982, 0.014032915234565735), 56: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 57: (1.2127117011155155, 0.25611049657681384, 0.014113497734069835), 58: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 59: (1.7320749330356022, 0.11730236002488194, 0.016027060151100225), 60: (1.2250884396890571, 0.2516361392022721, 0.011804008483886741), 61: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 62: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 63: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 64: (1.3524324432160792, 0.20923663597226802, 0.013714814186096214), 65: (1.207983408257259, 0.2578368670848937, 0.010349023342132613), 66: (1.0696250703873664, 0.3126346616232136, 0.011452090740203869), 67: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 68: (1.1524813196613046, 0.2788147386507328, 0.011603665351867687), 69: (1.137393340057812, 0.2847476458175558, 0.012291699647903442), 70: (1.2444432373122565, 0.24476752527845105, 0.01334513425827022), 71: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 72: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 73: (1.0440114176934014, 0.3237100492876409, 0.010314714908599809), 74: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 75: (1.2216244908644383, 0.25288191962479567, 0.009443157911300626), 76: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 77: (1.1135252547197316, 0.29433701718942434, 0.010051617026329063), 78: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 79: (1.2417819182356142, 0.24570273655767255, 0.010424494743347168), 80: (0.9263823178852398, 0.37841931834657005, 0.011364886164665244), 81: (1.4483064567465782, 0.18145675239684195, 0.021374163031578075), 82: (1.0548361444603112, 0.3189932578383086, 0.012901440262794495), 83: (0.5076225527682159, 0.6239236805496233, 0.008730635046958923), 84: (2.894407698951993, 0.017756339567876013, 0.04467357993125917), 85: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 86: (2.894407698951993, 0.017756339567876013, 0.04467357993125917), 87: (3.013370645684239, 0.014635682938161457, 0.048122626543045055), 88: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 89: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 90: (2.1060548842956632, 0.06448146155403724, 0.03805774450302124), 91: (2.2033485657290766, 0.05503917768304185, 0.03558392226696011), 92: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 93: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 94: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 95: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 96: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 97: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 98: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 99: (2.446681986732943, 0.03695732707497869, 0.03842214047908782), 100: (2.6467234192687865, 0.026618778820155658, 0.04456419050693511), 101: (2.408500757573574, 0.03934563074886143, 0.041527169942855824), 102: (2.4687665745111733, 0.03564232369271045, 0.04298895597457886), 103: (2.3990178598115888, 0.0399621853187128, 0.04548675417900083), 104: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 105: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 106: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 107: (0.6103225142216757, 0.5567434719512216, 0.005341175198555037), 108: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 109: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 110: (0.6937772128886472, 0.5053323531323073, 0.00855525135993962), 111: (0.5108092118769949, 0.6217780356510954, 0.005040934681892428), 112: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 113: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 114: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 115: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 116: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 117: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 118: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 119: (0.2727569654588054, 0.7911950651209458, 0.0022219240665435347), 120: (0.42626033178645306, 0.6799318623249235, 0.003239944577217102), 121: (0.6279777912011617, 0.5456211375801904, 0.005877128243446328), 122: (0.5451556351158041, 0.598894181824965, 0.004873842000961304), 123: (0.662451935092854, 0.5242811499325539, 0.007979604601860035), 124: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 125: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 126: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 127: (0.9574801818219801, 0.36333689627181376, 0.012267100811004683), 128: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 129: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 130: (1.0147349169071984, 0.3367336492726059, 0.013134312629699718), 131: (0.7447867858634004, 0.47539841641828073, 0.010077017545700062), 132: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 133: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 134: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 135: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 136: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 137: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 138: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 139: (1.395036536895664, 0.1964714564670174, 0.00986538827419281), 140: (0.9961169162223953, 0.3452193196021255, 0.008227831125259488), 141: (1.6477864903052022, 0.133799927532065, 0.010629636049270696), 142: (2.0859573671878606, 0.06661865209177747, 0.009048116207122758), 143: (1.732440577383377, 0.11723504124284848, 0.011292290687561046)}, 'original_noun_phrase': {0: (16.221565546438303, 1.7798935688953818e-54, 0.017407106111447024)}}, 'dann': {'named': {0: (1.5746176509057868, 0.13184926305204134, 0.003214170038700115), 1: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 2: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 3: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 4: (1.1920304712684011, 0.2479257359003706, 0.0031599968671798817), 5: (0.5200779651273902, 0.6090163247364517, 0.001845601201057423), 6: (1.206054210112573, 0.2426008625369625, 0.0025039613246917725), 7: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 8: (0.47155000450879714, 0.6426177506381161, 0.001234653592109669), 9: (1.2247327050016992, 0.23564408602523226, 0.002675969898700725), 10: (1.201259793210151, 0.24441146928813237, 0.0026369109749793673), 11: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 12: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 13: (1.54360541965144, 0.13917635992975658, 0.003611910343170155), 14: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 15: (1.0152075004772516, 0.3227612405256235, 0.002449846267700173), 16: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 17: (1.3775253617312382, 0.18436132922410667, 0.002908635139465321), 18: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 19: (0.6885673381448015, 0.4994176755029808, 0.0023790627717971136), 20: (1.046287630939748, 0.3085571843597269, 0.0037065461277961953), 21: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 22: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 23: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 24: (1.000591330100209, 0.32959808234996657, 0.004472000896930728), 25: (0.6392760200676714, 0.5302760130804316, 0.0023487478494644276), 26: (0.9890859229571602, 0.3350506730103011, 0.002826303243637085), 27: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 28: (0.4683580328936644, 0.6448569654083677, 0.0015384703874588013), 29: (1.0974643109697455, 0.2861529009170869, 0.004083611071109772), 30: (0.772173717216145, 0.4495103347018585, 0.0024793311953544284), 31: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 32: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 33: (1.2597847802601188, 0.22300226077128277, 0.003996308147907246), 34: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 35: (0.8985685678690583, 0.38012915270937386, 0.0037279784679413175), 36: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 37: (0.8712614738398252, 0.3944873573023582, 0.00401660650968555), 38: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 39: (0.780226275068901, 0.4448696063686006, 0.003804713487625122), 40: (1.3736051264808316, 0.18555597501671686, 0.003539626300334886), 41: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 42: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 43: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 44: (1.758097856772768, 0.09482974069465373, 0.004365961253643025), 45: (0.6825310836190819, 0.5031405378657355, 0.0013693228363990673), 46: (1.2041798717946441, 0.24330748797831458, 0.003801923990249645), 47: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 48: (0.695622925269803, 0.49508624427654435, 0.001280762255191803), 49: (1.8432428301415822, 0.08095054192318862, 0.004096028208732561), 50: (1.2878114195851305, 0.21327659581953656, 0.0027184084057808033), 51: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 52: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 53: (1.1744685509945894, 0.25471809302092624, 0.0031597867608070263), 54: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 55: (1.2757436705880303, 0.21742302360071808, 0.0034473687410354614), 56: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 57: (1.1007890922782775, 0.2847394620571734, 0.0032403737306594405), 58: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 59: (1.1131175543418497, 0.279542918384095, 0.0035638988018036333), 60: (0.1915994441670849, 0.8500887228278105, 0.0004647448658943176), 61: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 62: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 63: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 64: (0.3236575634047364, 0.7497327770576242, 0.0005711078643799161), 65: (-0.02577210760347539, 0.97970787065479, -4.477500915528454e-05), 66: (0.0041640076295935205, 0.996721022131268, 8.733570575680805e-06), 67: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 68: (0.07551291390310966, 0.9405960573805252, 0.0001857444643973971), 69: (0.24132461775516534, 0.8118887345903755, 0.0005281269550323486), 70: (-0.06500013956140589, 0.9488529272401054, -0.00013585984706876442), 71: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 72: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 73: (0.10544986645382569, 0.9171241677134373, 0.00019060075283050537), 74: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 75: (0.23750506615096562, 0.8148072250609198, 0.0004985958337783702), 76: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 77: (0.10373080844069092, 0.9184700871410466, 0.00020903944969175026), 78: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 79: (0.27946346039111863, 0.782908671272175, 0.0005801826715469693), 80: (1.0969986685495292, 0.28635126411165845, 0.002052229642868053), 81: (-0.18077443392733775, 0.8584589639021618, -0.0003736421465873607), 82: (0.5641841057069512, 0.579228588538415, 0.0013973131775855685), 83: (-0.40957301815228875, 0.6866998398235482, -0.001840552687644964), 84: (18.638280067115037, 1.142972942234969e-13, 0.04939173609018327), 85: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 86: (18.638280067115037, 1.142972942234969e-13, 0.04939173609018327), 87: (11.157918897342645, 8.763492435193886e-10, 0.044972747564315796), 88: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 89: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 90: (4.726836066232786, 0.00014676288203370317, 0.017240795493125932), 91: (3.9594122265237783, 0.0008406267631679267, 0.01665395945310591), 92: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 93: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 94: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 95: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 96: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 97: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 98: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 99: (2.9341613190767846, 0.008514053732936375, 0.017832455039024364), 100: (3.5015153952942675, 0.0023871232577999447, 0.02095486819744108), 101: (6.307254546414991, 4.705343143619616e-06, 0.035032862424850486), 102: (4.06992614386743, 0.0006530922463351053, 0.027826881408691384), 103: (3.5060261182862744, 0.002362809989338763, 0.02212550938129426), 104: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 105: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 106: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 107: (0.3758450788003259, 0.71119563930838, 0.0008998572826385554), 108: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 109: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 110: (0.6371552716925805, 0.531626834397389, 0.001433078944683086), 111: (0.4111837667433367, 0.6855385756628762, 0.0012284129858016746), 112: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 113: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 114: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 115: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 116: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 117: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 118: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 119: (-0.12399070660995282, 0.9026249668448508, -0.0003284260630607161), 120: (0.17936267587213414, 0.8595518712321037, 0.0005316376686095858), 121: (0.15088294245970307, 0.8816584474742779, 0.0005153030157089011), 122: (0.114409037303277, 0.9101139217444596, 0.0004038020968437084), 123: (0.3521864721013083, 0.7285746708127132, 0.0013536572456359641), 124: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 125: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 126: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 127: (0.12741347758222332, 0.899951987034084, 0.00035066604614258923), 128: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 129: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 130: (-0.04000747782820315, 0.9685045710815892, -0.0001647800207138228), 131: (-0.5102105433586565, 0.6157801497262663, -0.0024284034967422263), 132: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 133: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 134: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 135: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 136: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 137: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 138: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 139: (0.7164851927341819, 0.482406587480824, 0.0013258129358292292), 140: (-0.2512823221189932, 0.8042934998481082, -0.0005313009023666382), 141: (0.6968525837654181, 0.49433358037327757, 0.001538360118865989), 142: (0.27887685878877855, 0.7833520692090833, 0.0006208002567290594), 143: (0.37910217570356103, 0.7088154508059843, 0.0008354976773262135)}, 'noun_phrase': {0: (1.1897449735598769, 0.2645847721167414, 0.017573592066764843), 1: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 2: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 3: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 4: (1.3464183864381682, 0.2110942468276465, 0.015633621811866727), 5: (1.605223219851201, 0.14290750452770626, 0.01092868745326997), 6: (0.8667449107217655, 0.40859471483042686, 0.014970198273658752), 7: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 8: (0.9424232596385866, 0.37058368822804644, 0.010049128532409635), 9: (1.0004200719366165, 0.3432439337875035, 0.009594476222991921), 10: (1.1263998455452213, 0.2891332885974849, 0.017584273219108604), 11: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 12: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 13: (0.9933854623068189, 0.346477613185726, 0.012622731924056996), 14: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 15: (1.6032436342688692, 0.1433443742741506, 0.009411555528640703), 16: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 17: (1.158788151635794, 0.2763641563963549, 0.011333853006362915), 18: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 19: (1.9863680270079935, 0.0782530758132061, 0.010486459732055642), 20: (0.9766771897685304, 0.35424916066525003, 0.016275402903556835), 21: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 22: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 23: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 24: (1.0100004178142068, 0.3388764893485081, 0.01473958194255831), 25: (0.5924654780939295, 0.5681237185281386, 0.0063070297241210604), 26: (1.306671575638153, 0.22372519722970752, 0.016701975464820884), 27: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 28: (1.1269562258989507, 0.2889100552558865, 0.012668097019195523), 29: (0.9463363969787679, 0.3686902544162144, 0.010827291011810292), 30: (0.9142781364825604, 0.3844107065168235, 0.013658565282821666), 31: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 32: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 33: (0.7457949494362639, 0.4748184715790391, 0.011152601242065463), 34: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 35: (1.0477120521213235, 0.3220915580067869, 0.010223370790481523), 36: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 37: (0.8485865491571907, 0.4181090388139137, 0.009635287523269587), 38: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 39: (0.9280528929400361, 0.37759773112532347, 0.008633512258529707), 40: (1.3766956038409206, 0.20188278038668978, 0.020707631111145042), 41: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 42: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 43: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 44: (1.5073811767069394, 0.16598281928998024, 0.020500892400741544), 45: (1.6723040652145509, 0.12879548325218018, 0.012533774971962008), 46: (1.4666378051409232, 0.1765254005834594, 0.018508574366569497), 47: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 48: (1.6064828699659295, 0.14263013911645447, 0.018048653006553617), 49: (1.5463738395386384, 0.1564169887486045, 0.01534509062767031), 50: (1.3981889279428554, 0.19555401474455414, 0.018730267882347107), 51: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 52: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 53: (1.4226334528464957, 0.1885642222067885, 0.018465134501457203), 54: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 55: (1.9228400604573, 0.08666129622920002, 0.016585296392440774), 56: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 57: (1.623967010507898, 0.13882998936749893, 0.016398394107818515), 58: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 59: (1.7481542345898482, 0.11437536821340766, 0.015274143218994118), 60: (1.0490359251389871, 0.32151406212143196, 0.01535017490386964), 61: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 62: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 63: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 64: (1.3047798270533912, 0.22434187708656605, 0.014936184883117654), 65: (1.317960548529843, 0.22007466973219486, 0.014754098653793346), 66: (1.2617869061939107, 0.2387443941887252, 0.01689254939556123), 67: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 68: (1.2272916074156415, 0.25084640292998533, 0.015773829817771923), 69: (1.2265172246768885, 0.2511237527348488, 0.016437518596649214), 70: (1.2861969294266826, 0.23047558902018975, 0.018531644344329856), 71: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 72: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 73: (0.9991969600185951, 0.3438045461857614, 0.012521785497665416), 74: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 75: (1.1056965806537367, 0.2975371016397301, 0.012827748060226407), 76: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 77: (0.9991693337686526, 0.34381721657381925, 0.011863422393798895), 78: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 79: (0.9921678862571092, 0.3470396146081677, 0.01198123693466191), 80: (1.8635080862094613, 0.09527941169686853, 0.021046599745750405), 81: (1.6094299972768031, 0.14198309472141932, 0.019358164072036776), 82: (1.0479494079279454, 0.3219879609813807, 0.016733840107917786), 83: (0.35083009179502844, 0.7337877965758419, 0.009211620688438421), 84: (3.5329053514837865, 0.006384768927710425, 0.06510847508907319), 85: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 86: (3.5329053514837865, 0.006384768927710425, 0.06510847508907319), 87: (2.6267171046948254, 0.027506310672767986, 0.06364955008029938), 88: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 89: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 90: (1.5052862576449235, 0.16651107699783177, 0.03877555429935453), 91: (1.3697137814615197, 0.2039759430223708, 0.03738438487052914), 92: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 93: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 94: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 95: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 96: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 97: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 98: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 99: (1.1970975045130632, 0.2618474505146704, 0.03918782472610477), 100: (1.42327253408557, 0.18838441099826245, 0.04804551005363461), 101: (1.764074502143604, 0.11154378618190322, 0.05211286842823026), 102: (1.5055933118825775, 0.16643355723289166, 0.053024378418922435), 103: (1.4646924927840583, 0.17704310755118133, 0.04850500524044038), 104: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 105: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 106: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 107: (0.6855236510909435, 0.5102836227256967, 0.009697437286376953), 108: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 109: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 110: (0.7425844256789769, 0.47666690700989356, 0.010901454091072071), 111: (0.6136733515292012, 0.5546225904841315, 0.009904259443283103), 112: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 113: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 114: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 115: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 116: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 117: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 118: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 119: (0.39073274802622565, 0.7050828077030289, 0.005680873990058899), 120: (0.5407255398252496, 0.6018205539470665, 0.008237031102180492), 121: (0.5927683310151038, 0.5679296256557602, 0.009882375597953796), 122: (0.5545378575092976, 0.5927217120499975, 0.009223684668540955), 123: (0.6759325534544358, 0.5160745360370119, 0.01272856295108793), 124: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 125: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 126: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 127: (0.7646227956182511, 0.4640717712710538, 0.01223587095737455), 128: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 129: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 130: (0.5039502108633557, 0.6264009988096672, 0.011995682120323148), 131: (0.4691593497211488, 0.6501130732324156, 0.009293520450592008), 132: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 133: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 134: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 135: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 136: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 137: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 138: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 139: (0.5229328421333339, 0.6136495085651087, 0.004523599147796653), 140: (0.3408077548751734, 0.7410690126275548, 0.0035796403884887917), 141: (0.7773226886502699, 0.4569132428552066, 0.005795419216156006), 142: (0.7700970896635472, 0.4609771275072918, 0.004961055517196611), 143: (1.0665779213271587, 0.31393674043831626, 0.008490797877311718)}, 'original_noun_phrase': {0: (14.357576138134364, 8.680784544688741e-44, 0.01910032100147674)}}}\n",
            "{'EI_sadness': {'non_dann': {'named': {0: (2.0761933504409655, 0.05168978321260658, 0.0029319673776626587), 1: (2.0277227426424362, 0.05684802149348772, 0.0024926632642746083), 2: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 3: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 4: (2.2230811810478572, 0.03854150183671055, 0.0023595303297042625), 5: (2.0112032728744835, 0.05870902687924929, 0.001079910993576072), 6: (2.1806215388692425, 0.0419872916426721, 0.002160996198654175), 7: (2.046200341527533, 0.05482936063760963, 0.002086377143859841), 8: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 9: (2.0647403157958726, 0.05286898145200738, 0.001883125305175759), 10: (2.1844555404719377, 0.04166494885466178, 0.001084744930267334), 11: (2.3645098043050004, 0.028853162664240845, 0.0022659599781036377), 12: (1.5175673531338714, 0.145586931890424, 0.0010704368352890126), 13: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 14: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 15: (1.850029026805524, 0.0799249809879829, 0.001398104429244984), 16: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 17: (1.3749531839684273, 0.18514447446028678, 0.000617067515850056), 18: (2.204628766848036, 0.04000585868865231, 0.00216032415628431), 19: (2.285793149570828, 0.03392496127451366, 0.0012482210993766674), 20: (2.0167980614106953, 0.05807271163781755, 0.006875231862068176), 21: (1.5988626632260454, 0.12634834921979857, 0.004185765981674194), 22: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 23: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 24: (2.0546288368370345, 0.05393017024255865, 0.00546366870403292), 25: (1.7069376241864904, 0.10412447151487597, 0.0029117465019226074), 26: (2.174048713849316, 0.04254520564317553, 0.006127482652664162), 27: (1.7169097546770051, 0.10225364327072171, 0.0035044163465499656), 28: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 29: (2.019615061442269, 0.05775467206481685, 0.005211299657821633), 30: (1.9330421431608467, 0.06827436833571221, 0.004680752754211426), 31: (1.8648201405792866, 0.07772911882885637, 0.004628813266754128), 32: (1.8797381679169154, 0.07556833471115386, 0.004890909790992715), 33: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 34: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 35: (1.8528679297404185, 0.07949934680260273, 0.003478109836578369), 36: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 37: (1.8766312627773414, 0.0760139372792857, 0.00439032316207888), 38: (1.8970254808345208, 0.07313074540923355, 0.004736888408660878), 39: (1.8984042073379923, 0.07293936206286997, 0.004009491205215432), 40: (1.20506807804393, 0.2429724396998576, 0.0018940389156341664), 41: (0.9762495415897414, 0.34120776495641103, 0.0010327056050300487), 42: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 43: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 44: (1.5638420023981912, 0.13435774633190134, 0.0014817893505096325), 45: (2.2668341225678175, 0.03526386490010458, 0.001438423991203308), 46: (1.6782686708220131, 0.10966722697059246, 0.0023178547620773537), 47: (1.310394631237746, 0.205683103215663, 0.0019497275352478027), 48: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 49: (2.0883570808072833, 0.05046347909114234, 0.0029065310955047607), 50: (0.6101184095267318, 0.5490118916170734, 0.0005402684211731068), 51: (1.1397273294732755, 0.26856489875868883, 0.0011490106582641713), 52: (1.16153920208516, 0.25980743666427664, 0.0012454301118850486), 53: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 54: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 55: (1.215851117000124, 0.23893278648489205, 0.0011919990181922802), 56: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 57: (1.151776444529962, 0.26370044738777193, 0.0009692117571830638), 58: (0.5154450303025641, 0.612187618588006, 0.00045802146196366467), 59: (-0.28249320707092423, 0.7806197754390511, -0.00021852850914000355), 60: (1.762578428279938, 0.09405096323231914, 0.001563867926597573), 61: (2.0080108707067215, 0.0590749050332689, 0.002247375249862682), 62: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 63: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 64: (1.9615926117590434, 0.06463036269842726, 0.0018849849700927623), 65: (2.1399687369678557, 0.04554794710656452, 0.0025827020406723467), 66: (1.6637068337975134, 0.11257778388879425, 0.0015673279762268288), 67: (1.5051181251873107, 0.148737187493084, 0.001271191239357039), 68: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 69: (2.150386999841326, 0.044610134602192375, 0.0023679733276367188), 70: (2.177118213515657, 0.04228382339911018, 0.0028291165828704945), 71: (1.851836012904142, 0.07965383042593033, 0.0019317537546157948), 72: (2.126903002088096, 0.04674941447782704, 0.002539578080177296), 73: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 74: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 75: (2.0987947680643417, 0.049432237599766615, 0.002668848633766152), 76: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 77: (2.0930106082908817, 0.05000132978705774, 0.002096055448055245), 78: (1.4666823812522731, 0.1588193928181782, 0.0009457528591155784), 79: (2.2137610714328213, 0.03927487568851686, 0.002266727387905121), 80: (-1.2297251309277186, 0.23381072735893818, -0.0009175911545753479), 81: (1.5770916026973085, 0.13127891211348372, 0.0027214109897613636), 82: (-1.9033034001047069, 0.07226287284710739, -0.0013722851872444153), 83: (2.0248964150987545, 0.05716261700759523, 0.0013909146189689525), 84: (0.7499794368533861, 0.46245361722017764, 0.0003775402903556935), 85: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 86: (0.7499794368533861, 0.46245361722017764, 0.0003775402903556935), 87: (0.8267080693582394, 0.4186641253443355, 0.0003182619810104259), 88: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 89: (2.2402202926119856, 0.03722558439101236, 0.002292880415916465), 90: (2.0223167206031394, 0.057451123976569936, 0.001519373059272744), 91: (2.261036607936422, 0.03568291136125278, 0.002285835146904003), 92: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 93: (2.2804006018370484, 0.0343009354037606, 0.0016447007656097412), 94: (2.245660840473207, 0.03681657862016815, 0.002138602733612105), 95: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 96: (2.1620934635302254, 0.04357737110447765, 0.002022719383239724), 97: (2.2327728738093473, 0.03779223294436587, 0.0018279343843460305), 98: (2.06902232230206, 0.05242529547033867, 0.0018476366996765248), 99: (2.1544990001643627, 0.04424485304084849, 0.001421475410461448), 100: (2.032488673768521, 0.05632105241724866, 0.001361405849456765), 101: (1.5283930400747785, 0.14289255239327028, 0.0006372183561325073), 102: (2.115812886331568, 0.047791727971066664, 0.0013906747102737649), 103: (2.007473343935898, 0.05913671110023691, 0.0012248843908310159), 104: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 105: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 106: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 107: (0.0020121291648187547, 0.9984155306688118, 1.6719102859386048e-06), 108: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 109: (1.1050655128485936, 0.2829289719820194, 0.0009811639785766157), 110: (1.1368937517123074, 0.2697184801885618, 0.0011129364371299522), 111: (1.3696790666518044, 0.18675858532937661, 0.001445630192756675), 112: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 113: (1.4954607557796367, 0.15121951132493028, 0.0015002578496933205), 114: (0.36569502516584806, 0.7186324318973236, 0.0003187239170074352), 115: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 116: (0.5292246097569755, 0.6027786487650441, 0.00042713284492490455), 117: (0.5142802482705426, 0.6129861646620078, 0.00041597187519071266), 118: (1.4696636014771989, 0.1580178641681456, 0.0013391733169555442), 119: (0.7178184135979354, 0.48160281132001537, 0.0006729722023010032), 120: (1.3948773774889318, 0.17914722026440263, 0.0013889968395233154), 121: (0.8983249623027905, 0.38025569019303307, 0.0008771687746048085), 122: (1.0055171789009922, 0.3272827294300725, 0.0010290488600731007), 123: (1.2707327346974095, 0.21916305747624937, 0.0013425901532173046), 124: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 125: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 126: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 127: (1.5217129237402338, 0.14455022513035917, 0.0019379243254661338), 128: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 129: (1.9502220733005475, 0.06606035550226323, 0.003574529290199302), 130: (2.257082874008286, 0.035971313423737696, 0.0035756126046180836), 131: (2.006714851562998, 0.05922402288379378, 0.0032355964183807817), 132: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 133: (1.5921861739093308, 0.12784357900410126, 0.0019223928451538308), 134: (1.3387098671311615, 0.19646404439327628, 0.0020498543977737205), 135: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 136: (1.0242504981564373, 0.31858165840198993, 0.0012687504291534202), 137: (2.0459741649498424, 0.05485367511894138, 0.003114047646522511), 138: (1.8147542867908093, 0.08538270044406286, 0.0027948886156082264), 139: (1.246129678593975, 0.22786341362474055, 0.001814487576484669), 140: (2.2130333231733244, 0.03933267532463067, 0.003469802439212799), 141: (1.59845966430835, 0.1264381837596209, 0.0017410978674888833), 142: (2.316546549269402, 0.03185256188279949, 0.004329253733158123), 143: (1.6008080372334728, 0.12591545072725402, 0.0018596306443214305)}, 'noun_phrase': {0: (4.416285964467372, 0.0016802065013952865, 0.0157653748989105), 1: (5.053638609689071, 0.0006869433416155954, 0.018537551164627075), 2: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 3: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 4: (2.6385147819837624, 0.026979388143290196, 0.010085684061050426), 5: (3.8865595174708805, 0.0036942739930930454, 0.012511950731277421), 6: (2.761585012181802, 0.022054913275819255, 0.008653420209884688), 7: (2.7105990080739124, 0.023973973042005197, 0.010546064376831032), 8: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 9: (3.0383359306990423, 0.014055712684936238, 0.011872017383575462), 10: (2.4775676124384827, 0.0351313640520545, 0.011167210340499945), 11: (2.30448516428659, 0.046655701485290364, 0.009044277667999312), 12: (3.076748054633628, 0.013209175998730217, 0.012794214487075795), 13: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 14: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 15: (3.1591619817473666, 0.011565798032961561, 0.01454648971557615), 16: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 17: (2.0391274779866837, 0.0718664088830766, 0.009420758485794045), 18: (2.193453298505174, 0.055934477109315484, 0.01103978157043456), 19: (3.1052140879654035, 0.012615876104423013, 0.01338036060333253), 20: (3.2659050260927107, 0.009746295218372463, 0.015316778421402), 21: (3.4949274347865944, 0.006777387618012797, 0.01486323475837703), 22: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 23: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 24: (3.0551497099124587, 0.01367848967421171, 0.013470512628555253), 25: (3.978960748655893, 0.00321083842861956, 0.014046573638915927), 26: (3.7146413003442715, 0.004810320006575784, 0.013544982671737649), 27: (2.9601293574172027, 0.01595616452942292, 0.01156689524650567), 28: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 29: (2.7294445130844096, 0.02324566456488856, 0.011514449119567893), 30: (2.729707535927594, 0.02323565995765799, 0.013818293809890747), 31: (3.4048450222022812, 0.007813134063093983, 0.012937533855438277), 32: (3.241925761631311, 0.01012738885873162, 0.013584524393081665), 33: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 34: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 35: (2.6942361002948796, 0.024625063165150767, 0.011142772436141979), 36: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 37: (2.40979128825808, 0.03926245654856135, 0.009671843051910434), 38: (2.869612152651806, 0.01848838237639765, 0.011248195171356168), 39: (3.0408490762270968, 0.013998655336479288, 0.014194577932357788), 40: (3.5282488522525606, 0.0064315987021017155, 0.020213115215301447), 41: (3.540509723288857, 0.006309059642745521, 0.0206573188304901), 42: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 43: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 44: (3.139242691532513, 0.011942581116984919, 0.018650662899017345), 45: (4.231198576921104, 0.0022027128241340296, 0.016053479909896917), 46: (3.5889064563962347, 0.005848767455705855, 0.014558517932891868), 47: (2.422097782700182, 0.03847802488113382, 0.012305146455764748), 48: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 49: (2.9479736158808096, 0.016274435480833523, 0.014781582355499201), 50: (3.098010984589562, 0.01276335748321234, 0.01675570011138916), 51: (3.109653253721714, 0.012525862890179026, 0.01677399277687075), 52: (2.9627464276545528, 0.015888481787377604, 0.016157895326614324), 53: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 54: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 55: (3.177091057619296, 0.011237179503860398, 0.016597312688827526), 56: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 57: (3.034893196641776, 0.014134263923135663, 0.015046232938766446), 58: (2.84866046080801, 0.019130942005185753, 0.014237165451049805), 59: (2.815390245351479, 0.0201984694568075, 0.016350775957107544), 60: (2.839192886611046, 0.01942873235920257, 0.006291621923446622), 61: (3.11326114958753, 0.012453194070788634, 0.007118368148803733), 62: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 63: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 64: (2.9669450703085363, 0.015780511185964227, 0.006942373514175426), 65: (1.4561753110822764, 0.17932542287163092, 0.0018079161643981267), 66: (2.698245957275624, 0.024463871275924615, 0.004881149530410744), 67: (3.2574274365048974, 0.009879295174801486, 0.00820679664611823), 68: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 69: (2.7540433641090347, 0.022328640184350632, 0.005718529224395752), 70: (1.7046740075279663, 0.12244931858753924, 0.0026186466217040794), 71: (2.497536079891916, 0.03399900051068607, 0.005833053588867154), 72: (3.2582103359915013, 0.009866934085741942, 0.00625405311584476), 73: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 74: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 75: (2.9056865701533283, 0.017433224383815584, 0.0073753297328948975), 76: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 77: (2.690496370487274, 0.024776364575125966, 0.007022315263748191), 78: (2.751874503031505, 0.022407996607576468, 0.008711409568786632), 79: (2.24868660308401, 0.051112820961218254, 0.005518060922622692), 80: (2.711303188253064, 0.023946347728852103, 0.014744377136230447), 81: (3.461143738892126, 0.0071478995398634075, 0.012177711725235019), 82: (2.0203259378538676, 0.07408300087981214, 0.005611547827720664), 83: (2.636527310779811, 0.027067437024558542, 0.005813160538673423), 84: (2.4655943021651554, 0.03582830649843939, 0.00879108607769008), 85: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 86: (2.4655943021651554, 0.03582830649843939, 0.00879108607769008), 87: (1.5642090515574891, 0.1522070860796841, 0.005417627096176103), 88: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 89: (1.2679911142067402, 0.2366197689945075, 0.003012627363204956), 90: (2.7518234768895606, 0.022409867045439586, 0.007333815097808838), 91: (2.7015366476263636, 0.024332387034338154, 0.005179184675216719), 92: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 93: (2.5476090434245555, 0.03131735666801436, 0.005592763423919678), 94: (3.5406372105246993, 0.006307798427916721, 0.008065050840377896), 95: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 96: (2.0803944850609946, 0.06722217924025395, 0.005379730463027976), 97: (3.7145350378047395, 0.0048111106743607066, 0.008877068758010864), 98: (2.9523209650575724, 0.016159868571200005, 0.0082339346408844), 99: (3.4755318428902466, 0.006987576347940334, 0.007993972301483188), 100: (3.3246350129385323, 0.008874564146913682, 0.008616235852241527), 101: (2.6128540996488807, 0.028138744989167987, 0.008272904157638539), 102: (3.626378017474533, 0.005516655986738787, 0.009155404567718517), 103: (2.932885811474453, 0.016678549378825964, 0.00849389433860781), 104: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 105: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 106: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 107: (-0.0016871017238068403, 0.9986906919628202, -5.674362182572779e-06), 108: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 109: (0.2850577485601285, 0.7820509244092041, 0.0009132444858551692), 110: (1.148653444973492, 0.28031053394824124, 0.002852267026901223), 111: (0.4914509220986568, 0.6348698895571083, 0.0012215912342071755), 112: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 113: (0.202365480741529, 0.844131589418614, 0.000528854131698675), 114: (0.39156709903185344, 0.7044876319585958, 0.0013040482997894731), 115: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 116: (0.11173623981498304, 0.9134848656399938, 0.0003580749034881592), 117: (0.5211298502601197, 0.6148548745782575, 0.0017965734004974365), 118: (0.5718106056858795, 0.5814483781183453, 0.001604634523391768), 119: (0.6256134362802805, 0.5471030883124561, 0.0020198166370392068), 120: (0.527354480549568, 0.6106986724501338, 0.0014766275882721058), 121: (0.597677215553958, 0.5647888152550127, 0.0016816914081573264), 122: (0.762842554036261, 0.4650810698386909, 0.002107131481170643), 123: (0.8334508277291102, 0.42615579589916486, 0.0021722793579102007), 124: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 125: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 126: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 127: (4.2173074188264605, 0.002248379496694524, 0.0185785233974457), 128: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 129: (3.776966159094484, 0.004369360251735088, 0.020553475618362405), 130: (3.8910995144390377, 0.003668804882592309, 0.014036139845848061), 131: (2.2774326269052807, 0.04876680009354322, 0.010569620132446333), 132: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 133: (3.025012628087391, 0.014362223259487624, 0.014482587575912476), 134: (3.8711233221875028, 0.003782278370286225, 0.017132323980331376), 135: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 136: (3.3154945767064916, 0.009004718042169724, 0.014977973699569658), 137: (4.650113863752602, 0.0012019215246106532, 0.021829116344451838), 138: (3.6557698313769347, 0.005270047017565919, 0.018977212905883722), 139: (5.010929337147533, 0.0007280326694162593, 0.021607166528701816), 140: (4.338673978181536, 0.001881104887787527, 0.017895638942718506), 141: (4.510406264374862, 0.001466853700376692, 0.019268441200256337), 142: (4.868648511259469, 0.0008851704002848073, 0.016163876652717568), 143: (4.967373579495973, 0.0007726779571079604, 0.0189771890640259)}, 'original_noun_phrase': {0: (31.041602740335975, 2.2266901078377605e-162, 0.010461641165117441)}}, 'dann': {'named': {0: (2.01532610131715, 0.05823952037759897, 0.0028388768434524647), 1: (2.3937613389328503, 0.027154986646530727, 0.005228689312934853), 2: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 3: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 4: (2.426389194977716, 0.025370888472331172, 0.00452912151813506), 5: (2.3297592966902, 0.030998683237986997, 0.004616954922676109), 6: (2.4405702855153164, 0.02463011044813779, 0.0066686451435089555), 7: (1.416219786976402, 0.1728972332063728, 0.0020775735378265603), 8: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 9: (2.11143569677307, 0.048208894467454196, 0.004332017898559526), 10: (1.24517038514806, 0.2282079619182438, 0.000608849525451649), 11: (2.3606027800539082, 0.02908731423116455, 0.005291995406150829), 12: (2.01593781649579, 0.05817014609882399, 0.003174147009849526), 13: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 14: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 15: (1.8072895125560662, 0.08657853214144638, 0.0030585378408432007), 16: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 17: (1.2188731534410913, 0.23780987192114464, 0.001443845033645641), 18: (2.40993471804713, 0.0262564864059559, 0.005618496239185311), 19: (2.13736459397006, 0.04578514782518903, 0.003176182508468628), 20: (2.164109208244593, 0.043401754689803555, 0.010798591375350941), 21: (1.6517520430579264, 0.11501617805171431, 0.005485408008098602), 22: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 23: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 24: (2.176314111631599, 0.04235215429215568, 0.00798928886651995), 25: (1.7429509364768534, 0.09750390333213964, 0.005237537622451827), 26: (2.2665373380547083, 0.035285206016914664, 0.01138500571250911), 27: (0.7054593449252535, 0.4890840053264909, 0.0017155617475509422), 28: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 29: (1.9580768996614084, 0.06506953614003887, 0.007999703288078308), 30: (1.6680398867674768, 0.11170491203935211, 0.006096550822258018), 31: (1.9502761436575369, 0.06605348934337371, 0.008092659711837757), 32: (2.038245653280442, 0.05569036147685382, 0.00889438986778257), 33: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 34: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 35: (2.2132863361613513, 0.03931257154751434, 0.009669522941112507), 36: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 37: (1.9643642516473796, 0.06428600071253636, 0.006490138173103344), 38: (1.7598620907461924, 0.09452243188078542, 0.004673919081687905), 39: (2.1117255573522806, 0.0481811676145444, 0.009024541079998005), 40: (2.0202140444542622, 0.05768724869350837, 0.0076436251401901245), 41: (1.3963430625753228, 0.17871227383506366, 0.003674662113189675), 42: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 43: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 44: (2.036378888496478, 0.055894170684916984, 0.005913336575031292), 45: (1.6288618613030326, 0.11981032806296979, 0.0035395383834838645), 46: (2.0142430573881716, 0.05836252983789694, 0.0068226397037506326), 47: (-1.0170673951861304, 0.3218984735820919, -0.0025250256061554177), 48: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 49: (1.7295258552294348, 0.09992824728740321, 0.005457466840743996), 50: (0.8065240214084098, 0.42992105654917767, 0.001941654086112965), 51: (1.9540167850045052, 0.06558002054843416, 0.0070449799299240334), 52: (1.6923123458249807, 0.10692133007251438, 0.004972368478775024), 53: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 54: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 55: (2.0298190221000896, 0.05661569245948826, 0.006736303865909565), 56: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 57: (1.5868120525906177, 0.12905790921482516, 0.004140032827854134), 58: (0.6950144481264574, 0.4954589327172636, 0.0012076124548912048), 59: (1.7529241918818776, 0.09573592190747847, 0.005203597247600555), 60: (-2.1724247453840166, 0.04268409010139422, -0.004670247435569763), 61: (-2.053375634010933, 0.05406302117809778, -0.0032308146357536427), 62: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 63: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 64: (-2.0635792197542355, 0.052989872553715775, -0.0032930344343185536), 65: (-1.9388825291268506, 0.06751441908581773, -0.00319840610027311), 66: (-2.145999025135268, 0.04500295945127394, -0.004016786813735962), 67: (-1.8407659386046358, 0.08132772532097549, -0.0035571694374084695), 68: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 69: (-2.2469269310086237, 0.03672199187058787, -0.003964000940322898), 70: (-1.7500687431562394, 0.09623926067370431, -0.0019540101289748923), 71: (-2.118770958586775, 0.04751166764970733, -0.0032920002937316672), 72: (-1.7269327925044, 0.10040244767592066, -0.0024043619632720947), 73: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 74: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 75: (-1.9528039182617378, 0.06573320622283765, -0.002909339964389801), 76: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 77: (-2.0920171213945404, 0.05009967174142531, -0.0023684099316597096), 78: (-1.2446992246694102, 0.2283773348158189, -0.000778646767139457), 79: (-1.9239583107104954, 0.06947144811174345, -0.0020939484238624573), 80: (-1.9991168592591095, 0.060105039270328696, -0.005574475228786457), 81: (1.9148777496002638, 0.07068666076787984, 0.007803726196289051), 82: (-1.1946232401436556, 0.24693464386413358, -0.0009208932518959267), 83: (-0.5529496291589154, 0.5867455751417274, -0.0005741193890571594), 84: (-1.8157868859926778, 0.08521842185070586, -0.0021478980779647827), 85: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 86: (-1.8157868859926778, 0.08521842185070586, -0.0021478980779647827), 87: (-1.655716333627156, 0.11420266146388568, -0.0019031599164008872), 88: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 89: (-1.6914418611041173, 0.10708980588289103, -0.0021917402744293657), 90: (-0.11672843707965531, 0.9083002780071767, -8.38652253151162e-05), 91: (-1.1929590220244564, 0.24757045002732167, -0.0011555373668670765), 92: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 93: (-1.585995832035679, 0.12924318546206304, -0.0017868906259536632), 94: (-0.21888448298201132, 0.8290741064818753, -0.0001829683780670277), 95: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 96: (-1.4381877173421733, 0.1666492015369222, -0.0014763176441192627), 97: (-1.3366598449217524, 0.1971203456188626, -0.0011707395315170288), 98: (-1.0740606540032465, 0.296247294002853, -0.0009630531072616355), 99: (-1.2068922613930937, 0.24228542206416326, -0.0007560327649116516), 100: (-1.8241346710220254, 0.08390045940619115, -0.0015145212411880604), 101: (-2.162748627591851, 0.04352022071524088, -0.0031302958726883157), 102: (-1.489069034507623, 0.15288111178888808, -0.0010386317968368308), 103: (0.4879570829901477, 0.6311636218014774, 0.00037415921688077614), 104: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 105: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 106: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 107: (-1.8329722874020595, 0.08252461492946188, -0.0030451223254203685), 108: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 109: (-2.207693062964958, 0.03975920328626011, -0.0060792297124863115), 110: (-0.8567234573029678, 0.4022743766450734, -0.001001754403114341), 111: (-1.8489248817913835, 0.08009106389357762, -0.0034164220094680675), 112: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 113: (-2.022022549360925, 0.05748410630394622, -0.0035925269126891868), 114: (-2.058580376498248, 0.05351319540114414, -0.004383644461631753), 115: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 116: (-2.0918168873748777, 0.050119513423664816, -0.0043361157178878895), 117: (-1.8880092509539255, 0.07439327390504862, -0.0033483386039733776), 118: (-1.8845179117026338, 0.07488730328602192, -0.003114837408065785), 119: (-1.4077962655542242, 0.17534266557861433, -0.0020655572414398082), 120: (-1.445918525367151, 0.1644945165218859, -0.0017020985484123008), 121: (-0.8477953606071333, 0.4071055922693776, -0.001069238781928994), 122: (-1.4542216688839156, 0.16220561384271925, -0.0020685195922851562), 123: (-0.7631757474097437, 0.454730854196873, -0.0009519368410110252), 124: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 125: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 126: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 127: (2.091691171130639, 0.05013197459014095, 0.004751838743686676), 128: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 129: (0.22604622603829608, 0.8235793506895409, 0.0003679931163788064), 130: (2.2922422033695056, 0.03348032202314858, 0.005952678620815277), 131: (1.79555298972163, 0.0884882159161502, 0.0035826370120048967), 132: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 133: (0.4395252360809602, 0.6652393790344514, 0.00045618414878845215), 134: (1.8653859226823029, 0.07764618740447601, 0.004578793048858665), 135: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 136: (1.5533996276693478, 0.13682655089143395, 0.003414085507392861), 137: (1.6481172478840815, 0.11576639986583508, 0.0031228095293044933), 138: (2.1212558186185766, 0.04727756169697215, 0.005397960543632507), 139: (1.6916909059557774, 0.10704158193148378, 0.0034637153148651123), 140: (0.5932870793574698, 0.5599859458691933, 0.0007807597517967002), 141: (2.0521489793761125, 0.05419334328703193, 0.004835659265518211), 142: (2.2843473063096478, 0.03402539210292163, 0.007252700626850128), 143: (2.3434558118307067, 0.03013596737161015, 0.007905696332454692)}, 'noun_phrase': {0: (2.7928348399845992, 0.02095653855450993, 0.009369206428527788), 1: (2.4825458430697602, 0.03484558612922075, 0.006868648529052779), 2: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 3: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 4: (1.8794675129502518, 0.09288520951504105, 0.0053334832191467285), 5: (1.219989457682996, 0.25347169411169973, 0.0045654237270355), 6: (1.9960019785135974, 0.07704779239938649, 0.005563408136367798), 7: (1.7306079537933463, 0.1175728030364332, 0.00759871602058404), 8: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 9: (3.4834750370425698, 0.006900683327810753, 0.009609246253967196), 10: (2.5924618483451045, 0.02909574678015594, 0.007367825508117765), 11: (2.7953362548751906, 0.020871059544309557, 0.006409856677055403), 12: (2.1016549387940735, 0.06494362432702382, 0.006048253178596474), 13: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 14: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 15: (2.8663712240341725, 0.018586317783867108, 0.00808035135269164), 16: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 17: (1.4226751415459677, 0.1885524881969071, 0.003962308168411255), 18: (1.8265563241436442, 0.10104654781125397, 0.009212589263916049), 19: (2.8811647274014542, 0.018143551348810388, 0.0064402103424072155), 20: (1.2556551355654462, 0.24085969882602667, 0.008592197299003623), 21: (0.8192182566574282, 0.4338185494672727, 0.005452871322631836), 22: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 23: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 24: (0.7392531250297657, 0.47858976225286753, 0.004844743013382002), 25: (0.233867260298727, 0.8203210664518852, 0.0013445198535918523), 26: (0.6132645153604004, 0.5548811109657461, 0.0033368468284606934), 27: (0.9082148293525399, 0.38743743568611744, 0.0030472695827483687), 28: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 29: (0.6974568744262327, 0.5031345430587105, 0.00404183566570282), 30: (1.1111425905488965, 0.29530809018706083, 0.005777087807655401), 31: (1.16375891274626, 0.27444487939113144, 0.008001649379730247), 32: (0.4919021572730726, 0.6345631666685482, 0.0034994781017303467), 33: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 34: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 35: (0.39284214929279937, 0.7035784983380782, 0.0032848954200744296), 36: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 37: (0.2943463250143888, 0.7751692720323028, 0.0020151317119598278), 38: (1.2788944418573331, 0.2329239318564792, 0.006669062376022383), 39: (0.7515970584041647, 0.4714896875247243, 0.006257641315460194), 40: (1.9945685680042773, 0.07722599577720352, 0.011468788981437772), 41: (1.8283585583843929, 0.100757866179108, 0.009940785169601452), 42: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 43: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 44: (1.4015295999610728, 0.19458579777046728, 0.008382248878478993), 45: (0.9456705601318364, 0.3690119307893027, 0.0041120052337646484), 46: (1.4931575226360712, 0.1695986699885476, 0.00494876503944397), 47: (0.9119512608914013, 0.38557024297308085, 0.004036653041839644), 48: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 49: (1.7729505719066518, 0.10999334378301455, 0.007531911134719849), 50: (2.275759927636859, 0.04890036967411337, 0.008783435821533181), 51: (1.9529107903724439, 0.08257973794432565, 0.009775882959365823), 52: (1.0998215140874008, 0.2999564894963836, 0.0051892101764678955), 53: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 54: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 55: (1.5999303300794778, 0.1440782652838534, 0.007100117206573497), 56: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 57: (1.0451172089021257, 0.3232257756949092, 0.003931707143783547), 58: (1.5798374174541316, 0.1486014826298566, 0.006007325649261486), 59: (1.649514996205371, 0.13344141223553385, 0.008565112948417664), 60: (1.3295881275022043, 0.21636716175173012, 0.0024265706539154386), 61: (1.4173873856403525, 0.19004586466961876, 0.002405273914337147), 62: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 63: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 64: (1.2900967106111363, 0.2291768979657518, 0.0023651361465454324), 65: (-0.6394964562408798, 0.5384349236163265, -0.000882583856582686), 66: (0.9342767272221216, 0.3745482158358, 0.0027553081512451394), 67: (1.0726513518879688, 0.3113456338090456, 0.0038314461708068848), 68: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 69: (-0.5696281779426863, 0.5828662636022648, -0.0006015062332153098), 70: (-0.2745948075034577, 0.789826666639445, -0.00036676228046417236), 71: (0.7238457528963413, 0.4875475233179146, 0.0011965095996856467), 72: (0.29145151643111444, 0.7773117660790252, 0.00044726133346556507), 73: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 74: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 75: (2.3429201321187887, 0.04381023422088989, 0.0038973510265349898), 76: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 77: (0.9711736549982152, 0.3568371376290034, 0.0013588905334472878), 78: (2.0072032701641356, 0.07566868388287618, 0.003750687837600686), 79: (1.5745269444394807, 0.1498180017765327, 0.002507019042968739), 80: (1.647618663053722, 0.13383478360750184, 0.011279281973838817), 81: (1.6738667774425795, 0.12848235166552172, 0.010548812150955211), 82: (1.1621081060048455, 0.27508109068113357, 0.002224203944206249), 83: (-0.25193420792078114, 0.8067507294293463, -0.0005240231752395297), 84: (0.0412125746255518, 0.968026221265819, 0.00013266801834105335), 85: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 86: (0.0412125746255518, 0.968026221265819, 0.00013266801834105335), 87: (-0.720503825232584, 0.48950445267006704, -0.002070850133895852), 88: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 89: (-1.3861570545292106, 0.19907553981411108, -0.004951936006545976), 90: (-0.41728301673572293, 0.6862493445482392, -0.001135048270225536), 91: (-0.25408668538698265, 0.8051384178876207, -0.000785475969314553), 92: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 93: (-0.8060328331453194, 0.441000463827958, -0.0032617926597595104), 94: (0.23516574824941155, 0.8193435829365103, 0.0006491661071777455), 95: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 96: (0.023711144409944818, 0.9816004121274581, 8.725523948671654e-05), 97: (-0.030663587039654484, 0.9762070583974061, -0.00010018348693846546), 98: (0.43235741452339665, 0.6756562027226033, 0.0015580117702483909), 99: (1.7325756622545776, 0.11721017975366686, 0.003394353389739979), 100: (0.8870298995968616, 0.39814625288237826, 0.002258834242820751), 101: (-0.7524762354220925, 0.4709866065858548, -0.002182847261428811), 102: (1.225072725136357, 0.2516417794579439, 0.003177434206008911), 103: (-0.3083385384218838, 0.7648420365418974, -0.0006639301776886208), 104: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 105: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 106: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 107: (1.1827276187047462, 0.26721883946446306, 0.0033135801553725863), 108: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 109: (0.4677473899243689, 0.6510845019015793, 0.0023932158946990967), 110: (1.5543537521556468, 0.1545207289732867, 0.004358538985252347), 111: (0.33219457880664965, 0.7473483386668133, 0.0012724012136459129), 112: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 113: (0.5554129924996416, 0.5921477184652227, 0.0020010501146316417), 114: (0.5985961044900063, 0.5642019814694244, 0.002261972427368142), 115: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 116: (0.33314913057822887, 0.7466514536968696, 0.0012956619262695646), 117: (0.665602535423863, 0.5223561435016453, 0.002299004793167081), 118: (0.5939834392591454, 0.567151259835008, 0.0021283745765686146), 119: (0.9116082024107136, 0.3857414089404295, 0.0024249464273452537), 120: (1.2625107604083745, 0.23849570004409668, 0.0037049204111099243), 121: (0.6278400661121015, 0.545707397946765, 0.001571732759475697), 122: (0.8226113127614556, 0.4319832956577777, 0.002193671464920055), 123: (1.540657301774562, 0.15778809242077027, 0.003776147961616516), 124: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 125: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 126: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 127: (1.373430046054174, 0.2028595056854633, 0.006783539056777921), 128: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 129: (1.563314433817932, 0.15241582591032174, 0.004777562618255682), 130: (0.46319681215618164, 0.6542200303688117, 0.002609437704086337), 131: (-1.563110616436469, 0.1524634180112219, -0.005553948879241988), 132: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 133: (-0.12147064624780947, 0.9059871249278382, -0.00040838122367858887), 134: (0.1549571102303764, 0.8802746457107468, 0.0008236527442931907), 135: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 136: (-0.16280450007914946, 0.8742692334005663, -0.0007202357053757158), 137: (1.3272705868823882, 0.21710188904661032, 0.00633070766925814), 138: (1.4925421439116233, 0.16975665927954725, 0.00777252912521359), 139: (2.202101872751473, 0.0551512016007396, 0.009638789296150252), 140: (1.896985815993702, 0.0903222932096059, 0.008707857131958041), 141: (1.6469051935356773, 0.13398305609894848, 0.006484252214431718), 142: (1.5642716309109284, 0.1521924940783004, 0.006479287147521995), 143: (1.5814696097453336, 0.14822936067621462, 0.010344752669334434)}, 'original_noun_phrase': {0: (13.300901960438123, 3.703059968082085e-38, 0.004469728635417114)}}}, 'EI_anger': {'non_dann': {'named': {0: (2.9999473312352563, 0.007362582726028531, 0.008020885288715363), 1: (2.8668234774506334, 0.009871819488213418, 0.007182353734970115), 2: (3.00154155812073, 0.007336638454529805, 0.007494708895683266), 3: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 4: (2.853858915689559, 0.010156129568114373, 0.007833994925022125), 5: (2.828059928865708, 0.010745468775168335, 0.007886649668216728), 6: (3.02450282733906, 0.0069726453063536685, 0.009996812045574177), 7: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 8: (2.8598644963104904, 0.010023463730840692, 0.008409228920936596), 9: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 10: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 11: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 12: (2.5998013893153016, 0.017592679093099937, 0.005481539666652657), 13: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 14: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 15: (2.9326297210120362, 0.008542831810533981, 0.005548548698425304), 16: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 17: (2.940704275103277, 0.008392164112157185, 0.0060208141803741455), 18: (3.151965011996052, 0.005249422356308179, 0.006964576244354259), 19: (2.287671406170058, 0.03379490281125083, 0.0036009758710860984), 20: (3.0413286735531093, 0.006717086069541028, 0.014911824464797996), 21: (3.069448373548996, 0.006310132602520979, 0.0162633419036865), 22: (3.0021290719939473, 0.007327099649944999, 0.012680222094059002), 23: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 24: (3.09062885600122, 0.006019513433223663, 0.01488557308912275), 25: (2.956577974398133, 0.008103409424171643, 0.010112801194191001), 26: (2.9864008493337164, 0.007586648071914168, 0.011366653442382801), 27: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 28: (2.973399068655112, 0.0078078980268998345, 0.012191291153430928), 29: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 30: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 31: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 32: (3.034858735548319, 0.006814261596819409, 0.014548160135746002), 33: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 34: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 35: (2.9552849489291804, 0.008126567023332459, 0.013237571716308572), 36: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 37: (2.936128540797658, 0.008477227650781594, 0.011788879334926616), 38: (3.0385378587619694, 0.006758836922482435, 0.01182446479797361), 39: (2.7435982420580585, 0.012912897795710858, 0.009320780634880066), 40: (3.0405868166075836, 0.006728159852842062, 0.014032751321792603), 41: (3.060608797193966, 0.006435410659359709, 0.014685365557670582), 42: (2.9596167573971948, 0.008049236583012171, 0.012382332980632793), 43: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 44: (3.0580771508201217, 0.00647173267937202, 0.012605533003807068), 45: (2.82915858949088, 0.010719715967741848, 0.007928590476512898), 46: (2.9653970838404464, 0.007947153849508864, 0.01044829487800597), 47: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 48: (2.6817851636071484, 0.014757382691585958, 0.010272590816020943), 49: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 50: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 51: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 52: (2.86993360871273, 0.00980476144910828, 0.010063776373863242), 53: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 54: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 55: (2.916662743131008, 0.008848485787575558, 0.011233554780483268), 56: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 57: (2.743987235061825, 0.012902018625834148, 0.007454979419708241), 58: (2.897997026527674, 0.009219171524230591, 0.012199313938617729), 59: (2.594337170862807, 0.01779893404523318, 0.006748458743095376), 60: (2.914960139530747, 0.008881693746406093, 0.006869411468505837), 61: (2.8507182375464906, 0.010226178604013208, 0.006061425805091836), 62: (2.841321460699797, 0.010438544261103, 0.00706293582916262), 63: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 64: (2.885798778955244, 0.00946945042613447, 0.006650090217590332), 65: (2.7587514163403224, 0.012495511330205291, 0.006261751055717468), 66: (2.7396773779959207, 0.013023044734662113, 0.005785688757896423), 67: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 68: (2.812318299243337, 0.011121034507954432, 0.006750799715518951), 69: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 70: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 71: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 72: (2.8672447468516307, 0.00986271068265795, 0.006573969125747703), 73: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 74: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 75: (2.817415509292445, 0.010998067813829594, 0.006316101551055886), 76: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 77: (2.8505265911786, 0.010230468034281626, 0.00683790147304536), 78: (2.8741127243361118, 0.009715344205031512, 0.007124258577823617), 79: (2.7607764414751292, 0.012440719137313207, 0.006465397775173187), 80: (0.8877558925020694, 0.3857725591115787, 0.001051077246665949), 81: (2.801768111398139, 0.01137974983785741, 0.0159024238586426), 82: (-1.5057271346943946, 0.1485817808527447, -0.0008097499608993308), 83: (1.9700449211964866, 0.06358531425883647, 0.002071602642536141), 84: (-22.95339218952276, 2.5686339773041418e-15, -0.022832025587558757), 85: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 86: (-22.95339218952276, 2.5686339773041418e-15, -0.022832025587558757), 87: (-22.86662846998824, 2.7536733557052225e-15, -0.01856851130723952), 88: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 89: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 90: (-16.768518789837042, 7.632218059649364e-13, -0.017311701178550742), 91: (-11.611768393844091, 4.5059977084044113e-10, -0.009996645152568817), 92: (-23.866649352713203, 1.2532478803632706e-15, -0.020687639713287354), 93: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 94: (-24.985537608446055, 5.384084308191323e-16, -0.01994720846414566), 95: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 96: (-19.523207824261274, 4.935981580158765e-14, -0.018804195523262013), 97: (-23.407583424122, 1.7917538260793985e-15, -0.014919529855251301), 98: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 99: (-26.935806838095047, 1.3401717265174718e-16, -0.023001615703105938), 100: (-29.05155747805047, 3.290875147552598e-17, -0.01941395699977877), 101: (-20.952578853822185, 1.364592063799021e-14, -0.015068808197975181), 102: (-23.755402610337182, 1.3658353886189715e-15, -0.022684668004512798), 103: (-17.6441388240095, 3.067566632212257e-13, -0.016543084383010886), 104: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 105: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 106: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 107: (-0.6701712489599319, 0.5108126714636266, -0.0008711561560630909), 108: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 109: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 110: (0.2285465785879622, 0.8216631703527664, 0.00027125924825666115), 111: (-1.1693969055389009, 0.25670543531739254, -0.0009917482733726724), 112: (-0.5160453000378008, 0.6117762825661357, -0.0006043612957000732), 113: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 114: (0.19566382318731748, 0.8469506675785499, 0.00022938847541809082), 115: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 116: (-0.4111320283226256, 0.6855758638595414, -0.0005095556378364452), 117: (-0.5973129280740079, 0.5573506555723196, -0.0007590308785438427), 118: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 119: (0.5016432707791384, 0.6216815313223802, 0.0006431668996810802), 120: (-0.7605603194798696, 0.4562551943917086, -0.0009556487202644348), 121: (0.41965824946936614, 0.6794421954453768, 0.0005446463823318703), 122: (-0.088499390132571, 0.9304059320710668, -0.00011604279279708862), 123: (0.44557433780413114, 0.6609402438153469, 0.0005996599793434143), 124: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 125: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 126: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 127: (2.7777349607598487, 0.011990792200825029, 0.005483637750148762), 128: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 129: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 130: (3.0837643105296504, 0.006112248291725002, 0.00957365781068803), 131: (3.1351439301505257, 0.00545050683362242, 0.011689433455467246), 132: (3.0435190939944987, 0.006684492448388404, 0.008233727514743827), 133: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 134: (3.0966805509407807, 0.00593889477616171, 0.00858931094408033), 135: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 136: (2.6523439052590074, 0.015721598320941495, 0.005913108587265015), 137: (3.0953838504201574, 0.005956080152424374, 0.009752169251441956), 138: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 139: (2.657918666541735, 0.015534546577635117, 0.0047150939702987615), 140: (2.5123701025065484, 0.02118038133567884, 0.004118752479553234), 141: (2.708947810106843, 0.013917895751007657, 0.003966300189495092), 142: (3.151285618819743, 0.005257402111263108, 0.00846958756446839), 143: (2.965174451191324, 0.007951062361363352, 0.007695800065994268)}, 'noun_phrase': {0: (-1.823182545892379, 0.10158903404476334, -0.004346001148223921), 1: (0.6529009636033906, 0.5301427638966068, 0.0014689952135086282), 2: (-1.9147767672455225, 0.08778776509585746, -0.004484418034553517), 3: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 4: (-1.8334472014081977, 0.09994691694819158, -0.004479753971099842), 5: (-1.6768964109221438, 0.12787725657828872, -0.006013435125350963), 6: (-2.411025152323756, 0.03918309767510654, -0.006692928075790416), 7: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 8: (-2.50267900383429, 0.033713307978660734, -0.0077323317527771), 9: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 10: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 11: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 12: (-1.2215763576396204, 0.2528992657800809, -0.0032196015119552723), 13: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 14: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 15: (-2.0185489614696026, 0.07429584774720312, -0.005984464287757896), 16: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 17: (-2.502975107022215, 0.03369693249034014, -0.006320160627365068), 18: (-0.007360628600401206, 0.9942876956158053, -1.968741416930042e-05), 19: (-0.8907364716036349, 0.3962576215910443, -0.0022437006235123125), 20: (-2.8025004219687366, 0.020628201662149168, -0.011005443334579423), 21: (-2.107034519266076, 0.06437899542473148, -0.008922708034515447), 22: (-2.9943248021882756, 0.015094652822419669, -0.01277201473712919), 23: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 24: (-3.3972888461876676, 0.007907222552312736, -0.014024832844734214), 25: (-4.1748166136782485, 0.0023944354171003945, -0.012944406270980868), 26: (-4.1962723641365995, 0.002319461381102061, -0.01413772106170652), 27: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 28: (-2.695548618638821, 0.024572183386920812, -0.010166427493095376), 29: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 30: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 31: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 32: (-2.843643522099154, 0.01928815883461188, -0.009994786977767955), 33: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 34: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 35: (-3.7646858982640334, 0.004452747333857424, -0.011769860982894897), 36: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 37: (-3.7324423984291, 0.004679780409504576, -0.014772346615791354), 38: (-2.2455867675896406, 0.051372315964701445, -0.008109477162361156), 39: (-3.2359231103348005, 0.010225186578521093, -0.009877172112464916), 40: (-2.3616352973704924, 0.04248737155882671, -0.005944323539733842), 41: (-0.6965706128165664, 0.5036633522655196, -0.0019025415182113425), 42: (-2.1819167419098173, 0.056996194905469984, -0.006671530008315996), 43: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 44: (-1.8999677088194737, 0.08989273023884006, -0.006697237491607666), 45: (-2.887450189083182, 0.017958705520828136, -0.008153745532035794), 46: (-3.6971182160452765, 0.004942572347528052, -0.009962809085845903), 47: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 48: (-1.7840686423691154, 0.10807941437847667, -0.0044749736785889005), 49: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 50: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 51: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 52: (-2.012770107772844, 0.07499209109599361, -0.005148887634277344), 53: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 54: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 55: (-3.3358841727430946, 0.008717067187864204, -0.01025363802909851), 56: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 57: (-3.0324117689742924, 0.014191161631230537, -0.010666310787200928), 58: (-1.0603304316979425, 0.3166194368147802, -0.0034190744161605724), 59: (-2.709559987535502, 0.024014793251107003, -0.008456566929817166), 60: (-5.612223135587408, 0.00032908633523572386, -0.007830667495727472), 61: (-3.044537086563111, 0.01391535583938568, -0.004270273447036788), 62: (-4.466322721932532, 0.0015629322764594033, -0.004947292804717951), 63: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 64: (-2.9114347754278325, 0.017270871413094906, -0.004106640815734863), 65: (-3.3891489240165122, 0.008009904035483717, -0.005237913131713845), 66: (-4.262865332703453, 0.00210226306433529, -0.006344011425972018), 67: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 68: (-2.183246878448915, 0.05687278800943512, -0.0033423155546188354), 69: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 70: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 71: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 72: (-2.5803190667246017, 0.029681082044315344, -0.0034605830907821766), 73: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 74: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 75: (-2.563400996839954, 0.03051638168160957, -0.004253998398780823), 76: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 77: (-2.2429798918897146, 0.051591542039749805, -0.004107862710952759), 78: (-2.935586107463613, 0.016605475817213253, -0.00466219484806063), 79: (-2.8804512051376103, 0.018164657500522638, -0.004243367910385154), 80: (-3.446982093301815, 0.0073094595679489595, -0.0070547282695770375), 81: (-2.877209148867109, 0.018260874752402068, -0.009688264131546054), 82: (-4.375781209611118, 0.0017820273124026433, -0.005826637148857117), 83: (-4.227395409459643, 0.002215116531872715, -0.008993631601333596), 84: (-10.859157548412043, 1.794588733464013e-06, -0.028087219595909096), 85: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 86: (-10.859157548412043, 1.794588733464013e-06, -0.028087219595909096), 87: (-13.425354861271588, 2.9440041519051927e-07, -0.028906109929084767), 88: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 89: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 90: (-16.15389910304737, 5.917210079387948e-08, -0.026733589172363237), 91: (-10.429215707109279, 2.5193237097096096e-06, -0.02062155306339264), 92: (-15.871086648344093, 6.902775576577911e-08, -0.026989573240280174), 93: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 94: (-19.213494334267327, 1.2930383331406867e-08, -0.0260120451450348), 95: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 96: (-17.396849352413383, 3.094297015510351e-08, -0.027425014972686734), 97: (-11.630878088616768, 1.0043174949080335e-06, -0.023780626058578502), 98: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 99: (-16.256511459884884, 5.5990327970512016e-08, -0.03293475806713103), 100: (-14.966067719944089, 1.1505091346154834e-07, -0.02911258637905123), 101: (-8.9014716568645, 9.34175689190008e-06, -0.02603101432323457), 102: (-12.9312189034619, 4.06388779550763e-07, -0.02966231703758243), 103: (-11.619817167101273, 1.0124617705326627e-06, -0.025008597970008872), 104: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 105: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 106: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 107: (-0.9049840849981707, 0.38905713129157093, -0.0011850744485855103), 108: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 109: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 110: (0.14430588605679148, 0.8884386416264047, 0.00020279288291930042), 111: (-0.9779956544240272, 0.3536312369845376, -0.0014018923044205045), 112: (-0.6577968932510961, 0.52713316186464, -0.000915491580963157), 113: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 114: (-1.73562370385813, 0.11665049488657796, -0.002127626538276639), 115: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 116: (-0.7291060706599362, 0.48447730366659025, -0.0009533852338791116), 117: (-1.276532090034004, 0.2337205874074845, -0.0018397957086563332), 118: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 119: (-0.11016672130588094, 0.9146946252160408, -0.00012453794479372338), 120: (0.6337352230370069, 0.5420222418426753, 0.000764855742454551), 121: (0.6038074636841404, 0.5608803614850112, 0.0007824897766113281), 122: (-0.0975518694745494, 0.9244261027674239, -0.0001659691333770752), 123: (0.6025133045306404, 0.5617041956500191, 0.0009518772363663053), 124: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 125: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 126: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 127: (-1.5586334828057054, 0.15351219246910677, -0.003723523020744357), 128: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 129: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 130: (-0.8889873387795206, 0.39714807548087205, -0.0023648232221603616), 131: (1.8731454777310295, 0.09382679279907638, 0.005485448241233815), 132: (-1.4602469096557182, 0.17823119122489203, -0.00560216009616854), 133: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 134: (-1.2349301099673444, 0.24812404639205596, -0.0043000727891922), 135: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 136: (-0.48362082848308674, 0.6402040030990057, -0.0012556225061416404), 137: (-0.8892996750279333, 0.3969889665798001, -0.0023857623338698897), 138: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 139: (-4.450203928550094, 0.001599725184121625, -0.008053421974182129), 140: (-4.422306758793129, 0.0016656100862283232, -0.009977713227272034), 141: (-2.3976431527953115, 0.04005235709894506, -0.007599052786827071), 142: (0.0546395040071408, 0.9576193598538241, 0.00018808543682097278), 143: (-1.7572022516970278, 0.11275805348644675, -0.005658695101737943)}, 'original_noun_phrase': {0: (-24.1029332123197, 4.213490669717725e-108, -0.007616794750922251)}}, 'dann': {'named': {0: (3.1541497723996583, 0.00522384135791289, 0.00986942350864406), 1: (3.181309271590398, 0.004915805191278162, 0.011457297205924977), 2: (3.127365912520824, 0.005546005199175376, 0.009590110182762124), 3: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 4: (3.1493556275382018, 0.005280135194466702, 0.010423678159713734), 5: (3.1297181110943186, 0.00551695442450737, 0.007126376032829285), 6: (3.08688816725016, 0.006069876481733846, 0.010540437698364269), 7: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 8: (3.1645958707087574, 0.00510319942100133, 0.011332765221595764), 9: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 10: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 11: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 12: (3.102131411924078, 0.005867179401643754, 0.0061305731534958), 13: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 14: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 15: (3.0000584814268514, 0.007360771004279333, 0.0068900465965270885), 16: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 17: (3.0841412656010374, 0.006107120088498524, 0.006706812977790855), 18: (3.148477894461567, 0.00529050553967907, 0.008803820610046365), 19: (2.5196051941625384, 0.020859143591782148, 0.0034940391778945923), 20: (3.139819430130235, 0.005393873686063911, 0.01886967718601229), 21: (3.154179233456912, 0.005223497234744051, 0.02260468602180482), 22: (3.1507253686005186, 0.0052639913739709054, 0.016295456886291526), 23: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 24: (3.1363917765936224, 0.005435335588589249, 0.01601330339908602), 25: (3.0916877516329184, 0.006005330815079033, 0.00884194076061251), 26: (3.017501051367118, 0.0070817458098385395, 0.007745276391506184), 27: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 28: (3.1120514476799865, 0.005738819214552431, 0.014920613169670083), 29: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 30: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 31: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 32: (3.1078968924867683, 0.005792241140374358, 0.01412692666053772), 33: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 34: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 35: (3.0885493840039504, 0.006047460221819138, 0.010939228534698475), 36: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 37: (3.1264344405222126, 0.005557550435755567, 0.013928736746311177), 38: (3.0801806473784654, 0.006161210862369186, 0.01008434295654298), 39: (2.721222332581404, 0.013553645016633994, 0.005284606665372837), 40: (3.068690031293318, 0.00632078661724058, 0.019550687074661233), 41: (3.07003739288343, 0.006301869452771459, 0.02186712622642517), 42: (3.0644857210587464, 0.006380170879024848, 0.019834151864051797), 43: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 44: (3.0976252131600672, 0.005926405378420299, 0.018958660960197438), 45: (2.8919449369729633, 0.009342539390275371, 0.013088151812553406), 46: (3.0509629683921577, 0.00657486941097741, 0.010790181159973156), 47: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 48: (2.9229009235242236, 0.00872783447418792, 0.01431553065776825), 49: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 50: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 51: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 52: (2.9671368452040445, 0.007916674678809254, 0.01232005059719088), 53: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 54: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 55: (3.070699551364976, 0.006292592803193917, 0.016034062951803207), 56: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 57: (3.0368559536937076, 0.006784119775119118, 0.013155999779701222), 58: (3.082664852250908, 0.006127229540655791, 0.014980928599834464), 59: (2.875630327042398, 0.009683067762781711, 0.008324480056762673), 60: (2.7486062148184214, 0.012773504283733491, 0.006257417798042342), 61: (2.9762263224875394, 0.0077592624524099535, 0.007529836893081621), 62: (3.0148540153590613, 0.00712342071843546, 0.007782182097434975), 63: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 64: (2.989777061076018, 0.007530194496840481, 0.007344353199005116), 65: (2.777215221805072, 0.012004347233363365, 0.007192438840866067), 66: (2.614099029701356, 0.017063641108465092, 0.005363926291465759), 67: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 68: (2.8197206933326515, 0.010942885865725937, 0.007550516724586509), 69: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 70: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 71: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 72: (2.8579679658821235, 0.01006517796201528, 0.006564566493034385), 73: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 74: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 75: (2.8578427461103284, 0.010067938036389864, 0.00712003856897353), 76: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 77: (2.926692507871646, 0.008655278882439317, 0.00737684965133667), 78: (3.0870022862232513, 0.006068333994685831, 0.008602914214134227), 79: (2.945411434880791, 0.008305517094331305, 0.007217621803283669), 80: (2.9549685909975034, 0.008132242598424416, 0.006953896582126612), 81: (3.005514411265544, 0.007272369120876625, 0.022204640507698048), 82: (2.707395073279422, 0.01396463290410086, 0.0027575403451919334), 83: (3.0113191208907693, 0.007179444602253337, 0.006955680251121532), 84: (-7.325341937968756, 6.045422435606574e-07, -0.0066195636987685935), 85: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 86: (-7.325341937968756, 6.045422435606574e-07, -0.0066195636987685935), 87: (-10.74069959243424, 1.6448399515383815e-09, -0.008333107829093911), 88: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 89: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 90: (0.18239733846981665, 0.8572029605082729, 0.00031536519527436413), 91: (1.7362255918841576, 0.09871197028057596, 0.003092189133167278), 92: (-0.940070890721652, 0.3589800896324369, -0.0012825310230255016), 93: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 94: (3.0357114102967033, 0.00680137726430756, 0.0038804203271866067), 95: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 96: (-1.1773778726816657, 0.253583294746442, -0.001657232642173767), 97: (3.5997084988096124, 0.0019097129950801056, 0.004591581225395214), 98: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 99: (1.1089465269046965, 0.28129319148179616, 0.0018455713987350353), 100: (-0.5295364484976446, 0.6025665348471472, -0.0007632926106452831), 101: (1.5058625044790652, 0.1485472554812579, 0.0018052011728286743), 102: (0.2205974084685615, 0.8277590522365456, 0.0002595365047454723), 103: (4.310195331721349, 0.00037750741628889866, 0.0077728137373924144), 104: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 105: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 106: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 107: (0.8210562490126309, 0.4217971975292646, 0.0006834208965301736), 108: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 109: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 110: (1.8198202333697628, 0.08457938902254086, 0.0013935327529907005), 111: (0.4377222947367746, 0.6665230539813192, 0.00022646188735964135), 112: (0.20970731813703808, 0.8361283611725413, 0.00012572109699249268), 113: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 114: (-0.15158254894517134, 0.881114123565619, -0.0001502901315689198), 115: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 116: (2.0145092513411904, 0.05833227460376728, 0.001291951537132241), 117: (0.2962314517029351, 0.7702669343080781, 0.0002052307128906361), 118: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 119: (1.8139215348766842, 0.08551538619544562, 0.0013534307479858287), 120: (-1.0397772310328304, 0.31149499050162893, -0.0004953473806381115), 121: (0.6238769147308664, 0.5401272059841066, 0.0005101948976516946), 122: (0.8286400046540349, 0.41759656386945787, 0.0005371749401092418), 123: (2.0842931669980382, 0.05087023026204952, 0.0026114106178283802), 124: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 125: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 126: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 127: (3.0149132102670153, 0.007122486163198851, 0.0069400861859321705), 128: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 129: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 130: (3.1269969622232923, 0.005550575401041034, 0.012137611210346233), 131: (3.161449722975462, 0.005139245536346475, 0.012094923853874229), 132: (2.670957543312085, 0.015105268001117854, 0.007432873547077168), 133: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 134: (3.1207491378829744, 0.005628526362397582, 0.011614325642585743), 135: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 136: (2.4076877774607914, 0.026379633477613015, 0.006028772145509731), 137: (3.099804932215983, 0.0058976846234807204, 0.013817815482616402), 138: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 139: (2.681860483691142, 0.014754989598012682, 0.005324845761060709), 140: (2.621741612879915, 0.01678706553181637, 0.005616920441389062), 141: (2.7503886062482987, 0.012724240238317888, 0.006610753387212759), 142: (3.0975633595865384, 0.005927222363946263, 0.008682096004486106), 143: (3.0221484823384093, 0.00700914733911952, 0.008415175974369055)}, 'noun_phrase': {0: (-1.7014712871235786, 0.12306423285839684, -0.008207207918167092), 1: (-1.209152251513571, 0.2574092262783106, -0.005838519334793069), 2: (-2.2741258510416813, 0.04903120185809948, -0.008376097679138161), 3: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 4: (-2.4336529698323597, 0.03775566225680258, -0.008571654558181763), 5: (-2.974418762892723, 0.01559018013610164, -0.011515149474144004), 6: (-3.7429218799226067, 0.0046046824226910355, -0.011580252647399869), 7: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 8: (-2.6748416167039375, 0.025419995672536787, -0.014063483476638772), 9: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 10: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 11: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 12: (-0.868794681803827, 0.4075302724955586, -0.0038636088371276633), 13: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 14: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 15: (-2.565657162667428, 0.030403638037283857, -0.011824379861354817), 16: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 17: (-2.258488164901773, 0.050300726156232385, -0.011758840084075906), 18: (-2.2825129120722116, 0.048363317779386975, -0.009865096211433388), 19: (-1.8880463081963228, 0.09162170682924786, -0.005839028954505887), 20: (-2.4604038966831085, 0.03613469573733932, -0.014544641971588113), 21: (-2.8847849291391174, 0.018036850527221066, -0.01452497243881229), 22: (-2.551173735937456, 0.031134730890924927, -0.01110403537750243), 23: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 24: (-2.9892038014917492, 0.015220565122441269, -0.013150608539581332), 25: (-3.0358582416996263, 0.01411219946624356, -0.009422686696052562), 26: (-1.8660285997449984, 0.0948974771938708, -0.006441432237625111), 27: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 28: (-3.132727195674896, 0.012068561763517274, -0.015891933441162076), 29: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 30: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 31: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 32: (-2.4382151236003016, 0.03747418958339082, -0.010398060083389282), 33: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 34: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 35: (-0.4020122275284977, 0.6970547964026097, -0.001407685875892628), 36: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 37: (-2.3817349153601155, 0.041110618220405626, -0.0105152428150177), 38: (-1.257334395672945, 0.24027886271623936, -0.004917246103286754), 39: (-0.9283198803266377, 0.3774665466052247, -0.0028055936098098644), 40: (-1.2495992374355025, 0.24296399817492736, -0.006587874889373735), 41: (-0.8132883786986603, 0.4370386133713132, -0.0045864880084991455), 42: (-0.8147147390846828, 0.4362625933545672, -0.0038908302783965953), 43: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 44: (-0.8168166857324468, 0.43512071642609373, -0.004046750068664562), 45: (-1.849788845567326, 0.0973836808318736, -0.008271408081054699), 46: (-2.4891812967461524, 0.03446827802048692, -0.00911563038825991), 47: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 48: (-1.257612326326132, 0.24018284151870845, -0.005986660718917847), 49: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 50: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 51: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 52: (-0.5367128485920113, 0.6044777235518819, -0.0025108575820922963), 53: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 54: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 55: (-1.711825808156662, 0.1210863310467368, -0.008448512852191892), 56: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 57: (-1.4351811948788749, 0.18506084115120194, -0.008757948875427246), 58: (-1.64123672709301, 0.1351663686415412, -0.00761822462081907), 59: (-1.7385832043518836, 0.11610942142227056, -0.007238876819610551), 60: (-4.4708621725623, 0.0015527346020212072, -0.01286238431930542), 61: (-3.093124421481188, 0.012864420613802998, -0.009431958198547363), 62: (-2.509957558117125, 0.03331307548642898, -0.006254076957702637), 63: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 64: (-2.446109453748497, 0.03699205282025611, -0.0065238356590270885), 65: (-2.7155631563309925, 0.02377991331523115, -0.008957451581954934), 66: (-3.1404512583916464, 0.011919362713343784, -0.008159947395324718), 67: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 68: (-2.258420562695312, 0.050306283711191606, -0.008272564411163308), 69: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 70: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 71: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 72: (-2.137501827401781, 0.06126964941052555, -0.005974161624908436), 73: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 74: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 75: (-2.2966378673043333, 0.04725865309097473, -0.007664144039154053), 76: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 77: (-1.7607714994671344, 0.11212588279486928, -0.006605374813079823), 78: (-3.2872467532527385, 0.009419650337976296, -0.008584368228912342), 79: (-2.1628039190842365, 0.058798454376853486, -0.007389742136001576), 80: (0.6104773495640484, 0.556645368160736, 0.0022977083921432717), 81: (-2.054564425384406, 0.07009414087799887, -0.012084883451461748), 82: (-1.3624638417474084, 0.20616905327334664, -0.00208470821380613), 83: (-0.41301183616454257, 0.6892641446549439, -0.002105090022087086), 84: (-5.627079331263414, 0.00032289986025933146, -0.014079564809799217), 85: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 86: (-5.627079331263414, 0.00032289986025933146, -0.014079564809799217), 87: (-6.451965022375558, 0.00011787055596603068, -0.018509644269943226), 88: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 89: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 90: (-2.7796784174762608, 0.021412021024543516, -0.011031919717788663), 91: (-1.9531050197523963, 0.08255397686718234, -0.007644644379615817), 92: (-3.974534561372611, 0.0032323967288151565, -0.010690855979919422), 93: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 94: (-2.4375247727498524, 0.03751664799519189, -0.0077206671237945446), 95: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 96: (-3.6971469313038927, 0.004942352518800177, -0.011132961511611916), 97: (-2.409284665271311, 0.03929508737744707, -0.006687295436859142), 98: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 99: (-3.391759907387993, 0.007976816914009565, -0.011295613646507274), 100: (-4.550282689732428, 0.0013853788483211749, -0.01318627893924712), 101: (-1.9228295311738242, 0.08666275825938953, -0.00699996352195742), 102: (-3.2664646511666104, 0.00973758129845975, -0.008170241117477384), 103: (-0.8379698118004049, 0.4237422752964861, -0.0032378971576690896), 104: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 105: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 106: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 107: (-0.6687090925180541, 0.520462228362796, -0.0008792579174041748), 108: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 109: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 110: (1.874027241162766, 0.09369492998386465, 0.001990282535552945), 111: (2.5304714659685534, 0.03221044502910602, 0.0031304419040680265), 112: (2.2681340275158024, 0.04951388216076753, 0.0024760425090789573), 113: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 114: (-2.67396811839617, 0.025456402645859244, -0.004358720779418956), 115: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 116: (2.059597017358291, 0.06952548788412616, 0.002565145492553711), 117: (1.7672393612030144, 0.11098865588675123, 0.0019124507904053067), 118: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 119: (2.3449763280740075, 0.043662927431223436, 0.003461676836013783), 120: (2.7105711284566856, 0.023975067436201904, 0.0033237516880035844), 121: (2.6924024594040903, 0.02469913117707077, 0.0031828880310058594), 122: (1.7274638091581718, 0.1181543690344885, 0.0018701136112213579), 123: (1.4427718860170107, 0.1829689959341304, 0.002938568592071533), 124: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 125: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 126: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 127: (0.35754748266078845, 0.7289232559385432, 0.0015774309635162576), 128: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 129: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 130: (0.4769284775470119, 0.6447804326307979, 0.002782040834426902), 131: (1.5916127249552854, 0.14593548854062083, 0.009280022978782665), 132: (-0.1372929792438153, 0.8938215875311126, -0.0008468002080916914), 133: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 134: (0.06349382805961021, 0.9507611144000385, 0.0003700375556945912), 135: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 136: (0.363499115120205, 0.7246239293824269, 0.0017059653997421043), 137: (0.616811119120338, 0.5526407815822985, 0.0034369230270385742), 138: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 139: (0.6503904569933501, 0.5316899776332693, 0.002200816571712505), 140: (0.6637485050695717, 0.5234884334503043, 0.0028235316276550293), 141: (0.033023209048875894, 0.9743768586929915, 0.00017293840646748215), 142: (0.3273936848444332, 0.7508569702765883, 0.0019787043333053256), 143: (0.6544397522467309, 0.5291957439553789, 0.002611398696899414)}, 'original_noun_phrase': {0: (-15.686372171980068, 2.5908143315881138e-51, -0.00538205282969606)}}}, 'EI_fear': {'non_dann': {'named': {0: (-0.4536497069305446, 0.6552198366825752, -0.0006526619195937888), 1: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 2: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 3: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 4: (-0.8453837185365612, 0.40841698204839616, -0.0013110116124153137), 5: (-0.46427452094332555, 0.6477266892107059, -0.0005847990512848344), 6: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 7: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 8: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 9: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 10: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 11: (-0.11691070026151802, 0.9081577799239737, -0.000165420770645186), 12: (2.0225299646717434, 0.05742722578522014, 0.0029049932956695113), 13: (-0.26262674405991276, 0.7956648531240026, -0.0004011809825897883), 14: (-0.08360809439295097, 0.9342426460836097, -0.00010615885257725388), 15: (-0.709156222445198, 0.4868391424395345, -0.0005408331751823314), 16: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 17: (-0.7959533420944594, 0.43589159110764286, -0.0009384125471115223), 18: (-1.8659784441623357, 0.07755942000400019, -0.0018775373697280662), 19: (-0.7651367147586945, 0.45358998697214725, -0.0010971322655677906), 20: (1.5288874046307366, 0.142770504358485, 0.00632993280887606), 21: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 22: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 23: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 24: (1.5561215016818635, 0.13617942425602786, 0.006514191627502441), 25: (1.324449090361284, 0.2010655150470798, 0.005747288465499878), 26: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 27: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 28: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 29: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 30: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 31: (1.7506147286030171, 0.09614284165925152, 0.00850757062435159), 32: (1.8548369171790378, 0.07920530825837102, 0.00788701772689826), 33: (1.6009913019210766, 0.12587473375451938, 0.007541057467460699), 34: (1.3659567646552007, 0.18790451769818264, 0.005688571929931663), 35: (1.26755219530107, 0.2202730831554025, 0.005834899842739105), 36: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 37: (1.7587175226377527, 0.09472170382020201, 0.008782824873924289), 38: (1.864035766852779, 0.07784421991681233, 0.008328481018543277), 39: (0.9488361249336216, 0.3546174940700779, 0.004171487689018205), 40: (0.8765149718311525, 0.39169780491417294, 0.0021890848875045776), 41: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 42: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 43: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 44: (2.1282274166381874, 0.046626328415440045, 0.005648136138916016), 45: (1.6609366785571051, 0.11313885567545193, 0.003921613097190857), 46: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 47: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 48: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 49: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 50: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 51: (1.3946870168456393, 0.17920377269488452, 0.0037615150213241577), 52: (1.6353851739741603, 0.11842713792866925, 0.00456748902797699), 53: (1.3315823913263836, 0.1987533241794981, 0.004584217071533225), 54: (1.3371353570410147, 0.19696795927159966, 0.00397452712059021), 55: (1.779829401688538, 0.09110404205894308, 0.00402035713195803), 56: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 57: (2.019810376955619, 0.05773267898214161, 0.0058293938636779785), 58: (2.103828312244844, 0.04894177807965281, 0.00476861894130709), 59: (1.0962920103537341, 0.2866524914517763, 0.0026982814073562844), 60: (0.4370111105755945, 0.6670297001581933, 0.0006972163915633933), 61: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 62: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 63: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 64: (1.1558873260782965, 0.2620559169228458, 0.0017056524753570335), 65: (0.6085352404626192, 0.5500392182921371, 0.0007077634334564653), 66: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 67: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 68: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 69: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 70: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 71: (0.9027840809592305, 0.37794389342005685, 0.0010757893323898315), 72: (1.0765194416528894, 0.29517481630283526, 0.0015606909990311113), 73: (1.1249983108921853, 0.27460131610512795, 0.0015131771564483865), 74: (0.9582896293698356, 0.3499530367509156, 0.0014142096042633057), 75: (-0.3494323682524638, 0.730607818680475, -0.00045030713081362084), 76: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 77: (0.43274634082896746, 0.6700713494942525, 0.0006299242377281189), 78: (0.4679228281870684, 0.6451625383412054, 0.0006492644548415916), 79: (0.831870192340097, 0.41581548127152934, 0.0010650619864464028), 80: (-0.6664111234719685, 0.5131597453870926, -0.0017973065376281627), 81: (0.7780572169891012, 0.44611673216458503, 0.0031023234128951804), 82: (-0.2686312437643469, 0.791108615990461, -0.00022893697023396165), 83: (-0.5457728927551285, 0.5915729724571748, -0.0005119055509567594), 84: (4.561650474819855, 0.00021319750591981511, 0.00431002676486969), 85: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 86: (4.561650474819855, 0.00021319750591981511, 0.00431002676486969), 87: (11.010716603364635, 1.0921299854413208e-09, 0.010568979382514998), 88: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 89: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 90: (1.0637210480277177, 0.30078805955814025, 0.0011882573366165383), 91: (3.6135226538296683, 0.001850616143029893, 0.003304967284202487), 92: (7.622456405355375, 3.410296896217827e-07, 0.008830562233924866), 93: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 94: (4.938953194680991, 9.112475991874698e-05, 0.005027011036872864), 95: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 96: (6.714881111632672, 2.0348426215097325e-06, 0.007181236147880576), 97: (-0.07731640103103846, 0.9391802415001849, -8.628368377683326e-05), 98: (2.79280976762182, 0.011603946348508714, 0.0030788868665695412), 99: (2.903617370634803, 0.009106010357973707, 0.0030685946345329063), 100: (0.9654002716532625, 0.34647240489796993, 0.001101657748222351), 101: (10.59050226920981, 2.0724162787332835e-09, 0.010231825709342945), 102: (3.7472086613254545, 0.001364640158664261, 0.004019924998283353), 103: (5.4843523237003415, 2.7265752737184905e-05, 0.005727683007717144), 104: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 105: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 106: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 107: (-0.3181158623863308, 0.7538670422588711, -0.00038250088691710316), 108: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 109: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 110: (-0.17260063152673366, 0.8647906774407321, -0.00022515356540681042), 111: (0.1146094829785176, 0.9099571640600346, 0.00012142956256866455), 112: (0.08284497610249766, 0.9348413845528509, 9.741485118863746e-05), 113: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 114: (0.09216537345907856, 0.9275314887096396, 0.00010473728179938302), 115: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 116: (0.4632378052925146, 0.648456153909601, 0.0005057364702224509), 117: (-0.26042867881764836, 0.7973346475213109, -0.00029164254665370315), 118: (-0.3750008117769526, 0.7118131023296859, -0.000460708141326871), 119: (-0.6469147561060148, 0.5254261353596277, -0.000817073881626107), 120: (-0.29662372851395696, 0.7699719612142981, -0.000387200713157676), 121: (-0.7101735937355383, 0.4862224188674523, -0.0010588198900222667), 122: (-0.44416024420047034, 0.6619441742361193, -0.0005719110369681868), 123: (-0.4844556320193189, 0.6336001696899646, -0.000822190940380052), 124: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 125: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 126: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 127: (2.2785103645191667, 0.03443363204092818, 0.009405428171157859), 128: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 129: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 130: (2.359254180695483, 0.029168546047316737, 0.012106120586395264), 131: (2.665775666150978, 0.015274503537344412, 0.01082838624715804), 132: (2.640052341317529, 0.016141602275097384, 0.011544437706470456), 133: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 134: (2.359823315241125, 0.029134239010212195, 0.00960416495800015), 135: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 136: (2.2719409701125346, 0.03489850091617077, 0.008913415670394942), 137: (2.410227595417562, 0.026240474368313817, 0.01069551706314087), 138: (2.4547128014911554, 0.023911529244833432, 0.009658692777156863), 139: (2.243085573650609, 0.03700966254519938, 0.010553579777479172), 140: (1.861165540870239, 0.07826668314737244, 0.007289951294660535), 141: (2.410029655559331, 0.026251295034089187, 0.007672186940908404), 142: (2.3381135612914767, 0.030469781765021322, 0.00903195366263393), 143: (2.314148366568837, 0.032009860677918146, 0.009153123199939772)}, 'noun_phrase': {0: (-1.9359197349281163, 0.08486303438398021, -0.003261709213256858), 1: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 2: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 3: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 4: (-3.5921207239926622, 0.005819472709776622, -0.006515455245971691), 5: (-2.2405191888039777, 0.05179931555336764, -0.004590368270874001), 6: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 7: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 8: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 9: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 10: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 11: (-2.177198306446434, 0.057436064972269864, -0.005819413065910328), 12: (-1.400019122170044, 0.19502306335776495, -0.004387927055358953), 13: (-3.8281904159932822, 0.004038902993587881, -0.005794817209243797), 14: (-3.0418381044554876, 0.013976266320790747, -0.005463844537734963), 15: (-3.002390611876544, 0.014898501704831292, -0.006923961639404286), 16: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 17: (-1.3965123499270924, 0.19604149075064167, -0.005478394031524669), 18: (-1.1061521175095483, 0.2973501494858802, -0.005515959858894359), 19: (-1.5320431542102702, 0.15987433423037603, -0.0039048135280608798), 20: (-1.2667867509932154, 0.23703097219497393, -0.004638332128524714), 21: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 22: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 23: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 24: (-1.5919673048439271, 0.1458558787983439, -0.005894666910171487), 25: (-1.9018239322840609, 0.08962630091028609, -0.007182878255844183), 26: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 27: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 28: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 29: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 30: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 31: (-2.231033720451802, 0.0526079234418805, -0.009498947858810391), 32: (-1.6722600173987396, 0.12880431947882942, -0.006319308280944846), 33: (-1.319104625064257, 0.21970751880154052, -0.005503982305526733), 34: (-1.6426000220670234, 0.13488091489643397, -0.006459599733352639), 35: (-1.8324739912421704, 0.10010153939268446, -0.007775568962097135), 36: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 37: (-0.5298460653769466, 0.6090391421319425, -0.0025731325149536133), 38: (-1.1459079265504366, 0.2813873161218451, -0.006542104482650746), 39: (-2.0563281051041438, 0.06989434897981066, -0.0072820544242858665), 40: (-1.832659175296179, 0.100072100381963, -0.0058657824993133545), 41: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 42: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 43: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 44: (-1.8572277489032036, 0.09623735235256793, -0.00609959363937379), 45: (-2.0199085350068584, 0.07413294507286643, -0.005737346410751298), 46: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 47: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 48: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 49: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 50: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 51: (-2.0686929741325577, 0.06850892653616228, -0.010611301660537698), 52: (-2.1190318543237137, 0.06313680911664347, -0.0076056540012360285), 53: (-1.371374748224241, 0.20347631027779792, -0.00496740341186519), 54: (-1.670398188835708, 0.1291783171103463, -0.00642936229705815), 55: (-1.805673082097354, 0.10444831758628668, -0.009003195166587818), 56: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 57: (-1.1323926277300542, 0.28673600315853054, -0.005244308710098289), 58: (-0.9862841080332114, 0.34976500439605274, -0.007236728072166465), 59: (-1.2051019454800171, 0.2588935665517983, -0.00519210398197173), 60: (-1.3078051242097022, 0.2233563599285183, -0.004501736164092995), 61: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 62: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 63: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 64: (-1.1304764999833943, 0.287500792462536, -0.004508697986602805), 65: (-1.5776562043906954, 0.14910008006886857, -0.0054946839809417725), 66: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 67: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 68: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 69: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 70: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 71: (-1.1228025481450146, 0.2905799023735358, -0.0034292519092559703), 72: (-1.1442219595457226, 0.2820501762831403, -0.002496677637100153), 73: (-1.39756984179323, 0.19573389666515814, -0.004101824760437078), 74: (-1.782239322829124, 0.10839218870326162, -0.0048563122749327725), 75: (-1.1621705134074587, 0.2750570177717406, -0.004986456036567721), 76: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 77: (-1.4777500256179747, 0.17359339189294204, -0.005267497897148143), 78: (-1.4780535824896877, 0.17351389868873646, -0.005110841989517223), 79: (-1.3501294547030407, 0.20994633313990962, -0.004570305347442627), 80: (-0.41295578067509575, 0.6893037499489926, -0.002099892497062661), 81: (-1.7588500575013584, 0.11246579297791844, -0.009781035780906722), 82: (-2.0992377338378363, 0.06519888246336426, -0.0063045263290405495), 83: (-1.7171570209124136, 0.12007936926462812, -0.006237471103668235), 84: (-0.5440740393161605, 0.5996079486356212, -0.002246636152267445), 85: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 86: (-0.5440740393161605, 0.5996079486356212, -0.002246636152267445), 87: (1.0527153678445045, 0.3199131931574301, 0.0033967435359955056), 88: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 89: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 90: (-2.143597626272751, 0.06066523565255921, -0.008459538221359253), 91: (-2.2476325636466004, 0.05120091292210157, -0.008722174167633101), 92: (-0.2006075316872387, 0.8454655225549137, -0.0007541239261626975), 93: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 94: (-1.4932396741932192, 0.1695775885824614, -0.005306470394134455), 95: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 96: (-0.3191203618275257, 0.7569175311159203, -0.0011212348937987837), 97: (-2.458981689327924, 0.03621910324283895, -0.008778303861618042), 98: (-1.5699961994350784, 0.15086293024603037, -0.005399644374847412), 99: (-1.7332341226006547, 0.11708906381472337, -0.006020474433898915), 100: (-2.2478215731877276, 0.051185105408517506, -0.007329279184341453), 101: (0.9242215188599492, 0.3794839117881119, 0.004103481769561768), 102: (-1.3804999785748928, 0.2007499668059926, -0.005717629194259666), 103: (-1.2435497732198746, 0.2450811694075575, -0.005191737413406394), 104: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 105: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 106: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 107: (-1.2584645766151907, 0.23988859851978603, -0.004110944271087658), 108: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 109: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 110: (-1.8641328798290278, 0.09518460229791703, -0.006200087070465099), 111: (-1.4991464007925193, 0.1680678520803307, -0.00435821413993831), 112: (-1.755279233997687, 0.11310001292177661, -0.005064666271209717), 113: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 114: (-1.6163046921037871, 0.14048400703572178, -0.005287939310073764), 115: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 116: (-1.9711279406636157, 0.08019647814243068, -0.005731606483459495), 117: (-1.75484549632847, 0.11317727421496512, -0.00519672632217405), 118: (-1.6664082176427797, 0.12998313803203515, -0.0054109632968902255), 119: (-2.0442336127660954, 0.07127548469948174, -0.005514851212501504), 120: (-1.7009025325257665, 0.12317372724695272, -0.004597327113151528), 121: (-1.5645063437346745, 0.15213777580428148, -0.004361140727996815), 122: (-1.6353236057412899, 0.13641082715340458, -0.004772144556045521), 123: (-1.6247418312440645, 0.13866371532846725, -0.004948762059211742), 124: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 125: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 126: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 127: (-1.3397891031696871, 0.2131580692015096, -0.004450920224189747), 128: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 129: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 130: (-0.6524386219967896, 0.5304275011164683, -0.0029736489057540783), 131: (-1.5652547968287462, 0.15196340716105336, -0.005452835559844937), 132: (-0.9499563651753489, 0.366944972691554, -0.003337734937667869), 133: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 134: (-0.331746526385179, 0.7476755298176827, -0.0010100603103637695), 135: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 136: (-1.5532771974724229, 0.15477535040653126, -0.005829417705535911), 137: (0.173603556619036, 0.8660189516820461, 0.0005457580089569536), 138: (-0.7888195050695923, 0.45049597174297196, -0.003311330080032371), 139: (-1.5504657179857682, 0.15544206940481914, -0.006878229975700367), 140: (-1.7797678230287417, 0.10881609423344454, -0.008536991477012645), 141: (-1.4496366575264021, 0.18109491798781593, -0.007257568836212147), 142: (-1.1939255379395972, 0.26302553686328756, -0.007501396536827076), 143: (-0.9296431144648586, 0.37681686074414045, -0.004888626933097828)}, 'original_noun_phrase': {0: (-15.651398643433845, 4.1453538019672003e-51, -0.0055239785255657425)}}, 'dann': {'named': {0: (2.6103947862637535, 0.017199240439614367, 0.011928600072860673), 1: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 2: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 3: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 4: (2.5092497314562876, 0.02132035644276648, 0.012463900446891829), 5: (2.4316064916403395, 0.025095964440944134, 0.008169776201248147), 6: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 7: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 8: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 9: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 10: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 11: (2.7206603552292705, 0.013570121500998907, 0.013262182474136297), 12: (2.7599366760771935, 0.012463413199479363, 0.01201771795749662), 13: (2.6464823962790263, 0.015920576282805184, 0.011606037616729736), 14: (2.6262554643444473, 0.016625715351621783, 0.010800504684448264), 15: (2.7682107276193775, 0.012241527969973616, 0.013800801336765312), 16: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 17: (2.7885554708035167, 0.011711892006608559, 0.015732431411743186), 18: (2.854763108132108, 0.010136048251224801, 0.009570011496543906), 19: (2.8352092689387693, 0.01057894462261949, 0.014922112226486206), 20: (2.768750157576103, 0.012227193965040123, 0.02500676512718203), 21: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 22: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 23: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 24: (2.93476167269629, 0.008502798875669767, 0.02939695119857788), 25: (2.738110081888915, 0.013067324427681905, 0.024826714396476768), 26: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 27: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 28: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 29: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 30: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 31: (2.9634761132464313, 0.007980939359257441, 0.02686447501182554), 32: (2.8407244653589068, 0.010452178354582193, 0.023518845438957214), 33: (2.775746657367221, 0.01204272733819661, 0.023004117608070307), 34: (2.8277620864617217, 0.01075246047528403, 0.022682592272758484), 35: (2.88838547813698, 0.009415837176344984, 0.03005471378564839), 36: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 37: (2.9733964670835156, 0.007807942915604268, 0.032468363642692566), 38: (3.028675001880239, 0.006908411367128458, 0.02547063231468194), 39: (2.906120344098321, 0.009056046626788558, 0.0336635142564774), 40: (2.80842970923703, 0.01121572907259239, 0.016974622011184715), 41: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 42: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 43: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 44: (2.8924318658678465, 0.009332555163869228, 0.02221711874008181), 45: (2.7029789524994787, 0.014098376107217862, 0.015183374285697937), 46: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 47: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 48: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 49: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 50: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 51: (2.89587139025981, 0.00926232144924328, 0.018863475322723477), 52: (2.7971994432079477, 0.011493564463744527, 0.018300509452819802), 53: (2.6929188384939966, 0.014407617635695123, 0.01884995400905609), 54: (2.6942788236446074, 0.014365438288492592, 0.016951128840446472), 55: (2.9185447165477694, 0.008811918607938592, 0.02505885809659958), 56: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 57: (3.007555189737084, 0.007239567422610325, 0.029827497899532318), 58: (3.0742500481934614, 0.006243077090385201, 0.021926903724670377), 59: (2.9490198539813113, 0.008239680467161007, 0.027023300528526306), 60: (2.9294106654178353, 0.0086036226508914, 0.010767298936843916), 61: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 62: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 63: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 64: (2.993402595610379, 0.00747002451250534, 0.012270322442054837), 65: (3.0601517867655366, 0.006441952809940185, 0.009791091084480286), 66: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 67: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 68: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 69: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 70: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 71: (2.92869606273098, 0.008617174175294142, 0.0105192571878433), 72: (2.277012206788573, 0.034539140453250156, 0.004699712991714455), 73: (2.831245353720505, 0.010670964778161365, 0.005946746468543984), 74: (2.7295140761014784, 0.013312747369964885, 0.007367062568664595), 75: (3.1058799658170972, 0.005818350172150957, 0.015683779120445274), 76: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 77: (3.0892671588081413, 0.006037799562176934, 0.012800940871238675), 78: (2.9608404644985993, 0.008027520312143406, 0.008423709869384788), 79: (3.087240881130914, 0.006065110261453242, 0.01329335868358611), 80: (2.8087124765138056, 0.01120881718131954, 0.016108225286006916), 81: (2.833286273988715, 0.010623490409024463, 0.03871192485094066), 82: (2.4806467052111603, 0.02264456595340393, 0.0043008685111999845), 83: (2.5829132463192552, 0.018237557057120556, 0.0034660324454307556), 84: (4.54841894094851, 0.00021968685465574152, 0.008455431461334206), 85: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 86: (4.54841894094851, 0.00021968685465574152, 0.008455431461334206), 87: (5.769151100855724, 1.4705844669189098e-05, 0.011277711391449008), 88: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 89: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 90: (0.6769574592804383, 0.5065920974120413, 0.001984626054763794), 91: (5.214256783919982, 4.93760898309573e-05, 0.008036795258521967), 92: (6.056797547522969, 7.960969881339664e-06, 0.015093770623207048), 93: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 94: (-0.40490702966731434, 0.6900682738154246, -0.0008324533700942993), 95: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 96: (-0.8440543326556268, 0.40914102931184826, -0.0015962868928909302), 97: (-0.021390838269522268, 0.9831569308204549, -4.547834396362305e-05), 98: (0.7861387266930034, 0.4414810991375544, 0.001652923226356462), 99: (4.285325112910361, 0.00039951537069062866, 0.012760338187217724), 100: (1.694590781568973, 0.10648142938167585, 0.004414877295494057), 101: (10.453739572099726, 2.563048905570711e-09, 0.02470133304595945), 102: (-1.5984145490382646, 0.12644824397666987, -0.0032364934682845847), 103: (4.785291290120442, 0.00012865458575207346, 0.01689343750476835), 104: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 105: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 106: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 107: (3.0197293044775324, 0.00704684737387263, 0.01095967292785649), 108: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 109: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 110: (2.9887173755638496, 0.007547869581981068, 0.011412858963012695), 111: (2.9949393585336357, 0.007444660741199274, 0.007520383596420266), 112: (2.980428862712128, 0.007687509455643306, 0.010929712653160062), 113: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 114: (2.997267297196953, 0.007406397525509584, 0.009008508920669511), 115: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 116: (3.033095790477973, 0.006840975632980589, 0.00854403972625728), 117: (3.0559468368881655, 0.006502450773708548, 0.008743429183959983), 118: (2.969353781850851, 0.007877999433326299, 0.009646764397621133), 119: (2.972589402427881, 0.00782188042582904, 0.016182480752468142), 120: (2.9553898016429723, 0.008124686772074364, 0.014857803285121929), 121: (2.9641410442447254, 0.007969229059858068, 0.01723177880048754), 122: (3.036369606976501, 0.006791447745007415, 0.008858245611190807), 123: (3.0032123391275105, 0.007309543316983239, 0.019232231378555287), 124: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 125: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 126: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 127: (2.9494314684755825, 0.008232202534938372, 0.02923231124877934), 128: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 129: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 130: (2.9540510825213544, 0.008148724687837637, 0.03158140927553177), 131: (2.9624466600605515, 0.007999102167196308, 0.018715083599090576), 132: (2.9772518632058964, 0.007741693167467521, 0.024149402976036072), 133: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 134: (2.903855433908864, 0.009101246795777613, 0.021802923083305337), 135: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 136: (2.8992656468373372, 0.009193511126860463, 0.02026311159133909), 137: (2.8638766807406766, 0.00993576246164692, 0.022373551130294822), 138: (2.969027447358234, 0.007883680972499465, 0.023039574921131123), 139: (2.9340452536128225, 0.008516231274771047, 0.023069849610328652), 140: (2.7874978474710157, 0.011738876234434108, 0.01798664256930349), 141: (2.9419650728470366, 0.008368870824083162, 0.01703978329896927), 142: (2.9837752688099948, 0.007630833086294239, 0.025397874414920807), 143: (2.978592531378548, 0.00771878319812897, 0.022190096974372853)}, 'noun_phrase': {0: (1.5211540598530064, 0.16254649062513027, 0.013754546642303467), 1: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 2: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 3: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 4: (0.984389581324264, 0.35064594117528725, 0.010298395156860374), 5: (0.6632973004745379, 0.5237642157995729, 0.0070555627346039484), 6: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 7: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 8: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 9: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 10: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 11: (1.0370261838546138, 0.3267820036368045, 0.00874339342117314), 12: (0.7119061384825222, 0.49456175001248004, 0.00777845382690423), 13: (0.9371818472161219, 0.3731309140840815, 0.007833760976791448), 14: (0.7366210450925132, 0.48011253405961407, 0.006562745571136452), 15: (0.8343167047122595, 0.42569261636890376, 0.00946221351623533), 16: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 17: (1.0512282292622237, 0.32055948669042184, 0.011078271269798268), 18: (0.9889449511963347, 0.3485305221125199, 0.004496330022811856), 19: (1.2560797061214373, 0.2407127354829818, 0.013133659958839417), 20: (0.6197988663892074, 0.5507575215599216, 0.006505572795867831), 21: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 22: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 23: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 24: (0.5314321588295272, 0.6079839464159491, 0.00651753544807443), 25: (0.24727993101355267, 0.8102403009419568, 0.0027957201004028542), 26: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 27: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 28: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 29: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 30: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 31: (0.1951014486755195, 0.8496468908155581, 0.001885938644409224), 32: (0.08608159956273065, 0.9332862053204737, 0.0009582996368407981), 33: (0.2217119948379997, 0.8294870516588212, 0.002364957332610995), 34: (0.21154382157197257, 0.8371756779670659, 0.0023240566253661665), 35: (0.1916696407314516, 0.8522555602874563, 0.0023022353649139737), 36: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 37: (0.4321730201921053, 0.6757853326831198, 0.006130862236022994), 38: (-0.22520643133486828, 0.8268491220608349, -0.0021021664142608643), 39: (0.5592106457537548, 0.5896603452047438, 0.0070510983467102495), 40: (0.5588404923242061, 0.5899025381298684, 0.003660529851913452), 41: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 42: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 43: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 44: (0.42468507042674847, 0.6810385105607396, 0.003106921911239624), 45: (0.3690807508222823, 0.7206011217092118, 0.002863657474517778), 46: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 47: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 48: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 49: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 50: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 51: (-0.0456223529946904, 0.9646075102381807, -0.0002785503864287664), 52: (0.34091357953439466, 0.7409919871619243, 0.002949303388595559), 53: (0.403134994753946, 0.6962578372112948, 0.0028584957122802512), 54: (0.4314506422032745, 0.676291316411693, 0.003278601169586115), 55: (0.38669031903503914, 0.7079694287177205, 0.004895877838134777), 56: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 57: (0.6550953575024775, 0.5287925704894851, 0.009386119246482805), 58: (0.10722816840008649, 0.9169602360533933, 0.0005818247795105203), 59: (0.8946026226027306, 0.3942944517389294, 0.010824564099311818), 60: (1.1624310662910748, 0.2749565308335149, 0.007143926620483354), 61: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 62: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 63: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 64: (1.4330775157735591, 0.18564423250548448, 0.008354246616363525), 65: (0.8255857678848993, 0.43037880484130064, 0.0045719683170318826), 66: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 67: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 68: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 69: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 70: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 71: (1.1667007049543583, 0.27331405258306735, 0.007150387763977029), 72: (0.9869411346801803, 0.34945987843095927, 0.0037491142749787043), 73: (0.3678523045748961, 0.7214857162649786, 0.0013000845909117986), 74: (0.9355467605241871, 0.37392813308729367, 0.004311722517013505), 75: (1.363227620845901, 0.20593706702985365, 0.012658190727233898), 76: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 77: (1.1017450409429674, 0.2991626799477583, 0.008647975325584423), 78: (0.7482202833323687, 0.4734251701483594, 0.0041672050952911155), 79: (1.2084413701632182, 0.2576692452722908, 0.008719712495803833), 80: (1.0238216628886645, 0.33264965296422166, 0.008077451586723294), 81: (1.9762407340205639, 0.0795394356984782, 0.02270984649658203), 82: (-0.42703446245694854, 0.6793883185306457, -0.0011041760444641113), 83: (1.532474106169191, 0.159769384656312, 0.003844776749610923), 84: (0.0879514617355009, 0.9318411509571123, 0.0004138648509979248), 85: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 86: (0.0879514617355009, 0.9318411509571123, 0.0004138648509979248), 87: (0.15523944596788888, 0.8800584383822896, 0.0006642401218414085), 88: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 89: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 90: (-3.8067711291385278, 0.004173753314251861, -0.011523932218551636), 91: (-1.8029863878824919, 0.10489362292100228, -0.0071912050247191495), 92: (0.6411449947774289, 0.5374110134642108, 0.0022733032703399214), 93: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 94: (-3.226996053276431, 0.01037244631668259, -0.01360133886337278), 95: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 96: (-2.6142804669791535, 0.028073003494980337, -0.012852048873901345), 97: (-3.146274040846368, 0.011808150660718974, -0.012410771846771307), 98: (-2.969709395751982, 0.015709836245144774, -0.010099208354949929), 99: (1.0111902941119997, 0.33833698593321376, 0.002703249454498291), 100: (-2.1807562313493887, 0.05710407711809115, -0.005730390548706055), 101: (3.3910737644395006, 0.007985498073479418, 0.014367562532424938), 102: (-2.6306614721924566, 0.027328998091301647, -0.011934328079223677), 103: (0.845008227377277, 0.4200018922314048, 0.0031632542610168235), 104: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 105: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 106: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 107: (1.503233054789495, 0.1670302522608637, 0.00598465204238896), 108: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 109: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 110: (1.0801275731534432, 0.3081788129116308, 0.0037075042724609153), 111: (1.350372644189398, 0.20987129463835882, 0.003793919086456321), 112: (1.1409329986178038, 0.2833468488875774, 0.004815900325775169), 113: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 114: (0.938123675525445, 0.3726722678854003, 0.0029580175876616766), 115: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 116: (0.4418870143661213, 0.6689980321458654, 0.0010536968708039218), 117: (1.0624567868430348, 0.31570439647134824, 0.003084981441497825), 118: (1.1882036759612773, 0.26516151588337133, 0.004590630531311035), 119: (1.4773634891828835, 0.17369466132529682, 0.010195052623748757), 120: (1.541044659092739, 0.15769484933100672, 0.009900480508804321), 121: (1.6310872575661979, 0.13730875715834567, 0.011794212460517872), 122: (1.613215614443618, 0.14115583526110093, 0.004842764139175393), 123: (1.325637573581265, 0.21762086424884264, 0.01053376793861388), 124: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 125: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 126: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 127: (0.5937930491209755, 0.5672731790101048, 0.0066876918077469205), 128: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 129: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 130: (0.5426486941236397, 0.6005492527688636, 0.0045343518257141), 131: (-0.8042424472502977, 0.44198180042872004, -0.007315874099731445), 132: (0.3308831718874723, 0.7483061466157258, 0.0032659351825714), 133: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 134: (0.4624585358432608, 0.6547294103416952, 0.004889035224914573), 135: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 136: (-0.34970864312488015, 0.7346011484279015, -0.002848786115646429), 137: (0.6464103164936413, 0.534148417373987, 0.005934375524520863), 138: (0.2691786656064834, 0.793861518024378, 0.003027743101119973), 139: (0.17735762485888104, 0.863154868091551, 0.001822125911712602), 140: (-0.37085576442258356, 0.7193237238085466, -0.003197172284126304), 141: (-0.05608851125686871, 0.9564967421496149, -0.00037859976291654274), 142: (-0.3790054764489656, 0.7134706037627666, -0.0035865873098373524), 143: (-0.23611853117342302, 0.8186265508544835, -0.0017890527844429127)}, 'original_noun_phrase': {0: (3.7088827910822393, 0.00021609055086301644, 0.0022620030678809355)}}}, 'V': {'non_dann': {'named': {0: (-0.9655923860156405, 0.34637869716316627, -0.0015480875968932883), 1: (-1.0741254413589432, 0.29621899886218495, -0.0027149647474289385), 2: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 3: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 4: (-1.4234530845842852, 0.1708193823853086, -0.003223337233066559), 5: (-1.5367179668232358, 0.1408488075207118, -0.0025740280747413857), 6: (-1.5653479220113784, 0.13400480155122227, -0.003283877670764934), 7: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 8: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 9: (-0.19233909614082156, 0.8495174542603634, -0.00021315068006516613), 10: (-0.6684984996365081, 0.5118560541591541, -0.0013095587491989136), 11: (-1.2836756202863069, 0.21469064283757888, -0.00285803079605107), 12: (-0.8837081892317356, 0.38789931416530654, -0.001225583255290985), 13: (-0.8763453230689969, 0.3917876841049176, -0.0012641027569770813), 14: (-1.3433628505638238, 0.19498083875532457, -0.0034336537122726107), 15: (0.377900318366655, 0.7096933759595105, 0.0003549039363861528), 16: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 17: (-0.05933765597262203, 0.9533028378635036, -7.38292932510598e-05), 18: (-1.6240763804220337, 0.12083370119748225, -0.005450773239135731), 19: (0.16783953998993503, 0.868483143283497, 0.00023294389247896508), 20: (-1.1688359416468115, 0.25692596352159114, -0.0050141215324401855), 21: (-1.2758445829146297, 0.21738809261491887, -0.0051697060465812905), 22: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 23: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 24: (-1.0716031571730849, 0.29732202084518844, -0.0033075079321860934), 25: (-0.5141001827100227, 0.6131096575601327, -0.0010094165802002064), 26: (-0.5208521245718691, 0.6084871752646555, -0.0013989895582199319), 27: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 28: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 29: (-0.3114520296882886, 0.7588485536702768, -0.0008454144001007191), 30: (-0.9634391729116165, 0.3474299688920349, -0.0041394203901291005), 31: (-0.7690457529567646, 0.45132096835816593, -0.002662914991378773), 32: (-0.24857079295377213, 0.8063597754513214, -0.0007111832499503978), 33: (-1.066340630532658, 0.2996329236992549, -0.00435973554849628), 34: (-1.307672890608816, 0.20658688109408466, -0.00601511597633364), 35: (-0.5262795155351091, 0.6047837023066619, -0.0013959199190139993), 36: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 37: (-0.8581215038175375, 0.4015212353378972, -0.0022276133298874123), 38: (-1.1347974567091397, 0.2705742661790784, -0.004529349505901337), 39: (-0.40565957624116045, 0.6895245499887311, -0.0012224495410918523), 40: (-0.6406568097495143, 0.5293975255966856, -0.001646570861339569), 41: (-0.5853719355747461, 0.5651861053067004, -0.0012907266616821733), 42: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 43: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 44: (-0.9489773932241476, 0.35454747994196123, -0.0018764838576317278), 45: (-1.0683973297232137, 0.29872824067494963, -0.002051642537116982), 46: (-0.8369860284159304, 0.4130046106090671, -0.0017104774713516402), 47: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 48: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 49: (0.5390550742003877, 0.5961094768520061, 0.0008374646306038014), 50: (0.509430426665857, 0.6163164151134919, 0.0011194631457328574), 51: (-0.45904937428945386, 0.6514069685670868, -0.0009944468736648449), 52: (1.1920889842998112, 0.24790333603105016, 0.002108065783977542), 53: (-0.009001506377464103, 0.9929117780184165, -1.8292665481589587e-05), 54: (-0.8938228627007104, 0.38259927561296503, -0.002469956874847412), 55: (-0.7409374164917433, 0.4677905743603378, -0.001771286129951477), 56: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 57: (-0.189613780755316, 0.8516227653999674, -0.0003061845898628124), 58: (-1.0901153668773997, 0.28929525424952246, -0.0031058117747306824), 59: (0.08010588384267328, 0.9369907869551097, 0.0001716315746307373), 60: (-0.9435540905679378, 0.357242098328733, -0.0012674659490585216), 61: (-1.2222354524469252, 0.23656526113198942, -0.001689542829990398), 62: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 63: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 64: (-1.3416058181990118, 0.1955398725124688, -0.002584441006183602), 65: (-1.2939101689160104, 0.2112046721344239, -0.0025296270847320335), 66: (-1.4975951364366846, 0.15066797542950505, -0.0037569612264632957), 67: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 68: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 69: (-0.8947327697137233, 0.38212485012500774, -0.0015893116593361234), 70: (-1.0199802863067917, 0.3205505139576873, -0.0019883275032043235), 71: (-1.3887894251705182, 0.180962945560633, -0.003145812451839425), 72: (-1.2717330742018775, 0.21881483186899028, -0.0028291866183280945), 73: (-1.084865105941498, 0.29155555727453236, -0.001851402223110199), 74: (-1.2807612794340442, 0.2156914457213122, -0.0027890652418136597), 75: (-1.4655199689952445, 0.1591328174936716, -0.0026975095272063765), 76: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 77: (-1.4125480364169472, 0.17395977281344674, -0.0030394211411476357), 78: (-1.3956229928847093, 0.1789258505199363, -0.003412824869155895), 79: (-1.4035888265476122, 0.176574526295681, -0.002786573767662115), 80: (-1.9538375680947229, 0.06560263570584778, -0.006239135563373577), 81: (-0.9555164699181337, 0.35131696447438465, -0.0018814221024513467), 82: (-1.4043987162522156, 0.17633686542850158, -0.0023443534970283286), 83: (-1.3046279959727924, 0.2076016532099325, -0.002861945331096627), 84: (-1.246634060229046, 0.2276824161542174, -0.0028518155217170382), 85: (-1.2310797936963471, 0.2333151456783793, -0.002442561089992523), 86: (-1.246634060229046, 0.2276824161542174, -0.0028518155217170382), 87: (-1.4632692623450536, 0.15974111921326092, -0.003566589951515209), 88: (-1.2310797936963471, 0.2333151456783793, -0.002442561089992523), 89: (-1.2807847243339239, 0.21568338013571242, -0.002878469228744518), 90: (-1.2411009999355749, 0.22967401659770825, -0.0032365977764129417), 91: (-1.1329422043775377, 0.27133332550132955, -0.0024403154850006215), 92: (-1.055336190633787, 0.3045069992126154, -0.0014326974749565013), 93: (-0.6382379117121378, 0.5309370059872862, -0.0010516881942749245), 94: (-1.1996396689886066, 0.245025619144954, -0.0029798626899719682), 95: (-1.2310797936963471, 0.2333151456783793, -0.002442561089992523), 96: (-1.2310797936963471, 0.2333151456783793, -0.002442561089992523), 97: (-1.271510855089988, 0.21889215087735586, -0.003068020939826943), 98: (-1.0164371450382368, 0.32219065132596236, -0.002341513335704759), 99: (-0.9881038745787944, 0.3355189738627954, -0.0011626988649369174), 100: (-0.8175833210164493, 0.42372975642343835, -0.0007899612188339011), 101: (-1.150245744457188, 0.26431475506382923, -0.0010504990816117), 102: (-1.2234240597852992, 0.23612647059559277, -0.001219561696052529), 103: (-1.2639236569598344, 0.22154477783200927, -0.002597288787364982), 104: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 105: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 106: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 107: (-1.332367803736338, 0.1985000284689346, -0.001470197737216905), 108: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 109: (-1.3690711449738875, 0.18694535568447485, -0.0014252185821533314), 110: (-1.6968615586433409, 0.10604454993733481, -0.0023965135216712508), 111: (-1.6758607670247108, 0.11014403108217709, -0.002761526405811343), 112: (-1.5574983394136035, 0.13585305366721342, -0.0022291630506515725), 113: (-1.6498443215634835, 0.1154094152679027, -0.0020239174365997425), 114: (-1.325939630449497, 0.2005806263497934, -0.002689258754253354), 115: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 116: (-1.713140214147639, 0.10295740800574543, -0.002138154208660148), 117: (-1.646052449263405, 0.11619442187733992, -0.002467985451221455), 118: (-1.373653937332699, 0.1855410625486748, -0.0016740038990974426), 119: (-1.5631035035748297, 0.13453111299807083, -0.0018570870161056519), 120: (-1.478995505174084, 0.15553026955592655, -0.0020525068044662254), 121: (-1.6110691092376708, 0.12365268822716842, -0.0019341111183166504), 122: (-1.6883917964470028, 0.10768191643732017, -0.0021598696708680087), 123: (-1.7141499880632753, 0.1027684786821179, -0.0023778527975082397), 124: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 125: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 126: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 127: (-1.4938415223919361, 0.1516390377816124, -0.004572677612304676), 128: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 129: (-1.3129193599736824, 0.20484752227316716, -0.005023352801799774), 130: (-0.8583608420079871, 0.40139239324552134, -0.0018903136253357045), 131: (-1.1694472967345249, 0.2566856322955363, -0.003980498015880574), 132: (-1.1360628993963449, 0.2700574248940383, -0.004126235842704773), 133: (-1.6375335808188212, 0.11797456457409827, -0.008619713783264149), 134: (-1.2194356546819272, 0.23760130541591554, -0.004548071324825265), 135: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 136: (-1.2371013758037184, 0.23112198127659128, -0.004518668353557576), 137: (-1.2420058982862812, 0.22934738959328374, -0.004030098021030437), 138: (-1.482196568669953, 0.15468439566172634, -0.005874595046043374), 139: (-0.7966730898476708, 0.4354834302361442, -0.0019156903028488825), 140: (-1.1543869206064388, 0.2626552548600694, -0.003194358944892839), 141: (-0.8446116343344517, 0.408837395689407, -0.0022171080112457497), 142: (-0.7843797043545297, 0.4424875467617805, -0.00208998024463658), 143: (-1.146384039521738, 0.2658692879092321, -0.003623847663402624)}, 'noun_phrase': {0: (-0.059404197340731345, 0.9539283013239545, -0.00019150078296659157), 1: (0.2827684870284805, 0.7837500846544987, 0.0008828014135360829), 2: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 3: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 4: (-0.16916203883537503, 0.8694101768114813, -0.0005187541246414185), 5: (2.7188632924943, 0.023651784815193116, 0.009782162308692888), 6: (0.9267465809386195, 0.37824006408654753, 0.002643743157386802), 7: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 8: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 9: (0.1553281404548011, 0.8799905200007769, 0.0003853142261505127), 10: (2.042603803188805, 0.0714635916706121, 0.006852361559867848), 11: (0.11978999502582492, 0.9072809340766865, 0.00039168596267702416), 12: (1.7358431873080704, 0.11661028811499242, 0.004445970058441162), 13: (0.8352533038787895, 0.4251919941350234, 0.0024510204792022594), 14: (0.5276772076773238, 0.6104835859831382, 0.0019851565361022616), 15: (2.623989835452748, 0.027629588005256053, 0.00590239763259881), 16: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 17: (0.899885400693973, 0.39162312295973234, 0.0020317703485489003), 18: (0.19112814437559877, 0.8526673500261107, 0.0006267488002777433), 19: (1.8851414600043277, 0.09204771789818098, 0.0039432287216186745), 20: (1.9481795871832923, 0.0832096083036202, 0.009910395741462685), 21: (2.388838197839223, 0.04063470690309999, 0.01304997205734254), 22: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 23: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 24: (1.986108497553953, 0.07828579223734768, 0.01063987910747527), 25: (2.9010552828347635, 0.01756516657000177, 0.019302809238433805), 26: (1.9697079522156795, 0.08037987401938257, 0.01144414544105532), 27: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 28: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 29: (3.006855421113125, 0.014791051196128932, 0.015049532055854797), 30: (2.9458463949005096, 0.016330797875466346, 0.018118998408317544), 31: (2.225783411275994, 0.05306078927680244, 0.011234748363494917), 32: (2.1868453891588238, 0.05654022876027513, 0.014129883050918557), 33: (2.1398547510197097, 0.06103566280519921, 0.012387192249298129), 34: (2.1289182482555518, 0.062130632265630044, 0.012444487214088418), 35: (2.3401750146396885, 0.04400766060431682, 0.010740101337432861), 36: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 37: (1.4027578105564296, 0.19423086702518264, 0.006725168228149392), 38: (2.189229792682724, 0.05632091576506185, 0.011613276600837719), 39: (2.715432548796257, 0.023784998621499708, 0.011719131469726474), 40: (1.8455854852216242, 0.09803705351169469, 0.010246938467025735), 41: (2.0597568391954546, 0.06950750176674306, 0.011862045526504494), 42: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 43: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 44: (2.1407480241861574, 0.06094705815844367, 0.013551744818687428), 45: (2.821955705898563, 0.019983100357021288, 0.019610649347305276), 46: (2.0108135877737956, 0.0752292248974375, 0.012152406573295582), 47: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 48: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 49: (2.791275816514626, 0.021009994005131174, 0.01436871588230132), 50: (2.899181632937714, 0.01761883543492855, 0.018244919180870012), 51: (2.2642454795287064, 0.049829620819470316, 0.011322516202926625), 52: (2.2631511194968574, 0.04991883509184121, 0.014994788169860884), 53: (2.1331777822232794, 0.06170192204766262, 0.01296908259391788), 54: (1.9738854362225988, 0.07984147514886539, 0.012726208567619357), 55: (3.2270638975436823, 0.010371318905303612, 0.01232927441596987), 56: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 57: (2.3059216084842595, 0.04654615390440723, 0.011069506406784002), 58: (1.8617153707809084, 0.0955519393192057, 0.011489596962928728), 59: (3.1403809797085143, 0.011920711594495271, 0.012796837091445856), 60: (2.4737431050712355, 0.035352498876522205, 0.002737727761268627), 61: (1.6113005706540569, 0.14157378011701163, 0.0019494831562042458), 62: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 63: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 64: (1.53743688384478, 0.1585651949016809, 0.002130213379859913), 65: (0.6035986920680937, 0.5610132145872897, 0.0003473639488220659), 66: (1.0077287109915996, 0.339908305180983, 0.0011471956968307495), 67: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 68: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 69: (1.6021500606216978, 0.14358622667512966, 0.001747226715087924), 70: (1.578855155842922, 0.14882583018562254, 0.001754996180534374), 71: (-0.9331597303937214, 0.3750941961722959, -0.0008138090372085682), 72: (1.5852476471891817, 0.14737120760043948, 0.0018799364566802756), 73: (-0.7271373912647012, 0.4856248944885333, -0.0007396459579467329), 74: (-1.0830423046211022, 0.30695096566719837, -0.0011349618434906006), 75: (0.4343451327383166, 0.6742649308269852, 0.0006856977939605491), 76: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 77: (0.3532057330609686, 0.7320659772780975, 0.0005770146846771351), 78: (0.859673839666449, 0.41228160487660115, 0.0012849241495132557), 79: (-0.788284034364636, 0.45079352471553624, -0.0010927379131316917), 80: (1.3336937425899686, 0.21507071520896828, 0.0051479965448379406), 81: (1.9807669198305076, 0.07896205495606001, 0.012577366828918468), 82: (0.06972756394308077, 0.9459352163099144, 0.00014835894107817493), 83: (-1.1598508863626962, 0.27595292042177283, -0.003138679265975941), 84: (2.502593835967706, 0.03371801951659803, 0.009901961684226968), 85: (2.4071531852960577, 0.039432667879380455, 0.007238087058067333), 86: (2.502593835967706, 0.03371801951659803, 0.009901961684226968), 87: (2.1891701035114055, 0.05632639574221948, 0.007605543732643161), 88: (2.4071531852960577, 0.039432667879380455, 0.007238087058067333), 89: (2.2839470834087656, 0.04825000877654509, 0.006980195641517639), 90: (2.1001663651565923, 0.06510070417770156, 0.007232803106307972), 91: (2.217126449173077, 0.05381582701278829, 0.006793224811553922), 92: (2.9082126428940716, 0.01736168617648168, 0.005811285972595226), 93: (2.4387499353236053, 0.03744133009823867, 0.007071739435195901), 94: (2.007662306976666, 0.07561267274669725, 0.006354174017906167), 95: (2.4071531852960577, 0.039432667879380455, 0.007238087058067333), 96: (2.4071531852960577, 0.039432667879380455, 0.007238087058067333), 97: (1.634712174810816, 0.13654009651084903, 0.005153477191925049), 98: (2.630136944145678, 0.02735251082667631, 0.008446931838989258), 99: (1.9107332969923785, 0.0883578431583259, 0.0041037440299988015), 100: (2.290628562098974, 0.047725556273620136, 0.004274970293045022), 101: (2.0933821116848716, 0.06582125831616147, 0.006068557500839233), 102: (1.627603621013519, 0.13805114116980974, 0.0039044678211211936), 103: (2.585950424413753, 0.02940816869870594, 0.008170664310455322), 104: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 105: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 106: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 107: (0.7945155788504622, 0.4473388366512432, 0.0008462429046631192), 108: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 109: (0.9942624898356804, 0.34607322156604914, 0.0014589965343475009), 110: (1.455289556974256, 0.17956424097009338, 0.0012722373008727805), 111: (-0.40311714339013993, 0.6962705053346807, -0.00046482086181637294), 112: (-1.3995010159139174, 0.19517324349859388, -0.0016595661640167458), 113: (0.31736921700451814, 0.7582025900197144, 0.0004179954528809038), 114: (0.48352736126717916, 0.6402678088763173, 0.0006562054157256969), 115: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 116: (0.3778781950419369, 0.7142790483131387, 0.0004267096519470215), 117: (-0.2685125850444927, 0.794358177071614, -0.0003172755241394043), 118: (0.17690180491132731, 0.8635025141437851, 0.0002203315496444591), 119: (0.10644048277647157, 0.9175676736078939, 0.0001502275466918057), 120: (-1.1374421926809106, 0.2847282753703033, -0.0014120459556580256), 121: (-0.26892403073778803, 0.7940513734382388, -0.00041577816009519264), 122: (-0.7879673008075888, 0.4509695902220746, -0.000934952497482322), 123: (0.7339417108574199, 0.48166582547252856, 0.001118680834770236), 124: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 125: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 126: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 127: (1.3815475677348008, 0.20043898665844112, 0.008650729060173057), 128: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 129: (1.7839885104955835, 0.10809309766930562, 0.012804698944091808), 130: (1.993676236811333, 0.07733712993116255, 0.010171923041343678), 131: (1.3413500042767823, 0.2126705993332483, 0.009033668041229292), 132: (2.9252832409934597, 0.01688605966307884, 0.01588654816150664), 133: (0.8375860459376804, 0.42394687359118666, 0.006955501437187206), 134: (2.509183986679589, 0.033355385995736835, 0.014794331789016735), 135: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 136: (1.2116436551718137, 0.25649963092484557, 0.0063264548778533825), 137: (2.432500896362837, 0.03782707395283051, 0.015213516354560863), 138: (1.9800145535322926, 0.07905775295108508, 0.014388802647590682), 139: (2.494999426821794, 0.03414080250541585, 0.009348744153976463), 140: (2.19400933262387, 0.055883794369236614, 0.009948676824569791), 141: (2.526709734292557, 0.03240986873880421, 0.01006338596343992), 142: (2.5065990466500887, 0.03349715943896789, 0.009605455398559615), 143: (1.9558113216511723, 0.0821958264018493, 0.0078074634075164795)}, 'original_noun_phrase': {0: (17.928403897297034, 4.934233780617678e-65, 0.006354058471818802)}}, 'dann': {'named': {0: (-0.9867294195570829, 0.33617516260200075, -0.0019272759556770436), 1: (-1.2452428997465435, 0.22818190293584237, -0.0020560294389724842), 2: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 3: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 4: (-1.3566084641525438, 0.19080716428991337, -0.0022081181406974904), 5: (-0.5066744948466355, 0.6182126598611556, -0.000635613501071941), 6: (-0.9807570537436318, 0.33903683077187907, -0.0012205190956592615), 7: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 8: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 9: (0.38819756753275503, 0.7021851295065851, 0.0004766911268234142), 10: (-1.1755225931788928, 0.2543065174989379, -0.0024114027619361877), 11: (-0.9917942822648065, 0.3337615197075454, -0.0015550673007965199), 12: (0.2774077479936443, 0.7844628678651137, 0.00034148842096326515), 13: (-0.8339300993217477, 0.41468221042413356, -0.0013277605175971985), 14: (-1.5202488930803648, 0.14491564372650456, -0.004161497950553872), 15: (1.6972759586236632, 0.10596498853229763, 0.003535556793212935), 16: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 17: (1.0016507492575955, 0.32909914609836277, 0.0016669109463691711), 18: (-1.4578890030812197, 0.16120294861894072, -0.00389276146888734), 19: (1.6277437842392213, 0.12004876977937895, 0.0032092809677123357), 20: (-1.5009283623118574, 0.1498099651304123, -0.006498688459396329), 21: (-1.6109635955680688, 0.12367578045278915, -0.0068493291735649), 22: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 23: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 24: (-1.2891871810569517, 0.2128078300899039, -0.003938792645931222), 25: (-0.999616474258732, 0.33005766075827636, -0.0026086762547492537), 26: (-1.3924457849298786, 0.17987067769141993, -0.002627967298030859), 27: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 28: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 29: (-0.636680581148872, 0.5319294488758434, -0.001921905577182803), 30: (-1.0990154391398026, 0.2854928455095363, -0.0049043744802474976), 31: (-0.8290505966473767, 0.41736989975170846, -0.0022954314947128407), 32: (-1.0159921359762007, 0.3223970657175921, -0.003628966212272655), 33: (-1.2450212130949883, 0.22826157586858026, -0.005895775556564287), 34: (-1.5628644896088133, 0.13458726283948527, -0.00883282124996182), 35: (-0.2775163543425045, 0.7843807340327269, -0.0007456392049789429), 36: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 37: (-0.9951581226427932, 0.33216518239122017, -0.0030926048755645086), 38: (-1.3996153864589451, 0.17774426418474745, -0.006066711246967338), 39: (-0.5049933294009642, 0.6193707536363772, -0.0014558494091033714), 40: (-1.234195357923345, 0.23217842040776085, -0.0030386224389076233), 41: (-0.8005745624243296, 0.4332750978182609, -0.0009572267532348411), 42: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 43: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 44: (-0.8849455143984167, 0.38724837754189967, -0.0010334163904189841), 45: (-0.9632115368751197, 0.34754123658872504, -0.0013620018959045743), 46: (-0.844525009189141, 0.4088845819892061, -0.0009750984609127045), 47: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 48: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 49: (1.712832759416862, 0.10301499218695129, 0.0028555780649184848), 50: (-0.7001632511296481, 0.49231043837757804, -0.001612403988838218), 51: (0.6411093264132413, 0.5291097990458402, 0.0008345276117325273), 52: (1.1293342529446195, 0.2728139954959521, 0.0013565793633460999), 53: (-0.7914214552349883, 0.43846704390652413, -0.0017496749758720287), 54: (-1.79079785815554, 0.08927231509681495, -0.007404179871082328), 55: (0.7673307186972625, 0.45231561228052297, 0.001419311761856057), 56: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 57: (-0.4790246383966941, 0.6373879263513851, -0.0009406745433807595), 58: (-1.236138871462387, 0.231471475936924, -0.003477537631988492), 59: (1.5413282816567015, 0.13972747015120882, 0.003300172090530351), 60: (1.4461796521472785, 0.16442213427483043, 0.002373544871807076), 61: (-0.2265780880045984, 0.8231716551620237, -0.00032638758420944214), 62: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 63: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 64: (-0.31440272416523185, 0.7566414288566066, -0.0005337491631507985), 65: (0.2993878914671035, 0.7678944669361603, 0.00044022351503375523), 66: (0.8265976246560939, 0.41872520800562596, 0.0012576088309287914), 67: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 68: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 69: (0.6203414116234746, 0.5424028527539361, 0.000960740447044417), 70: (1.1831346456523935, 0.25134904624556187, 0.0019820079207419905), 71: (-0.12513206171331423, 0.9017335024184183, -0.0002091348171234242), 72: (-0.32320675290749284, 0.7500688061850564, -0.0005764201283455228), 73: (-0.193046127530431, 0.8489714599354892, -0.0003615766763687134), 74: (1.032006781588347, 0.3150274090199826, 0.0016774669289589261), 75: (-0.6984208369015937, 0.49337462560092404, -0.0011544555425644365), 76: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 77: (-1.0163627556385761, 0.3222251498500238, -0.0023468554019928645), 78: (0.21944534007987893, 0.8286434657475691, 0.0002961188554763683), 79: (-1.0705322236151504, 0.29779124771528703, -0.0024858862161636353), 80: (-1.1413412513774956, 0.26790949133339526, -0.002657562494277954), 81: (-0.09323127588617472, 0.9266959202957807, -0.00019965767860408157), 82: (-0.7831413718382906, 0.44319692478841277, -0.0005579873919487333), 83: (-0.7579305183526267, 0.45779103790086184, -0.0009718358516693226), 84: (-1.207548910324389, 0.2420384788511308, -0.001984208822250366), 85: (-1.099413381128593, 0.285323687638757, -0.0022287189960479847), 86: (-1.207548910324389, 0.2420384788511308, -0.001984208822250366), 87: (-1.3110906270483678, 0.2054524906583894, -0.003223280608654011), 88: (-1.099413381128593, 0.285323687638757, -0.0022287189960479847), 89: (-0.9996574191640942, 0.3300383489980435, -0.0009561315178870933), 90: (-1.0516519210829798, 0.3061514750470006, -0.003016935288906064), 91: (-0.9291803564474265, 0.3644510598083106, -0.00220405012369157), 92: (-1.175401010869867, 0.25435396664616183, -0.0025725707411766163), 93: (-0.06055605743868714, 0.9523452087615489, -5.860477685926124e-05), 94: (-0.8225972785487173, 0.42094146169540103, -0.0009408704936504364), 95: (-1.099413381128593, 0.285323687638757, -0.0022287189960479847), 96: (-1.099413381128593, 0.285323687638757, -0.0022287189960479847), 97: (-1.0611938437803885, 0.30190549491190133, -0.0031025767326355314), 98: (-0.9387229241331986, 0.359654215235255, -0.002385479211807262), 99: (-1.1336384223047353, 0.2710482898355101, -0.0032224059104919434), 100: (-1.2423371530833762, 0.2292279110528442, -0.0028264492750167403), 101: (-1.1430441857127842, 0.267219224605781, -0.002621537446975797), 102: (-1.326179076988338, 0.2005028178374613, -0.003980955481529214), 103: (-1.0804694565232806, 0.29345778095464037, -0.0030408114194869773), 104: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 105: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 106: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 107: (1.1370978192396186, 0.2696352794586459, 0.0008532896637916676), 108: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 109: (0.9661359028032891, 0.34611368012936705, 0.0007006019353866799), 110: (0.5873272559320057, 0.5638991515795342, 0.0004150778055190818), 111: (-0.8768927671793506, 0.3914976991308914, -0.0010815337300300598), 112: (-0.492585715107945, 0.6279493442868852, -0.0005521222949028126), 113: (-0.8876679884782366, 0.38581866399279774, -0.001028694212436676), 114: (1.9919857371898635, 0.0609425756947396, 0.0017527014017104658), 115: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 116: (-0.4936055765692642, 0.6272421392862972, -0.00036787688732142776), 117: (-0.17356616266441877, 0.8640422455233715, -0.000146953761577584), 118: (0.05262973954484554, 0.9585763353239114, 4.5648217201210706e-05), 119: (-0.7937550885372135, 0.4371396708869957, -0.0008041948080061978), 120: (-0.8291497433376069, 0.4173151783385879, -0.0008700907230377863), 121: (-0.8888635595557307, 0.38519190917442914, -0.0010397553443908247), 122: (-0.8103474787237596, 0.4277741836374571, -0.0008706778287886907), 123: (-0.4076069663627391, 0.6881183421446317, -0.00041303932666780785), 124: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 125: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 126: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 127: (-1.6980236247314056, 0.10582157209362963, -0.00728069692850114), 128: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 129: (-1.6014726497406484, 0.1257678426921422, -0.0063585817813873735), 130: (-1.389156437285621, 0.18085306802344098, -0.006095549464225802), 131: (-1.203391578778529, 0.2436051407849732, -0.004777027666568789), 132: (-0.7429261706312886, 0.4666135784705717, -0.0031158849596977234), 133: (-1.4174551873754109, 0.17254091080034387, -0.007822236418724038), 134: (-0.4066098503932951, 0.6888382131061397, -0.0015775725245475991), 135: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 136: (-1.341924812338639, 0.19543828416746917, -0.0055364444851875305), 137: (-1.0550217584710877, 0.30464709845296795, -0.003987997770309448), 138: (-1.216419014520918, 0.23872146186183196, -0.005159489810466766), 139: (-0.6273524459348028, 0.537895201199105, -0.002090153098106451), 140: (-1.2759050472617128, 0.21736716485991053, -0.004835143685340881), 141: (-1.0370987066099429, 0.3127094386870817, -0.004217070341110185), 142: (-0.929467327108055, 0.3643061783437216, -0.0036227077245712946), 143: (-0.7362115149546515, 0.47059460455096525, -0.0027908772230148537)}, 'noun_phrase': {0: (0.32176681478719044, 0.7549769578469689, 0.0012385070323944203), 1: (1.4766226480042148, 0.17388890034183893, 0.003506255149841264), 2: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 3: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 4: (0.918911793265633, 0.38210910874179593, 0.0020947158336639404), 5: (3.0882193176317525, 0.012966697975766992, 0.011990487575531006), 6: (2.5676534241826596, 0.03030423078546963, 0.008141294121742249), 7: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 8: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 9: (1.547013422172735, 0.15626424608948053, 0.005729144811630227), 10: (2.4550281157408893, 0.036454779706908066, 0.007289832830429055), 11: (1.2572070136387585, 0.2403228820471545, 0.002852046489715543), 12: (1.9338228423299695, 0.08514892994019035, 0.0050469756126403365), 13: (1.2390259791919658, 0.24667429300686539, 0.0036864370107650424), 14: (0.48488703356022395, 0.6393399309607386, 0.0018735915422439797), 15: (3.0180731625373034, 0.014524585908708705, 0.01241346597671511), 16: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 17: (2.2035700060731642, 0.05501930296620119, 0.008129897713661238), 18: (0.9132758823472046, 0.38490984600892886, 0.0031307876110077126), 19: (4.048596889831625, 0.002891013585731314, 0.012764960527420044), 20: (1.8079507621380635, 0.1040721817494618, 0.01314978301525116), 21: (2.081269316041891, 0.0671269195079296, 0.013036927580833413), 22: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 23: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 24: (1.3841530851981612, 0.19966731786475816, 0.009304749965667769), 25: (2.3562657772333533, 0.042862819899520725, 0.019379985332489025), 26: (1.8930350680862085, 0.09089440506378557, 0.012767596542835213), 27: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 28: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 29: (2.0354686831459485, 0.07229272014087024, 0.013244447112083446), 30: (2.1598565954741553, 0.059081257858952, 0.015896314382553067), 31: (1.6534747802033491, 0.13262338073074043, 0.01141127645969392), 32: (1.6880543973538569, 0.12567104980516453, 0.012637373805046093), 33: (1.8361282815985915, 0.09952210292351608, 0.012894773483276334), 34: (1.3368283216280035, 0.21408532009435072, 0.011225506663322449), 35: (2.609624460078373, 0.028288172896220688, 0.013556665182113625), 36: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 37: (1.91107212434062, 0.08830993899457726, 0.011751183867454551), 38: (1.4523067103124898, 0.1803705184257729, 0.010884010791778587), 39: (2.148512238971029, 0.06018215146050553, 0.01318176388740544), 40: (2.0638939744849827, 0.06904346740782925, 0.015153759717941251), 41: (2.218225015959506, 0.05371943471530602, 0.01549370288848878), 42: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 43: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 44: (1.9175249664254657, 0.08740228790429246, 0.012787795066833474), 45: (2.826005784456006, 0.01985141251957473, 0.022183382511138938), 46: (2.400549429629872, 0.03986196143203122, 0.015419840812683105), 47: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 48: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 49: (2.505639567322363, 0.033549936127881964, 0.01723344624042511), 50: (2.986644043743537, 0.015283907001565568, 0.01938931047916409), 51: (2.1677734204354304, 0.05832459243502238, 0.013565436005592346), 52: (2.0865272636900545, 0.06655711860338713, 0.014177829027175903), 53: (2.297315725716465, 0.04720626843717642, 0.01569410562515261), 54: (1.9330067188475204, 0.08526044851507485, 0.014338076114654541), 55: (3.033151673430603, 0.01417417142098351, 0.015086108446121238), 56: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 57: (2.593551740533509, 0.029043780052692654, 0.014308959245681763), 58: (2.2853531705382273, 0.048139171777373625, 0.015052309632301308), 59: (2.8422391488625958, 0.019332403640807263, 0.014718294143676758), 60: (2.3478558177617104, 0.04345746117319644, 0.0067915409803390725), 61: (1.534866195829691, 0.15918794928573535, 0.004163616895675626), 62: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 63: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 64: (0.9611441817827515, 0.3615892674135417, 0.0029742836952209584), 65: (0.44765017690051645, 0.6649861385672213, 0.0012340337038039828), 66: (0.3762017454388502, 0.7154820324859454, 0.0012960508465766907), 67: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 68: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 69: (1.1967354350238133, 0.2619817086495285, 0.003531971573829673), 70: (1.0992289864297298, 0.30020134809395893, 0.0034032225608826017), 71: (0.1587061438478116, 0.8774045689061314, 0.0005131065845489502), 72: (0.42648493986091657, 0.6797741371312713, 0.0011184543371201006), 73: (0.2655708572575739, 0.7965528273142781, 0.0005496859550476185), 74: (0.5492131345664989, 0.5962205939672494, 0.0016154468059539906), 75: (0.9987362359534044, 0.3440158964787752, 0.0021351695060729536), 76: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 77: (0.8569000352002448, 0.41373418445484245, 0.0017239898443222046), 78: (0.9949937974038945, 0.34573629026476604, 0.00306892693042754), 79: (0.3522973172050324, 0.7327241932984645, 0.0006840765476227029), 80: (2.0212493136417597, 0.07397262899774995, 0.01194812059402467), 81: (1.9236852369726192, 0.08654401633515652, 0.016407230496406577), 82: (-0.47962578956006724, 0.6429340136693806, -0.0010462343692779763), 83: (-0.4520202174282119, 0.6619515180399729, -0.0013109803199767844), 84: (2.009315991639913, 0.07541122018983434, 0.009284698963165305), 85: (1.5616612390054907, 0.15280223525836137, 0.007558640837669384), 86: (2.009315991639913, 0.07541122018983434, 0.009284698963165305), 87: (2.3551862846862446, 0.04293869528060917, 0.01398688554763794), 88: (1.5616612390054907, 0.15280223525836137, 0.007558640837669384), 89: (3.543341051175119, 0.006281111838804635, 0.009105792641639743), 90: (2.2544266917526525, 0.050635688571756834, 0.012923109531402577), 91: (2.0509966809881472, 0.07049997744201736, 0.012570643424987815), 92: (2.3482289288575813, 0.04343090776926426, 0.010921820998191833), 93: (2.525897669445957, 0.0324530813336942, 0.013159114122390791), 94: (3.6530274871827073, 0.005292558199144063, 0.009726472198963165), 95: (1.5616612390054907, 0.15280223525836137, 0.007558640837669384), 96: (1.5616612390054907, 0.15280223525836137, 0.007558640837669384), 97: (2.2342428055361436, 0.052332986349772126, 0.011621281504631042), 98: (2.3261772832574, 0.04502809643828023, 0.011416107416152954), 99: (1.0855901183385217, 0.3058808007821362, 0.006092655658721857), 100: (0.8517691553364071, 0.4164304708552665, 0.0038651406764984575), 101: (0.9305035802434644, 0.37639482087757603, 0.005082440376281805), 102: (0.42063581835907643, 0.6838868765081995, 0.002441000938415594), 103: (1.2413875033361923, 0.24584158719440594, 0.007783329486847013), 104: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 105: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 106: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 107: (1.1461810666772596, 0.28128004427667713, 0.0015783518552780484), 108: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 109: (0.8042179440478184, 0.4419952411606892, 0.0012110233306885099), 110: (2.0171298896813723, 0.07446624454084619, 0.0024679958820342796), 111: (-0.1809439041613578, 0.8604207660427458, -0.0002918362617492787), 112: (-0.03487466536290386, 0.972940919216302, -4.5865774154663086e-05), 113: (0.020356245105158034, 0.9842033447427685, 3.411471843722813e-05), 114: (1.9362754442796217, 0.08481462632865278, 0.0023768365383148304), 115: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 116: (1.5074059428512663, 0.16597658306890198, 0.0015840351581573597), 117: (0.656416529562219, 0.5279806554409376, 0.0010124027729034313), 118: (-0.06917983691502126, 0.946359153392213, -0.0001084446907043124), 119: (-0.6111665245114267, 0.5562088264271956, -0.00088534355163572), 120: (-0.29190712472856417, 0.7769744306615747, -0.0003355681896209939), 121: (-1.2099638399245203, 0.2571126327336307, -0.0020089626312256303), 122: (-1.2187103075203463, 0.25393388037841697, -0.0016553401947021484), 123: (-0.11224944773483804, 0.9130893441532238, -0.0001426100730895774), 124: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 125: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 126: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 127: (0.8188097810674024, 0.43403984418158514, 0.008369690179824807), 128: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 129: (1.344219612684229, 0.21177688246528342, 0.014003562927246083), 130: (1.2182077609202966, 0.25411565011020515, 0.010748335719108604), 131: (1.4284011620972987, 0.18694678849210875, 0.014729905128478993), 132: (2.448221417005585, 0.03686411709070902, 0.02071168422698977), 133: (1.307978779444202, 0.22329990056242666, 0.014133965969085671), 134: (1.8508231489365952, 0.0972235339920648, 0.016493734717369035), 135: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 136: (1.1066228491541468, 0.29715705817775806, 0.010728123784065235), 137: (1.9938421708074958, 0.07731645237533531, 0.018206831812858537), 138: (1.3997402905893874, 0.19510387413129615, 0.01494083404541019), 139: (1.6489407277297508, 0.13356042651209937, 0.00880095362663269), 140: (1.617289305446289, 0.1402704745815911, 0.009810954332351685), 141: (2.1037029662360927, 0.06472810658485369, 0.012146306037902899), 142: (2.1458646654182423, 0.06044193061800628, 0.01131196022033687), 143: (2.2606987112332058, 0.05011932905829078, 0.016005074977874734)}, 'original_noun_phrase': {0: (17.549553772156735, 1.2464257307518868e-62, 0.008245057353956864)}}}, 'EI_joy': {'non_dann': {'named': {0: (0.927130428671265, 0.3654871282317217, 0.0019718199968338235), 1: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 2: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 3: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 4: (1.2531693205122543, 0.22534723569449014, 0.0024197280406951793), 5: (1.3174986525278145, 0.2033387726702804, 0.002490296959877014), 6: (1.2436306837720958, 0.22876181256660205, 0.0026371866464614535), 7: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 8: (1.2518942904312103, 0.22580137345216844, 0.0027422189712524303), 9: (1.3042185510974138, 0.20773840637308943, 0.0022828787565231656), 10: (1.327117228975429, 0.2001981932160694, 0.002530188858509086), 11: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 12: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 13: (1.7428398231497895, 0.09752375837723172, 0.0035621538758278115), 14: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 15: (1.3023021240927164, 0.20837942480472954, 0.0024586543440818787), 16: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 17: (1.4053663411093298, 0.17605325521431578, 0.003052820265293077), 18: (1.3486637530996801, 0.1933019257809402, 0.0028375104069709722), 19: (1.334510768970373, 0.1978102185004114, 0.0025804668664932695), 20: (0.9648078850354396, 0.3467614634674251, 0.003345428407192208), 21: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 22: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 23: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 24: (1.1873555411945744, 0.24972033392870072, 0.004133212566375721), 25: (1.0341270734019747, 0.31406072117995326, 0.0029740095138549583), 26: (0.9954487568479664, 0.33202751012987175, 0.003394991159439087), 27: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 28: (1.0148970597038753, 0.3229054061666836, 0.0026780590415000916), 29: (1.1008968462715054, 0.2846937389346681, 0.0031256809830665366), 30: (0.8866396932483286, 0.38635826577565757, 0.0030403494834900235), 31: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 32: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 33: (1.1269897254497583, 0.2737793640146807, 0.0042918160557746665), 34: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 35: (1.2727763621511254, 0.21845211311102822, 0.0037619456648826377), 36: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 37: (1.1800763632204914, 0.2525341357443367, 0.003792211413383484), 38: (1.0915672108272454, 0.28867247361033194, 0.0041485175490379445), 39: (1.5784224237170579, 0.13097295691677174, 0.004382364451885223), 40: (0.3706764954299417, 0.7149789285371184, 0.001211306452751193), 41: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 42: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 43: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 44: (0.9510429695683896, 0.35352483469689244, 0.0026509761810302734), 45: (0.20321680659982658, 0.841126104553226, 0.00047275424003601074), 46: (0.8183434249209005, 0.42330630820690995, 0.002901148796081554), 47: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 48: (0.4200164302561195, 0.6791850216458598, 0.0011544987559318765), 49: (0.830738390994025, 0.4164389892854148, 0.0021710038185119296), 50: (0.6555897233779486, 0.5199482142674257, 0.0017548486590385215), 51: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 52: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 53: (0.6798450782278772, 0.5048022137549548, 0.0026078701019287), 54: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 55: (0.6442546163386427, 0.5271122871918197, 0.0016197532415390126), 56: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 57: (0.89902941782725, 0.3798898471765356, 0.0022531986236571933), 58: (0.6787233338558956, 0.50549709623957, 0.0024676561355590487), 59: (1.0190850405952432, 0.3209643699571856, 0.002433972060680356), 60: (-0.16750666698169162, 0.868741420067475, -0.00022678077220916748), 61: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 62: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 63: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 64: (0.13922735134295997, 0.8907356759180701, 0.00022866278886796154), 65: (-0.3883816998147466, 0.702051151937652, -0.0005484327673912492), 66: (0.02193939548499107, 0.9827250708715176, 3.829300403596081e-05), 67: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 68: (0.002459680991383545, 0.9980631026103626, 5.003809928849634e-06), 69: (0.07916098447993353, 0.9377323781415308, 0.00017678886651989467), 70: (0.006313327585592243, 0.995028544256352, 1.0462105274222644e-05), 71: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 72: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 73: (-0.01168985920938709, 0.9907949286564823, -1.865178346632801e-05), 74: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 75: (-0.014069772953011189, 0.9889210067818506, -2.6115775108359607e-05), 76: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 77: (0.09061458634754754, 0.9287473156172278, 0.00014399439096451916), 78: (0.0459218582261186, 0.9638517649562787, 8.82759690284951e-05), 79: (0.25524464813261455, 0.8012767347316481, 0.00042962878942487404), 80: (0.4944255105613076, 0.6266738386933018, 0.0017054766416549905), 81: (0.4015194111857567, 0.6925180059712798, 0.0017325416207313316), 82: (1.4284660577708723, 0.16939123354105534, 0.002391044795513164), 83: (-0.18513527348539655, 0.8550848963311205, -0.00041247904300689697), 84: (13.623805562448862, 2.953336520776732e-11, 0.03214100897312164), 85: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 86: (13.623805562448862, 2.953336520776732e-11, 0.03214100897312164), 87: (15.613965963745777, 2.712259102454546e-12, 0.03620264828205105), 88: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 89: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 90: (10.718198861612956, 1.7025185784121546e-09, 0.022274567186832434), 91: (10.278879605250857, 3.372997728430722e-09, 0.020667591691017145), 92: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 93: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 94: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 95: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 96: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 97: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 98: (12.351255036659008, 1.5891359055687137e-10, 0.02758397608995436), 99: (17.278907545928526, 4.4642351018466344e-13, 0.0325523316860199), 100: (16.329994079427188, 1.2240473252310238e-12, 0.033421632647514354), 101: (11.532314218673289, 5.055202430326208e-10, 0.03211134076118466), 102: (12.56817891732165, 1.1814773159409936e-10, 0.031706155836582206), 103: (11.170516588095923, 8.600819301000773e-10, 0.03029073625802997), 104: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 105: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 106: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 107: (0.614321461231838, 0.5462894812398227, 0.0005189865827560092), 108: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 109: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 110: (0.5925784878290913, 0.5604504593874424, 0.0006949156522750854), 111: (0.8340101957405962, 0.41463818475912606, 0.000979569554328874), 112: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 113: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 114: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 115: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 116: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 117: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 118: (1.1260432968668515, 0.2741697736016839, 0.0013793587684631237), 119: (-0.7430493506231183, 0.46654073581996913, -0.0005963727831840293), 120: (-0.26132919939103477, 0.7966504313686013, -0.00017389357089997448), 121: (0.31679291549251387, 0.7548551317327175, 0.0003045767545700184), 122: (0.07705892819510853, 0.939382356282123, 7.993727922439575e-05), 123: (0.8649705897222905, 0.39784479313516674, 0.0009871751070022472), 124: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 125: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 126: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 127: (0.7967294372923546, 0.4354514862913137, 0.002296024560928367), 128: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 129: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 130: (0.06481738391512246, 0.9489965212009155, 0.00022808462381362915), 131: (-0.1603517261507232, 0.8742965632691216, -0.00047191381454469994), 132: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 133: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 134: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 135: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 136: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 137: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 138: (0.8698153803932404, 0.39525749190247417, 0.002507357299327856), 139: (1.6146163075395017, 0.12287847791729747, 0.002999137341976099), 140: (0.305663680124198, 0.7631844067937561, 0.00042246878147123024), 141: (1.177061576252688, 0.2537064835219378, 0.0021041855216026306), 142: (1.292512658049242, 0.21167805377758545, 0.002440388500690449), 143: (0.6976292430085975, 0.4938585345161479, 0.0015214845538139565)}, 'noun_phrase': {0: (1.0442462382617037, 0.3236071648886541, 0.014378252625465382), 1: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 2: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 3: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 4: (1.068156744919508, 0.3132615708445877, 0.014390206336975131), 5: (1.3491836965086035, 0.21023837259357941, 0.012973392009735074), 6: (0.9708868300900558, 0.3569723962343001, 0.013869673013687134), 7: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 8: (1.045011391917099, 0.32327209352648073, 0.011891117691993691), 9: (0.9160655739855799, 0.38352168605558257, 0.011061030626296986), 10: (1.0883179518932313, 0.3047382407508142, 0.016650095582008362), 11: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 12: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 13: (0.9440753908926197, 0.3697834154061227, 0.012642705440521207), 14: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 15: (1.165337749743547, 0.2738375081624465, 0.00994248390197755), 16: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 17: (1.133729001318476, 0.28620356708756717, 0.012761783599853471), 18: (0.982225291781785, 0.35165433371508403, 0.01728561520576477), 19: (1.2184084745740587, 0.2540430397974921, 0.010925072431564375), 20: (1.1388626197105938, 0.2841655220278896, 0.01670609414577484), 21: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 22: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 23: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 24: (1.1641932803103003, 0.27427767268470876, 0.015377753973007224), 25: (1.1262662246092308, 0.2891869208633998, 0.009866386651992798), 26: (1.2568708386939498, 0.24043908581333848, 0.01549505889415742), 27: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 28: (1.2433798432522831, 0.24514085951029313, 0.011981427669525146), 29: (1.3581336617829196, 0.207488489275405, 0.01307763159275055), 30: (1.1808753318955496, 0.267917640443583, 0.01745899319648747), 31: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 32: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 33: (1.0007150294443807, 0.3431088428374259, 0.014056950807571411), 34: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 35: (1.4016136397818648, 0.1945614940081961, 0.011961144208908037), 36: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 37: (0.9822887533739848, 0.35162473484834944, 0.010575699806213368), 38: (1.288707398321475, 0.2296388599983207, 0.019293841719627336), 39: (1.4805615219909143, 0.17285835724538356, 0.012446621060371421), 40: (1.336733785263309, 0.21411498289476213, 0.01734680533409122), 41: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 42: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 43: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 44: (1.4575218114632529, 0.17896290802183798, 0.018263766169548057), 45: (1.4098880955745712, 0.19218133050503033, 0.010134425759315446), 46: (1.2970440402707297, 0.2268784565464511, 0.01639994382858273), 47: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 48: (1.5330357203201004, 0.15963270659450096, 0.0149380803108215), 49: (1.4428694090638343, 0.18294225463509742, 0.015479716658592213), 50: (1.4701353173956515, 0.17559793168985668, 0.01913298964500426), 51: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 52: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 53: (1.3821539006476835, 0.20025918322600134, 0.01667459011077882), 54: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 55: (1.647582437547556, 0.13384230836982, 0.014032915234565735), 56: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 57: (1.2127117011155155, 0.25611049657681384, 0.014113497734069835), 58: (1.3717511957472106, 0.20336321699281482, 0.02029756903648372), 59: (1.7320749330356022, 0.11730236002488194, 0.016027060151100225), 60: (1.2250884396890571, 0.2516361392022721, 0.011804008483886741), 61: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 62: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 63: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 64: (1.3524324432160792, 0.20923663597226802, 0.013714814186096214), 65: (1.207983408257259, 0.2578368670848937, 0.010349023342132613), 66: (1.0696250703873664, 0.3126346616232136, 0.011452090740203869), 67: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 68: (1.1524813196613046, 0.2788147386507328, 0.011603665351867687), 69: (1.137393340057812, 0.2847476458175558, 0.012291699647903442), 70: (1.2444432373122565, 0.24476752527845105, 0.01334513425827022), 71: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 72: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 73: (1.0440114176934014, 0.3237100492876409, 0.010314714908599809), 74: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 75: (1.2216244908644383, 0.25288191962479567, 0.009443157911300626), 76: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 77: (1.1135252547197316, 0.29433701718942434, 0.010051617026329063), 78: (1.181210499491626, 0.2677910847614826, 0.01295053064823154), 79: (1.2417819182356142, 0.24570273655767255, 0.010424494743347168), 80: (0.9263823178852398, 0.37841931834657005, 0.011364886164665244), 81: (1.4483064567465782, 0.18145675239684195, 0.021374163031578075), 82: (1.0548361444603112, 0.3189932578383086, 0.012901440262794495), 83: (0.5076225527682159, 0.6239236805496233, 0.008730635046958923), 84: (2.894407698951993, 0.017756339567876013, 0.04467357993125917), 85: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 86: (2.894407698951993, 0.017756339567876013, 0.04467357993125917), 87: (3.013370645684239, 0.014635682938161457, 0.048122626543045055), 88: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 89: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 90: (2.1060548842956632, 0.06448146155403724, 0.03805774450302124), 91: (2.2033485657290766, 0.05503917768304185, 0.03558392226696011), 92: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 93: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 94: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 95: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 96: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 97: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 98: (2.558416524816187, 0.030766951889784996, 0.04213587641716002), 99: (2.446681986732943, 0.03695732707497869, 0.03842214047908782), 100: (2.6467234192687865, 0.026618778820155658, 0.04456419050693511), 101: (2.408500757573574, 0.03934563074886143, 0.041527169942855824), 102: (2.4687665745111733, 0.03564232369271045, 0.04298895597457886), 103: (2.3990178598115888, 0.0399621853187128, 0.04548675417900083), 104: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 105: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 106: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 107: (0.6103225142216757, 0.5567434719512216, 0.005341175198555037), 108: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 109: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 110: (0.6937772128886472, 0.5053323531323073, 0.00855525135993962), 111: (0.5108092118769949, 0.6217780356510954, 0.005040934681892428), 112: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 113: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 114: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 115: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 116: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 117: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 118: (0.7170704471532611, 0.4915200941050609, 0.007924893498420682), 119: (0.2727569654588054, 0.7911950651209458, 0.0022219240665435347), 120: (0.42626033178645306, 0.6799318623249235, 0.003239944577217102), 121: (0.6279777912011617, 0.5456211375801904, 0.005877128243446328), 122: (0.5451556351158041, 0.598894181824965, 0.004873842000961304), 123: (0.662451935092854, 0.5242811499325539, 0.007979604601860035), 124: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 125: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 126: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 127: (0.9574801818219801, 0.36333689627181376, 0.012267100811004683), 128: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 129: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 130: (1.0147349169071984, 0.3367336492726059, 0.013134312629699718), 131: (0.7447867858634004, 0.47539841641828073, 0.010077017545700062), 132: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 133: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 134: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 135: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 136: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 137: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 138: (1.0832905866829015, 0.3068465513157683, 0.013978892564773526), 139: (1.395036536895664, 0.1964714564670174, 0.00986538827419281), 140: (0.9961169162223953, 0.3452193196021255, 0.008227831125259488), 141: (1.6477864903052022, 0.133799927532065, 0.010629636049270696), 142: (2.0859573671878606, 0.06661865209177747, 0.009048116207122758), 143: (1.732440577383377, 0.11723504124284848, 0.011292290687561046)}, 'original_noun_phrase': {0: (16.221565546438303, 1.7798935688953818e-54, 0.017407106111447024)}}, 'dann': {'named': {0: (1.5746176509057868, 0.13184926305204134, 0.003214170038700115), 1: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 2: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 3: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 4: (1.1920304712684011, 0.2479257359003706, 0.0031599968671798817), 5: (0.5200779651273902, 0.6090163247364517, 0.001845601201057423), 6: (1.206054210112573, 0.2426008625369625, 0.0025039613246917725), 7: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 8: (0.47155000450879714, 0.6426177506381161, 0.001234653592109669), 9: (1.2247327050016992, 0.23564408602523226, 0.002675969898700725), 10: (1.201259793210151, 0.24441146928813237, 0.0026369109749793673), 11: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 12: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 13: (1.54360541965144, 0.13917635992975658, 0.003611910343170155), 14: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 15: (1.0152075004772516, 0.3227612405256235, 0.002449846267700173), 16: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 17: (1.3775253617312382, 0.18436132922410667, 0.002908635139465321), 18: (0.9653309357174993, 0.3465062292109887, 0.0021351978182792553), 19: (0.6885673381448015, 0.4994176755029808, 0.0023790627717971136), 20: (1.046287630939748, 0.3085571843597269, 0.0037065461277961953), 21: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 22: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 23: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 24: (1.000591330100209, 0.32959808234996657, 0.004472000896930728), 25: (0.6392760200676714, 0.5302760130804316, 0.0023487478494644276), 26: (0.9890859229571602, 0.3350506730103011, 0.002826303243637085), 27: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 28: (0.4683580328936644, 0.6448569654083677, 0.0015384703874588013), 29: (1.0974643109697455, 0.2861529009170869, 0.004083611071109772), 30: (0.772173717216145, 0.4495103347018585, 0.0024793311953544284), 31: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 32: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 33: (1.2597847802601188, 0.22300226077128277, 0.003996308147907246), 34: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 35: (0.8985685678690583, 0.38012915270937386, 0.0037279784679413175), 36: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 37: (0.8712614738398252, 0.3944873573023582, 0.00401660650968555), 38: (0.6522022478311991, 0.5220834867681785, 0.0022854521870612765), 39: (0.780226275068901, 0.4448696063686006, 0.003804713487625122), 40: (1.3736051264808316, 0.18555597501671686, 0.003539626300334886), 41: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 42: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 43: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 44: (1.758097856772768, 0.09482974069465373, 0.004365961253643025), 45: (0.6825310836190819, 0.5031405378657355, 0.0013693228363990673), 46: (1.2041798717946441, 0.24330748797831458, 0.003801923990249645), 47: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 48: (0.695622925269803, 0.49508624427654435, 0.001280762255191803), 49: (1.8432428301415822, 0.08095054192318862, 0.004096028208732561), 50: (1.2878114195851305, 0.21327659581953656, 0.0027184084057808033), 51: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 52: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 53: (1.1744685509945894, 0.25471809302092624, 0.0031597867608070263), 54: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 55: (1.2757436705880303, 0.21742302360071808, 0.0034473687410354614), 56: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 57: (1.1007890922782775, 0.2847394620571734, 0.0032403737306594405), 58: (1.1501546724856806, 0.2643513380626418, 0.00244247168302536), 59: (1.1131175543418497, 0.279542918384095, 0.0035638988018036333), 60: (0.1915994441670849, 0.8500887228278105, 0.0004647448658943176), 61: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 62: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 63: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 64: (0.3236575634047364, 0.7497327770576242, 0.0005711078643799161), 65: (-0.02577210760347539, 0.97970787065479, -4.477500915528454e-05), 66: (0.0041640076295935205, 0.996721022131268, 8.733570575680805e-06), 67: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 68: (0.07551291390310966, 0.9405960573805252, 0.0001857444643973971), 69: (0.24132461775516534, 0.8118887345903755, 0.0005281269550323486), 70: (-0.06500013956140589, 0.9488529272401054, -0.00013585984706876442), 71: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 72: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 73: (0.10544986645382569, 0.9171241677134373, 0.00019060075283050537), 74: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 75: (0.23750506615096562, 0.8148072250609198, 0.0004985958337783702), 76: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 77: (0.10373080844069092, 0.9184700871410466, 0.00020903944969175026), 78: (0.011688681947886774, 0.9907958556357035, 2.6200711727097925e-05), 79: (0.27946346039111863, 0.782908671272175, 0.0005801826715469693), 80: (1.0969986685495292, 0.28635126411165845, 0.002052229642868053), 81: (-0.18077443392733775, 0.8584589639021618, -0.0003736421465873607), 82: (0.5641841057069512, 0.579228588538415, 0.0013973131775855685), 83: (-0.40957301815228875, 0.6866998398235482, -0.001840552687644964), 84: (18.638280067115037, 1.142972942234969e-13, 0.04939173609018327), 85: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 86: (18.638280067115037, 1.142972942234969e-13, 0.04939173609018327), 87: (11.157918897342645, 8.763492435193886e-10, 0.044972747564315796), 88: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 89: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 90: (4.726836066232786, 0.00014676288203370317, 0.017240795493125932), 91: (3.9594122265237783, 0.0008406267631679267, 0.01665395945310591), 92: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 93: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 94: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 95: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 96: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 97: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 98: (9.479253743156212, 1.2367389255714281e-08, 0.03457566052675248), 99: (2.9341613190767846, 0.008514053732936375, 0.017832455039024364), 100: (3.5015153952942675, 0.0023871232577999447, 0.02095486819744108), 101: (6.307254546414991, 4.705343143619616e-06, 0.035032862424850486), 102: (4.06992614386743, 0.0006530922463351053, 0.027826881408691384), 103: (3.5060261182862744, 0.002362809989338763, 0.02212550938129426), 104: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 105: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 106: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 107: (0.3758450788003259, 0.71119563930838, 0.0008998572826385554), 108: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 109: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 110: (0.6371552716925805, 0.531626834397389, 0.001433078944683086), 111: (0.4111837667433367, 0.6855385756628762, 0.0012284129858016746), 112: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 113: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 114: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 115: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 116: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 117: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 118: (0.565939889268659, 0.5780582204092528, 0.001401010155677812), 119: (-0.12399070660995282, 0.9026249668448508, -0.0003284260630607161), 120: (0.17936267587213414, 0.8595518712321037, 0.0005316376686095858), 121: (0.15088294245970307, 0.8816584474742779, 0.0005153030157089011), 122: (0.114409037303277, 0.9101139217444596, 0.0004038020968437084), 123: (0.3521864721013083, 0.7285746708127132, 0.0013536572456359641), 124: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 125: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 126: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 127: (0.12741347758222332, 0.899951987034084, 0.00035066604614258923), 128: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 129: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 130: (-0.04000747782820315, 0.9685045710815892, -0.0001647800207138228), 131: (-0.5102105433586565, 0.6157801497262663, -0.0024284034967422263), 132: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 133: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 134: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 135: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 136: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 137: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 138: (0.72640652448611, 0.4764440973518601, 0.001512168347835574), 139: (0.7164851927341819, 0.482406587480824, 0.0013258129358292292), 140: (-0.2512823221189932, 0.8042934998481082, -0.0005313009023666382), 141: (0.6968525837654181, 0.49433358037327757, 0.001538360118865989), 142: (0.27887685878877855, 0.7833520692090833, 0.0006208002567290594), 143: (0.37910217570356103, 0.7088154508059843, 0.0008354976773262135)}, 'noun_phrase': {0: (1.1897449735598769, 0.2645847721167414, 0.017573592066764843), 1: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 2: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 3: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 4: (1.3464183864381682, 0.2110942468276465, 0.015633621811866727), 5: (1.605223219851201, 0.14290750452770626, 0.01092868745326997), 6: (0.8667449107217655, 0.40859471483042686, 0.014970198273658752), 7: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 8: (0.9424232596385866, 0.37058368822804644, 0.010049128532409635), 9: (1.0004200719366165, 0.3432439337875035, 0.009594476222991921), 10: (1.1263998455452213, 0.2891332885974849, 0.017584273219108604), 11: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 12: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 13: (0.9933854623068189, 0.346477613185726, 0.012622731924056996), 14: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 15: (1.6032436342688692, 0.1433443742741506, 0.009411555528640703), 16: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 17: (1.158788151635794, 0.2763641563963549, 0.011333853006362915), 18: (0.9548318465524074, 0.3646039365541912, 0.0157816022634506), 19: (1.9863680270079935, 0.0782530758132061, 0.010486459732055642), 20: (0.9766771897685304, 0.35424916066525003, 0.016275402903556835), 21: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 22: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 23: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 24: (1.0100004178142068, 0.3388764893485081, 0.01473958194255831), 25: (0.5924654780939295, 0.5681237185281386, 0.0063070297241210604), 26: (1.306671575638153, 0.22372519722970752, 0.016701975464820884), 27: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 28: (1.1269562258989507, 0.2889100552558865, 0.012668097019195523), 29: (0.9463363969787679, 0.3686902544162144, 0.010827291011810292), 30: (0.9142781364825604, 0.3844107065168235, 0.013658565282821666), 31: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 32: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 33: (0.7457949494362639, 0.4748184715790391, 0.011152601242065463), 34: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 35: (1.0477120521213235, 0.3220915580067869, 0.010223370790481523), 36: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 37: (0.8485865491571907, 0.4181090388139137, 0.009635287523269587), 38: (1.0628019539673974, 0.3155560521011496, 0.01639834642410276), 39: (0.9280528929400361, 0.37759773112532347, 0.008633512258529707), 40: (1.3766956038409206, 0.20188278038668978, 0.020707631111145042), 41: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 42: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 43: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 44: (1.5073811767069394, 0.16598281928998024, 0.020500892400741544), 45: (1.6723040652145509, 0.12879548325218018, 0.012533774971962008), 46: (1.4666378051409232, 0.1765254005834594, 0.018508574366569497), 47: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 48: (1.6064828699659295, 0.14263013911645447, 0.018048653006553617), 49: (1.5463738395386384, 0.1564169887486045, 0.01534509062767031), 50: (1.3981889279428554, 0.19555401474455414, 0.018730267882347107), 51: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 52: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 53: (1.4226334528464957, 0.1885642222067885, 0.018465134501457203), 54: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 55: (1.9228400604573, 0.08666129622920002, 0.016585296392440774), 56: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 57: (1.623967010507898, 0.13882998936749893, 0.016398394107818515), 58: (1.3827513068726704, 0.20008216195922973, 0.019965374469757102), 59: (1.7481542345898482, 0.11437536821340766, 0.015274143218994118), 60: (1.0490359251389871, 0.32151406212143196, 0.01535017490386964), 61: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 62: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 63: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 64: (1.3047798270533912, 0.22434187708656605, 0.014936184883117654), 65: (1.317960548529843, 0.22007466973219486, 0.014754098653793346), 66: (1.2617869061939107, 0.2387443941887252, 0.01689254939556123), 67: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 68: (1.2272916074156415, 0.25084640292998533, 0.015773829817771923), 69: (1.2265172246768885, 0.2511237527348488, 0.016437518596649214), 70: (1.2861969294266826, 0.23047558902018975, 0.018531644344329856), 71: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 72: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 73: (0.9991969600185951, 0.3438045461857614, 0.012521785497665416), 74: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 75: (1.1056965806537367, 0.2975371016397301, 0.012827748060226407), 76: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 77: (0.9991693337686526, 0.34381721657381925, 0.011863422393798895), 78: (1.2660158145542335, 0.2372945021417972, 0.01612878143787383), 79: (0.9921678862571092, 0.3470396146081677, 0.01198123693466191), 80: (1.8635080862094613, 0.09527941169686853, 0.021046599745750405), 81: (1.6094299972768031, 0.14198309472141932, 0.019358164072036776), 82: (1.0479494079279454, 0.3219879609813807, 0.016733840107917786), 83: (0.35083009179502844, 0.7337877965758419, 0.009211620688438421), 84: (3.5329053514837865, 0.006384768927710425, 0.06510847508907319), 85: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 86: (3.5329053514837865, 0.006384768927710425, 0.06510847508907319), 87: (2.6267171046948254, 0.027506310672767986, 0.06364955008029938), 88: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 89: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 90: (1.5052862576449235, 0.16651107699783177, 0.03877555429935453), 91: (1.3697137814615197, 0.2039759430223708, 0.03738438487052914), 92: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 93: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 94: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 95: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 96: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 97: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 98: (2.4781380841962766, 0.03509849756354863, 0.05549254715442653), 99: (1.1970975045130632, 0.2618474505146704, 0.03918782472610477), 100: (1.42327253408557, 0.18838441099826245, 0.04804551005363461), 101: (1.764074502143604, 0.11154378618190322, 0.05211286842823026), 102: (1.5055933118825775, 0.16643355723289166, 0.053024378418922435), 103: (1.4646924927840583, 0.17704310755118133, 0.04850500524044038), 104: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 105: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 106: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 107: (0.6855236510909435, 0.5102836227256967, 0.009697437286376953), 108: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 109: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 110: (0.7425844256789769, 0.47666690700989356, 0.010901454091072071), 111: (0.6136733515292012, 0.5546225904841315, 0.009904259443283103), 112: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 113: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 114: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 115: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 116: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 117: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 118: (0.7597416686096554, 0.4668425181929913, 0.010781642794609081), 119: (0.39073274802622565, 0.7050828077030289, 0.005680873990058899), 120: (0.5407255398252496, 0.6018205539470665, 0.008237031102180492), 121: (0.5927683310151038, 0.5679296256557602, 0.009882375597953796), 122: (0.5545378575092976, 0.5927217120499975, 0.009223684668540955), 123: (0.6759325534544358, 0.5160745360370119, 0.01272856295108793), 124: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 125: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 126: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 127: (0.7646227956182511, 0.4640717712710538, 0.01223587095737455), 128: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 129: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 130: (0.5039502108633557, 0.6264009988096672, 0.011995682120323148), 131: (0.4691593497211488, 0.6501130732324156, 0.009293520450592008), 132: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 133: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 134: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 135: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 136: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 137: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 138: (0.6381215696461847, 0.5392897460186654, 0.009442603588104226), 139: (0.5229328421333339, 0.6136495085651087, 0.004523599147796653), 140: (0.3408077548751734, 0.7410690126275548, 0.0035796403884887917), 141: (0.7773226886502699, 0.4569132428552066, 0.005795419216156006), 142: (0.7700970896635472, 0.4609771275072918, 0.004961055517196611), 143: (1.0665779213271587, 0.31393674043831626, 0.008490797877311718)}, 'original_noun_phrase': {0: (14.357576138134364, 8.680784544688741e-44, 0.01910032100147674)}}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list_sentence_pairs = ['named','noun_phrase']\n",
        "# dict_t_test ={}\n",
        "# for model_type, loaded_model in dict_loaded_model.items():\n",
        "#   dict_t_test[str(model_type)+\"_noun_phrase\"] = two_sample_test(dict_sentence_pairs =dict_noun_phrase_sentence_pair,text_pipeline = text_pipeline, loaded_model = loaded_model, loaded_model_device = 'cpu')\n",
        "#   dict_t_test[str(model_type)+\"_named\"] = two_sample_test(dict_sentence_pairs =dict_list_named_sentence_pairs,text_pipeline = text_pipeline, loaded_model = loaded_model, loaded_model_device = 'cpu')\n"
      ],
      "metadata": {
        "id": "wjHHAwkEH1XV"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dict_t_test.items()"
      ],
      "metadata": {
        "id": "8S3dfoFSKa9g"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dict_t_test_noun_phrase_sentence_pair = two_sample_test(dict_sentence_pairs =dict_noun_phrase_sentence_pair,text_pipeline = text_pipeline, loaded_model = loaded_model, loaded_model_device = 'cpu')\n",
        "# dict_t_test_named_sentence_pairs = two_sample_test(dict_sentence_pairs =dict_list_named_sentence_pairs,text_pipeline = text_pipeline, loaded_model = loaded_model, loaded_model_device = 'cpu')"
      ],
      "metadata": {
        "id": "AxBEsY7kjuI_"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dict_result_named_sentence_pair ={}\n",
        "\n",
        "# for key, value in dict_list_named_sentence_pairs.items():\n",
        "#   female_list = value[0]\n",
        "#   male_list = value[1]\n",
        "#   female_list_indices = [ text_pipeline(tweet_example)for tweet_example in female_list]\n",
        "#   male_list_indices = [text_pipeline(tweet_example)for tweet_example in male_list]\n",
        "\n",
        "#   female_list_output = [predict(sentence, loaded_model,text_pipeline,device= loaded_model_device) for sentence in female_list]\n",
        "#   male_list_output = [predict(sentence, loaded_model,text_pipeline,device= loaded_model_device) for sentence in male_list]\n",
        "#   # for sentence in female_list:\n",
        "#   #   female_list_output.append(predict(sentence, loaded_model,text_pipeline)\n",
        "#   # print(female_list,\"\\n\",female_list_indices,\"\\n\", female_list_output)\n",
        "#   # print(male_list,\"\\n\",male_list_indices,\"\\n\", male_list_output)\n",
        "#   t_test_result = stats.ttest_rel(female_list_output, male_list_output)\n",
        "#   dict_result_named_sentence_pair[key] = (t_test_result.statistic, t_test_result.pvalue,mean(female_list_output)-mean(male_list_output))\n",
        "#   # print(type(stats.ttest_rel(female_list_output, male_list_output)))\n",
        "\n",
        "#   # break\n",
        "\n",
        "# print((dict_result_named_sentence_pair))"
      ],
      "metadata": {
        "id": "aaXd_3lrzUYY"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #without named people\n",
        "# dict_result_sentence_pair ={}\n",
        "# # for key, value in dict_sentence_pair:\n",
        "# #   if len(value[0])\n",
        "# print(len(dict_sentence_pair))\n",
        "\n",
        "# for key, value in dict_sentence_pair.items():\n",
        "#   female_list = [value[0]]\n",
        "#   male_list = [value[1]]\n",
        "#   # if len(female_list)!=len(male_list):\n",
        "#   #   print(\"key:\", key)\n",
        "#   #   print(female_list,\"\\n\",male_list)\n",
        "#   #   print(len(female_list),\"-\",len(male_list))\n",
        "#   #   print(text_pipeline(female_list[0]),\"\\n\",text_pipeline(male_list[0]))\n",
        "#   #   break\n",
        "\n",
        "#   female_list_indices = [ text_pipeline(tweet_example) for tweet_example in female_list]\n",
        "#   male_list_indices = [text_pipeline(tweet_example) for tweet_example in male_list]\n",
        "\n",
        "#   female_list_output = [predict(sentence, loaded_model,text_pipeline,device= loaded_model_device) for sentence in female_list]\n",
        "#   male_list_output = [predict(sentence, loaded_model,text_pipeline,device= loaded_model_device) for sentence in male_list]\n",
        "#   # for sentence in female_list:\n",
        "#   #   female_list_output.append(predict(sentence, loaded_model,text_pipeline)\n",
        "#   # print(female_list,\"\\n\",female_list_indices,\"\\n\", female_list_output)\n",
        "#   # print(male_list,\"\\n\",male_list_indices,\"\\n\", male_list_output)\n",
        "#   t_test_result = stats.ttest_rel(female_list_output, male_list_output)\n",
        "#   dict_result_sentence_pair[key] = (t_test_result.statistic, t_test_result.pvalue,mean(female_list_output)-mean(male_list_output))\n",
        "#   # print(type(stats.ttest_rel(female_list_output, male_list_output)))\n",
        "\n",
        "#   # break\n",
        "\n",
        "# print(dict_result_sentence_pair)"
      ],
      "metadata": {
        "id": "t0b9Yil67vmp"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of results (based on semval paper)"
      ],
      "metadata": {
        "id": "VChKXEKUlPfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dict_t_test_noun_phrase_sentence_pair\n",
        "# dict_t_test_named_sentence_pairs"
      ],
      "metadata": {
        "id": "o3wanFg3lOz_"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(dict_t_test_noun_phrase_sentence_pair),len(dict_t_test_named_sentence_pairs)"
      ],
      "metadata": {
        "id": "27s07k7pCEX-"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analysis_t_test(dict_t_test_sentence_pairs, threshold = 0.05):\n",
        "  list_output =[]\n",
        "  for key, test_output in dict_t_test_sentence_pairs.items():\n",
        "    significant=True\n",
        "    t_statistic = test_output[0]\n",
        "    p_value = test_output[1]\n",
        "    f_m_diff = test_output[2]\n",
        "    if (float(p_value) > float(threshold) or float(p_value) == float(threshold)):\n",
        "      significant=False\n",
        "      category = 'f_equals_m'\n",
        "    else:\n",
        "      significant=True\n",
        "      \n",
        "      if f_m_diff > 0:\n",
        "        category='f_high_m_low'\n",
        "      else:\n",
        "        category = 'f_low_m_high' \n",
        "    list_output.append([key,t_statistic,p_value,significant,f_m_diff,category])\n",
        "    \n",
        "  df_columns = ['key','t_statistic','p_value', 'significant','delta','category']\n",
        "  df_output = pd.DataFrame(list_output, columns = df_columns)\n",
        "\n",
        "  list_category = list(df_output['category'].unique())\n",
        "  list_statistics =[]\n",
        "  for category in list_category:\n",
        "    df_temp = df_output[df_output['category']==category]\n",
        "    average = df_temp['delta'].mean()\n",
        "    # print(category,len(df_temp), average)\n",
        "    list_statistics.append([category,len(df_temp), average])\n",
        "  df_statistics = pd.DataFrame(list_statistics, columns = ['category', 'num_pairs','average_difference'])\n",
        "  return df_statistics\n",
        "\n",
        "\n",
        "# print(analysis_t_test(dict_t_test_noun_phrase_sentence_pair))\n",
        "# print(analysis_t_test(dict_t_test_named_sentence_pairs))\n"
      ],
      "metadata": {
        "id": "rX-ek3UXCNmd"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{'EI_anger': {\n",
        "    'non_dann': {\n",
        "        'original_noun_phrase': {\n",
        "            0: (0.1998956871564016, 0.8415904073105785, 3.9207107490946136e-05)}}, \n",
        "    'dann': {\n",
        "        'original_noun_phrase': {\n",
        "            0: (-3.055498578204014, 0.002288182511007486, -0.0005662351846695279)}}}, 'EI_sadness': {'non_dann': {'original_noun_phrase': {0: (0.697356464894513, 0.4856923878774828, 0.0001477275302426695)}}, 'dann': {'original_noun_phrase': {0: (2.6524225900747918, 0.008079505029578445, 0.0012747823571165329)}}}, 'EI_fear': {'non_dann': {'original_noun_phrase': {0: (-1.897698720306802, 0.057935765936073504, -0.0004837316667868352)}}, 'dann': {'original_noun_phrase': {0: (-4.093950892599476, 4.476596480751689e-05, -0.0010545446744395504)}}}, 'EI_joy': {'non_dann': {'original_noun_phrase': {0: (-1.2515370594455935, 0.210942010423521, -0.0002394391637708937)}}, 'dann': {'original_noun_phrase': {0: (-6.649931016376002, 4.1525086430524637e-11, -0.0012206101997030983)}}}, 'V': {'non_dann': {'original_noun_phrase': {0: (-0.6159244491387837, 0.5380417924786083, -0.00019533265795973476)}}, 'dann': {'original_noun_phrase': {0: (-1.5534286604033418, 0.12054065128353071, -0.0005289117702179658)}}}}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTw-p3pIG3DW",
        "outputId": "6fb36251-659b-42b3-c78c-cc641fe726b1"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'EI_anger': {'non_dann': {'original_noun_phrase': {0: (0.1998956871564016,\n",
              "     0.8415904073105785,\n",
              "     3.9207107490946136e-05)}},\n",
              "  'dann': {'original_noun_phrase': {0: (-3.055498578204014,\n",
              "     0.002288182511007486,\n",
              "     -0.0005662351846695279)}}},\n",
              " 'EI_sadness': {'non_dann': {'original_noun_phrase': {0: (0.697356464894513,\n",
              "     0.4856923878774828,\n",
              "     0.0001477275302426695)}},\n",
              "  'dann': {'original_noun_phrase': {0: (2.6524225900747918,\n",
              "     0.008079505029578445,\n",
              "     0.0012747823571165329)}}},\n",
              " 'EI_fear': {'non_dann': {'original_noun_phrase': {0: (-1.897698720306802,\n",
              "     0.057935765936073504,\n",
              "     -0.0004837316667868352)}},\n",
              "  'dann': {'original_noun_phrase': {0: (-4.093950892599476,\n",
              "     4.476596480751689e-05,\n",
              "     -0.0010545446744395504)}}},\n",
              " 'EI_joy': {'non_dann': {'original_noun_phrase': {0: (-1.2515370594455935,\n",
              "     0.210942010423521,\n",
              "     -0.0002394391637708937)}},\n",
              "  'dann': {'original_noun_phrase': {0: (-6.649931016376002,\n",
              "     4.1525086430524637e-11,\n",
              "     -0.0012206101997030983)}}},\n",
              " 'V': {'non_dann': {'original_noun_phrase': {0: (-0.6159244491387837,\n",
              "     0.5380417924786083,\n",
              "     -0.00019533265795973476)}},\n",
              "  'dann': {'original_noun_phrase': {0: (-1.5534286604033418,\n",
              "     0.12054065128353071,\n",
              "     -0.0005289117702179658)}}}}"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_statistics={}\n",
        "for name, dict_model_type_sentence_pair in dict_t_test.items():\n",
        "  dict_statistics_l1={}\n",
        "  for model_type, dict_sentence_pair in dict_model_type_sentence_pair.items():\n",
        "    dict_statistics_l2={}\n",
        "    for sentence_pair,t_test_dict in dict_sentence_pair.items():\n",
        "      df_statistics = analysis_t_test(t_test_dict, threshold = 0.05)\n",
        "      print(name+\"_\"+model_type+\"_\"+sentence_pair)\n",
        "      print(df_statistics)\n",
        "      print(50*\"=\")\n",
        "      dict_statistics_l2[sentence_pair] = df_statistics\n",
        "    dict_statistics_l1[model_type] = dict_statistics_l2\n",
        "  dict_statistics[name] = dict_statistics_l1\n",
        "  # df_statistics = analysis_t_test(t_test_dict, threshold = 0.05)\n",
        "  # dict_statistics[model_type_sentence_pair_name] = df_statistics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPHjuLtbFJw3",
        "outputId": "6e649539-c186-4543-bdb4-04d314e57d55"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EI_sadness_non_dann_named\n",
            "       category  num_pairs  average_difference\n",
            "0    f_equals_m        107            0.001881\n",
            "1  f_high_m_low         37            0.002548\n",
            "==================================================\n",
            "EI_sadness_non_dann_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0  f_high_m_low        109            0.012491\n",
            "1    f_equals_m         35            0.004142\n",
            "==================================================\n",
            "EI_sadness_non_dann_original_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0  f_high_m_low          1            0.010462\n",
            "==================================================\n",
            "EI_sadness_dann_named\n",
            "       category  num_pairs  average_difference\n",
            "0    f_equals_m        115            0.000876\n",
            "1  f_high_m_low         23            0.006459\n",
            "2  f_low_m_high          6           -0.004192\n",
            "==================================================\n",
            "EI_sadness_dann_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0  f_high_m_low          9            0.007425\n",
            "1    f_equals_m        135            0.004273\n",
            "==================================================\n",
            "EI_sadness_dann_original_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0  f_high_m_low          1             0.00447\n",
            "==================================================\n",
            "EI_anger_non_dann_named\n",
            "       category  num_pairs  average_difference\n",
            "0  f_high_m_low        101            0.009140\n",
            "1    f_equals_m         23           -0.000049\n",
            "2  f_low_m_high         20           -0.019936\n",
            "==================================================\n",
            "EI_anger_non_dann_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0    f_equals_m         80           -0.002499\n",
            "1  f_low_m_high         64           -0.014013\n",
            "==================================================\n",
            "EI_anger_non_dann_original_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0  f_low_m_high          1           -0.007617\n",
            "==================================================\n",
            "EI_anger_dann_named\n",
            "       category  num_pairs  average_difference\n",
            "0  f_high_m_low        107            0.010282\n",
            "1  f_low_m_high          3           -0.007191\n",
            "2    f_equals_m         34            0.000522\n",
            "==================================================\n",
            "EI_anger_dann_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0    f_equals_m         81           -0.002594\n",
            "1  f_low_m_high         58           -0.010008\n",
            "2  f_high_m_low          5            0.003115\n",
            "==================================================\n",
            "EI_anger_dann_original_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0  f_low_m_high          1           -0.005382\n",
            "==================================================\n",
            "EI_fear_non_dann_named\n",
            "       category  num_pairs  average_difference\n",
            "0    f_equals_m         97            0.001881\n",
            "1  f_high_m_low         47            0.006925\n",
            "==================================================\n",
            "EI_fear_non_dann_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0    f_equals_m        139           -0.005482\n",
            "1  f_low_m_high          5           -0.006695\n",
            "==================================================\n",
            "EI_fear_non_dann_original_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0  f_low_m_high          1           -0.005524\n",
            "==================================================\n",
            "EI_fear_dann_named\n",
            "       category  num_pairs  average_difference\n",
            "0  f_high_m_low        132            0.016822\n",
            "1    f_equals_m         12           -0.001153\n",
            "==================================================\n",
            "EI_fear_dann_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0    f_equals_m        132            0.003360\n",
            "1  f_low_m_high         11           -0.012008\n",
            "2  f_high_m_low          1            0.014368\n",
            "==================================================\n",
            "EI_fear_dann_original_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0  f_high_m_low          1            0.002262\n",
            "==================================================\n",
            "V_non_dann_named\n",
            "     category  num_pairs  average_difference\n",
            "0  f_equals_m        144           -0.002707\n",
            "==================================================\n",
            "V_non_dann_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0    f_equals_m        109            0.004857\n",
            "1  f_high_m_low         35            0.011016\n",
            "==================================================\n",
            "V_non_dann_original_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0  f_high_m_low          1            0.006354\n",
            "==================================================\n",
            "V_dann_named\n",
            "     category  num_pairs  average_difference\n",
            "0  f_equals_m        144           -0.002116\n",
            "==================================================\n",
            "V_dann_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0    f_equals_m        115            0.006796\n",
            "1  f_high_m_low         29            0.013990\n",
            "==================================================\n",
            "V_dann_original_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0  f_high_m_low          1            0.008245\n",
            "==================================================\n",
            "EI_joy_non_dann_named\n",
            "       category  num_pairs  average_difference\n",
            "0    f_equals_m        124            0.001946\n",
            "1  f_high_m_low         20            0.028967\n",
            "==================================================\n",
            "EI_joy_non_dann_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0    f_equals_m        126            0.013768\n",
            "1  f_high_m_low         18            0.042879\n",
            "==================================================\n",
            "EI_joy_non_dann_original_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0  f_high_m_low          1            0.017407\n",
            "==================================================\n",
            "EI_joy_dann_named\n",
            "       category  num_pairs  average_difference\n",
            "0    f_equals_m        124            0.001686\n",
            "1  f_high_m_low         20            0.032359\n",
            "==================================================\n",
            "EI_joy_dann_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0    f_equals_m        131            0.015280\n",
            "1  f_high_m_low         13            0.057599\n",
            "==================================================\n",
            "EI_joy_dann_original_noun_phrase\n",
            "       category  num_pairs  average_difference\n",
            "0  f_high_m_low          1              0.0191\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict_statistics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cByyTVHIJMQ1",
        "outputId": "53a61de4-07f0-4607-fffe-d76563a1532c"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'EI_sadness': {'non_dann': {'named':        category  num_pairs  average_difference\n",
            "0    f_equals_m        107            0.001881\n",
            "1  f_high_m_low         37            0.002548, 'noun_phrase':        category  num_pairs  average_difference\n",
            "0  f_high_m_low        109            0.012491\n",
            "1    f_equals_m         35            0.004142, 'original_noun_phrase':        category  num_pairs  average_difference\n",
            "0  f_high_m_low          1            0.010462}, 'dann': {'named':        category  num_pairs  average_difference\n",
            "0    f_equals_m        115            0.000876\n",
            "1  f_high_m_low         23            0.006459\n",
            "2  f_low_m_high          6           -0.004192, 'noun_phrase':        category  num_pairs  average_difference\n",
            "0  f_high_m_low          9            0.007425\n",
            "1    f_equals_m        135            0.004273, 'original_noun_phrase':        category  num_pairs  average_difference\n",
            "0  f_high_m_low          1             0.00447}}, 'EI_anger': {'non_dann': {'named':        category  num_pairs  average_difference\n",
            "0  f_high_m_low        101            0.009140\n",
            "1    f_equals_m         23           -0.000049\n",
            "2  f_low_m_high         20           -0.019936, 'noun_phrase':        category  num_pairs  average_difference\n",
            "0    f_equals_m         80           -0.002499\n",
            "1  f_low_m_high         64           -0.014013, 'original_noun_phrase':        category  num_pairs  average_difference\n",
            "0  f_low_m_high          1           -0.007617}, 'dann': {'named':        category  num_pairs  average_difference\n",
            "0  f_high_m_low        107            0.010282\n",
            "1  f_low_m_high          3           -0.007191\n",
            "2    f_equals_m         34            0.000522, 'noun_phrase':        category  num_pairs  average_difference\n",
            "0    f_equals_m         81           -0.002594\n",
            "1  f_low_m_high         58           -0.010008\n",
            "2  f_high_m_low          5            0.003115, 'original_noun_phrase':        category  num_pairs  average_difference\n",
            "0  f_low_m_high          1           -0.005382}}, 'EI_fear': {'non_dann': {'named':        category  num_pairs  average_difference\n",
            "0    f_equals_m         97            0.001881\n",
            "1  f_high_m_low         47            0.006925, 'noun_phrase':        category  num_pairs  average_difference\n",
            "0    f_equals_m        139           -0.005482\n",
            "1  f_low_m_high          5           -0.006695, 'original_noun_phrase':        category  num_pairs  average_difference\n",
            "0  f_low_m_high          1           -0.005524}, 'dann': {'named':        category  num_pairs  average_difference\n",
            "0  f_high_m_low        132            0.016822\n",
            "1    f_equals_m         12           -0.001153, 'noun_phrase':        category  num_pairs  average_difference\n",
            "0    f_equals_m        132            0.003360\n",
            "1  f_low_m_high         11           -0.012008\n",
            "2  f_high_m_low          1            0.014368, 'original_noun_phrase':        category  num_pairs  average_difference\n",
            "0  f_high_m_low          1            0.002262}}, 'V': {'non_dann': {'named':      category  num_pairs  average_difference\n",
            "0  f_equals_m        144           -0.002707, 'noun_phrase':        category  num_pairs  average_difference\n",
            "0    f_equals_m        109            0.004857\n",
            "1  f_high_m_low         35            0.011016, 'original_noun_phrase':        category  num_pairs  average_difference\n",
            "0  f_high_m_low          1            0.006354}, 'dann': {'named':      category  num_pairs  average_difference\n",
            "0  f_equals_m        144           -0.002116, 'noun_phrase':        category  num_pairs  average_difference\n",
            "0    f_equals_m        115            0.006796\n",
            "1  f_high_m_low         29            0.013990, 'original_noun_phrase':        category  num_pairs  average_difference\n",
            "0  f_high_m_low          1            0.008245}}, 'EI_joy': {'non_dann': {'named':        category  num_pairs  average_difference\n",
            "0    f_equals_m        124            0.001946\n",
            "1  f_high_m_low         20            0.028967, 'noun_phrase':        category  num_pairs  average_difference\n",
            "0    f_equals_m        126            0.013768\n",
            "1  f_high_m_low         18            0.042879, 'original_noun_phrase':        category  num_pairs  average_difference\n",
            "0  f_high_m_low          1            0.017407}, 'dann': {'named':        category  num_pairs  average_difference\n",
            "0    f_equals_m        124            0.001686\n",
            "1  f_high_m_low         20            0.032359, 'noun_phrase':        category  num_pairs  average_difference\n",
            "0    f_equals_m        131            0.015280\n",
            "1  f_high_m_low         13            0.057599, 'original_noun_phrase':        category  num_pairs  average_difference\n",
            "0  f_high_m_low          1              0.0191}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dict_statistics={}\n",
        "# for model_type_sentence_pair_name, t_test_dict in dict_t_test.items():\n",
        "#   df_statistics = analysis_t_test(t_test_dict, threshold = 0.05)\n",
        "#   dict_statistics[model_type_sentence_pair_name] = df_statistics\n"
      ],
      "metadata": {
        "id": "ttOYeuMiK27C"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for model_type_sentence_pair_name, df_statistics in dict_statistics.items():\n",
        "#   print(model_type_sentence_pair_name,\"\\n\",df_statistics)\n",
        "#   print(50*\"=\")"
      ],
      "metadata": {
        "id": "_4Dtpv0PL4wN"
      },
      "execution_count": 99,
      "outputs": []
    }
  ]
}